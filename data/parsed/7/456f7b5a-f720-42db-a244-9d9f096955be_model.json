[
    [
        {
            "type": "title",
            "bbox": [
                0.275,
                0.416,
                0.635,
                0.442
            ],
            "angle": 0,
            "content": "最优化：建模、算法与理论"
        },
        {
            "type": "text",
            "bbox": [
                0.294,
                0.485,
                0.615,
                0.503
            ],
            "angle": 0,
            "content": "刘浩洋、户将、李勇锋、文再文编著"
        }
    ],
    [],
    [
        {
            "type": "title",
            "bbox": [
                0.418,
                0.252,
                0.491,
                0.283
            ],
            "angle": 0,
            "content": "前言"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.334,
                0.738,
                0.474
            ],
            "angle": 0,
            "content": "最优化计算方法是运筹学、计算数学、机器学习和数据科学与大数据技术等专业的一门核心课程。最优化问题通常需要对实际需求进行定性和定量分析，建立恰当的数学模型来描述该问题，设计合适的计算方法来寻找问题的最优解，探索研究模型和算法的理论性质，考察算法的计算性能等。最优化算法广泛应用于科学与工程计算、数据科学、机器学习、人工智能、图像和信号处理、金融和经济、管理科学等众多领域。本书将介绍最优化的基本概念、典型案例、基本算法和理论，培养学生解决实际问题的能力。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.479,
                0.738,
                0.661
            ],
            "angle": 0,
            "content": "本书可作为数学优化、运筹学、计算数学、机器学习、人工智能、计算机科学和数据科学等专业的本科生、研究生和相关研究人员的教材或参考书目。通过本书的学习，希望读者能掌握最优化的基本概念、最优性理论、一些典型的最优化问题（如凸优化，无约束优化，约束优化，复合优化，等等）的建模或判别、相关优化问题的基本计算方法、能学会调用基于MATLAB或Python等语言的典型优化软件程序求解一些标准的优化问题，可以灵活运用所讲授的算法和理论求解一些非标准的优化问题，并锻炼对将实际问题建立合适最优化模型、选择合适的现有软件包和算法、遇到没有现成算法自己实现简单算法等能力。"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.665,
                0.738,
                0.724
            ],
            "angle": 0,
            "content": "考虑到不同层次的需求，本书另有简化版（书名：《最优化计算方法》），主要区别是简化版中不涉及一些复杂的概念、详细的例子和证明。在第一章简要介绍最优化基本概念之后，本书从四个方面进行讲述。"
        },
        {
            "type": "text",
            "bbox": [
                0.194,
                0.733,
                0.738,
                0.853
            ],
            "angle": 0,
            "content": "- 基础知识：第二章介绍最优化建模和算法中经常需要使用的一些基础知识，包括范数、导数、凸集、凸函数、次梯度、共轭函数等。此外为了内容的完整性，也在附录部分简要概述了一些基础知识，其中线性代数部分包含矩阵、特征值、广义逆、SMW公式、Schur补等，数值代数部分包含范数、方程组求解、矩阵分解、数值代数软件包等，概率论部分包含随机变量、期望、方差、条件期望等重要概念和结论。"
        },
        {
            "type": "page_number",
            "bbox": [
                0.451,
                0.87,
                0.459,
                0.882
            ],
            "angle": 0,
            "content": "i"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.274,
                0.131
            ],
            "angle": 0,
            "content": "ii"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.157,
                0.826,
                0.277
            ],
            "angle": 0,
            "content": "- 优化建模：第三章阐述一些典型的优化建模方法，并以科学工程计算和机器学习中一些典型问题为例介绍如何建立优化模型。第四章给出了最优化问题的一些典型分类和判别技巧，如线性规划、半定规划、最小二乘问题、复合优化、矩阵优化、随机优化等。一个实际问题根据其侧重点可以由不同的优化模型来描述，一种优化模型也可以对应很多不同的实际应用。"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.293,
                0.825,
                0.372
            ],
            "angle": 0,
            "content": "- 最优性理论：第五章介绍最优性理论，包括最优解的存在性和唯一性、无约束可微问题、无约束不可微问题、带约束优化问题和凸优化问题的一阶或二阶最优性条件、对偶理论、带广义不等式约束（如半定规划问题）的对偶理论。"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.389,
                0.825,
                0.53
            ],
            "angle": 0,
            "content": "- 最优化算法: 第六章介绍无约束优化算法, 包括线搜索方法、梯度类算法、次梯度算法、牛顿 (Newton) 类算法、拟牛顿类算法、信赖域算法、非线性最小二乘问题算法。第七章介绍约束优化算法, 包括罚函数法、增广拉格朗日（Lagrange）函数法及其在典型凸优化问题的原始问题和对偶问题上的具体应用、线性规划内点法。第八章介绍复合优化算法, 包括近似点梯度法、Nesterov 加速算法、近似点算法、分块坐标下降法、对偶算法、交替方向乘子法、随机优化算法。"
        },
        {
            "type": "list",
            "bbox": [
                0.281,
                0.157,
                0.826,
                0.53
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.545,
                0.825,
                0.666
            ],
            "angle": 0,
            "content": "本书主要概念配有详细的例子来解释，主要优化算法的介绍包含算法描述、应用举例和收敛性分析三个方面。在算法描述方面，本书侧重于算法的基本思想和直观解释；在应用举例方面，针对几乎所有算法写出了其在稀疏优化或逻辑回归等典型问题中的具体形式和求解过程，给出了最优性度量与迭代步数关系等数值结果。相关程序也可以从作者主页下载，读者可方便地比较各种算法的特点。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.671,
                0.825,
                0.811
            ],
            "angle": 0,
            "content": "本书各部分内容的难易程度有些差异，比较难的部分在小节标题标注星号。理论和算法涉及的基础知识也有较大差异，比如向量导数、凸集、凸函数、线性代数等在低年级课程中大多已经覆盖，但是矩阵函数及其导数、共轭函数、次梯度等可能讲述很少。因此讲授或阅读时可以根据具体情况进行选择，不一定要按照章节的顺序进行。例如次梯度、无约束不可微问题的最优性理论和次梯度算法等涉及非光滑函数的基础部分可以考虑放在光滑函数的梯度类算法之后再讲授或阅读。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "最优化理论与算法内涵十分丰富，本书涉及的各方面仍然比较初步和浅略。更全面的应用场景，更深入的理论探讨和更详细的算法设计需读者进"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.719,
                0.119,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "iii"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.157,
                0.744,
                0.277
            ],
            "angle": 0,
            "content": "一步查阅相关章节给出的参考文献。由于篇幅限制，有很多重要内容没有讲述，如连续优化里的共轭梯度算法、逐步二次规划、无导数优化、线性规划单纯形法和更详细的内点法、二次锥规划和半定规划的内点法、非线性规划的内点法等。本书也没有讲述带微分方程约束优化、流形约束优化、鲁棒优化、整数规划、组合优化、次模优化、动态规划等应用广泛的知识，感兴趣的读者可以阅读相关文献。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.281,
                0.744,
                0.382
            ],
            "angle": 0,
            "content": "诚挚感谢袁亚湘院士多年来的精心指导和悉心关怀，对本书的规划和内容给予的宝贵意见。特别感谢张平文院士、马志明院士、徐宗本院士等专家对本书的指导和支持。非常感谢北京大学北京国际数学研究中心和数学科学学院、国家自然科学基金、北京智源人工智能研究院等对课题组的长期资助和支持。"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.384,
                0.756,
                0.506
            ],
            "angle": 0,
            "content": "本书写作参考了袁亚湘院士和孙文瑜教授的《最优化理论与方法》，Jorge Nocedal 教授和 Stephen Wright 教授的 Numerical Optimization, Stephen Boyd 教授和 Lieven Vandenberghe 教授的 Convex Optimization 等经典教材。Lieven Vandenberghe 教授在加州大学洛杉矶分校多门课程的讲义对本书的整理帮助很大。也特别感谢加州大学洛杉矶分校印卧涛教授慷慨分享稀疏优化、交替方向乘子法、坐标下降法等很多方面的内容。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.509,
                0.744,
                0.651
            ],
            "angle": 0,
            "content": "本书内容在北京大学数学科学学院多次开设的“凸优化”和“大数据分析中的算法”课程中使用，感谢课题组同学在初稿整理方面的支持，如刘普凡在内容简介，金泽宇在数值代数基础和Nesterov加速算法，许东在数学分析基础，杨明瀚在无约束光滑函数优化方法，柳伊扬在无约束非光滑函数优化算法，柳昊明在近似点梯度法，刘德斌在罚函数法，赵明明在对偶函数方法，王金鑫在交替方向乘子法及其变形，陈铖和谢中林在书稿后期整理等方面的帮助。同时也感谢高等教育出版社刘荣编辑精心细致的校稿和修改。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.653,
                0.735,
                0.672
            ],
            "angle": 0,
            "content": "限于作者的知识水平，书中恐有不妥之处，恳请读者不吝批评和指正。"
        },
        {
            "type": "text",
            "bbox": [
                0.493,
                0.708,
                0.738,
                0.724
            ],
            "angle": 0,
            "content": "刘浩洋、户将、李勇锋、文再文"
        },
        {
            "type": "text",
            "bbox": [
                0.591,
                0.728,
                0.736,
                0.744
            ],
            "angle": 0,
            "content": "北京，2020年7月"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.12,
                0.278,
                0.131
            ],
            "angle": 0,
            "content": "iv"
        }
    ],
    [
        {
            "type": "title",
            "bbox": [
                0.42,
                0.253,
                0.493,
                0.283
            ],
            "angle": 0,
            "content": "目录"
        },
        {
            "type": "title",
            "bbox": [
                0.172,
                0.35,
                0.739,
                0.366
            ],
            "angle": 0,
            "content": "第一章 最优化简介 1"
        },
        {
            "type": "text",
            "bbox": [
                0.198,
                0.371,
                0.739,
                0.388
            ],
            "angle": 0,
            "content": "1.1 最优化问题概括 1"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.392,
                0.737,
                0.409
            ],
            "angle": 0,
            "content": "1.1.1 最优化问题的一般形式 1"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.414,
                0.739,
                0.43
            ],
            "angle": 0,
            "content": "1.1.2 最优化问题的类型与应用背景 2"
        },
        {
            "type": "list",
            "bbox": [
                0.236,
                0.392,
                0.739,
                0.43
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.198,
                0.436,
                0.739,
                0.452
            ],
            "angle": 0,
            "content": "1.2 实例：稀疏优化 3"
        },
        {
            "type": "text",
            "bbox": [
                0.198,
                0.457,
                0.739,
                0.473
            ],
            "angle": 0,
            "content": "1.3 实例：低秩矩阵恢复 7"
        },
        {
            "type": "text",
            "bbox": [
                0.198,
                0.479,
                0.739,
                0.494
            ],
            "angle": 0,
            "content": "1.4 实例：深度学习 8"
        },
        {
            "type": "list",
            "bbox": [
                0.198,
                0.436,
                0.739,
                0.494
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.5,
                0.737,
                0.516
            ],
            "angle": 0,
            "content": "1.4.1 多层感知机 8"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.522,
                0.739,
                0.537
            ],
            "angle": 0,
            "content": "1.4.2 卷积神经网络 10"
        },
        {
            "type": "list",
            "bbox": [
                0.236,
                0.5,
                0.739,
                0.537
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.198,
                0.543,
                0.739,
                0.559
            ],
            "angle": 0,
            "content": "1.5 最优化的基本概念 12"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.565,
                0.737,
                0.58
            ],
            "angle": 0,
            "content": "1.5.1 连续和离散优化问题 13"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.586,
                0.739,
                0.602
            ],
            "angle": 0,
            "content": "1.5.2 无约束和约束优化问题 14"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.608,
                0.739,
                0.623
            ],
            "angle": 0,
            "content": "1.5.3 随机和确定性优化问题 14"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.629,
                0.739,
                0.645
            ],
            "angle": 0,
            "content": "1.5.4 线性和非线性规划问题 15"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.651,
                0.739,
                0.666
            ],
            "angle": 0,
            "content": "1.5.5 凸和非凸优化问题 15"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.673,
                0.739,
                0.688
            ],
            "angle": 0,
            "content": "1.5.6 全局和局部最优解 16"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.694,
                0.739,
                0.709
            ],
            "angle": 0,
            "content": "1.5.7 优化算法 16"
        },
        {
            "type": "list",
            "bbox": [
                0.236,
                0.565,
                0.739,
                0.709
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.198,
                0.716,
                0.739,
                0.732
            ],
            "angle": 0,
            "content": "1.6 总结 21"
        },
        {
            "type": "text",
            "bbox": [
                0.198,
                0.737,
                0.739,
                0.753
            ],
            "angle": 0,
            "content": "习题1 22"
        },
        {
            "type": "title",
            "bbox": [
                0.172,
                0.772,
                0.739,
                0.788
            ],
            "angle": 0,
            "content": "第二章 基础知识 23"
        },
        {
            "type": "text",
            "bbox": [
                0.198,
                0.794,
                0.739,
                0.81
            ],
            "angle": 0,
            "content": "2.1 范数 23"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.815,
                0.737,
                0.831
            ],
            "angle": 0,
            "content": "2.1.1 向量范数 23"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.836,
                0.739,
                0.852
            ],
            "angle": 0,
            "content": "2.1.2 矩阵范数 24"
        },
        {
            "type": "list",
            "bbox": [
                0.236,
                0.815,
                0.739,
                0.852
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "page_number",
            "bbox": [
                0.448,
                0.872,
                0.461,
                0.882
            ],
            "angle": 0,
            "content": "V"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.12,
                0.278,
                0.13
            ],
            "angle": 0,
            "content": "vi"
        },
        {
            "type": "header",
            "bbox": [
                0.789,
                0.118,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "目录"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.158,
                0.825,
                0.173
            ],
            "angle": 0,
            "content": "2.1.3 矩阵内积 25"
        },
        {
            "type": "text",
            "bbox": [
                0.286,
                0.178,
                0.825,
                0.194
            ],
            "angle": 0,
            "content": "2.2 导数 26"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.2,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "2.2.1 梯度与海瑟矩阵 26"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.22,
                0.825,
                0.235
            ],
            "angle": 0,
            "content": "2.2.2 矩阵变量函数的导数 30"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.241,
                0.825,
                0.256
            ],
            "angle": 0,
            "content": "2.2.3 自动微分 32"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.2,
                0.825,
                0.256
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.286,
                0.262,
                0.825,
                0.277
            ],
            "angle": 0,
            "content": "2.3 广义实值函数 34"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.283,
                0.825,
                0.298
            ],
            "angle": 0,
            "content": "2.3.1 适当函数 35"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.303,
                0.825,
                0.318
            ],
            "angle": 0,
            "content": "2.3.2 闭函数 35"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.283,
                0.825,
                0.318
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.286,
                0.324,
                0.825,
                0.339
            ],
            "angle": 0,
            "content": "2.4 凸集 38"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.345,
                0.825,
                0.36
            ],
            "angle": 0,
            "content": "2.4.1 凸集的相关定义 38"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.366,
                0.825,
                0.381
            ],
            "angle": 0,
            "content": "2.4.2 重要的凸集 41"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.387,
                0.825,
                0.402
            ],
            "angle": 0,
            "content": "2.4.3 保凸的运算 43"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.407,
                0.825,
                0.423
            ],
            "angle": 0,
            "content": "2.4.4 分离超平面定理 44"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.345,
                0.825,
                0.423
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.286,
                0.428,
                0.825,
                0.443
            ],
            "angle": 0,
            "content": "2.5 凸函数 46"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.449,
                0.825,
                0.464
            ],
            "angle": 0,
            "content": "2.5.1 凸函数的定义 46"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.47,
                0.825,
                0.485
            ],
            "angle": 0,
            "content": "2.5.2 凸函数判定定理 48"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.491,
                0.825,
                0.506
            ],
            "angle": 0,
            "content": "2.5.3 保凸的运算 53"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.511,
                0.825,
                0.526
            ],
            "angle": 0,
            "content": "2.5.4 凸函数的性质 56"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.449,
                0.825,
                0.526
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.286,
                0.533,
                0.825,
                0.548
            ],
            "angle": 0,
            "content": "2.6 共轭函数 58"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.553,
                0.825,
                0.568
            ],
            "angle": 0,
            "content": "2.6.1 共轭函数的定义和例子 58"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.574,
                0.825,
                0.589
            ],
            "angle": 0,
            "content": "2.6.2 二次共轭函数 60"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.553,
                0.825,
                0.589
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.286,
                0.595,
                0.825,
                0.61
            ],
            "angle": 0,
            "content": "2.7 次梯度 61"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.616,
                0.825,
                0.631
            ],
            "angle": 0,
            "content": "2.7.1 次梯度的定义 61"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.636,
                0.825,
                0.652
            ],
            "angle": 0,
            "content": "2.7.2 次梯度的性质 64"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.657,
                0.825,
                0.672
            ],
            "angle": 0,
            "content": "2.7.3 凸函数的方向导数 66"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.678,
                0.825,
                0.693
            ],
            "angle": 0,
            "content": "2.7.4 次梯度的计算规则 68"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.616,
                0.825,
                0.693
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.286,
                0.699,
                0.825,
                0.714
            ],
            "angle": 0,
            "content": "2.8 总结 75"
        },
        {
            "type": "text",
            "bbox": [
                0.288,
                0.719,
                0.825,
                0.735
            ],
            "angle": 0,
            "content": "习题2 76"
        },
        {
            "type": "title",
            "bbox": [
                0.261,
                0.753,
                0.825,
                0.769
            ],
            "angle": 0,
            "content": "第三章 优化建模 79"
        },
        {
            "type": "text",
            "bbox": [
                0.286,
                0.775,
                0.825,
                0.79
            ],
            "angle": 0,
            "content": "3.1 建模技术 80"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.795,
                0.825,
                0.811
            ],
            "angle": 0,
            "content": "3.1.1 目标函数的设计 80"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.816,
                0.825,
                0.832
            ],
            "angle": 0,
            "content": "3.1.2 约束的设计 84"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.795,
                0.825,
                0.832
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.286,
                0.837,
                0.825,
                0.852
            ],
            "angle": 0,
            "content": "3.2 回归分析 86"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.173,
                0.118,
                0.21,
                0.133
            ],
            "angle": 0,
            "content": "目录"
        },
        {
            "type": "header",
            "bbox": [
                0.715,
                0.119,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "vii"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.156,
                0.739,
                0.173
            ],
            "angle": 0,
            "content": "3.2.1 概述 86"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.178,
                0.739,
                0.194
            ],
            "angle": 0,
            "content": "3.2.2 线性回归模型 87"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.199,
                0.739,
                0.216
            ],
            "angle": 0,
            "content": "3.2.3 正则化线性回归模型 88"
        },
        {
            "type": "list",
            "bbox": [
                0.235,
                0.156,
                0.739,
                0.216
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.219,
                0.736,
                0.236
            ],
            "angle": 0,
            "content": "3.3 逻辑回归 91"
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.24,
                0.737,
                0.257
            ],
            "angle": 0,
            "content": "3.4 支持向量机 93"
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.261,
                0.737,
                0.277
            ],
            "angle": 0,
            "content": "3.5 概率图模型 95"
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.282,
                0.737,
                0.299
            ],
            "angle": 0,
            "content": "3.6 相位恢复 98"
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.303,
                0.737,
                0.319
            ],
            "angle": 0,
            "content": "3.7 主成分分析 101"
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.323,
                0.737,
                0.34
            ],
            "angle": 0,
            "content": "3.8 矩阵分离问题 103"
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.344,
                0.737,
                0.361
            ],
            "angle": 0,
            "content": "3.9 字典学习 104"
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.365,
                0.737,
                0.382
            ],
            "angle": 0,
            "content": "3.10 K-均值聚类 106"
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.386,
                0.737,
                0.403
            ],
            "angle": 0,
            "content": "3.11 图像处理中的全变差模型 ..... 108"
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.407,
                0.737,
                0.423
            ],
            "angle": 0,
            "content": "3.12 小波模型 111"
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.428,
                0.737,
                0.444
            ],
            "angle": 0,
            "content": "3.13 强化学习 113"
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.449,
                0.737,
                0.465
            ],
            "angle": 0,
            "content": "3.14 总结 115"
        },
        {
            "type": "list",
            "bbox": [
                0.197,
                0.219,
                0.737,
                0.465
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.199,
                0.469,
                0.737,
                0.485
            ],
            "angle": 0,
            "content": "习题3 117"
        },
        {
            "type": "title",
            "bbox": [
                0.172,
                0.504,
                0.736,
                0.52
            ],
            "angle": 0,
            "content": "第四章 典型优化问题 121"
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.524,
                0.736,
                0.541
            ],
            "angle": 0,
            "content": "4.1 线性规划 121"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.545,
                0.736,
                0.561
            ],
            "angle": 0,
            "content": "4.1.1 基本形式和应用背景 121"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.566,
                0.737,
                0.582
            ],
            "angle": 0,
            "content": "4.1.2 应用举例 122"
        },
        {
            "type": "list",
            "bbox": [
                0.235,
                0.545,
                0.737,
                0.582
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.586,
                0.736,
                0.603
            ],
            "angle": 0,
            "content": "4.2 最小二乘问题 125"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.607,
                0.737,
                0.624
            ],
            "angle": 0,
            "content": "4.2.1 基本形式和应用背景 125"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.629,
                0.737,
                0.645
            ],
            "angle": 0,
            "content": "4.2.2 应用举例 125"
        },
        {
            "type": "list",
            "bbox": [
                0.235,
                0.607,
                0.737,
                0.645
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.649,
                0.736,
                0.666
            ],
            "angle": 0,
            "content": "4.3 复合优化问题 130"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.67,
                0.737,
                0.686
            ],
            "angle": 0,
            "content": "4.3.1 基本形式和应用背景 130"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.691,
                0.737,
                0.707
            ],
            "angle": 0,
            "content": "4.3.2 应用举例 132"
        },
        {
            "type": "list",
            "bbox": [
                0.235,
                0.67,
                0.737,
                0.707
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.712,
                0.736,
                0.728
            ],
            "angle": 0,
            "content": "4.4 随机优化问题 134"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.732,
                0.737,
                0.748
            ],
            "angle": 0,
            "content": "4.4.1 基本形式和应用背景 134"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.754,
                0.737,
                0.77
            ],
            "angle": 0,
            "content": "4.4.2 应用举例 134"
        },
        {
            "type": "list",
            "bbox": [
                0.235,
                0.732,
                0.737,
                0.77
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.774,
                0.736,
                0.79
            ],
            "angle": 0,
            "content": "4.5 半定规划 136"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.795,
                0.737,
                0.811
            ],
            "angle": 0,
            "content": "4.5.1 基本形式和应用背景 136"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.816,
                0.737,
                0.832
            ],
            "angle": 0,
            "content": "4.5.2 应用举例 137"
        },
        {
            "type": "list",
            "bbox": [
                0.235,
                0.795,
                0.737,
                0.832
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.197,
                0.836,
                0.736,
                0.852
            ],
            "angle": 0,
            "content": "4.6 矩阵优化 140"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.26,
                0.119,
                0.288,
                0.131
            ],
            "angle": 0,
            "content": "viii"
        },
        {
            "type": "header",
            "bbox": [
                0.789,
                0.118,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "目录"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.158,
                0.826,
                0.173
            ],
            "angle": 0,
            "content": "4.6.1 基本形式和应用背景 140"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.179,
                0.825,
                0.194
            ],
            "angle": 0,
            "content": "4.6.2 应用举例 142"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.158,
                0.826,
                0.194
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.199,
                0.826,
                0.215
            ],
            "angle": 0,
            "content": "4.7 整数规划 145"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.22,
                0.825,
                0.235
            ],
            "angle": 0,
            "content": "4.7.1 基本形式和应用背景 145"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.241,
                0.825,
                0.256
            ],
            "angle": 0,
            "content": "4.7.2 应用举例 145"
        },
        {
            "type": "list",
            "bbox": [
                0.326,
                0.22,
                0.825,
                0.256
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.262,
                0.825,
                0.277
            ],
            "angle": 0,
            "content": "4.8 典型优化算法软件介绍 148"
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.283,
                0.825,
                0.298
            ],
            "angle": 0,
            "content": "4.9 优化模型语言 149"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.304,
                0.825,
                0.318
            ],
            "angle": 0,
            "content": "4.9.1 CVX 149"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.324,
                0.825,
                0.339
            ],
            "angle": 0,
            "content": "4.9.2 AMPL 150"
        },
        {
            "type": "list",
            "bbox": [
                0.326,
                0.304,
                0.825,
                0.339
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.345,
                0.825,
                0.36
            ],
            "angle": 0,
            "content": "4.10 总结 151"
        },
        {
            "type": "title",
            "bbox": [
                0.287,
                0.366,
                0.825,
                0.381
            ],
            "angle": 0,
            "content": "习题4 152"
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.399,
                0.826,
                0.416
            ],
            "angle": 0,
            "content": "第五章 最优性理论 157"
        },
        {
            "type": "text",
            "bbox": [
                0.285,
                0.421,
                0.825,
                0.436
            ],
            "angle": 0,
            "content": "5.1 最优化问题解的存在性 157"
        },
        {
            "type": "text",
            "bbox": [
                0.285,
                0.442,
                0.825,
                0.457
            ],
            "angle": 0,
            "content": "5.2 无约束可微问题的最优性理论 160"
        },
        {
            "type": "list",
            "bbox": [
                0.285,
                0.421,
                0.825,
                0.457
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.463,
                0.825,
                0.478
            ],
            "angle": 0,
            "content": "5.2.1 一阶最优性条件 160"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.484,
                0.825,
                0.498
            ],
            "angle": 0,
            "content": "5.2.2 二阶最优性条件 161"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.505,
                0.825,
                0.52
            ],
            "angle": 0,
            "content": "5.2.3 实例 163"
        },
        {
            "type": "list",
            "bbox": [
                0.326,
                0.463,
                0.825,
                0.52
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.525,
                0.825,
                0.54
            ],
            "angle": 0,
            "content": "5.3 无约束不可微问题的最优性理论 ..... 164"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.546,
                0.825,
                0.561
            ],
            "angle": 0,
            "content": "5.3.1 凸优化问题一阶充要条件 164"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.567,
                0.825,
                0.582
            ],
            "angle": 0,
            "content": "5.3.2 复合优化问题的一阶必要条件 165"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.588,
                0.825,
                0.603
            ],
            "angle": 0,
            "content": "*5.3.3 非光滑非凸问题的最优性条件 ..... 166"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.609,
                0.825,
                0.624
            ],
            "angle": 0,
            "content": "5.3.4 实例 168"
        },
        {
            "type": "list",
            "bbox": [
                0.326,
                0.546,
                0.825,
                0.624
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.629,
                0.825,
                0.644
            ],
            "angle": 0,
            "content": "5.4 对偶理论 169"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.65,
                0.825,
                0.665
            ],
            "angle": 0,
            "content": "5.4.1 拉格朗日函数与对偶问题 169"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.671,
                0.825,
                0.686
            ],
            "angle": 0,
            "content": "5.4.2 带广义不等式约束优化问题的对偶 171"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.692,
                0.825,
                0.706
            ],
            "angle": 0,
            "content": "5.4.3 实例 174"
        },
        {
            "type": "list",
            "bbox": [
                0.326,
                0.65,
                0.825,
                0.706
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.712,
                0.825,
                0.727
            ],
            "angle": 0,
            "content": "5.5 一般约束优化问题的最优性理论 ..... 179"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.733,
                0.825,
                0.748
            ],
            "angle": 0,
            "content": "5.5.1 一阶最优性条件 179"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.754,
                0.825,
                0.769
            ],
            "angle": 0,
            "content": "5.5.2 二阶最优性条件 189"
        },
        {
            "type": "list",
            "bbox": [
                0.326,
                0.733,
                0.825,
                0.769
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.775,
                0.825,
                0.79
            ],
            "angle": 0,
            "content": "5.6 带约束凸优化问题的最优性理论 ..... 191"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.796,
                0.825,
                0.811
            ],
            "angle": 0,
            "content": "5.6.1 Slater约束品性与强对偶原理 192"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.817,
                0.825,
                0.832
            ],
            "angle": 0,
            "content": "5.6.2 一阶充要条件 195"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.837,
                0.825,
                0.852
            ],
            "angle": 0,
            "content": "*5.6.3 一阶充要条件：必要性的证明 ..... 196"
        },
        {
            "type": "list",
            "bbox": [
                0.326,
                0.796,
                0.825,
                0.852
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.173,
                0.118,
                0.21,
                0.133
            ],
            "angle": 0,
            "content": "目录"
        },
        {
            "type": "header",
            "bbox": [
                0.72,
                0.12,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "ix"
        },
        {
            "type": "title",
            "bbox": [
                0.198,
                0.157,
                0.739,
                0.173
            ],
            "angle": 0,
            "content": "5.7 约束优化最优性理论应用实例 203"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.178,
                0.737,
                0.194
            ],
            "angle": 0,
            "content": "5.7.1 仿射空间的投影问题 203"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.199,
                0.737,
                0.216
            ],
            "angle": 0,
            "content": "5.7.2 线性规划问题 204"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.22,
                0.737,
                0.236
            ],
            "angle": 0,
            "content": "5.7.3 基追踪 205"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.24,
                0.737,
                0.257
            ],
            "angle": 0,
            "content": "5.7.4 最大割问题的半定规划松弛及其非凸分解模型 207"
        },
        {
            "type": "list",
            "bbox": [
                0.236,
                0.178,
                0.737,
                0.257
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.198,
                0.262,
                0.739,
                0.278
            ],
            "angle": 0,
            "content": "5.8 总结 209"
        },
        {
            "type": "title",
            "bbox": [
                0.2,
                0.282,
                0.737,
                0.298
            ],
            "angle": 0,
            "content": "习题5 210"
        },
        {
            "type": "title",
            "bbox": [
                0.172,
                0.316,
                0.739,
                0.333
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法 215"
        },
        {
            "type": "title",
            "bbox": [
                0.198,
                0.337,
                0.739,
                0.353
            ],
            "angle": 0,
            "content": "6.1 线搜索方法 215"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.358,
                0.737,
                0.374
            ],
            "angle": 0,
            "content": "6.1.1 线搜索准则 216"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.379,
                0.737,
                0.395
            ],
            "angle": 0,
            "content": "6.1.2 线搜索算法 221"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.399,
                0.737,
                0.416
            ],
            "angle": 0,
            "content": "6.1.3 收敛性分析 222"
        },
        {
            "type": "list",
            "bbox": [
                0.236,
                0.358,
                0.737,
                0.416
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.198,
                0.421,
                0.739,
                0.437
            ],
            "angle": 0,
            "content": "6.2 梯度类算法 224"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.442,
                0.737,
                0.457
            ],
            "angle": 0,
            "content": "6.2.1 梯度下降法 225"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.462,
                0.737,
                0.478
            ],
            "angle": 0,
            "content": "6.2.2 Barzilar-Borwein 方法 229"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.483,
                0.737,
                0.499
            ],
            "angle": 0,
            "content": "6.2.3 应用举例 231"
        },
        {
            "type": "list",
            "bbox": [
                0.236,
                0.442,
                0.737,
                0.499
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.198,
                0.504,
                0.739,
                0.52
            ],
            "angle": 0,
            "content": "6.3 次梯度算法 237"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.525,
                0.737,
                0.541
            ],
            "angle": 0,
            "content": "6.3.1 次梯度算法结构 237"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.545,
                0.737,
                0.561
            ],
            "angle": 0,
            "content": "6.3.2 收敛性分析 237"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.566,
                0.737,
                0.582
            ],
            "angle": 0,
            "content": "6.3.3 应用举例 242"
        },
        {
            "type": "list",
            "bbox": [
                0.236,
                0.525,
                0.737,
                0.582
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.198,
                0.587,
                0.739,
                0.603
            ],
            "angle": 0,
            "content": "6.4 牛顿类算法 245"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.608,
                0.737,
                0.624
            ],
            "angle": 0,
            "content": "6.4.1 经典牛顿法 245"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.629,
                0.737,
                0.645
            ],
            "angle": 0,
            "content": "6.4.2 收敛性分析 245"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.649,
                0.737,
                0.665
            ],
            "angle": 0,
            "content": "6.4.3 修正牛顿法 247"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.67,
                0.737,
                0.686
            ],
            "angle": 0,
            "content": "6.4.4 非精确牛顿法 249"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.691,
                0.737,
                0.707
            ],
            "angle": 0,
            "content": "6.4.5 应用举例 250"
        },
        {
            "type": "list",
            "bbox": [
                0.236,
                0.608,
                0.737,
                0.707
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.198,
                0.712,
                0.739,
                0.728
            ],
            "angle": 0,
            "content": "6.5 拟牛顿类算法 252"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.732,
                0.737,
                0.748
            ],
            "angle": 0,
            "content": "6.5.1 割线方程 253"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.754,
                0.737,
                0.77
            ],
            "angle": 0,
            "content": "6.5.2 拟牛顿矩阵更新方式 255"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.774,
                0.737,
                0.79
            ],
            "angle": 0,
            "content": "6.5.3 拟牛顿法的全局收敛性 258"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.795,
                0.737,
                0.811
            ],
            "angle": 0,
            "content": "6.5.4 有限内存BFGS方法 260"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.816,
                0.737,
                0.832
            ],
            "angle": 0,
            "content": "6.5.5 应用举例 264"
        },
        {
            "type": "list",
            "bbox": [
                0.236,
                0.732,
                0.737,
                0.832
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.198,
                0.837,
                0.739,
                0.853
            ],
            "angle": 0,
            "content": "6.6 信赖域算法 ..... 266"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.121,
                0.272,
                0.131
            ],
            "angle": 0,
            "content": "X"
        },
        {
            "type": "header",
            "bbox": [
                0.789,
                0.118,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "目录"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.156,
                0.826,
                0.173
            ],
            "angle": 0,
            "content": "6.6.1 信赖域算法框架 267"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.178,
                0.826,
                0.194
            ],
            "angle": 0,
            "content": "6.6.2 信赖域子问题求解 269"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.199,
                0.826,
                0.216
            ],
            "angle": 0,
            "content": "6.6.3 收敛性分析 276"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.221,
                0.826,
                0.236
            ],
            "angle": 0,
            "content": "6.6.4 应用举例 280"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.156,
                0.826,
                0.236
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.241,
                0.826,
                0.258
            ],
            "angle": 0,
            "content": "6.7 非线性最小二乘问题算法 ..... 280"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.262,
                0.826,
                0.278
            ],
            "angle": 0,
            "content": "6.7.1 非线性最小二乘问题 281"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.283,
                0.826,
                0.3
            ],
            "angle": 0,
            "content": "6.7.2 高斯－牛顿算法 282"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.305,
                0.826,
                0.321
            ],
            "angle": 0,
            "content": "6.7.3 Levenberg-Marquardt 方法 285"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.326,
                0.826,
                0.342
            ],
            "angle": 0,
            "content": "6.7.4 大残量问题的拟牛顿法 288"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.347,
                0.826,
                0.363
            ],
            "angle": 0,
            "content": "6.7.5 应用举例 290"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.262,
                0.826,
                0.363
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.368,
                0.826,
                0.384
            ],
            "angle": 0,
            "content": "6.8 总结 291"
        },
        {
            "type": "text",
            "bbox": [
                0.287,
                0.389,
                0.826,
                0.405
            ],
            "angle": 0,
            "content": "习题6 294"
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.423,
                0.826,
                0.439
            ],
            "angle": 0,
            "content": "第七章 约束优化算法 297"
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.444,
                0.826,
                0.46
            ],
            "angle": 0,
            "content": "7.1 罚函数法 297"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.465,
                0.826,
                0.482
            ],
            "angle": 0,
            "content": "7.1.1 等式约束的二次罚函数法 297"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.487,
                0.826,
                0.502
            ],
            "angle": 0,
            "content": "7.1.2 收敛性分析 300"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.508,
                0.826,
                0.523
            ],
            "angle": 0,
            "content": "7.1.3 一般约束问题的二次罚函数法 ..... 303"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.529,
                0.826,
                0.544
            ],
            "angle": 0,
            "content": "7.1.4 应用举例 305"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.55,
                0.826,
                0.565
            ],
            "angle": 0,
            "content": "7.1.5 其他类型的罚函数法 308"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.465,
                0.826,
                0.565
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.57,
                0.826,
                0.586
            ],
            "angle": 0,
            "content": "7.2 增广拉格朗日函数法 311"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.592,
                0.826,
                0.607
            ],
            "angle": 0,
            "content": "7.2.1 等式约束优化问题的增广拉格朗日函数法 311"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.613,
                0.826,
                0.629
            ],
            "angle": 0,
            "content": "7.2.2 一般约束优化问题的增广拉格朗日函数法 317"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.634,
                0.826,
                0.65
            ],
            "angle": 0,
            "content": "7.2.3 凸优化问题的增广拉格朗日函数法 320"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.655,
                0.826,
                0.671
            ],
            "angle": 0,
            "content": "7.2.4 基追踪问题的增广拉格朗日函数法 323"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.676,
                0.826,
                0.692
            ],
            "angle": 0,
            "content": "7.2.5 半定规划问题的增广拉格朗日函数法 330"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.592,
                0.826,
                0.692
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.697,
                0.826,
                0.713
            ],
            "angle": 0,
            "content": "7.3 线性规划内点法 332"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.718,
                0.826,
                0.734
            ],
            "angle": 0,
            "content": "7.3.1 原始-对偶算法 333"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.739,
                0.826,
                0.755
            ],
            "angle": 0,
            "content": "7.3.2 路径追踪算法 335"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.718,
                0.826,
                0.755
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.76,
                0.826,
                0.776
            ],
            "angle": 0,
            "content": "7.4 总结 338"
        },
        {
            "type": "text",
            "bbox": [
                0.287,
                0.781,
                0.826,
                0.796
            ],
            "angle": 0,
            "content": "习题7 339"
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.815,
                0.826,
                0.832
            ],
            "angle": 0,
            "content": "第八章 复合优化算法 343"
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.836,
                0.826,
                0.852
            ],
            "angle": 0,
            "content": "8.1 近似点梯度法 343"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.173,
                0.118,
                0.21,
                0.133
            ],
            "angle": 0,
            "content": "目录"
        },
        {
            "type": "header",
            "bbox": [
                0.72,
                0.12,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "xi"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.156,
                0.739,
                0.173
            ],
            "angle": 0,
            "content": "8.1.1 邻近算子 344"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.178,
                0.739,
                0.195
            ],
            "angle": 0,
            "content": "8.1.2 近似点梯度法 349"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.199,
                0.739,
                0.216
            ],
            "angle": 0,
            "content": "8.1.3 应用举例 351"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.221,
                0.739,
                0.237
            ],
            "angle": 0,
            "content": "8.1.4 收敛性分析 354"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.242,
                0.739,
                0.259
            ],
            "angle": 0,
            "content": "*8.1.5 非凸函数的邻近算子与近似点梯度法 ..... 357"
        },
        {
            "type": "list",
            "bbox": [
                0.235,
                0.156,
                0.739,
                0.259
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.197,
                0.263,
                0.739,
                0.28
            ],
            "angle": 0,
            "content": "8.2 Nesterov加速算法 359"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.284,
                0.739,
                0.301
            ],
            "angle": 0,
            "content": "8.2.1 FISTA算法 359"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.306,
                0.739,
                0.323
            ],
            "angle": 0,
            "content": "8.2.2 其他加速算法 364"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.327,
                0.739,
                0.344
            ],
            "angle": 0,
            "content": "8.2.3 应用举例 366"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.348,
                0.739,
                0.365
            ],
            "angle": 0,
            "content": "8.2.4 收敛性分析 369"
        },
        {
            "type": "list",
            "bbox": [
                0.235,
                0.284,
                0.739,
                0.365
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.197,
                0.369,
                0.739,
                0.386
            ],
            "angle": 0,
            "content": "8.3 近似点算法 373"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.39,
                0.739,
                0.407
            ],
            "angle": 0,
            "content": "8.3.1 近似点算法 374"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.412,
                0.739,
                0.429
            ],
            "angle": 0,
            "content": "8.3.2 与增广拉格朗日函数法的关系 ..... 375"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.433,
                0.739,
                0.45
            ],
            "angle": 0,
            "content": "8.3.3 应用举例 377"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.454,
                0.739,
                0.471
            ],
            "angle": 0,
            "content": "8.3.4 收敛性分析 382"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.475,
                0.739,
                0.492
            ],
            "angle": 0,
            "content": "8.3.5 Moreau-Yosida 正则化 384"
        },
        {
            "type": "list",
            "bbox": [
                0.235,
                0.39,
                0.739,
                0.492
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.197,
                0.497,
                0.739,
                0.514
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法 387"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.518,
                0.739,
                0.535
            ],
            "angle": 0,
            "content": "8.4.1 问题描述 388"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.539,
                0.739,
                0.556
            ],
            "angle": 0,
            "content": "8.4.2 算法结构 390"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.56,
                0.739,
                0.577
            ],
            "angle": 0,
            "content": "8.4.3 应用举例 393"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.582,
                0.739,
                0.599
            ],
            "angle": 0,
            "content": "*8.4.4 收敛性分析 400"
        },
        {
            "type": "list",
            "bbox": [
                0.235,
                0.518,
                0.739,
                0.599
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.197,
                0.603,
                0.739,
                0.62
            ],
            "angle": 0,
            "content": "8.5 对偶算法 412"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.624,
                0.739,
                0.641
            ],
            "angle": 0,
            "content": "8.5.1 对偶近似点梯度法 413"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.645,
                0.739,
                0.662
            ],
            "angle": 0,
            "content": "8.5.2 原始-对偶混合梯度算法 418"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.666,
                0.739,
                0.683
            ],
            "angle": 0,
            "content": "8.5.3 应用举例 420"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.687,
                0.739,
                0.704
            ],
            "angle": 0,
            "content": "8.5.4 收敛性分析 424"
        },
        {
            "type": "list",
            "bbox": [
                0.235,
                0.624,
                0.739,
                0.704
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.197,
                0.709,
                0.739,
                0.726
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法 430"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.73,
                0.739,
                0.747
            ],
            "angle": 0,
            "content": "8.6.1 交替方向乘子法 430"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.752,
                0.739,
                0.769
            ],
            "angle": 0,
            "content": "8.6.2 Douglas-Rachford Splitting 算法 436"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.773,
                0.739,
                0.789
            ],
            "angle": 0,
            "content": "8.6.3 常见变形和技巧 440"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.794,
                0.739,
                0.811
            ],
            "angle": 0,
            "content": "8.6.4 应用举例 443"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.815,
                0.739,
                0.832
            ],
            "angle": 0,
            "content": "*8.6.5 收敛性分析 455"
        },
        {
            "type": "list",
            "bbox": [
                0.235,
                0.73,
                0.739,
                0.832
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.197,
                0.836,
                0.739,
                0.853
            ],
            "angle": 0,
            "content": "8.7 随机优化算法 461"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.26,
                0.119,
                0.282,
                0.131
            ],
            "angle": 0,
            "content": "xii"
        },
        {
            "type": "header",
            "bbox": [
                0.789,
                0.118,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "目录"
        },
        {
            "type": "text",
            "bbox": [
                0.325,
                0.158,
                0.825,
                0.173
            ],
            "angle": 0,
            "content": "8.7.1 随机梯度下降算法 463"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.179,
                0.825,
                0.194
            ],
            "angle": 0,
            "content": "8.7.2 应用举例 470"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.201,
                0.825,
                0.216
            ],
            "angle": 0,
            "content": "8.7.3 收敛性分析 473"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.222,
                0.825,
                0.236
            ],
            "angle": 0,
            "content": "8.7.4 方差减小技术 482"
        },
        {
            "type": "list",
            "bbox": [
                0.325,
                0.158,
                0.825,
                0.236
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.285,
                0.242,
                0.825,
                0.259
            ],
            "angle": 0,
            "content": "8.8 总结 489"
        },
        {
            "type": "text",
            "bbox": [
                0.286,
                0.263,
                0.825,
                0.28
            ],
            "angle": 0,
            "content": "习题8 491"
        },
        {
            "type": "title",
            "bbox": [
                0.259,
                0.298,
                0.826,
                0.315
            ],
            "angle": 0,
            "content": "附录A 符号表 495"
        },
        {
            "type": "title",
            "bbox": [
                0.259,
                0.333,
                0.826,
                0.35
            ],
            "angle": 0,
            "content": "附录B 数学基础 499"
        },
        {
            "type": "text",
            "bbox": [
                0.285,
                0.355,
                0.825,
                0.371
            ],
            "angle": 0,
            "content": "B.1 线性代数基础 499"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.376,
                0.825,
                0.392
            ],
            "angle": 0,
            "content": "B.1.1 矩阵内积与迹 499"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.398,
                0.825,
                0.413
            ],
            "angle": 0,
            "content": "B.1.2 正交矩阵与（半）正定矩阵 499"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.419,
                0.825,
                0.435
            ],
            "angle": 0,
            "content": "B.1.3 矩阵的秩 500"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.441,
                0.825,
                0.456
            ],
            "angle": 0,
            "content": "B.1.4 像空间和零空间 500"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.462,
                0.825,
                0.477
            ],
            "angle": 0,
            "content": "B.1.5 行列式 501"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.483,
                0.825,
                0.498
            ],
            "angle": 0,
            "content": "B.1.6 特征值与特征向量 501"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.504,
                0.825,
                0.52
            ],
            "angle": 0,
            "content": "B.1.7 广义逆 502"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.525,
                0.825,
                0.541
            ],
            "angle": 0,
            "content": "B.1.8 Sherman-Morrison-Woodbury 公式 503"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.547,
                0.825,
                0.562
            ],
            "angle": 0,
            "content": "B.1.9 Schur补 504"
        },
        {
            "type": "list",
            "bbox": [
                0.326,
                0.376,
                0.825,
                0.562
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.285,
                0.567,
                0.825,
                0.584
            ],
            "angle": 0,
            "content": "B.2 数值代数基础 505"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.589,
                0.825,
                0.604
            ],
            "angle": 0,
            "content": "B.2.1 解线性方程组 505"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.611,
                0.825,
                0.626
            ],
            "angle": 0,
            "content": "B.2.2 系数矩阵为特殊矩阵的方程组解法 508"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.632,
                0.825,
                0.647
            ],
            "angle": 0,
            "content": "B.2.3 特征值分解与奇异值分解 511"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.653,
                0.825,
                0.668
            ],
            "angle": 0,
            "content": "B.2.4 数值代数软件 513"
        },
        {
            "type": "list",
            "bbox": [
                0.326,
                0.589,
                0.825,
                0.668
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.285,
                0.674,
                0.825,
                0.69
            ],
            "angle": 0,
            "content": "B.3 概率基础 518"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.696,
                0.825,
                0.711
            ],
            "angle": 0,
            "content": "B.3.1 概率空间 518"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.718,
                0.825,
                0.732
            ],
            "angle": 0,
            "content": "B.3.2 随机变量 519"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.738,
                0.825,
                0.754
            ],
            "angle": 0,
            "content": "B.3.3 条件期望 525"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.76,
                0.825,
                0.775
            ],
            "angle": 0,
            "content": "B.3.4 随机变量的收敛性 529"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.781,
                0.825,
                0.796
            ],
            "angle": 0,
            "content": "B.3.5 随机过程 529"
        },
        {
            "type": "text",
            "bbox": [
                0.326,
                0.802,
                0.825,
                0.817
            ],
            "angle": 0,
            "content": "B.3.6 概率不等式 531"
        },
        {
            "type": "list",
            "bbox": [
                0.326,
                0.696,
                0.825,
                0.817
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.836,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "参考文献 535"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.118,
                0.21,
                0.133
            ],
            "angle": 0,
            "content": "目录"
        },
        {
            "type": "header",
            "bbox": [
                0.709,
                0.119,
                0.737,
                0.132
            ],
            "angle": 0,
            "content": "xiii"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.158,
                0.21,
                0.173
            ],
            "angle": 0,
            "content": "索引"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.158,
                0.737,
                0.171
            ],
            "angle": 0,
            "content": "557"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.26,
                0.12,
                0.287,
                0.131
            ],
            "angle": 0,
            "content": "xiv"
        },
        {
            "type": "header",
            "bbox": [
                0.789,
                0.118,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "目录"
        }
    ],
    [
        {
            "type": "title",
            "bbox": [
                0.29,
                0.255,
                0.618,
                0.287
            ],
            "angle": 0,
            "content": "第一章 最优化简介"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.34,
                0.744,
                0.504
            ],
            "angle": 0,
            "content": "最优化问题（也称优化问题）泛指定量决策问题，主要关心如何对有限资源进行有效分配和控制，并达到某种意义上的最优。它通常需要对需求进行定性和定量分析，建立恰当的数学模型来描述该问题，设计合适的计算方法来寻找问题的最优解，探索研究模型和算法的理论性质，考察算法的计算性能等。由于很多数学问题难以直接给出显式解，最优化模型就成为人们最常见的选择，计算机的高速发展也为最优化方法提供了有力辅助工具。因此最优化方法被广泛应用于科学与工程计算、金融与经济、管理科学、工业生产、图像与信号处理、数据分析与人工智能、计算物理与化学等众多领域。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.509,
                0.741,
                0.548
            ],
            "angle": 0,
            "content": "本章将介绍最优化问题的一般形式和一些重要的基本概念，并通过实际应用中的例子让读者更加直观地理解最优化问题。"
        },
        {
            "type": "title",
            "bbox": [
                0.338,
                0.594,
                0.572,
                0.617
            ],
            "angle": 0,
            "content": "1.1 最优化问题概括"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.64,
                0.433,
                0.659
            ],
            "angle": 0,
            "content": "1.1.1 最优化问题的一般形式"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.677,
                0.419,
                0.694
            ],
            "angle": 0,
            "content": "最优化问题一般可以描述为"
        },
        {
            "type": "equation",
            "bbox": [
                0.401,
                0.712,
                0.738,
                0.74
            ],
            "angle": 0,
            "content": "\\[\n\\min  f (x), \\tag {1.1.1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.413,
                0.738,
                0.505,
                0.753
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & x \\in \\mathcal {X}, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(x = (x_{1},x_{2},\\dots ,x_{n})^{\\mathrm{T}}\\in \\mathbb{R}^{n}\\) 是决策变量， \\(f\\colon \\mathbb{R}^n\\to \\mathbb{R}\\) 是目标函数， \\(\\mathcal{X}\\subseteq \\mathbb{R}^n\\) 是约束集合或可行域，可行域包含的点称为可行解或可行点．记号s.t.是“subject to”的缩写，专指约束条件．当 \\(\\mathcal{X} = \\mathbb{R}^n\\) 时，问题(1.1.1)称为无约束优化问题．集合 \\(\\mathcal{X}\\) 通常可以由约束函数 \\(c_{i}(x)\\colon \\mathbb{R}^{n}\\to \\mathbb{R},i = 1,2,\\dots ,m + l\\)"
        },
        {
            "type": "page_number",
            "bbox": [
                0.449,
                0.87,
                0.46,
                0.882
            ],
            "angle": 0,
            "content": "1"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.272,
                0.13
            ],
            "angle": 0,
            "content": "2"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第一章 最优化简介"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.425,
                0.174
            ],
            "angle": 0,
            "content": "表达为如下具体形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.185,
                0.652,
                0.204
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {X} = \\left\\{x \\in \\mathbb {R} ^ {n} \\mid c _ {i} (x) \\leqslant 0, i = 1, 2, \\dots , m, \\right.\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.452,
                0.21,
                0.753,
                0.229
            ],
            "angle": 0,
            "content": "\\[\nc _ {i} (x) = 0, \\quad i = m + 1, m + 2 \\dots , m + l \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.239,
                0.827,
                0.422
            ],
            "angle": 0,
            "content": "在所有满足约束条件的决策变量中，使目标函数取最小值的变量 \\(x^{*}\\) 称为优化问题(1.1.1)的最优解，即对任意 \\(x\\in \\mathcal{X}\\) 都有 \\(f(x)\\geqslant f(x^{*})\\) ．如果我们求解在约束集合 \\(\\mathcal{X}\\) 上目标函数 \\(f(x)\\) 的最大值，则问题(1.1.1)的“min”应相应地替换为“max”.注意到在集合 \\(\\mathcal{X}\\) 上，函数 \\(f\\) 的最小（最大）值不一定存在，但是其下（上）确界“inf \\(f(\\sup f)\\) ”总是存在的．因此，当目标函数的最小（最大）值不存在时，我们便关心其下（上）确界，即将问题(1.1.1)中的“min(max)”改为“infSup”).为了叙述简便，问题(1.1.1)中 \\(x\\) 为 \\(\\mathbb{R}^n\\) 空间中的向量．实际上，根据具体应用和需求， \\(x\\) 还可以是矩阵、多维数组或张量等，本书介绍的很多理论和算法可以相应推广."
        },
        {
            "type": "text",
            "bbox": [
                0.283,
                0.443,
                0.803,
                0.543
            ],
            "angle": 0,
            "content": "由于本书涉及较多公式，请读者根据上下文区分公式中的标量、向量、矩阵。在不加说明的情况下，向量一般用小写英文字母或希腊字母表示，矩阵一般用大写英文字母或希腊字母表示。公式中的标量可能使用多种记号，需要根据上下文确定。读者也可参考附录A符号表。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.577,
                0.578,
                0.596
            ],
            "angle": 0,
            "content": "1.1.2 最优化问题的类型与应用背景"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.609,
                0.828,
                0.854
            ],
            "angle": 0,
            "content": "最优化问题 (1.1.1) 的具体形式非常丰富，我们可以按照目标函数、约束函数以及解的性质将其分类。按照目标函数和约束函数的形式来分：当目标函数和约束函数均为线性函数时，问题 (1.1.1) 称为线性规划；当目标函数和约束函数中至少有一个为非线性函数时，相应的问题称为非线性规划；如果目标函数是二次函数而约束函数是线性函数则称为二次规划；包含非光滑函数的问题称为非光滑优化；不能直接求导数的问题称为无导数优化；变量只能取整数的问题称为整数规划；在线性约束下极小化关于半正定矩阵的线性函数的问题称为半定规划，其广义形式为锥规划。按照最优解的性质来分：最优解只有少量非零元素的问题称为稀疏优化；最优解是低秩矩阵的问题称为低秩矩阵优化。此外还有几何优化、二次锥规划、张量优化、鲁棒优化、全局优化、组合优化、网络规划、随机优化、动态规划、带微分方程约束优化、微分流形约束优化、分布式优化等。就具体应用而言，问题"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "1.2 实例：稀疏优化"
        },
        {
            "type": "page_number",
            "bbox": [
                0.725,
                0.119,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "3"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.157,
                0.736,
                0.195
            ],
            "angle": 0,
            "content": "(1.1.1) 可涵盖统计学习、压缩感知、最优运输、信号处理、图像处理、机器学习、强化学习、模式识别、金融工程、电力系统等领域的优化模型。"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.199,
                0.739,
                0.381
            ],
            "angle": 0,
            "content": "需要指出的是，数学建模很容易给出应用问题不同的模型，可以对应性质很不相同的问题，其求解难度和需要的算法也将差别很大。在投资组合优化中，人们希望通过寻求最优的投资组合以降低风险、提高收益。这时决策变量 \\( x_{i} \\) 表示在第 \\( i \\) 项资产上的投资额，向量 \\( x \\in \\mathbb{R}^{n} \\) 表示整体的投资分配。约束条件可能为总资金数、每项资产的最大（最小）投资额、最低收益等。目标函数通常是某种风险度量。如果是极小化收益的方差，则该问题是典型的二次规划；如果极小化风险价值(value at risk)函数，则该问题是混合整合规划；如果极小化条件风险价值(conditional value at risk)函数，则该问题是非光滑优化，也可以进一步化成线性规划。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.385,
                0.74,
                0.526
            ],
            "angle": 0,
            "content": "在本章后面的三节和第三章中，我们通过一些实际应用中的例子更直观、深入地理解最优化问题。由于篇幅限制，我们通常只简要给出它们的一些典型形式，而且叙述并不严格，主要是提供这些应用的大致形式，详细的定义和描述请读者参考本书后面的章节或者相关参考文献。而在第四章中，我们会将它们按优化问题分类。本书的目标之一是使得读者通过学习本书的理论和算法，能用算法软件包来求解这些模型，并了解这些算法有哪些优缺点，更进一步地，使得读者能独立设计类似问题的算法。"
        },
        {
            "type": "title",
            "bbox": [
                0.334,
                0.555,
                0.567,
                0.576
            ],
            "angle": 0,
            "content": "1.2 实例：稀疏优化"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.594,
                0.405,
                0.61
            ],
            "angle": 0,
            "content": "考虑线性方程组求解问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.423,
                0.623,
                0.737,
                0.639
            ],
            "angle": 0,
            "content": "\\[\nA x = b, \\tag {1.2.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.65,
                0.74,
                0.854
            ],
            "angle": 0,
            "content": "其中向量 \\(x \\in \\mathbb{R}^n\\) ，\\(b \\in \\mathbb{R}^m\\) ，矩阵 \\(A \\in \\mathbb{R}^{m \\times n}\\) ，且向量 \\(b\\) 的维数远小于向量 \\(x\\) 的维数，即 \\(m \\ll n\\) 。在自然科学和工程中常常遇到已知向量 \\(b\\) 和矩阵 \\(A\\) ，想要重构向量 \\(x\\) 的问题。例如在信号传输过程中，希望通过接收到长度为 \\(m\\) 的数字信号精确地重构原始信号。注意到由于 \\(m \\ll n\\) ，方程组 (1.2.1) 是欠定的，因此存在无穷多个解，重构出原始信号看似很难。所幸的是，这些解当中大部分是我们不感兴趣的，真正有用的解是所谓的“稀疏解”，即原始信号中有较多的零元素。如果加上稀疏性这一先验信息，且矩阵 \\(A\\) 以及原问题的解 \\(u\\) 满足某些条件，那么我们可以通过求解稀疏优化问题把 \\(u\\) 与方程组 (1.2.1) 的其他解区别开。这类技术广泛应用于压缩感知（compressive sensing），即通过部分信息恢复全部信息的解决方案。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.272,
                0.13
            ],
            "angle": 0,
            "content": "4"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第一章 最优化简介"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.157,
                0.748,
                0.174
            ],
            "angle": 0,
            "content": "先来看一个具体的例子. 在 MATLAB 环境里构造 \\(A, u\\) 和 \\(b\\):"
        },
        {
            "type": "equation",
            "bbox": [
                0.246,
                0.183,
                0.498,
                0.263
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} m = 1 2 8; n = 2 5 6; \\\\ \\mathrm {2} \\mid \\mathrm {A} = \\text {r a n d n} (\\mathrm {m}, \\mathrm {n}); \\\\ 3 \\left| u = \\text {s p r a n d n} (n, 1, 0. 1) \\right.; \\\\ 4 \\mid b = A * u; \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.281,
                0.827,
                0.36
            ],
            "angle": 0,
            "content": "在这个例子中，我们构造了一个 \\(128 \\times 256\\) 矩阵 \\(A\\)，它的每个元素都服从高斯（Gauss）随机分布。精确解 \\(u\\) 只有 \\(10\\%\\) 的元素非零，每一个非零元素也服从高斯分布。这些特征可以在理论上保证 \\(u\\) 是方程组 (1.2.1) 唯一的非零元素最少的解，即 \\(u\\) 是如下 \\(\\ell_0\\) 范数问题的最优解："
        },
        {
            "type": "equation",
            "bbox": [
                0.485,
                0.373,
                0.825,
                0.419
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x \\in \\mathbb {R} ^ {n}} \\| x \\| _ {0}, \\tag {1.2.2} \\\\ \\begin{array}{l l} \\text {s . t .} & A x = b. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.433,
                0.825,
                0.563
            ],
            "angle": 0,
            "content": "其中 \\(\\| x\\| _0\\) 是指 \\(x\\) 中非零元素的个数．由于 \\(\\| x\\| _0\\) 是不连续的函数，且取值只可能是整数，问题(1.2.2)实际上是NP（non-deterministic polynomial）难的，求解起来非常困难．因此当 \\(n\\) 较大时通过直接求解问题(1.2.2)来恢复出原始信号 \\(u\\) 是行不通的．那有没有替代的方法呢？答案是有的．若定义 \\(\\ell_1\\) 范数： \\(\\| x\\| _1 = \\sum_{i = 1}^{n}|x_i|\\) ，并将其替换到问题(1.2.2)当中，我们得到了另一个形式上非常相似的问题（又称 \\(\\ell_1\\) 范数优化问题，基追踪问题）："
        },
        {
            "type": "equation",
            "bbox": [
                0.485,
                0.577,
                0.825,
                0.622
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x \\in \\mathbb {R} ^ {n}} \\| x \\| _ {1}, \\tag {1.2.3} \\\\ \\begin{array}{l l} \\text {s . t .} & A x = b. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.637,
                0.827,
                0.8
            ],
            "angle": 0,
            "content": "令人惊讶地是，可以从理论上证明：若 \\(A, b\\) 满足一定的条件（例如使用前面随机产生的 \\(A\\) 和 \\(b\\)），向量 \\(u\\) 也是 \\(\\ell_1\\) 范数优化问题 (1.2.3) 的唯一最优解。这一发现的重要之处在于，虽然问题 (1.2.3) 仍没有显式解，但与问题 (1.2.2) 相比难度已经大大降低。前面我们提到 \\(\\ell_0\\) 范数优化问题是 NP 难问题，但 \\(\\ell_1\\) 范数优化问题的解可以非常容易地通过现有优化算法得到！从这个例子不难发现，优化学科的研究能够极大程度上帮助我们攻克现有的困难问题。既然有如上令人兴奋的结果，我们是否能使用其他更容易求解的范数替代 \\(\\ell_0\\) 范数呢？事实并非如此。如果简单地把 \\(\\ell_1\\) 范数修改为 \\(\\ell_2\\) 范数："
        },
        {
            "type": "page_footnote",
            "bbox": [
                0.283,
                0.839,
                0.801,
                0.852
            ],
            "angle": 0,
            "content": "1实际上， \\(\\ell_0\\) 范数不是一个范数，这里为了叙述统一而采用了这个术语，读者应当注意这个区别"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "1.2 实例：稀疏优化"
        },
        {
            "type": "page_number",
            "bbox": [
                0.725,
                0.118,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "5"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.154,
                0.495,
                0.202
            ],
            "angle": 0,
            "content": "\\(\\| x\\| _2 = \\left(\\sum_{i = 1}^n x_i^2\\right)^{1 / 2}\\) ，即求解如下优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.21,
                0.737,
                0.24
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| x \\| _ {2}, \\tag {1.2.4}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.241,
                0.509,
                0.254
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A x = b. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.264,
                0.739,
                0.384
            ],
            "angle": 0,
            "content": "几何学的知识表明, 问题 (1.2.4) 实际上就是原点到仿射集 \\(A x = b\\) 的投影, 我们可以直接写出它的显式表达式. 但遗憾的是, \\(u\\) 并不是问题 (1.2.4) 的解事实上, 图 1.1(a)-(c) 分别给出了一组随机数据下的 \\(u\\), 以及问题 (1.2.3) 和问题 (1.2.4) 的数值解. 可以看出图 1.1(a) 和 (b) 是完全一样的, 而 (c) 则与 \\(u\\) 相去甚远, 虽然隐约能看出数据点的大致趋势, 但已经不可分辨非零元素的具体位置."
        },
        {
            "type": "image",
            "bbox": [
                0.175,
                0.398,
                0.357,
                0.511
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.229,
                0.519,
                0.301,
                0.531
            ],
            "angle": 0,
            "content": "(a) 精确解 \\(u\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.367,
                0.398,
                0.547,
                0.51
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.401,
                0.519,
                0.513,
                0.531
            ],
            "angle": 0,
            "content": "(b) 问题 (1.2.3) 的解"
        },
        {
            "type": "image",
            "bbox": [
                0.556,
                0.399,
                0.734,
                0.51
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.589,
                0.519,
                0.699,
                0.531
            ],
            "angle": 0,
            "content": "(c) 问题 (1.2.4) 的解"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.36,
                0.551,
                0.546,
                0.567
            ],
            "angle": 0,
            "content": "图1.1 稀疏优化的例子"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.588,
                0.74,
                0.854
            ],
            "angle": 0,
            "content": "为什么会出现这种情况呢？这要追溯到 \\(\\ell_0, \\ell_1, \\ell_2\\) 范数的性质。下面用图示的方式来直观说明为什么 \\(\\ell_1\\) 范数优化问题的解具有稀疏性而 \\(\\ell_2\\) 范数优化问题的解不具有该性质。为了方便起见，我们在二维空间上讨论求解欠定方程组 \\(Ax = b\\)，此时 \\(Ax = b\\) 是一条直线。在几何上，三种优化问题实际上要找到最小的 \\(C\\)，使得“范数球” \\(\\{x \\| |x| \\leqslant C\\}\\)（\\(\\|\\cdot\\|\\) 表示任何一种范数）恰好与 \\(Ax = b\\) 相交。而图 1.2 里分别展示了三种范数球的几何直观：对 \\(\\ell_0\\) 范数，当 \\(C = 2\\) 时 \\(\\{x \\| |x|_0 \\leqslant C\\}\\) 是全平面，它自然与 \\(Ax = b\\) 相交，而当 \\(C = 1\\) 时退化成两条直线（坐标轴），此时问题的解是 \\(Ax = b\\) 和这两条直线的交点；对 \\(\\ell_1\\) 范数，根据 \\(C\\) 不同 \\(\\{x \\| |x|_1 \\leqslant C\\}\\) 为一系列正方形，这些正方形的顶点恰好都在坐标轴上，而最小的 \\(C\\) 对应的正方形和直线 \\(Ax = b\\) 的交点一般都是顶点，因此 \\(\\ell_1\\) 范数的解有稀疏性；对 \\(\\ell_2\\) 范数，当 \\(C\\) 取值不同时 \\(\\{x \\| |x|_2 \\leqslant C\\}\\) 为一系列圆，而圆有光滑的边界，它和直线 \\(Ax = b\\) 的切点可以是圆周上的任何一点，所以 \\(\\ell_2\\) 范数优化问题一般不能保证解的稀疏性。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.12,
                0.272,
                0.131
            ],
            "angle": 0,
            "content": "6"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第一章 最优化简介"
        },
        {
            "type": "image",
            "bbox": [
                0.264,
                0.159,
                0.443,
                0.294
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.323,
                0.304,
                0.384,
                0.316
            ],
            "angle": 0,
            "content": "(a) \\(\\ell_0\\) 范数"
        },
        {
            "type": "image",
            "bbox": [
                0.456,
                0.159,
                0.635,
                0.294
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.515,
                0.304,
                0.576,
                0.316
            ],
            "angle": 0,
            "content": "(b) \\(\\ell_1\\) 范数"
        },
        {
            "type": "image",
            "bbox": [
                0.644,
                0.16,
                0.822,
                0.293
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.702,
                0.304,
                0.762,
                0.316
            ],
            "angle": 0,
            "content": "(c) \\(\\ell_2\\) 范数"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.397,
                0.335,
                0.686,
                0.352
            ],
            "angle": 0,
            "content": "图1.2 三种范数优化问题求解示意图"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.396,
                0.827,
                0.641
            ],
            "angle": 0,
            "content": "问题(1.2.3)的理论和算法研究在2006年左右带来了革命性的影响．理论上研究的课题包括什么条件下问题(1.2.3)的解具有稀疏性，如何改进这些条件，如何推广这些条件到其他应用．常见的数据矩阵 \\(A\\) 一般由离散余弦变换、小波变换、傅里叶(Fourier)变换等生成．虽然这些矩阵本身并没有稀疏性，但通常具有很好的分析性质，保证稀疏解的存在性．注意到绝对值函数在零点处不可微，问题(1.2.3)是非光滑优化问题．虽然它可以等价于线性规划问题，但是数据矩阵 \\(A\\) 通常是稠密矩阵，甚至 \\(A\\) 的元素未知或者不能直接存储，只能提供 \\(Ax\\) 或 \\(A^{\\mathrm{T}}y\\) 等运算结果．在这些特殊情况下，线性规划经典的单纯形法和内点法通常不太适用于求解大规模的问题(1.2.3).本书的一个主要目的就是根据这些问题的特点设计合适的算法进行求解．需要强调的是，问题(1.2.3)主要特点是其最优解是稀疏向量，它是稀疏优化的一种典型形式."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.653,
                0.614,
                0.671
            ],
            "angle": 0,
            "content": "本书还将考虑带 \\(\\ell_1\\) 范数正则项的优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.695,
                0.825,
                0.729
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\mu \\| x \\| _ {1} + \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2}, \\tag {1.2.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.753,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(\\mu > 0\\) 是给定的正则化参数. 问题 (1.2.5) 又称为 LASSO (least absolute shrinkage and selection operator), 该问题可以看成是问题 (1.2.3) 的二次罚函数形式. 由于它是无约束优化问题, 形式上看起来比问题 (1.2.3) 简单. 本书大部分数值算法都将针对问题 (1.2.3) 或问题 (1.2.5) 给出具体形式. 因此全面掌握它们的求解方法是掌握基本最优化算法的一个标志."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.368,
                0.133
            ],
            "angle": 0,
            "content": "1.3 实例：低秩矩阵恢复"
        },
        {
            "type": "page_number",
            "bbox": [
                0.725,
                0.118,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "7"
        },
        {
            "type": "title",
            "bbox": [
                0.313,
                0.155,
                0.594,
                0.177
            ],
            "angle": 0,
            "content": "1.3 实例：低秩矩阵恢复"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.194,
                0.737,
                0.253
            ],
            "angle": 0,
            "content": "某视频网站提供了约48万用户对1万7千多部电影的上亿条评级数据，希望对用户的电影评级进行预测，从而改进用户电影推荐系统，为每个用户更有针对性地推荐影片。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.256,
                0.739,
                0.378
            ],
            "angle": 0,
            "content": "显然每一个用户不可能看过所有的电影，每一部电影也不可能收集到全部用户的评级．电影评级由用户打分1星到5星表示，记为取值 \\(1\\sim 5\\) 的整数．我们将电影评级放在一个矩阵 \\(M\\) 中，矩阵 \\(M\\) 的每一行表示不同用户，每一列表示不同电影．由于用户只对看过的电影给出自己的评价，矩阵\\(M\\) 中很多元素是未知的．图1.3给出了用户电影评级矩阵 \\(M\\) 的一个简单示例．令 \\(\\Omega\\) 是矩阵 \\(M\\) 中所有已知评级元素的下标的集合，则该问题可以初步"
        },
        {
            "type": "table",
            "bbox": [
                0.174,
                0.39,
                0.613,
                0.542
            ],
            "angle": 0,
            "content": "<table><tr><td></td><td>电影1</td><td>电影2</td><td>电影3</td><td>电影4</td><td>...</td><td>电影n</td></tr><tr><td>用户1</td><td>4</td><td>?</td><td>?</td><td>3</td><td>...</td><td>?</td></tr><tr><td>用户2</td><td>?</td><td>2</td><td>4</td><td>?</td><td>...</td><td>?</td></tr><tr><td>用户3</td><td>3</td><td>?</td><td>?</td><td>?</td><td>...</td><td>?</td></tr><tr><td>用户4</td><td>2</td><td>?</td><td>5</td><td>?</td><td>...</td><td>?</td></tr><tr><td>:</td><td>:</td><td>:</td><td>:</td><td>:</td><td></td><td>:</td></tr><tr><td>用户m</td><td>?</td><td>3</td><td>?</td><td>4</td><td>...</td><td>?</td></tr></table>"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.321,
                0.554,
                0.585,
                0.57
            ],
            "angle": 0,
            "content": "图1.3 用户电影评级矩阵 \\(M\\) 示例"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.591,
                0.739,
                0.753
            ],
            "angle": 0,
            "content": "描述为构造一个矩阵 \\(X\\)，使得在给定位置的元素等于已知评级元素，即满足 \\(X_{ij} = M_{ij}, (i,j) \\in \\Omega\\)。不难看出满足这个条件的矩阵 \\(X\\) 有无穷多个，那么如何得到一个真正有价值的 \\(X\\) 呢？这就需要分析 \\(X\\) 应该具有什么样的结构。类型相似的电影获得的评分往往是类似的，这意味着这些电影在矩阵 \\(M\\) 中所对应的列也是相似的，因此矩阵 \\(M\\) 的列可能是亏秩的；同样地，相似人群对不同电影的评分也可能是相似的，它们在矩阵 \\(M\\) 中所对应的行也是相似的，因此矩阵 \\(M\\) 的行也可能是亏秩的。因此寻找一个低秩矩阵 \\(X\\) 可能给出很好的解。令 \\(\\operatorname{rank}(X)\\) 为矩阵 \\(X\\) 的秩，该问题可以表达为"
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.761,
                0.737,
                0.791
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in \\mathbb {R} ^ {m \\times n}} \\operatorname {r a n k} (X), \\tag {1.3.1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.369,
                0.792,
                0.564,
                0.809
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & X _ {i j} = M _ {i j}, (i, j) \\in \\Omega . \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "这类问题称为低秩矩阵恢复（low rank matrix completion）。其约束条件保证了构造的低秩矩阵 \\(X\\) 与 \\(M\\) 中的所有已知元素完全相同。但是极小化"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.272,
                0.13
            ],
            "angle": 0,
            "content": "8"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第一章 最优化简介"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.829,
                0.266
            ],
            "angle": 0,
            "content": "矩阵的秩是 NP 难的问题, 如何将其化成一个容易求解的问题呢? 这里仍然沿用稀疏优化的思想. 在稀疏优化问题中, 我们将 \\(\\ell_0\\) 范数换成了 \\(\\ell_1\\) 范数. 而 \\(\\operatorname{rank}(X)\\) 正好是矩阵 \\(X\\) 所有非零奇异值的个数, 根据稀疏优化的思想, 我们将其更换成所有奇异值的和, 即矩阵 \\(X\\) 的核范数 (nuclear norm): \\(\\| X \\|_* = \\sum_{i} \\sigma_i(X)\\). 因此问题 (1.3.1) 就变成"
        },
        {
            "type": "equation",
            "bbox": [
                0.436,
                0.284,
                0.824,
                0.314
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in \\mathbb {R} ^ {m \\times n}} \\| X \\| _ {*}, \\tag {1.3.2}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.459,
                0.315,
                0.65,
                0.331
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & X _ {i j} = M _ {i j}, (i, j) \\in \\Omega . \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.344,
                0.825,
                0.441
            ],
            "angle": 0,
            "content": "可以证明问题 (1.3.2) 是一个凸优化问题, 并且在一定条件下它与问题 (1.3.1) 等价。也可以将问题 (1.3.2) 转换为一个半定规划问题, 但是目前半定规划算法所能有效求解的问题规模限制了这种技术的实际应用。同样地, 考虑到观测可能出现误差, 对于给定的参数 \\(\\mu > 0\\), 我们也写出该问题的二次罚函数形式:"
        },
        {
            "type": "equation",
            "bbox": [
                0.403,
                0.442,
                0.824,
                0.479
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in \\mathbb {R} ^ {m \\times n}} \\quad \\mu \\| X \\| _ {*} + \\frac {1}{2} \\sum_ {(i, j) \\in \\Omega} \\left(X _ {i j} - M _ {i j}\\right) ^ {2}. \\tag {1.3.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.486,
                0.824,
                0.524
            ],
            "angle": 0,
            "content": "类似于稀疏优化问题 (1.2.3) 和 (1.2.5)，本书大部分数值算法都可以针对问题 (1.3.2) 或问题 (1.3.3) 给出具体形式。"
        },
        {
            "type": "title",
            "bbox": [
                0.425,
                0.556,
                0.657,
                0.578
            ],
            "angle": 0,
            "content": "1.4 实例：深度学习"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.595,
                0.827,
                0.756
            ],
            "angle": 0,
            "content": "深度学习（deep learning）的起源可以追溯至20世纪40年代，其雏形出现在控制论中。近十年来深度学习又重新走入了人们的视野，深度学习问题和算法的研究也经历了一次新的浪潮。虽然卷积网络的设计受到了生物学和神经科学的启发，但深度学习目前的发展早已超越了机器学习模型中的神经科学观点。它用相对简单的函数来表达复杂的表示，从低层特征概括到更加抽象的高层特征，让计算机从经验中挖掘隐含的信息和价值。本节我们将通过介绍多层感知机和卷积神经网络来了解优化模型在深度学习中的应用。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.784,
                0.421,
                0.801
            ],
            "angle": 0,
            "content": "1.4.1 多层感知机"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "多层感知机 (multi-layer perceptron, MLP) 也叫作深度前馈网络 (deep feedforward network) 或前馈神经网络 (feedforward neural network),"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.332,
                0.133
            ],
            "angle": 0,
            "content": "1.4 实例：深度学习"
        },
        {
            "type": "page_number",
            "bbox": [
                0.725,
                0.119,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "9"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.318
            ],
            "angle": 0,
            "content": "它通过已有的信息或者知识来对未知事物进行预测。在神经网络中，已知的信息通常用数据集来表示。数据集一般分为训练集和测试集：训练集是用来训练神经网络，从而使得神经网络能够掌握训练集上的信息；测试集是用来测试训练完的神经网络的预测准确性。一个常见的任务是分类问题。假设我们有一个猫和狗的图片集，将其划分成训练集和测试集（保证集合中猫和狗图片要有一定的比例）。神经网络是想逼近一个从图片到 \\(\\{0,1\\}\\) 的函数，这里 0 表示猫，1 表示狗。因为神经网络本身的结构和大量的训练集信息，训练得到的函数与真实结果具有非常高的吻合性。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.322,
                0.74,
                0.568
            ],
            "angle": 0,
            "content": "具体地，给定训练集 \\(D = \\{\\{a_{1},b_{1}\\} ,\\{a_{2},b_{2}\\} ,\\dots ,\\{a_{m},b_{m}\\} \\}\\) ，假设数据\\(a_i\\in \\mathbb{R}^p,b_i\\in \\mathbb{R}^q\\) ．为了方便处理模型里的偏差项，还假设 \\(a_i\\) 的第一个元素等于1，即 \\(a_{i1} = 1\\) ．图1.4给出了一种由 \\(p\\) 个输入单元和 \\(q\\) 个输出单元构成的\\((L + 2)\\) 层感知机，其含有一个输入层，一个输出层，和 \\(L\\) 个隐藏层．该感知机的第 \\(l\\) 个隐藏层共有 \\(m^{(l)}\\) 个神经元，为了方便我们用 \\(l = 0\\) 表示输入层，\\(l = L + 1\\) 表示输出层，并定义 \\(m^{(0)} = p\\) 和 \\(m^{(L + 1)} = q.\\) 设 \\(y^{(l)}\\in \\mathbb{R}^{m^{(l)}}\\) 为第 \\(l\\) 层的所有神经元，同样地，为了能够处理每一个隐藏层的信号偏差，除输出层外，我们令 \\(y^{(l)}\\) 的第一个元素等于1，即 \\(y_1^{(l)} = 1,0\\leqslant l\\leqslant L\\) ，而其余的元素则是通过上一层的神经元的值进行加权求和得到．令参数 \\(x = (x^{(1)},x^{(2)},\\dots ,x^{(L + 1)})\\) 表示网络中所有层之间的权重，其中 \\(x_{i,k}^{(l)}\\) 是第 \\((l - 1)\\) 隐藏层的第 \\(k\\) 个单元连接到第 \\(l\\) 隐藏层的第 \\(i\\) 个单元对应的权重，则在第 \\(l\\) 隐藏层中，第 \\(i\\) 个单元（ \\(i > 1\\) ，当 \\(l = L + 1\\) 时可取为 \\(i\\geqslant 1\\) ）计算输出信息 \\(y_{i}^{(l)}\\) 为"
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.576,
                0.737,
                0.617
            ],
            "angle": 0,
            "content": "\\[\ny _ {i} ^ {(l)} = t \\left(z _ {i} ^ {(l)}\\right), \\quad z _ {i} ^ {(l)} = \\sum_ {k = 1} ^ {m ^ {(l - 1)}} x _ {i, k} ^ {(l)} y _ {k} ^ {(l - 1)}. \\tag {1.4.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.624,
                0.609,
                0.641
            ],
            "angle": 0,
            "content": "这里函数 \\(t(\\cdot)\\) 称为激活函数，常见的类型有Sigmoid函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.648,
                0.534,
                0.683
            ],
            "angle": 0,
            "content": "\\[\nt (z) = \\frac {1}{1 + \\exp (- z)},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.69,
                0.291,
                0.705
            ],
            "angle": 0,
            "content": "Heaviside函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.371,
                0.705,
                0.518,
                0.756
            ],
            "angle": 0,
            "content": "\\[\nt (z) = \\left\\{ \\begin{array}{l l} 1, & z \\geqslant 0, \\\\ 0, & z <   0, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.76,
                0.296,
                0.776
            ],
            "angle": 0,
            "content": "以及ReLU函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.386,
                0.781,
                0.737,
                0.798
            ],
            "angle": 0,
            "content": "\\[\nt (z) = \\max  \\{0, z \\}. \\tag {1.4.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.807,
                0.33,
                0.823
            ],
            "angle": 0,
            "content": "整个过程可以描述为"
        },
        {
            "type": "equation",
            "bbox": [
                0.327,
                0.831,
                0.579,
                0.854
            ],
            "angle": 0,
            "content": "\\[\ny ^ {(0)} \\xrightarrow {x ^ {(1)}} z ^ {(1)} \\xrightarrow {t} y ^ {(1)} \\xrightarrow {x ^ {(2)}} \\dots \\xrightarrow {t} y ^ {(L + 1)}.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.281,
                0.131
            ],
            "angle": 0,
            "content": "10"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第一章 最优化简介"
        },
        {
            "type": "image",
            "bbox": [
                0.293,
                0.158,
                0.554,
                0.349
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image",
            "bbox": [
                0.56,
                0.159,
                0.797,
                0.348
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.257,
                0.362,
                0.825,
                0.4
            ],
            "angle": 0,
            "content": "图1.4 带 \\(p\\) 个输入单元和 \\(q\\) 个输出单位的 \\((L + 2)\\) 层感知机的网络图，第 \\(l\\) 个隐藏层包含 \\(m^{(l)}\\) 个神经元。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.428,
                0.826,
                0.546
            ],
            "angle": 0,
            "content": "容易看出，多层感知机的每一层输出实际就是由其上一层的数值作线性组合再逐分量作非线性变换得到的。若将 \\( y^{(0)} \\) 视为自变量，\\( y^{(L + 1)} \\) 视为因变量，则多层感知机实际上定义了一个以 \\( x \\) 为参数的函数 \\( h(a;x): \\mathbb{R}^p \\to \\mathbb{R}^q \\)，这里 \\( a \\) 为输入层 \\( y^{(0)} \\) 的取值。当输入数据为 \\( a_i \\) 时，其输出 \\( h(a_i;x) \\) 将作为真实标签 \\( b_i \\) 的估计。若选择平方误差为损失函数，则我们得到多层感知机的优化模型："
        },
        {
            "type": "equation",
            "bbox": [
                0.414,
                0.548,
                0.825,
                0.584
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad \\sum_ {i = 1} ^ {m} \\| h \\left(a _ {i}; x\\right) - b _ {i} \\| _ {2} ^ {2} + \\lambda r (x), \\tag {1.4.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.592,
                0.825,
                0.673
            ],
            "angle": 0,
            "content": "其中 \\(r(x)\\) 是正则项, 用来刻画解的某些性质, 如光滑性或稀疏性等; \\(\\lambda\\) 称为正则化参数, 用来平衡模型的拟合程度和解的性质. 如果 \\(\\lambda\\) 太小, 那么对解的性质没有起到改善作用; 如果 \\(\\lambda\\) 太大, 则模型与原问题相差很大, 可能是一个糟糕的逼近."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.701,
                0.44,
                0.719
            ],
            "angle": 0,
            "content": "1.4.2 卷积神经网络"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.733,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "卷积神经网络（convolutional neural network, CNN）是一种深度前馈人工神经网络，专门用来处理如时间序列数据或是图像等网格数据。CNN在计算机视觉、视频分析、自然语言处理等诸多领域有大量成功的应用。与图1.4对应的全连接网络（相邻两层之间的节点都是相连或相关的）不同，卷积神经网络的思想是通过局部连接以及共享参数的方式来大大减少参数量，从而减少对数据量的依赖以及提高训练的速度。典型的CNN网络结构"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.333,
                0.133
            ],
            "angle": 0,
            "content": "1.4 实例：深度学习"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.119,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "11"
        },
        {
            "type": "image",
            "bbox": [
                0.28,
                0.161,
                0.685,
                0.31
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.386,
                0.325,
                0.522,
                0.341
            ],
            "angle": 0,
            "content": "图1.5 卷积操作"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.367,
                0.744,
                0.488
            ],
            "angle": 0,
            "content": "通常由一个或多个卷积层、下采样层（subsampling）²和顶层的全连接层组成。全连接层的结构与多层感知机的结构相同。卷积层是一种特殊的网络层，它首先对输入数据进行卷积操作产生多个特征映射，之后使用非线性激活函数（比如ReLU）对每个特征进行变换。下采样层一般位于卷积层之后，它的作用是减小数据维数并提取数据的多尺度信息，其结果最终会输出到下一组变换。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.492,
                0.741,
                0.531
            ],
            "angle": 0,
            "content": "给定一个二维图像 \\(I \\in \\mathbb{R}^{n \\times n}\\) 和卷积核 \\(K \\in \\mathbb{R}^{k \\times k}\\)，我们定义一种简单的卷积操作 \\(S = I * K\\)，它的元素是"
        },
        {
            "type": "equation",
            "bbox": [
                0.311,
                0.544,
                0.738,
                0.563
            ],
            "angle": 0,
            "content": "\\[\n\\boldsymbol {S} _ {i, j} = \\left\\langle I (i: i + k - 1, j: j + k - 1), \\boldsymbol {K} \\right\\rangle , \\tag {1.4.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.576,
                0.741,
                0.664
            ],
            "angle": 0,
            "content": "其中两个矩阵 \\(X,Y\\) 的内积是它们相应元素乘积之和，即 \\(\\langle X,Y\\rangle = \\sum_{i,j}X_{ij}Y_{ij},\\) \\(\\pmb {I}(i:i + k - 1,j:j + k - 1)\\) 是矩阵 \\(\\pmb{I}\\) 从位置 \\((i,j)\\) 开始的一个 \\(k\\times k\\) 子矩阵.图1.5给出了一个例子．生成的结果 \\(\\mathbf{S}\\) 可以根据卷积核的维数、 \\(\\pmb{I}\\) 的边界是否填充、卷积操作时滑动的大小等相应变化."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.668,
                0.741,
                0.83
            ],
            "angle": 0,
            "content": "图1.6给出了下采样层的一个示例。第 \\(l\\) 特征层是第 \\((l - 1)\\) 特征层的下采样层。具体地，我们先将第 \\((l - 1)\\) 层的每个矩阵划分成若干子矩阵，之后将每个子矩阵里所有元素按照某种规则（例如取平均值或最大值）变换成一个元素。因此，第 \\((l - 1)\\) 特征层每个小框里所有元素的平均值或最大值就对应于第 \\(l\\) 特征层的一个元素。容易看出，下采样层实际上是用一个数代表一个子矩阵，经过下采样层变换后，前一特征层矩阵的维数会进一步降低。图1.7给出了一个简单的卷积神经网络示意图。输入图片通过不同的卷积核生成的不同矩阵，再经过非线性激活函数作用后生成第1层的特征；第2层"
        },
        {
            "type": "page_footnote",
            "bbox": [
                0.195,
                0.839,
                0.371,
                0.853
            ],
            "angle": 0,
            "content": "2有时也称为“池化”（pooling）"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.281,
                0.131
            ],
            "angle": 0,
            "content": "12"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第一章 最优化简介"
        },
        {
            "type": "image",
            "bbox": [
                0.35,
                0.165,
                0.737,
                0.325
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.457,
                0.338,
                0.626,
                0.355
            ],
            "angle": 0,
            "content": "图1.6 下采样层示例"
        },
        {
            "type": "image",
            "bbox": [
                0.303,
                0.377,
                0.784,
                0.535
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.413,
                0.551,
                0.669,
                0.568
            ],
            "angle": 0,
            "content": "图1.7 卷积神经网络一种示意图"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.597,
                0.825,
                0.674
            ],
            "angle": 0,
            "content": "是第 1 层的下采样层；第 3 层和第 4 层又是卷积层和下采样层；第 5 层是全连接层；第 6 层为输出层。实际的卷积神经网络可达几十层甚至更多，卷积核的大小，网络节点之间的连接方式也可以有很多变化，从而生成不一样的模型。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.681,
                0.825,
                0.74
            ],
            "angle": 0,
            "content": "给定一个训练集 \\(D = \\{\\{a_{1}, b_{1}\\}, \\{a_{2}, b_{2}\\}, \\dots, \\{a_{m}, b_{m}\\}\\}\\), 其中 \\(a_{i}\\) 是训练图片, \\(b_{i}\\) 是其对应的标签. 卷积神经网络对应的优化问题的形式仍可套用 (1.4.3), 但函数 \\(h(a_{i}; x)\\) 由卷积神经网络构成, 而 \\(x\\) 是卷积神经网络的参数."
        },
        {
            "type": "title",
            "bbox": [
                0.413,
                0.775,
                0.67,
                0.797
            ],
            "angle": 0,
            "content": "1.5 最优化的基本概念"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "一般来说，最优化算法研究可以分为：构造最优化模型、确定最优化问题的类型和设计算法、实现算法或调用优化算法软件包进行求解．最优化模"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.352,
                0.133
            ],
            "angle": 0,
            "content": "1.5 最优化的基本概念"
        },
        {
            "type": "page_number",
            "bbox": [
                0.718,
                0.119,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "13"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.744,
                0.423
            ],
            "angle": 0,
            "content": "型的构造和实际问题紧密相关，比如说，给定二维欧几里得（Euclid）空间的若干个离散点，假定它们可以通过一条直线分成两部分，也可以通过一条曲线分成两部分。那么分别使用直线与曲线所得到的最优化模型是不同的。在问题(1.1.1)中，目标函数 \\(f\\) 和约束函数 \\(c_{i}\\) 都是由模型来确定的。在确定模型之后，我们需要对模型对应的优化问题进行分类。这里，分类的必要性是因为不存在对于所有优化问题的一个统一的算法。因此我们需要针对具体优化问题所属的类别，来设计或者调用相应的算法求解器。最后就是模型的求解过程。同一类优化问题往往存在着不同的求解算法。对于具体的优化问题，我们需要充分利用问题的结构，并根据问题的需求（求解精度和速度等）来设计相应的算法。另外，根据算法得到的结果，我们可以来判别模型构造是否合理或者进一步地改进模型。如果构造的模型比较复杂，那么算法求解起来相对困难（时间慢或者精度差）。此时算法分析可以帮助我们设计替代模型，以确保快速且比较精确地求出问题的解。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.427,
                0.744,
                0.526
            ],
            "angle": 0,
            "content": "这三个部分的研究对于形成完备的最优化体系是必要的。实际应用导出的各种各样的最优化模型给最优化学科不断注入新鲜的血液，对现有的优化算法进行挑战并推动其向前发展。最优化算法的设计以及理论分析帮助实际问题建立更鲁棒稳定的模型。模型与算法相辅相成，使得最优化学科不断发展。"
        },
        {
            "type": "title",
            "bbox": [
                0.171,
                0.555,
                0.414,
                0.574
            ],
            "angle": 0,
            "content": "1.5.1 连续和离散优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.587,
                0.741,
                0.688
            ],
            "angle": 0,
            "content": "最优化问题可以分为连续和离散优化问题两大类。连续优化问题是指决策变量所在的可行集合是连续的，比如平面、区间等。如稀疏优化问题(1.2.2)一(1.2.5)的约束集合就是连续的。离散优化问题是指决策变量能在离散集合上取值，比如离散点集、整数集等。常见的离散优化问题有整数规划，其对应的决策变量的取值范围是整数集合。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.692,
                0.744,
                0.854
            ],
            "angle": 0,
            "content": "在连续优化问题中，基于决策变量取值空间以及约束和目标函数的连续性，我们可以从一个点处目标和约束函数的取值来估计该点可行领域内的取值情况。进一步地，可以根据邻域内的取值信息来判断该点是否最优。离散优化问题则不具备这个性质，因为决策变量是在离散集合上取值。因此在实际中往往比连续优化问题更难求解。实际中的离散优化问题往往可以转化为一系列连续优化问题来进行求解。比如线性整数规划问题中著名的分支定界方法，就是松弛成一系列线性规划问题来进行求解。因此连续优化问题的求解在最优化理论与算法中扮演着重要的角色。本书后续的内容也将围绕"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "14"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第一章 最优化简介"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.157,
                0.439,
                0.174
            ],
            "angle": 0,
            "content": "连续优化问题展开介绍"
        },
        {
            "type": "title",
            "bbox": [
                0.259,
                0.205,
                0.519,
                0.223
            ],
            "angle": 0,
            "content": "1.5.2 无约束和约束优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.238,
                0.827,
                0.358
            ],
            "angle": 0,
            "content": "最优化问题的另外一个重要的分类标准是约束是否存在。无约束优化问题的决策变量没有约束条件限制，即可行集合 \\(\\mathcal{X} = \\mathbb{R}^n\\)。相对地，约束优化问题是指带有约束条件的问题。在实际应用中，这两类优化问题广泛存在。无约束优化问题对应于在欧几里得空间中求解一个函数的最小值点。比如在 \\(\\ell_1\\) 正则化问题 (1.2.5) 中，决策变量的可行域是 \\(\\mathbb{R}^n\\)，其为一个无约束优化问题。在问题 (1.2.2) — (1.2.4) 中，可行集为 \\(\\{x \\mid Ax = b\\}\\)，其为约束优化问题。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.362,
                0.828,
                0.525
            ],
            "angle": 0,
            "content": "因为问题(1.1.1)可以通过将约束 \\((\\mathcal{X}\\neq \\mathbb{R}^n)\\) 罚到目标函数上转化为无约束问题，所以在某种程度上，约束优化问题就是无约束优化问题．很多约束优化问题的求解也是转化为一系列的无约束优化问题来做，常见方式有增广拉格朗日函数法、罚函数法等．尽管如此，约束优化问题的理论以及算法研究仍然是非常重要的．主要原因是，借助于约束函数，我们能够更好地描述可行域的几何性质，进而更有效地找到最优解．对于典型的约束和无约束优化模型，我们将会在本书的第四章中介绍，相应的理论以及算法会在第五一八章中给出."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.555,
                0.519,
                0.573
            ],
            "angle": 0,
            "content": "1.5.3 随机和确定性优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.588,
                0.827,
                0.791
            ],
            "angle": 0,
            "content": "伴随着近年来人工智能的发展，随机优化问题的研究得到了长足的发展。随机优化问题是指目标或者约束函数中涉及随机变量而带有不确定性的问题。不像确定性优化问题中目标和约束函数都是确定的，随机优化问题中总是包含一些未知的参数。在实际问题中，我们往往只能知道这些参数的某些估计。随机优化问题在机器学习、深度学习以及强化学习中有着重要应用，其优化问题的目标函数是关于一个未知参数的期望的形式。因为参数的未知性，实际中常用的方法是通过足够多的样本来逼近目标函数，得到一个新的有限和形式的目标函数。由于样本数量往往非常大，我们还是将这个问题看作相对于指标随机变量的期望形式，然后通过随机优化方法来进行求解。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.795,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "相比于确定性优化问题，随机优化问题的求解往往涉及更多的随机性。很多确定性优化算法都有相应的随机版本。随机性使得这些算法在特定问题上具有更低的计算复杂度或者更好的收敛性质。以目标函数为多项求和的优"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.352,
                0.133
            ],
            "angle": 0,
            "content": "1.5 最优化的基本概念"
        },
        {
            "type": "page_number",
            "bbox": [
                0.718,
                0.118,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "15"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.256
            ],
            "angle": 0,
            "content": "化问题为例，如果使用确定性优化算法，每一次计算目标函数的梯度都会引入昂贵的复杂度．但是对于随机优化问题，我们每次可能只计算和式中的一项或者几项，这大大减少了计算时间．同时我们还能保证算法求解的足够精确．具体的模型介绍会在第四章中给出．确定性优化算法会在第六一八章中给出，随机优化算法会在第八章中介绍."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.282,
                0.431,
                0.3
            ],
            "angle": 0,
            "content": "1.5.4 线性和非线性规划问题"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.313,
                0.744,
                0.413
            ],
            "angle": 0,
            "content": "线性规划是指问题 (1.1.1) 中目标函数和约束函数都是线性的。当目标函数和约束函数至少有一个是非线性的，那么对应的优化问题的称为非线性规划问题。线性规划问题在约束优化问题中具有较为简单的形式。类似于连续函数可以用分片线性函数来逼近一样，线性规划问题的理论分析与数值求解可以为非线性规划问题提供很好的借鉴和基础。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.417,
                0.747,
                0.558
            ],
            "angle": 0,
            "content": "线性规划问题的研究很早便得到了人们的关注。在1946—1947年，George Bernard Dantzig提出了线性规划的一般形式并提出了至今仍非常流行的单纯形方法。虽然单纯形方法在实际问题中经常表现出快速收敛，但是其复杂度并不是多项式的。1979年，Leonid Khachiyan证明了线性规划问题多项式时间算法的存在性。1984年，Narendra Karmarkar提出了多项式时间的内点法。后来，内点法也被推广到求解一般的非线性规划问题。目前，求解线性规划问题最流行的两类方法依然是单纯形法和内点法。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.583,
                0.394,
                0.601
            ],
            "angle": 0,
            "content": "1.5.5 凸和非凸优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.614,
                0.739,
                0.693
            ],
            "angle": 0,
            "content": "凸优化问题是指最小化问题 (1.1.1) 中的目标函数和可行域分别是凸函数和凸集。如果其中有一个或者两者都不是凸的，那么相应的最小化问题是非凸优化问题。因为凸优化问题的任何局部最优解都是全局最优解，其相应的算法设计以及理论分析相对非凸优化问题简单很多。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.705,
                0.738,
                0.763
            ],
            "angle": 0,
            "content": "注1.1 若问题(1.1.1)中的min改为max，且目标函数和可行域分别为凹函数和凸集，我们也称这样的问题为凸优化问题。这是因为对凹函数求极大等价于对其相反数（凸函数）求极小。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "在实际问题的建模中，我们经常更倾向于得到一个凸优化模型．另外，判断一个问题是否是凸问题也很重要．比如，给定一个非凸优化问题，一种方法是将其转化为一系列凸优化子问题来求解．此时需要清楚原非凸问题中的哪个或哪些函数导致了非凸性，之后考虑的是如何用凸优化模型来逼近"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.119,
                0.281,
                0.131
            ],
            "angle": 0,
            "content": "16"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第一章 最优化简介"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.826,
                0.256
            ],
            "angle": 0,
            "content": "原问题. 在压缩感知问题中, \\(\\ell_0\\) 范数是非凸的, 原问题对应的解的性质难以直接分析, 相应的全局收敛的算法也不容易构造. 利用 \\(\\ell_0\\) 范数和 \\(\\ell_1\\) 范数在某种意义上的等价性, 我们将原非凸问题转化为凸优化问题. 在一定的假设下, 我们通过求解 \\(\\ell_1\\) 范数对应的凸优化问题得到了原非凸优化问题的全局最优解."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.287,
                0.48,
                0.305
            ],
            "angle": 0,
            "content": "1.5.6 全局和局部最优解"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.32,
                0.802,
                0.336
            ],
            "angle": 0,
            "content": "在求解最优化问题之前，先介绍最小化问题(1.1.1)的最优解的定义"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.353,
                0.747,
                0.37
            ],
            "angle": 0,
            "content": "定义1.1 (最优解) 对于可行点 \\(\\bar{x}\\) (即 \\(\\bar{x} \\in \\mathcal{X}\\)), 定义如下概念:"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.386,
                0.827,
                0.423
            ],
            "angle": 0,
            "content": "(1) 如果 \\(f(\\bar{x}) \\leqslant f(x), \\forall x \\in \\mathcal{X}\\), 那么称 \\(\\bar{x}\\) 为问题 (1.1.1) 的全局极小解 (点), 有时也称为 (全局) 最优解或最小值点;"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.44,
                0.825,
                0.477
            ],
            "angle": 0,
            "content": "(2) 如果存在 \\(\\bar{x}\\) 的一个 \\(\\varepsilon\\) 邻域 \\(N_{\\varepsilon}(\\bar{x})\\) 使得 \\(f(\\bar{x}) \\leqslant f(x), \\forall x \\in N_{\\varepsilon}(\\bar{x}) \\cap \\mathcal{X}\\), 那么称 \\(\\bar{x}\\) 为问题 (1.1.1) 的局部极小解（点），有时也称为局部最优解；"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.495,
                0.825,
                0.532
            ],
            "angle": 0,
            "content": "(3) 进一步地, 如果有 \\(f(\\bar{x}) < f(x), \\forall x \\in N_{\\varepsilon}(\\bar{x}) \\cap \\mathcal{X}, x \\neq \\bar{x}\\) 成立, 则称 \\(\\bar{x}\\) 为问题 (1.1.1) 的严格局部极小解 (点)."
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.386,
                0.827,
                0.532
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.549,
                0.825,
                0.606
            ],
            "angle": 0,
            "content": "如果一个点是局部极小解，但不是严格局部极小解，我们称之为非严格局部极小解。在图1.8中，我们以一个简单的函数为例，指出了其全局与局部极小解。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.612,
                0.825,
                0.67
            ],
            "angle": 0,
            "content": "在问题 (1.1.1) 的求解中，我们想要得到的是其全局最优解，但是由于实际问题的复杂性，往往只能够得到其局部最优解。在第四章中，我们将会针对具体的优化问题来分析其全局与局部最优解。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.7,
                0.401,
                0.718
            ],
            "angle": 0,
            "content": "1.5.7 优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.733,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "在给定优化问题之后，我们要考虑如何求解。根据优化问题的不同形式，其求解的困难程度可能会有很大差别。对于一个优化问题，如果我们能用代数表达式给出其最优解，那么这个解称为显式解，对应的问题往往比较简单。例如二次函数在有界区间上的极小化问题，我们可以通过比较其在对称轴上和区间两个端点处的值得到最优解，这个解可以显式地写出。但实际问题往往是没有办法显式求解的，因此常采用迭代算法。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.351,
                0.133
            ],
            "angle": 0,
            "content": "1.5 最优化的基本概念"
        },
        {
            "type": "page_number",
            "bbox": [
                0.718,
                0.118,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "17"
        },
        {
            "type": "image",
            "bbox": [
                0.209,
                0.16,
                0.696,
                0.429
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.223,
                0.445,
                0.685,
                0.463
            ],
            "angle": 0,
            "content": "图1.8 函数的全局极小、严格局部极小和非严格局部极小解"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.487,
                0.739,
                0.608
            ],
            "angle": 0,
            "content": "迭代算法的基本思想是：从一个初始点 \\(x^0\\) 出发，按照某种给定的规则进行迭代，得到一个序列 \\(\\{x^k\\}\\)。如果迭代在有限步内终止，那么希望最后一个点就是优化问题的解。如果迭代点列是无穷集合，那么希望该序列的极限点（或者聚点）则为优化问题的解。为了使算法能在有限步内终止，我们一般会通过一些收敛准则来保证迭代停在问题的一定精度逼近解上。对于无约束优化问题，常用的收敛准则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.316,
                0.616,
                0.737,
                0.653
            ],
            "angle": 0,
            "content": "\\[\n\\frac {f \\left(x ^ {k}\\right) - f ^ {*}}{\\max  \\{| f ^ {*} | , 1 \\}} \\leqslant \\varepsilon_ {1}, \\quad \\| \\nabla f \\left(x ^ {k}\\right) \\| \\leqslant \\varepsilon_ {2}, \\tag {1.5.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.66,
                0.74,
                0.801
            ],
            "angle": 0,
            "content": "其中 \\(\\varepsilon_{1},\\varepsilon_{2}\\) 为给定的很小的正数， \\(\\| \\cdot \\|\\) 表示某种范数（这里可以简单理解为\\(\\ell_2\\) 范数： \\(\\| x\\| _2 = \\left(\\sum_{i = 1}^n x_i^2\\right)^{1 / 2}\\) ，第二章将会给出范数的一般定义）， \\(f^{*}\\) 为函数\\(f\\) 的最小值（假设已知或者以某种方式估计得到）以及 \\(\\nabla f(x^{k})\\) 表示函数 \\(f\\) 在点 \\(x\\) 处的梯度（光滑函数在局部最优点处梯度为零向量，第五章中会给出更多介绍）。对于约束优化问题，还需要考虑约束违反度．具体地，要求最后得到的点满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.305,
                0.811,
                0.606,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} c _ {i} \\left(x ^ {k}\\right) \\leqslant \\varepsilon_ {3}, i = 1, 2, \\dots , m, \\\\ \\left| c _ {i} \\left(x ^ {k}\\right) \\right| \\leqslant \\varepsilon_ {4}, i = m + 1, m + 2, \\dots , m + l, \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.119,
                0.28,
                0.13
            ],
            "angle": 0,
            "content": "18"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第一章 最优化简介"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.156,
                0.827,
                0.297
            ],
            "angle": 0,
            "content": "其中 \\(\\varepsilon_{3}, \\varepsilon_{4}\\) 为很小的正数，用来刻画 \\(x^{k}\\) 的可行性。除了约束违反度之外，我们也要考虑 \\(x^{k}\\) 与最优解之间的距离，如 (1.5.1) 式中给出的函数值与最优值的相对误差。由于一般情况下事先并不知道最优解，在最优解唯一的情形下一般使用某种基准算法来得到 \\(x^{*}\\) 的一个估计，之后计算其与 \\(x^{k}\\) 的距离以评价算法的性能。因为约束的存在，我们不能简单地用目标函数的梯度来判断最优性，实际中采用的判别准则是点的最优性条件的违反度（关于约束优化的最优性条件，会在第五章中给出）。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.302,
                0.825,
                0.36
            ],
            "angle": 0,
            "content": "对于一个具体的算法，根据其设计的出发点，我们不一定能得到一个高精度的逼近解。此时，为了避免无用的计算开销，我们还需要一些停机准则来及时停止算法的进行。常用的停机准则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.371,
                0.369,
                0.712,
                0.406
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\| x ^ {k + 1} - x ^ {k} \\|}{\\max  \\left\\{\\| x ^ {k} \\| , 1 \\right\\}} \\leqslant \\varepsilon_ {5}, \\quad \\frac {| f (x ^ {k + 1}) - f (x ^ {k}) |}{\\max  \\{| f (x ^ {k}) | , 1 \\}} \\leqslant \\varepsilon_ {6},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.416,
                0.825,
                0.474
            ],
            "angle": 0,
            "content": "这里的各个 \\(\\varepsilon\\) 一般互不相等。上面的准则分别表示相邻迭代点和其对应目标函数值的相对误差很小。在算法设计中，这两个条件往往只能反映迭代点列接近收敛，但不能代表收敛到优化问题的最优解。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.479,
                0.826,
                0.578
            ],
            "angle": 0,
            "content": "在算法设计中，一个重要的标准是算法产生的点列是否收敛到优化问题的解．对于问题(1.1.1)，其可能有很多局部极小解和全局极小解，但所有全局极小解对应的目标函数值，即优化问题的最小值 \\( f^{*} \\) 是一样的．考虑无约束的情形，对于一个算法，给定初始点 \\( x^0 \\)，记其迭代产生的点列为 \\( \\{x^k\\} \\) 如果 \\( \\{x^k\\} \\) 在某种范数 \\( \\| \\cdot \\| \\) 的意义下满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.47,
                0.593,
                0.612,
                0.618
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\| x ^ {k} - x ^ {*} \\| = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.629,
                0.825,
                0.667
            ],
            "angle": 0,
            "content": "且收敛的点 \\(x^{*}\\) 为一个局部（全局）极小解，那么我们称该点列收敛到局部（全局）极小解，相应的算法称为是依点列收敛到局部（全局）极小解的。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.671,
                0.826,
                0.811
            ],
            "angle": 0,
            "content": "在算法的收敛分析中，初始迭代点 \\( x^0 \\) 的选取也尤为重要。比如一般的牛顿法，只有在初始点足够接近局部（全局）最优解时，才能收敛。但是这样的初始点的选取往往比较困难，此时我们更想要的是一个从任何初始点出发都能收敛的算法。因此优化算法的研究包括如何设计全局化策略，将已有的可能发散的优化算法修改得到一个新的全局收敛到局部（全局）最优解的算法。比如通过采用合适的全局化策略，我们可以修正一般的牛顿法使得修改后的算法是全局收敛到局部（全局）最优解的。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.815,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "进一步地, 如果从任意初始点 \\(x^{0}\\) 出发, 算法都是依点列收敛到局部 (全局) 极小解的, 我们称该算法是全局依点列收敛到局部 (全局) 极小解的."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.351,
                0.133
            ],
            "angle": 0,
            "content": "1.5 最优化的基本概念"
        },
        {
            "type": "page_number",
            "bbox": [
                0.718,
                0.118,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "19"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.739,
                0.277
            ],
            "angle": 0,
            "content": "相应地，如果记对应的函数值序列为 \\(\\{f(x^{k})\\}\\)，我们还可以定义算法的（全局）依函数值收敛到局部（全局）极小值的概念。对于凸优化问题，因为其任何局部最优解都为全局最优解，算法的收敛性都是相对于其全局极小而言的。除了点列和函数值的收敛外，实际中常用的还有每个迭代点的最优性条件（如无约束优化问题中的梯度范数，约束优化问题中的最优性条件违反度等等）的收敛。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.28,
                0.739,
                0.36
            ],
            "angle": 0,
            "content": "对于带约束的情形，给定初始点 \\(x^0\\) ，算法产生的点列 \\(\\{x^k\\}\\) 不一定是可行的（即 \\(x^{k}\\in \\mathcal{X}\\) 未必对任意 \\(k\\) 成立）．考虑到约束违反的情形，我们需要保证 \\(\\{x^k\\}\\) 在收敛到 \\(x^{*}\\) 的时候，其违反度是可接受的．除此要求之外，算法的收敛性的定义和无约束情形相同."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.364,
                0.737,
                0.423
            ],
            "angle": 0,
            "content": "在设计优化算法时，我们有一些基本的准则或技巧。对于复杂的优化问题，基本的想法是将其转化为一系列简单的优化问题（其最优解容易计算或者有显式表达式）来逐步求解。常用的技巧有："
        },
        {
            "type": "text",
            "bbox": [
                0.208,
                0.448,
                0.714,
                0.549
            ],
            "angle": 0,
            "content": "(1) 泰勒（Taylor）展开。对于一个非线性的目标或者约束函数，我们通过其泰勒展开用简单的线性函数或者二次函数来逼近，从而得到一个简化的问题。因为该简化问题只在小邻域内逼近原始问题，所以我们需要根据迭代点的更新来重新构造相应的简化问题。"
        },
        {
            "type": "text",
            "bbox": [
                0.208,
                0.56,
                0.714,
                0.64
            ],
            "angle": 0,
            "content": "(2) 对偶。每个优化问题都有对应的对偶问题。特别是凸的情形，当原始问题比较难解的时候，其对偶问题可能很容易求解。通过求解对偶问题或者同时求解原始问题和对偶问题，我们可以简化原始问题的求解，从而设计更有效的算法。"
        },
        {
            "type": "text",
            "bbox": [
                0.208,
                0.651,
                0.714,
                0.692
            ],
            "angle": 0,
            "content": "(3) 拆分. 对于一个复杂的优化问题, 我们可以将变量进行拆分, 比如 \\(\\min_{x} h(x) + r(x)\\), 可以拆分成"
        },
        {
            "type": "list",
            "bbox": [
                0.208,
                0.448,
                0.714,
                0.692
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.35,
                0.699,
                0.598,
                0.723
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, y} \\quad h (x) + r (y), \\quad \\text {s . t .} \\quad x = y.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.236,
                0.73,
                0.714,
                0.789
            ],
            "angle": 0,
            "content": "通过引入更多的变量，我们可以得到每个变量的简单问题（较易求解或者解有显式表达式），从而通过交替求解等方式来得到原问题的解。"
        },
        {
            "type": "text",
            "bbox": [
                0.208,
                0.8,
                0.714,
                0.839
            ],
            "angle": 0,
            "content": "(4) 块坐标下降. 对于一个 \\(n\\) 维空间 \\((n\\) 很大）的优化问题，我们可以通过逐步求解分量的方式将其转化为多个低维空间中的优化"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "20"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第一章 最优化简介"
        },
        {
            "type": "text",
            "bbox": [
                0.324,
                0.167,
                0.801,
                0.226
            ],
            "angle": 0,
            "content": "问题．比如，对于 \\(n = 100\\) ，我们可以先固定第2—100个分量，来求解 \\(x_{1}\\) ；接着固定下标为1,3—100的分量来求解 \\(x_{2}\\) ；依次类推."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.248,
                0.8,
                0.265
            ],
            "angle": 0,
            "content": "关于这些技巧的具体应用，读者可以进一步阅读本书中的算法部分。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.269,
                0.826,
                0.349
            ],
            "angle": 0,
            "content": "对于同一个优化问题，其求解算法可以有很多。在设计和比较不同的算法时，另一个重要的指标是算法的渐进收敛速度。我们以点列的 Q-收敛速度（Q 的含义为“quotient”）为例（函数值的 Q-收敛速度可以类似地定义）。设 \\(\\{x^k\\}\\) 为算法产生的迭代点列且收敛于 \\(x^*\\)，若对充分大的 \\(k\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.433,
                0.355,
                0.653,
                0.391
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\left\\| x ^ {k + 1} - x ^ {*} \\right\\|}{\\left\\| x ^ {k} - x ^ {*} \\right\\|} \\leqslant a, \\quad a \\in (0, 1),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.399,
                0.597,
                0.416
            ],
            "angle": 0,
            "content": "则称算法（点列）是Q-线性收敛的；若满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.46,
                0.422,
                0.622,
                0.459
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\frac {\\left|\\left| x ^ {k + 1} - x ^ {*} \\right|\\right|}{\\left|\\left| x ^ {k} - x ^ {*} \\right|\\right|} = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.466,
                0.597,
                0.483
            ],
            "angle": 0,
            "content": "称算法（点列）是Q-超线性收敛的；若满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.46,
                0.489,
                0.622,
                0.526
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\frac {\\left|\\left| x ^ {k + 1} - x ^ {*} \\right|\\right|}{\\left|\\left| x ^ {k} - x ^ {*} \\right|\\right|} = 1,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.534,
                0.7,
                0.551
            ],
            "angle": 0,
            "content": "称算法（点列）是Q-次线性收敛的．若对充分大的 \\(k\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.446,
                0.557,
                0.638,
                0.593
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\left\\| x ^ {k + 1} - x ^ {*} \\right\\|}{\\left\\| x ^ {k} - x ^ {*} \\right\\| ^ {2}} \\leqslant a, \\quad a > 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.601,
                0.827,
                0.703
            ],
            "angle": 0,
            "content": "则称算法（点列）是 Q-二次收敛的。类似地，也可定义更一般的 Q-r 次收敛 \\((r > 1)\\)。我们举例来更直观地展示不同的 Q-收敛速度，参见图 1.9（图中对所考虑的点列作了适当的变换）。点列 \\(\\{2^{-k}\\}\\) 是 Q-线性收敛的，点列 \\(\\{2^{-2^k}\\}\\) 是 Q-二次收敛的（也是 Q-超线性收敛的），点列 \\(\\{\\frac{1}{k}\\}\\) 是 Q-次线性收敛的。一般来说，具有 Q-超线性收敛速度和 Q-二次收敛速度的算法是收敛较快的。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.727,
                0.826,
                0.785
            ],
            "angle": 0,
            "content": "除Q-收敛速度外，另一常用概念是R-收敛速度（R的含义为“root”）.以点列为例，设 \\(\\{x^k\\}\\) 为算法产生的迭代点且收敛于 \\(x^{*}\\) ，若存在Q-线性收敛于0的非负序列 \\(t_k\\) 并且"
        },
        {
            "type": "equation",
            "bbox": [
                0.487,
                0.788,
                0.597,
                0.807
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| x ^ {k} - x ^ {*} \\right\\| \\leqslant t _ {k}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "对任意的 \\(k\\) 成立，则称算法（点列）是R-线性收敛的．类似地，可定义R-超线性收敛和R-二次收敛等收敛速度．从R-收敛速度的定义可以看出序列"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "1.6 总结"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "21"
        },
        {
            "type": "image",
            "bbox": [
                0.175,
                0.159,
                0.357,
                0.266
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.222,
                0.275,
                0.308,
                0.288
            ],
            "angle": 0,
            "content": "(a) Q-线性收敛"
        },
        {
            "type": "image",
            "bbox": [
                0.367,
                0.159,
                0.549,
                0.266
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.413,
                0.275,
                0.501,
                0.288
            ],
            "angle": 0,
            "content": "(b) Q-二次收敛"
        },
        {
            "type": "image",
            "bbox": [
                0.556,
                0.159,
                0.736,
                0.266
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.595,
                0.275,
                0.694,
                0.289
            ],
            "angle": 0,
            "content": "(c) Q-次线性收敛"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.34,
                0.308,
                0.569,
                0.325
            ],
            "angle": 0,
            "content": "图1.9 不同Q-收敛速度比较"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.353,
                0.737,
                0.391
            ],
            "angle": 0,
            "content": "\\(\\{\\| x^{k} - x^{*}\\|\\}\\) 被另一趋于0的序列 \\(\\{t_k\\}\\) 控制．当知道 \\(t_k\\) 的形式时，我们也称算法（点列）的收敛速度为 \\(\\mathcal{O}(t_k)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.396,
                0.739,
                0.496
            ],
            "angle": 0,
            "content": "与收敛速度密切相关的概念是优化算法的复杂度 \\(N(\\varepsilon)\\)，即计算出给定精度 \\(\\varepsilon\\) 的解所需的迭代次数或浮点运算次数。在实际应用中，这两种定义复杂度的方式均很常见。如果能较准确地估计每次迭代的运算量，则可以由算法所需迭代次数推出所需浮点运算次数。我们用具体的例子来进一步解释算法复杂度。设某一算法产生的迭代序列 \\(\\{x^k\\}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.508,
                0.575,
                0.541
            ],
            "angle": 0,
            "content": "\\[\nf (x ^ {k}) - f (x ^ {*}) \\leqslant \\frac {c}{\\sqrt {k}}, \\quad \\forall k > 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.555,
                0.741,
                0.678
            ],
            "angle": 0,
            "content": "其中 \\(c > 0\\) 为常数，\\(x^{*}\\) 为全局极小点．如果需要计算算法满足精度 \\(f(x^{k}) - f(x^{*}) \\leqslant \\varepsilon\\) 所需的迭代次数，只需令 \\(\\frac{c}{\\sqrt{k}} \\leqslant \\varepsilon\\) 则得到 \\(k \\geqslant \\frac{c^2}{\\varepsilon^2}\\)，因此该优化算法对应的（迭代次数）复杂度为 \\(N(\\varepsilon) = \\mathcal{O}\\left(\\frac{1}{\\varepsilon^2}\\right)\\)．注意，渐进收敛速度更多的是考虑迭代次数充分大的情形，而复杂度给出了算法迭代有限步之后产生的解与最优解之间的定量关系，因此近年来受到人们广泛关注."
        },
        {
            "type": "title",
            "bbox": [
                0.399,
                0.713,
                0.51,
                0.734
            ],
            "angle": 0,
            "content": "1.6 总结"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.753,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "本章简要介绍了优化问题的应用背景、一般形式以及一些基本概念。对于优化问题的更多分类、优化领域关心的热点问题，我们会在第三、四章中进一步介绍。对于优化算法的收敛准则、收敛性以及收敛速度，我们会在介绍算法的时候再具体展开。本书也会围绕上面介绍的优化算法的四个设计技巧，针对不同类别的问题，来具体地展示相应的算法构造以及有效性分析。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "22"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第一章 最优化简介"
        },
        {
            "type": "title",
            "bbox": [
                0.507,
                0.155,
                0.578,
                0.176
            ],
            "angle": 0,
            "content": "习题1"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.194,
                0.825,
                0.232
            ],
            "angle": 0,
            "content": "1.1 考虑稀疏优化问题，我们已经直观地讨论了在 \\(\\ell_0\\) ， \\(\\ell_{1}\\) ， \\(\\ell_{2}\\) 三种范数下问题的解的可能形式．针对一般的 \\(\\ell_p\\) “范数”："
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.244,
                0.692,
                0.281
            ],
            "angle": 0,
            "content": "\\[\n\\| x \\| _ {p} \\stackrel {\\mathrm {d e f}} {=} \\left(\\sum_ {i = 1} ^ {n} | x | ^ {p}\\right) ^ {1 / p}, \\quad 0 <   p <   2,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.292,
                0.447,
                0.308
            ],
            "angle": 0,
            "content": "我们考虑优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.506,
                0.31,
                0.616,
                0.345
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\quad \\| x \\| _ {p}, \\\\ \\begin{array}{l l} \\text {s . t .} & A x = b. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.356,
                0.825,
                0.393
            ],
            "angle": 0,
            "content": "试着用几何直观的方式（类似于图1.2）来说明当 \\( p \\in (0, 2) \\) 取何值时，该优化问题的解可能具有稀疏性。"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.407,
                0.825,
                0.507
            ],
            "angle": 0,
            "content": "1.2 给定一个函数 \\( f(x): \\mathbb{R}^n \\to \\mathbb{R} \\) 及其一个局部最优点 \\( x^* \\)，则该点沿任何方向 \\( d \\in \\mathbb{R}^n \\) 也是局部最优的，即 0 为函数 \\( \\phi(\\alpha) \\stackrel{\\mathrm{def}}{=} f(x^* + \\alpha d) \\) 的一个局部最优解。反之，如果 \\( x^* \\) 沿任何方向 \\( d \\in \\mathbb{R}^n \\) 都是局部最优解，则 \\( x^* \\) 是否为 \\( f(x) \\) 的一个局部最优解？若是，请给出证明；若不是，请给出反例。"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.521,
                0.54,
                0.538
            ],
            "angle": 0,
            "content": "1.3 试给出如下点列的 Q-收敛速度："
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.544,
                0.49,
                0.576
            ],
            "angle": 0,
            "content": "(a) \\(x^{k} = \\frac{1}{k!}, k = 1,2,\\dots\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.307,
                0.579,
                0.331,
                0.594
            ],
            "angle": 0,
            "content": "(b)"
        },
        {
            "type": "equation",
            "bbox": [
                0.426,
                0.592,
                0.734,
                0.661
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k} = \\left\\{ \\begin{array}{l l} {\\left(\\frac {1}{4}\\right) ^ {2 ^ {k}},} & {k \\text {为 偶 数},} \\\\ {\\frac {x ^ {k - 1}}{k},} & {k \\text {为 奇 数}.} \\end{array} \\right., \\quad k = 1, 2, \\dots\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.669,
                0.825,
                0.717
            ],
            "angle": 0,
            "content": "1.4 考虑函数 \\( f(x) = x_1^2 + x_2^2 \\)，\\( x = (x_1, x_2) \\in \\mathbb{R}^2 \\)，以及迭代点列 \\( x^k = (1 + \\frac{1}{2^k})(\\cos k, \\sin k)^{\\mathrm{T}}, k = 1, 2, \\dots \\)，请说明"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.725,
                0.707,
                0.744
            ],
            "angle": 0,
            "content": "(a) \\(\\{f(x^{k + 1})\\}\\) 是否收敛？若收敛，给出Q-收敛速度；"
        },
        {
            "type": "text",
            "bbox": [
                0.304,
                0.751,
                0.679,
                0.769
            ],
            "angle": 0,
            "content": "(b) \\(\\{x^{k + 1}\\}\\) 是否收敛？若收敛，给出Q-收敛速度"
        },
        {
            "type": "list",
            "bbox": [
                0.304,
                0.725,
                0.707,
                0.769
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "title",
            "bbox": [
                0.308,
                0.252,
                0.6,
                0.283
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.335,
                0.744,
                0.456
            ],
            "angle": 0,
            "content": "在介绍具体的最优化模型、理论和算法之前，我们先介绍一些必备的基础知识。本章中从范数和导数讲起，接着介绍广义实值函数、凸集、凸函数、共轭函数和次梯度等凸分析方面的重要概念和相关结论。这一章的部分内容可能在较后的章节中才会用到，读者阅读时可按需选择，如共轭函数、次梯度可在学习相关优化算法时再阅读。一些更加基础的内容，例如线性代数、数值代数和概率方面的知识可参考附录B。"
        },
        {
            "type": "title",
            "bbox": [
                0.399,
                0.485,
                0.51,
                0.506
            ],
            "angle": 0,
            "content": "2.1 范数"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.523,
                0.741,
                0.562
            ],
            "angle": 0,
            "content": "和标量不同，我们不能简单地按照元素大小来比较不同的向量和矩阵。向量范数和矩阵范数给出了一种长度计量方式。我们首先介绍向量范数。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.586,
                0.315,
                0.604
            ],
            "angle": 0,
            "content": "2.1.1 向量范数"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.617,
                0.738,
                0.654
            ],
            "angle": 0,
            "content": "定义2.1(范数）称一个从向量空间 \\(\\mathbb{R}^n\\) 到实数域 \\(\\mathbb{R}\\) 的非负函数 \\(\\| \\cdot \\|\\) 为范数，如果它满足："
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.664,
                0.737,
                0.68
            ],
            "angle": 0,
            "content": "(1) 正定性：对于所有的 \\(v \\in \\mathbb{R}^n\\)，有 \\(\\| v \\| \\geqslant 0\\)，且 \\(\\| v \\| = 0\\) 当且仅当 \\(v = 0\\)；"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.692,
                0.648,
                0.709
            ],
            "angle": 0,
            "content": "(2) 齐次性：对于所有的 \\(v \\in \\mathbb{R}^n\\) 和 \\(\\alpha \\in \\mathbb{R}\\)，有 \\(\\| \\alpha v \\| = |\\alpha| \\| v \\|\\)；"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.721,
                0.695,
                0.737
            ],
            "angle": 0,
            "content": "(3) 三角不等式：对于所有的 \\(v\\)，\\(w \\in \\mathbb{R}^n\\)，有 \\(\\| v + w \\| \\leqslant \\| v \\| + \\| w \\|\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.18,
                0.664,
                0.737,
                0.737
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.748,
                0.49,
                0.765
            ],
            "angle": 0,
            "content": "最常用的向量范数为 \\(\\ell_p\\) 范数 \\((p\\geqslant 1)\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.316,
                0.773,
                0.594,
                0.795
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| v \\right\\| _ {p} = \\left(\\left| v _ {1} \\right| ^ {p} + \\left| v _ {2} \\right| ^ {p} + \\dots + \\left| v _ {n} \\right| ^ {p}\\right) ^ {\\frac {1}{p}};\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.805,
                0.394,
                0.822
            ],
            "angle": 0,
            "content": "当 \\(p = \\infty\\) 时，\\(\\ell_{\\infty}\\) 范数定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.832,
                0.518,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| v \\right\\| _ {\\infty} = \\max  _ {i} \\left| v _ {i} \\right|.\n\\]"
        },
        {
            "type": "page_number",
            "bbox": [
                0.444,
                0.87,
                0.464,
                0.882
            ],
            "angle": 0,
            "content": "23"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "24"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.156,
                0.827,
                0.236
            ],
            "angle": 0,
            "content": "其中 \\(p = 1,2,\\infty\\) 的情形最重要，分别记为 \\(\\| \\cdot \\| _1,\\| \\cdot \\| _2\\) 和 \\(\\| \\cdot \\|_{\\infty}\\) ，在不引起歧义的情况下，我们有时省略 \\(\\ell_2\\) 范数的角标，记为 \\(\\| \\cdot \\|\\) ，在最优化问题算法构造和分析中，也常常遇到由正定矩阵 \\(A\\) 诱导的范数，即 \\(\\| x\\| _A\\stackrel {\\mathrm{def}}{=}\\sqrt{x^{\\mathrm{T}}Ax}.\\) 根据正定矩阵的定义，很容易验证 \\(\\| \\cdot \\| _A\\) 定义了一个范数."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.24,
                0.729,
                0.257
            ],
            "angle": 0,
            "content": "对向量的 \\(\\ell_2\\) 范数，我们有常用的柯西（Cauchy）不等式："
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.269,
                0.592,
                0.285
            ],
            "angle": 0,
            "content": "命题2.1（柯西不等式）设 \\(a, b \\in \\mathbb{R}^n\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.475,
                0.298,
                0.609,
                0.317
            ],
            "angle": 0,
            "content": "\\[\n\\left| a ^ {\\mathrm {T}} b \\right| \\leqslant \\left\\| a \\right\\| _ {2} \\| b \\| _ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.33,
                0.525,
                0.346
            ],
            "angle": 0,
            "content": "等号成立当且仅当 \\(a\\) 与 \\(b\\) 线性相关."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.372,
                0.401,
                0.39
            ],
            "angle": 0,
            "content": "2.1.2 矩阵范数"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.403,
                0.825,
                0.481
            ],
            "angle": 0,
            "content": "和向量范数类似，矩阵范数是定义在矩阵空间上的非负函数，并且满足正定性、齐次性和三角不等式．向量的 \\(\\ell_p\\) 范数可以比较容易地推广到矩阵的 \\(\\ell_p\\) 范数，本书常用 \\(p = 1,2\\) 的情形．当 \\(p = 1\\) 时，矩阵 \\(A\\in \\mathbb{R}^{m\\times n}\\) 的 \\(\\ell_1\\) 范数定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.473,
                0.481,
                0.612,
                0.518
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| A \\right\\| _ {1} = \\sum_ {i = 1} ^ {m} \\sum_ {j = 1} ^ {n} \\left| a _ {i j} \\right|,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.523,
                0.825,
                0.582
            ],
            "angle": 0,
            "content": "即 \\(\\| A\\| _1\\) 为 \\(A\\) 中所有元素绝对值的和．当 \\(p = 2\\) 时，此时得到的是矩阵的Frobenius范数（下称 \\(F\\) 范数)，记为 \\(\\| A\\| _F\\) .它可以看成是向量的 \\(\\ell_2\\) 范数的推广，即所有元素平方和开根号："
        },
        {
            "type": "equation",
            "bbox": [
                0.426,
                0.59,
                0.825,
                0.627
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| A \\right\\| _ {F} = \\sqrt {\\operatorname {T r} \\left(A A ^ {\\mathrm {T}}\\right)} = \\sqrt {\\sum_ {i , j} a _ {i j} ^ {2}}. \\tag {2.1.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.635,
                0.825,
                0.672
            ],
            "angle": 0,
            "content": "这里，\\(\\operatorname{Tr}(X)\\) 表示方阵 \\(X\\) 的迹。矩阵的 \\(F\\) 范数具有正交不变性，即对于任意的正交矩阵 \\(U \\in \\mathbb{R}^{m \\times m}, V \\in \\mathbb{R}^{n \\times n}\\)，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.681,
                0.717,
                0.726
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\| U A V \\| _ {F} ^ {2} = \\operatorname {T r} \\left(U A V V ^ {\\mathrm {T}} A ^ {\\mathrm {T}} U ^ {\\mathrm {T}}\\right) = \\operatorname {T r} \\left(U A A ^ {\\mathrm {T}} U ^ {\\mathrm {T}}\\right) \\\\ = \\operatorname {T r} \\left(A A ^ {\\mathrm {T}} U ^ {\\mathrm {T}} U\\right) = \\operatorname {T r} \\left(A A ^ {\\mathrm {T}}\\right) = \\| A \\| _ {F} ^ {2}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.736,
                0.61,
                0.753
            ],
            "angle": 0,
            "content": "其中第三个等号成立是因为 \\(\\operatorname{Tr}(AB) = \\operatorname{Tr}(BA)\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.757,
                0.825,
                0.816
            ],
            "angle": 0,
            "content": "除了从向量范数直接推广以外，矩阵范数还可以由向量范数诱导出来，一般称这种范数为算子范数。给定矩阵 \\(A \\in \\mathbb{R}^{m \\times n}\\)，以及 \\(m\\) 维和 \\(n\\) 维空间的向量范数 \\(\\| \\cdot \\|_{(m)}\\) 和 \\(\\| \\cdot \\|_{(n)}\\)，其诱导的矩阵范数定义如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.424,
                0.828,
                0.66,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| A \\right\\| _ {(m, n)} = \\max  _ {x \\in \\mathbb {R} ^ {n}, \\| x \\| _ {(n)} = 1} \\left\\| A x \\right\\| _ {(m)},\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "2.1 范数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "25"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.156,
                0.739,
                0.215
            ],
            "angle": 0,
            "content": "容易验证 \\(\\| \\cdot \\|_{(m,n)}\\) 满足范数的定义．如果将 \\(\\| \\cdot \\|_{(m)}\\) 和 \\(\\| \\cdot \\|_{(n)}\\) 都取为相应向量空间的 \\(\\ell_p\\) 范数，我们可以得到矩阵的 \\(p\\) 范数．本书经常用到的是矩阵的2范数，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.358,
                0.218,
                0.55,
                0.244
            ],
            "angle": 0,
            "content": "\\[\n\\| A \\| _ {2} = \\max  _ {x \\in \\mathbb {R} ^ {n}, \\| x \\| _ {2} = 1} \\| A x \\| _ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.249,
                0.739,
                0.288
            ],
            "angle": 0,
            "content": "容易验证（见习题2.2），矩阵的2范数是该矩阵的最大奇异值。根据算子范数的定义，所有算子范数都满足如下性质："
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.298,
                0.737,
                0.318
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| A x \\right\\| _ {(m)} \\leqslant \\left\\| A \\right\\| _ {(m, n)} \\| x \\| _ {(n)}. \\tag {2.1.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.328,
                0.739,
                0.388
            ],
            "angle": 0,
            "content": "例如当 \\(m = n = 2\\) 时，\\(\\| Ax \\|_2 \\leqslant \\| A \\|_2 \\| x \\|_2\\)。性质(2.1.2)又被称为矩阵范数的相容性，即 \\(\\| \\cdot \\|_{(m,n)}\\) 与 \\(\\| \\cdot \\|_{(m)}\\) 和 \\(\\| \\cdot \\|_{(n)}\\) 是相容的。并非所有矩阵范数都与给定的向量范数相容，在今后的应用中读者需要注意这一问题。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.398,
                0.74,
                0.498
            ],
            "angle": 0,
            "content": "注2.1 和矩阵2范数类似，向量的 \\(\\ell_1\\) 范数以及 \\(\\ell_{\\infty}\\) 范数均可诱导出相应的矩阵范数（分别为矩阵的1范数和无穷范数），在多数数值代数教材中将它们记为 \\(\\| \\cdot \\| _1\\) 和 \\(\\| \\cdot \\|_{\\infty}\\) 。然而本书较少涉及这两个范数，因此我们将 \\(\\| A\\| _1\\) 定义为矩阵 \\(A\\) 中所有元素绝对值的和。读者应当注意它和其他数值代数教材中定义的不同。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.507,
                0.739,
                0.545
            ],
            "angle": 0,
            "content": "除了矩阵2范数以外，另一个常用的矩阵范数为核范数．给定矩阵 \\(A\\in\\) \\(\\mathbb{R}^{m\\times n}\\) ，其核范数定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.545,
                0.507,
                0.58
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| A \\right\\| _ {*} = \\sum_ {i = 1} ^ {r} \\sigma_ {i},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.584,
                0.739,
                0.664
            ],
            "angle": 0,
            "content": "其中 \\(\\sigma_{i}, i = 1,2,\\dots ,r\\) 为 \\(A\\) 的所有非零奇异值，\\(r = \\mathrm{rank}(A)\\)。类似于向量的 \\(\\ell_1\\) 范数的保稀疏性，我们也经常通过限制矩阵的核范数来保证矩阵的低秩性。同时，根据范数的三角不等式（下文中的凸性），相应的优化问题可以有效求解。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.688,
                0.315,
                0.706
            ],
            "angle": 0,
            "content": "2.1.3 矩阵内积"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.719,
                0.741,
                0.818
            ],
            "angle": 0,
            "content": "对于矩阵空间 \\(\\mathbb{R}^{m\\times n}\\) 的两个矩阵 \\(A\\) 和 \\(B\\) ，除了定义它们各自的范数以外，我们还可以定义它们之间的内积．范数一般用来衡量矩阵的模的大小，而内积一般用来表征两个矩阵（或其张成的空间）之间的夹角．这里，我们介绍一种常用的内积——Frobenius 内积．\\(m\\times n\\) 矩阵 \\(A\\) 和 \\(B\\) 的 Frobenius 内积定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.818,
                0.579,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\langle A, B \\rangle \\stackrel {\\text {d e f}} {=} \\operatorname {T r} (A B ^ {\\mathrm {T}}) = \\sum_ {i = 1} ^ {m} \\sum_ {j = 1} ^ {n} a _ {i j} b _ {i j}.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "26"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.118,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.827,
                0.195
            ],
            "angle": 0,
            "content": "易知其为两个矩阵逐分量相乘的和，因而满足内积的定义．当 \\(A = B\\) 时，\\(\\langle A, B \\rangle\\) 等于矩阵 \\(A\\) 的 \\(F\\) 范数的平方."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.199,
                0.715,
                0.215
            ],
            "angle": 0,
            "content": "和向量范数相似，我们也有矩阵范数对应的柯西不等式："
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.227,
                0.704,
                0.244
            ],
            "angle": 0,
            "content": "命题2.2（矩阵范数的柯西不等式）设 \\(A, B \\in \\mathbb{R}^{m \\times n}\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.457,
                0.257,
                0.629,
                0.276
            ],
            "angle": 0,
            "content": "\\[\n\\left| \\langle A, B \\rangle \\right| \\leqslant \\| A \\| _ {F} \\| B \\| _ {F},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.289,
                0.533,
                0.305
            ],
            "angle": 0,
            "content": "等号成立当且仅当 \\(A\\) 和 \\(B\\) 线性相关."
        },
        {
            "type": "title",
            "bbox": [
                0.486,
                0.336,
                0.597,
                0.357
            ],
            "angle": 0,
            "content": "2.2 导数"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.374,
                0.825,
                0.474
            ],
            "angle": 0,
            "content": "为了分析可微最优化问题的性质，我们需要知道目标函数和约束函数的导数信息。在算法设计中，当优化问题没有显式解时，我们也往往通过函数值和导数信息来构造容易求解的子问题。利用目标函数和约束函数的导数信息，可以确保构造的子问题具有很好的逼近性质，从而构造各种各样有效的算法。本节将介绍有关导数的内容。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.5,
                0.46,
                0.518
            ],
            "angle": 0,
            "content": "2.2.1 梯度与海瑟矩阵"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.531,
                0.825,
                0.588
            ],
            "angle": 0,
            "content": "在数学分析课程中，我们已经学过多元函数微分学．这一小节，我们首先回顾梯度和海瑟（Hessian）矩阵的定义，之后介绍多元可微函数的一些重要性质."
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.601,
                0.825,
                0.639
            ],
            "angle": 0,
            "content": "定义2.2（梯度）给定函数 \\(f:\\mathbb{R}^n\\to \\mathbb{R}\\) ，且 \\(f\\) 在点 \\(x\\) 的一个邻域内有意义，若存在向量 \\(g\\in \\mathbb{R}^n\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.421,
                0.646,
                0.825,
                0.684
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {p \\rightarrow 0} \\frac {f (x + p) - f (x) - g ^ {\\mathrm {T}} p}{\\| p \\|} = 0, \\tag {2.2.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.691,
                0.825,
                0.75
            ],
            "angle": 0,
            "content": "其中 \\(\\|\\cdot\\|\\) 是任意的向量范数，就称 \\(f\\) 在点 \\(x\\) 处可微（或 Fréchet 可微）。此时 \\(g\\) 称为 \\(f\\) 在点 \\(x\\) 处的梯度，记作 \\(\\nabla f(x)\\)。如果对区域 \\(D\\) 上的每一个点 \\(x\\) 都有 \\(\\nabla f(x)\\) 存在，则称 \\(f\\) 在 \\(D\\) 上可微。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.761,
                0.825,
                0.811
            ],
            "angle": 0,
            "content": "若 \\(f\\) 在点 \\(x\\) 处的梯度存在，在(2.2.1)式中令 \\(p = \\varepsilon e_i\\) ， \\(e_i\\) 是第 \\(i\\) 个分量为1的单位向量，可知 \\(\\nabla f(x)\\) 的第 \\(i\\) 个分量为 \\(\\frac{\\partial f(x)}{\\partial x_i}\\) 因此，"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.818,
                0.688,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x) = \\left[ \\frac {\\partial f (x)}{\\partial x _ {1}}, \\frac {\\partial f (x)}{\\partial x _ {2}}, \\dots , \\frac {\\partial f (x)}{\\partial x _ {n}} \\right] ^ {\\mathrm {T}}.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "2.2 导数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "27"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.744,
                0.195
            ],
            "angle": 0,
            "content": "如果只关心对一部分变量的梯度，可以通过对 \\(\\nabla\\) 加下标来表示．例如，\\(\\nabla_{x}f(x,y)\\) 表示将 \\(y\\) 视为常数时 \\(f\\) 关于 \\(x\\) 的梯度."
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.2,
                0.731,
                0.217
            ],
            "angle": 0,
            "content": "对应于一元函数的二阶导数，对于多元函数我们可以定义其海瑟矩阵"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.235,
                0.741,
                0.288
            ],
            "angle": 0,
            "content": "定义2.3(海瑟矩阵）如果函数 \\(f(x):\\mathbb{R}^n\\to \\mathbb{R}\\) 在点 \\(x\\) 处的二阶偏导数\\(\\frac{\\partial^2f(x)}{\\partial x_i\\partial x_j} i,j = 1,2,\\dots ,n\\) 都存在，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.25,
                0.299,
                0.655,
                0.429
            ],
            "angle": 0,
            "content": "\\[\n\\nabla^ {2} f (x) = \\left[ \\begin{array}{c c c c c} \\frac {\\partial^ {2} f (x)}{\\partial x _ {1} ^ {2}} & \\frac {\\partial^ {2} f (x)}{\\partial x _ {1} \\partial x _ {2}} & \\frac {\\partial^ {2} f (x)}{\\partial x _ {1} \\partial x _ {3}} & \\dots & \\frac {\\partial^ {2} f (x)}{\\partial x _ {1} \\partial x _ {n}} \\\\ \\frac {\\partial^ {2} f (x)}{\\partial x _ {2} \\partial x _ {1}} & \\frac {\\partial^ {2} f (x)}{\\partial x _ {2} ^ {2}} & \\frac {\\partial^ {2} f (x)}{\\partial x _ {2} \\partial x _ {3}} & \\dots & \\frac {\\partial^ {2} f (x)}{\\partial x _ {2} \\partial x _ {n}} \\\\ \\vdots & \\vdots & \\vdots & & \\vdots \\\\ \\frac {\\partial^ {2} f (x)}{\\partial x _ {n} \\partial x _ {1}} & \\frac {\\partial^ {2} f (x)}{\\partial x _ {n} \\partial x _ {2}} & \\frac {\\partial^ {2} f (x)}{\\partial x _ {n} \\partial x _ {3}} & \\dots & \\frac {\\partial^ {2} f (x)}{\\partial x _ {n} ^ {2}} \\end{array} \\right]\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.44,
                0.39,
                0.457
            ],
            "angle": 0,
            "content": "称为 \\(f\\) 在点 \\(x\\) 处的海瑟矩阵."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.474,
                0.741,
                0.533
            ],
            "angle": 0,
            "content": "当 \\(\\nabla^2 f(x)\\) 在区域 \\(D\\) 上的每个点 \\(x\\) 处都存在时，称 \\(f\\) 在 \\(D\\) 上二阶可微。若 \\(\\nabla^2 f(x)\\) 在 \\(D\\) 上还连续，则称 \\(f\\) 在 \\(D\\) 上二阶连续可微，可以证明此时海瑟矩阵是一个对称矩阵。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.538,
                0.741,
                0.577
            ],
            "angle": 0,
            "content": "当 \\(f:\\mathbb{R}^n\\to \\mathbb{R}^m\\) 是向量值函数时，我们可以定义它的雅可比（Jacobi）矩阵 \\(J(x)\\in \\mathbb{R}^{m\\times n}\\) ，它的第 \\(i\\) 行是分量 \\(f_{i}(x)\\) 梯度的转置，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.292,
                0.588,
                0.615,
                0.713
            ],
            "angle": 0,
            "content": "\\[\nJ (x) = \\left[ \\begin{array}{c c c c} \\frac {\\partial f _ {1} (x)}{\\partial x _ {1}} & \\frac {\\partial f _ {1} (x)}{\\partial x _ {2}} & \\dots & \\frac {\\partial f _ {1} (x)}{\\partial x _ {n}} \\\\ \\frac {\\partial f _ {2} (x)}{\\partial x _ {1}} & \\frac {\\partial f _ {2} (x)}{\\partial x _ {2}} & \\dots & \\frac {\\partial f _ {2} (x)}{\\partial x _ {n}} \\\\ \\vdots & \\vdots & & \\vdots \\\\ \\frac {\\partial f _ {m} (x)}{\\partial x _ {1}} & \\frac {\\partial f _ {m} (x)}{\\partial x _ {2}} & \\dots & \\frac {\\partial f _ {m} (x)}{\\partial x _ {n}} \\end{array} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.722,
                0.66,
                0.741
            ],
            "angle": 0,
            "content": "此外容易看出，梯度 \\(\\nabla f(x)\\) 的雅可比矩阵就是 \\(f(x)\\) 的海瑟矩阵"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.745,
                0.738,
                0.782
            ],
            "angle": 0,
            "content": "类似于一元函数的泰勒展开，对于多元函数，我们不加证明地给出如下形式的泰勒展开："
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.8,
                0.67,
                0.818
            ],
            "angle": 0,
            "content": "定理2.1 设 \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) 是连续可微的，\\(p \\in \\mathbb{R}^n\\) 为向量，那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.835,
                0.584,
                0.855
            ],
            "angle": 0,
            "content": "\\[\nf (\\boldsymbol {x} + \\boldsymbol {p}) = f (\\boldsymbol {x}) + \\nabla f (\\boldsymbol {x} + t \\boldsymbol {p}) ^ {\\mathrm {T}} \\boldsymbol {p},\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "28"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.696,
                0.174
            ],
            "angle": 0,
            "content": "其中 \\(0 < t < 1\\) ，进一步地，如果 \\(f\\) 是二阶连续可微的，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.186,
                0.674,
                0.22
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x + p) = \\nabla f (x) + \\int_ {0} ^ {1} \\nabla^ {2} f (x + t p) p d t,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.363,
                0.222,
                0.737,
                0.254
            ],
            "angle": 0,
            "content": "\\[\nf (\\boldsymbol {x} + \\boldsymbol {p}) = f (\\boldsymbol {x}) + \\nabla f (\\boldsymbol {x}) ^ {\\mathrm {T}} \\boldsymbol {p} + \\frac {1}{2} p ^ {\\mathrm {T}} \\nabla^ {2} f (\\boldsymbol {x} + t \\boldsymbol {p}) \\boldsymbol {p},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.264,
                0.374,
                0.28
            ],
            "angle": 0,
            "content": "其中 \\(0 < t < 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.298,
                0.825,
                0.356
            ],
            "angle": 0,
            "content": "在这一小节的最后，我们介绍一类特殊的可微函数——梯度利普希茨(Lipschitz)连续的函数.该类函数在很多优化算法收敛性证明中起着关键作用."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.373,
                0.825,
                0.411
            ],
            "angle": 0,
            "content": "定义2.4（梯度利普希茨连续）给定可微函数 \\(f\\) ，若存在 \\(L > 0\\) ，对任意的 \\(x,y\\in \\mathbf{dom}f\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.423,
                0.428,
                0.826,
                0.448
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| \\nabla f (x) - \\nabla f (y) \\right\\| \\leqslant L \\| x - y \\|, \\tag {2.2.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.464,
                0.825,
                0.501
            ],
            "angle": 0,
            "content": "则称 \\(f\\) 是梯度利普希茨连续的, 相应利普希茨常数为 \\(L\\). 有时也简记为梯度 \\(L\\)-利普希茨连续或 \\(L\\)-光滑."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.518,
                0.826,
                0.556
            ],
            "angle": 0,
            "content": "梯度利普希茨连续表明 \\(\\nabla f(x)\\) 的变化可以被自变量 \\(x\\) 的变化所控制，满足该性质的函数具有很多好的性质，一个重要的性质是其具有二次上界。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.572,
                0.825,
                0.61
            ],
            "angle": 0,
            "content": "引理2.1（二次上界）设可微函数 \\(f(x)\\) 的定义域 \\(\\mathbf{dom}f = \\mathbb{R}^n\\) ，且为梯度 \\(L\\) -利普希茨连续的，则函数 \\(f(x)\\) 有二次上界："
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.622,
                0.826,
                0.655
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\leqslant f (x) + \\nabla f (x) ^ {\\mathrm {T}} (y - x) + \\frac {L}{2} \\| y - x \\| ^ {2}, \\quad \\forall x, y \\in \\mathbf {d o m} f. \\tag {2.2.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.666,
                0.589,
                0.684
            ],
            "angle": 0,
            "content": "证明．对任意的 \\(x,y\\in \\mathbb{R}^n\\) ，构造辅助函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.41,
                0.701,
                0.826,
                0.72
            ],
            "angle": 0,
            "content": "\\[\ng (t) = f (x + t (y - x)), \\quad t \\in [ 0, 1 ]. \\tag {2.2.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.736,
                0.549,
                0.755
            ],
            "angle": 0,
            "content": "显然 \\(g(0) = f(x)\\)，\\(g(1) = f(y)\\)，以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.411,
                0.77,
                0.672,
                0.79
            ],
            "angle": 0,
            "content": "\\[\ng ^ {\\prime} (t) = \\nabla f (x + t (y - x)) ^ {T} (y - x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.807,
                0.317,
                0.823
            ],
            "angle": 0,
            "content": "由等式"
        },
        {
            "type": "equation",
            "bbox": [
                0.446,
                0.824,
                0.638,
                0.857
            ],
            "angle": 0,
            "content": "\\[\ng (1) - g (0) = \\int_ {0} ^ {1} g ^ {\\prime} (t) d t\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "2.2 导数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "29"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.157,
                0.212,
                0.173
            ],
            "angle": 0,
            "content": "可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.174,
                0.623,
                0.334
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (y) - f (x) - \\nabla f (x) ^ {\\mathrm {T}} (y - x) \\\\ = \\int_ {0} ^ {1} \\left(g ^ {\\prime} (t) - g ^ {\\prime} (0)\\right) d t \\\\ = \\int_ {0} ^ {1} (\\nabla f (x + t (y - x)) - \\nabla f (x)) ^ {\\mathrm {T}} (y - x) \\mathrm {d} t \\\\ \\leqslant \\int_ {0} ^ {1} \\| \\nabla f (x + t (y - x)) - \\nabla f (x) \\| \\| y - x \\| d t \\\\ \\leqslant \\int_ {0} ^ {1} L \\| y - x \\| ^ {2} t \\mathrm {d} t = \\frac {L}{2} \\| y - x \\| ^ {2}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.338,
                0.739,
                0.376
            ],
            "angle": 0,
            "content": "其中最后一行的不等式利用了梯度利普希茨连续的条件 (2.2.2). 整理可得 (2.2.3) 式成立. □"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.389,
                0.739,
                0.468
            ],
            "angle": 0,
            "content": "引理2.1实际上指的是 \\(f(x)\\) 可被一个二次函数上界所控制，即要求\\(f(x)\\) 的增长速度不超过二次．实际上，该引理对 \\(f(x)\\) 定义域的要求可减弱为 \\(\\mathbf{dom}f\\) 是凸集（见定义2.13），此条件的作用是保证证明中的 \\(g(t)\\) 当\\(t\\in [0,1]\\) 时是有定义的."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.472,
                0.741,
                0.531
            ],
            "angle": 0,
            "content": "若 \\(f\\) 是梯度利普希茨连续的，且有一个全局极小点 \\(x^{*}\\)，一个重要的推论就是我们能够利用二次上界(2.2.3)来估计 \\(f(x) - f(x^{*})\\) 的大小，其中 \\(x\\) 可以是定义域中的任意一点。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.542,
                0.741,
                0.581
            ],
            "angle": 0,
            "content": "推论2.1设可微函数 \\(f(x)\\) 的定义域为 \\(\\mathbb{R}^n\\) 且存在一个全局极小点 \\(x^{*}\\) 若 \\(f(x)\\) 为梯度 \\(L\\) -利普希茨连续的，则对任意的 \\(x\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.341,
                0.588,
                0.737,
                0.62
            ],
            "angle": 0,
            "content": "\\[\n\\frac {1}{2 L} \\| \\nabla f (x) \\| ^ {2} \\leqslant f (x) - f \\left(x ^ {*}\\right). \\tag {2.2.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.627,
                0.587,
                0.644
            ],
            "angle": 0,
            "content": "证明．由于 \\(x^{*}\\) 是全局极小点，应用二次上界(2.2.3)有"
        },
        {
            "type": "equation",
            "bbox": [
                0.255,
                0.651,
                0.652,
                0.682
            ],
            "angle": 0,
            "content": "\\[\nf (x ^ {*}) \\leqslant f (y) \\leqslant f (x) + \\nabla f (x) ^ {\\mathrm {T}} (y - x) + \\frac {L}{2} \\| y - x \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.689,
                0.738,
                0.726
            ],
            "angle": 0,
            "content": "在这里固定 \\(x\\)，注意到上式对于任意的 \\(y\\) 均成立，因此可对上式不等号右边取下确界："
        },
        {
            "type": "equation",
            "bbox": [
                0.258,
                0.733,
                0.648,
                0.8
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (x ^ {*}) \\leqslant \\inf  _ {y \\in \\mathbb {R} ^ {n}} \\left\\{f (x) + \\nabla f (x) ^ {\\mathrm {T}} (y - x) + \\frac {L}{2} \\| y - x \\| ^ {2} \\right\\} \\\\ = f (x) - \\frac {1}{2 L} \\| \\nabla f (x) \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.807,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "推论2.1证明的最后一步应用了二次函数的性质：当 \\(y = x - \\frac{\\nabla f(x)}{L}\\) 时取到最小值．有关二次函数最优性条件将在第五章中进一步讨论."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "30"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.499,
                0.174
            ],
            "angle": 0,
            "content": "2.2.2 矩阵变量函数的导数"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.187,
                0.825,
                0.225
            ],
            "angle": 0,
            "content": "多元函数梯度的定义可以推广到变量是矩阵的情形．对于以 \\(m\\times n\\) 矩阵 \\(X\\) 为自变量的函数 \\(f(X)\\) ，若存在矩阵 \\(G\\in \\mathbb{R}^{m\\times n}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.404,
                0.235,
                0.679,
                0.272
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {V \\rightarrow 0} \\frac {f (X + V) - f (X) - \\langle G , V \\rangle}{\\| V \\|} = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.282,
                0.825,
                0.34
            ],
            "angle": 0,
            "content": "其中 \\(\\|\\cdot\\|\\) 是任意矩阵范数，就称矩阵变量函数 \\(f\\) 在 \\(X\\) 处 Fréchet 可微，称 \\(G\\) 为 \\(f\\) 在 Fréchet 可微意义下的梯度。类似于向量情形，矩阵变量函数 \\(f(X)\\) 的梯度可以用其偏导数表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.349,
                0.685,
                0.469
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x) = \\left[ \\begin{array}{c c c c} \\frac {\\partial f}{\\partial x _ {1 1}} & \\frac {\\partial f}{\\partial x _ {1 2}} & \\dots & \\frac {\\partial f}{\\partial x _ {1 n}} \\\\ \\frac {\\partial f}{\\partial x _ {2 1}} & \\frac {\\partial f}{\\partial x _ {2 2}} & \\dots & \\frac {\\partial f}{\\partial x _ {2 n}} \\\\ \\vdots & \\vdots & & \\vdots \\\\ \\frac {\\partial f}{\\partial x _ {m 1}} & \\frac {\\partial f}{\\partial x _ {m 2}} & \\dots & \\frac {\\partial f}{\\partial x _ {m n}} \\end{array} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.478,
                0.522,
                0.512
            ],
            "angle": 0,
            "content": "其中 \\(\\frac{\\partial f}{\\partial x_{ij}}\\) 表示 \\(f\\) 关于 \\(x_{ij}\\) 的偏导数."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.513,
                0.825,
                0.551
            ],
            "angle": 0,
            "content": "在实际应用中，矩阵Fréchet可微的定义和使用往往比较繁琐，为此我们需要介绍另一种定义——Gâteaux可微。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.564,
                0.825,
                0.601
            ],
            "angle": 0,
            "content": "定义2.5（Gateaux可微）设 \\(f(X)\\) 为矩阵变量函数，如果存在矩阵 \\(G \\in \\mathbb{R}^{m \\times n}\\)，对任意方向 \\(V \\in \\mathbb{R}^{m \\times n}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.611,
                0.825,
                0.645
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {t \\rightarrow 0} \\frac {f (X + t V) - f (X) - t \\langle G , V \\rangle}{t} = 0, \\tag {2.2.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.655,
                0.825,
                0.692
            ],
            "angle": 0,
            "content": "则称 \\(f\\) 关于 \\(X\\) 是 Gâteaux 可微的．满足(2.2.6)式的 \\(G\\) 称为 \\(f\\) 在 \\(X\\) 处在 Gâteaux 可微意义下的梯度."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.706,
                0.827,
                0.857
            ],
            "angle": 0,
            "content": "和 Fréchet 可微的定义进行对比不难发现，Gateaux 可微实际上是方向导数的某种推广，它针对一元函数考虑极限，因此利用 Gateaux 可微计算梯度是更容易实现的。此外，从二者定义容易看出，若 \\(f\\) 是 Fréchet 可微的，则 \\(f\\) 也是 Gâteaux 可微的，且二者意义下的梯度相等。但这一命题反过来不一定成立。本书考虑的大多数可微函数都是 Fréchet 可微的，根据以上结论，我们无需具体区分 \\(f\\) 的导数究竟是在哪个意义下的。在不引起歧义的情况下，我们统一将矩阵变量函数 \\(f(X)\\) 的导数记为 \\(\\frac{\\partial f}{\\partial X}\\) 或 \\(\\nabla f(X)\\)。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "2.2 导数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.735,
                0.131
            ],
            "angle": 0,
            "content": "31"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.214
            ],
            "angle": 0,
            "content": "在实际中，由于Gateaux可微定义式更容易操作，因此通常是利用(2.2.6)式进行矩阵变量函数 \\(f(X)\\) 的求导运算．我们以下面的例子来具体说明."
        },
        {
            "type": "title",
            "bbox": [
                0.205,
                0.237,
                0.254,
                0.253
            ],
            "angle": 0,
            "content": "例2.1"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.273,
                0.741,
                0.313
            ],
            "angle": 0,
            "content": "(1) 考虑线性函数: \\( f(X) = \\operatorname{Tr}(AX^{\\mathrm{T}}B) \\), 其中 \\( A \\in \\mathbb{R}^{p \\times n}, B \\in \\mathbb{R}^{m \\times p}, X \\in \\mathbb{R}^{m \\times n} \\), 对任意方向 \\( V \\in \\mathbb{R}^{m \\times n} \\) 以及 \\( t \\in \\mathbb{R} \\), 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.243,
                0.326,
                0.705,
                0.384
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\lim  _ {t \\rightarrow 0} \\frac {f (X + t V) - f (X)}{t} = \\lim  _ {t \\rightarrow 0} \\frac {\\operatorname {T r} (A (X + t V) ^ {\\mathrm {T}} B) - \\operatorname {T r} (A X ^ {\\mathrm {T}} B)}{t} \\\\ = \\operatorname {T r} \\left(A V ^ {\\mathrm {T}} B\\right) = \\langle B A, V \\rangle . \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.401,
                0.373,
                0.419
            ],
            "angle": 0,
            "content": "因此， \\(\\nabla f(X) = BA\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.438,
                0.741,
                0.503
            ],
            "angle": 0,
            "content": "(2) 考虑二次函数: \\( f(X, Y) = \\frac{1}{2} \\| XY - A \\|_F^2 \\), 其中 \\((X, Y) \\in \\mathbb{R}^{m \\times p} \\times \\mathbb{R}^{p \\times n}\\), 对变量 \\(Y\\), 其中 \\(X \\in \\mathbb{R}^{m \\times p}\\), \\(Y \\in \\mathbb{R}^{p \\times n}\\), 取任意方向 \\(V\\) 以及充分小的 \\(t \\in \\mathbb{R}\\), 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.235,
                0.517,
                0.713,
                0.605
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (X, Y + t V) - f (X, Y) = \\frac {1}{2} \\| X (Y + t V) - A \\| _ {F} ^ {2} - \\frac {1}{2} \\| X Y - A \\| _ {F} ^ {2} \\\\ = \\langle t X V, X Y - A \\rangle + \\frac {1}{2} t ^ {2} \\| X V \\| _ {F} ^ {2} \\\\ = t \\left\\langle V, X ^ {T} (X Y - A) \\right\\rangle + \\mathcal {O} \\left(t ^ {2}\\right). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.622,
                0.453,
                0.652
            ],
            "angle": 0,
            "content": "由定义可知 \\(\\frac{\\partial f}{\\partial Y} = X^{\\mathrm{T}}(XY - A)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.652,
                0.591,
                0.667
            ],
            "angle": 0,
            "content": "对变量 \\(X\\) ，取任意方向 \\(V\\) 以及充分小的 \\(t\\in \\mathbb{R}\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.235,
                0.683,
                0.711,
                0.77
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (X + t V, Y) - f (X, Y) = \\frac {1}{2} \\| (X + t V) Y - A \\| _ {F} ^ {2} - \\frac {1}{2} \\| X Y - A \\| _ {F} ^ {2} \\\\ = \\langle t V Y, X Y - A \\rangle + \\frac {1}{2} t ^ {2} \\| V Y \\| _ {F} ^ {2} \\\\ = t \\left\\langle V, (X Y - A) Y ^ {\\mathrm {T}} \\right\\rangle + \\mathcal {O} \\left(t ^ {2}\\right). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.787,
                0.454,
                0.82
            ],
            "angle": 0,
            "content": "由定义可知 \\(\\frac{\\partial f}{\\partial X} = (XY - A)Y^{\\mathrm{T}}.\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.835,
                0.737,
                0.853
            ],
            "angle": 0,
            "content": "(3) 考虑 \\(\\ln\\)-det 函数: \\(f(X) = \\ln(\\operatorname{det}(X))\\), \\(X \\in S_{++}^{n}\\), 给定 \\(X \\succ 0\\), 对任意"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "32"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.157,
                0.547,
                0.174
            ],
            "angle": 0,
            "content": "方向 \\(V\\in \\mathcal{S}^n\\) 以及 \\(t\\in \\mathbb{R}\\) ，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.363,
                0.185,
                0.763,
                0.275
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (X + t V) - f (X) \\\\ = \\ln (\\det (X + t V)) - \\ln (\\det (X)) \\\\ = \\ln (\\det (X ^ {1 / 2} (I + t X ^ {- 1 / 2} V X ^ {- 1 / 2}) X ^ {1 / 2})) - \\ln (\\det (X)) \\\\ = \\ln (\\det (I + t X ^ {- 1 / 2} V X ^ {- 1 / 2})). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.285,
                0.824,
                0.324
            ],
            "angle": 0,
            "content": "由于 \\(X^{-1 / 2} V X^{-1 / 2}\\) 是对称矩阵，所以它可以正交对角化，不妨设它的特征值为 \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_n\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.441,
                0.334,
                0.684,
                0.481
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\ln (\\det (I + t X ^ {- 1 / 2} V X ^ {- 1 / 2})) \\\\ = \\ln \\prod_ {i = 1} ^ {n} (1 + t \\lambda_ {i}) \\\\ = \\sum_ {i = 1} ^ {n} \\ln (1 + t \\lambda_ {i}) = \\sum_ {i = 1} ^ {n} t \\lambda_ {i} + \\mathcal {O} (t ^ {2}) \\\\ = t \\operatorname {T r} \\left(X ^ {- 1 / 2} V X ^ {- 1 / 2}\\right) + \\mathcal {O} (t ^ {2}) \\\\ = t \\left\\langle \\left(X ^ {- 1}\\right) ^ {\\mathrm {T}}, V \\right\\rangle + \\mathcal {O} (t ^ {2}). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.493,
                0.824,
                0.546
            ],
            "angle": 0,
            "content": "上式中倒数第二个等号成立是因为 \\(\\operatorname{Tr}(A) = \\sum_{i=1}^{n} \\lambda_i(A)\\). 因此, 我们得到结论 \\(\\nabla f(X) = (X^{-1})^{\\mathrm{T}}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.556,
                0.825,
                0.656
            ],
            "angle": 0,
            "content": "在对函数求导的过程中，应当注意函数的自变量和相应的导数应该有相同的维数。例如自变量 \\(X \\in \\mathbb{R}^{m \\times n}\\)，那么其矩阵导数 \\(\\nabla f(X) \\in \\mathbb{R}^{m \\times n}\\)。这个要求在对矩阵变量函数求导时非常容易被忽略，检查一个矩阵变量函数的导数是否正确的第一步是要验证其维数是否和对应的自变量相符。读者可利用例2.1的(2)来加深对矩阵变量函数导数的理解。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.681,
                0.4,
                0.699
            ],
            "angle": 0,
            "content": "2.2.3 自动微分"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.712,
                0.827,
                0.853
            ],
            "angle": 0,
            "content": "自动微分是使用计算机计算导数的算法。在神经网络中，我们通过前向传播的方式将输入数据 \\(a\\) 转化为输出 \\(\\hat{y}\\)，也就是将输入数据 \\(a\\) 作为初始信息，将其传递到隐藏层的每个神经元，处理后得到输出 \\(\\hat{y}\\)。通过比较输出 \\(\\hat{y}\\) 与真实标签 \\(y\\)，可以定义一个损失函数 \\(f(x)\\)，其中 \\(x\\) 表示所有神经元对应的参数集合并且 \\(f(x)\\) 一般是多个函数复合的形式。为了找到最优的参数，我们需要通过优化算法来调整 \\(x\\) 使得 \\(f(x)\\) 达到最小。因此，对神经元参数 \\(x\\) 计算导数是不可避免的。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "2.2 导数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "33"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.744,
                0.238
            ],
            "angle": 0,
            "content": "对于一个由很多个简单函数复合而成的函数，根据复合函数的链式法则，可以通过每个简单函数的导数的乘积来计算对于各层变量的导数。我们先从一个简单的例子开始说起。考虑函数 \\( f(x_{1},x_{2}) = x_{1}x_{2} + \\sin x_{1} \\) 计算该函数的过程可以用计算图2.1来表示。"
        },
        {
            "type": "image",
            "bbox": [
                0.195,
                0.274,
                0.422,
                0.518
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.489,
                0.335,
                0.551,
                0.348
            ],
            "angle": 0,
            "content": "\\[\nw _ {1} = x _ {1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.491,
                0.36,
                0.552,
                0.373
            ],
            "angle": 0,
            "content": "\\[\nw _ {2} = x _ {2}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.491,
                0.384,
                0.574,
                0.397
            ],
            "angle": 0,
            "content": "\\[\nw _ {3} = w _ {1} w _ {2}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.491,
                0.407,
                0.577,
                0.422
            ],
            "angle": 0,
            "content": "\\[\nw _ {4} = \\sin w _ {1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.49,
                0.432,
                0.596,
                0.447
            ],
            "angle": 0,
            "content": "\\[\nw _ {5} = w _ {3} + w _ {4}\n\\]"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.325,
                0.541,
                0.583,
                0.558
            ],
            "angle": 0,
            "content": "图2.1 函数 \\( f(x_{1},x_{2}) \\) 的计算过程"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.584,
                0.52,
                0.601
            ],
            "angle": 0,
            "content": "利用计算导数的链式法则，我们可以依次计算"
        },
        {
            "type": "equation",
            "bbox": [
                0.247,
                0.611,
                0.316,
                0.644
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\partial f}{\\partial w _ {5}} = 1,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.246,
                0.646,
                0.401,
                0.68
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\partial f}{\\partial w _ {4}} = \\frac {\\partial f}{\\partial w _ {5}} \\frac {\\partial w _ {5}}{\\partial w _ {4}} = 1,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.246,
                0.681,
                0.401,
                0.715
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\partial f}{\\partial w _ {3}} = \\frac {\\partial f}{\\partial w _ {5}} \\frac {\\partial w _ {5}}{\\partial w _ {3}} = 1,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.246,
                0.716,
                0.45,
                0.749
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\partial f}{\\partial w _ {2}} = \\frac {\\partial f}{\\partial w _ {3}} \\frac {\\partial w _ {3}}{\\partial w _ {2}} = w _ {1} = x _ {1},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.246,
                0.75,
                0.664,
                0.785
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\partial f}{\\partial w _ {1}} = \\frac {\\partial f}{\\partial w _ {3}} \\frac {\\partial w _ {3}}{\\partial w _ {1}} + \\frac {\\partial f}{\\partial w _ {4}} \\frac {\\partial w _ {4}}{\\partial w _ {1}} = w _ {2} + \\cos w _ {1} = \\cos x _ {1} + x _ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.796,
                0.4,
                0.812
            ],
            "angle": 0,
            "content": "通过这种方式，就求得了导数"
        },
        {
            "type": "equation",
            "bbox": [
                0.34,
                0.822,
                0.569,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\partial f}{\\partial x _ {1}} = \\cos x _ {1} + x _ {2}, \\quad \\frac {\\partial f}{\\partial x _ {2}} = x _ {1}.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "34"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.277
            ],
            "angle": 0,
            "content": "在计算图2.1中, \\(w_{1}\\) 和 \\(w_{2}\\) 为自变量, \\(w_{3}\\) 和 \\(w_{4}\\) 为中间变量, \\(w_{5}\\) 代表最终的目标函数值。容易看出, 函数 \\(f\\) 计算过程中涉及的所有变量 \\(w_{1}, w_{2}, \\dots, w_{5}\\) 和它们之间的依赖关系构成了一个有向图: 每个变量 \\(w_{i}\\) 代表着图中的一个节点, 变量的依赖关系为该图的边。如果有一条从节点 \\(w_{i}\\) 指向 \\(w_{j}\\) 的边, 我们称 \\(w_{i}\\) 为 \\(w_{j}\\) 的父节点, \\(w_{j}\\) 为 \\(w_{i}\\) 的子节点。一个节点的值由其所有的父节点的值确定。则称从父节点的值推子节点值的计算流为前向传播。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.283,
                0.825,
                0.406
            ],
            "angle": 0,
            "content": "自动微分有两种方式：前向模式和后向模式。在前向模式中，根据计算图，可以依次计算每个中间变量的取值及其对父变量的偏导数值（例如由 \\(w_{1}\\) 和 \\(w_{2}\\) 的值，可以确定 \\(w_{3}\\) 的值，并确定 \\(\\frac{\\partial w_{3}}{\\partial w_{1}}\\) 和 \\(\\frac{\\partial w_{3}}{\\partial w_{2}}\\) 的值）。通过链式法则，可以复合得到每个中间变量对自变量的导数值。直至传播到最后一个子节点 \\((w_{5})\\) 时，就得到了最终的目标函数值以及目标函数关于自变量 \\((x_{1}, x_{2})\\) 的梯度值。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.413,
                0.825,
                0.533
            ],
            "angle": 0,
            "content": "不同于前向模式，后向模式的节点求值和导数计算不是同时进行的。它是先利用前向模式计算各个节点的值，然后再根据计算图逆向计算对函数 \\(f\\) 关于各个中间变量的偏导数。如前面给的计算例子，设节点 \\(w_{i}\\) 的值已经通过前向模式计算得到，为了计算梯度，我们首先计算 \\(f(w_{5})\\) 对其父节点 \\((w_{4}\\) 和 \\(w_{3}\\) )的导数。这样依次往下展开，就可以由子节点的导数得到对当前节点的导数，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.435,
                0.536,
                0.649,
                0.575
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\partial f}{\\partial w _ {i}} = \\sum_ {w _ {j} \\text {是} w _ {i} \\text {的 子 节 点}} \\frac {\\partial f}{\\partial w _ {j}} \\frac {\\partial w _ {j}}{\\partial w _ {i}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.59,
                0.825,
                0.69
            ],
            "angle": 0,
            "content": "对于前向模式而言，后向模式的梯度的计算复杂度更低。具体地，后向模式的梯度计算代价至多为函数值计算代价的5倍，但是前向模式的计算代价可能多达函数值计算代价的 \\(n\\)（\\(n\\) 为自变量维数）倍。这使得后向模式在实际中更加流行。对于神经网络中的优化问题，其自动微分采用的是后向模式，具体实现可以参考[1,47]。"
        },
        {
            "type": "title",
            "bbox": [
                0.436,
                0.732,
                0.646,
                0.753
            ],
            "angle": 0,
            "content": "2.3 广义实值函数"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.774,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "在数学分析课程中我们学习了函数的基本概念：函数是从向量空间 \\(\\mathbb{R}^n\\) 到实数域 \\(\\mathbb{R}\\) 的映射．而在最优化领域，经常涉及对某个函数其中的一个变量取inf（sup）操作，这导致函数的取值可能为无穷．为了能够更方便地描述优化问题，我们需要对函数的定义进行某种扩展."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "2.3 广义实值函数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "35"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.159,
                0.736,
                0.198
            ],
            "angle": 0,
            "content": "定义2.6（广义实值函数）令 \\(\\overline{\\mathbb{R}}\\stackrel {\\mathrm{def}}{=}\\mathbb{R}\\cup \\{\\pm \\infty \\}\\) 为广义实数空间，则映射\\(f:\\mathbb{R}^n\\to \\overline{\\mathbb{R}}\\) 称为广义实值函数"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.212,
                0.737,
                0.25
            ],
            "angle": 0,
            "content": "从广义实值函数的定义可以看出，其值域多了两个特殊的值 \\(\\pm \\infty\\)。和数学分析一样，我们规定"
        },
        {
            "type": "equation",
            "bbox": [
                0.356,
                0.267,
                0.554,
                0.283
            ],
            "angle": 0,
            "content": "\\[\n- \\infty <   a <   + \\infty , \\quad \\forall a \\in \\mathbb {R}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.301,
                0.211,
                0.317
            ],
            "angle": 0,
            "content": "以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.268,
                0.323,
                0.639,
                0.34
            ],
            "angle": 0,
            "content": "\\[\n(+ \\infty) + (+ \\infty) = + \\infty , \\quad + \\infty + a = + \\infty , \\forall a \\in \\mathbb {R}.\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.368,
                0.315,
                0.386
            ],
            "angle": 0,
            "content": "2.3.1 适当函数"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.399,
                0.737,
                0.438
            ],
            "angle": 0,
            "content": "适当函数是一类很重要的广义实值函数，很多最优化理论都是建立在适当函数之上的."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.452,
                0.737,
                0.51
            ],
            "angle": 0,
            "content": "定义2.7 (适当函数) 给定广义实值函数 \\(f\\) 和非空集合 \\(\\mathcal{X}\\). 如果存在 \\(x \\in \\mathcal{X}\\) 使得 \\(f(x) < +\\infty\\), 并且对任意的 \\(x \\in \\mathcal{X}\\), 都有 \\(f(x) > -\\infty\\), 那么称函数 \\(f\\) 关于集合 \\(\\mathcal{X}\\) 是适当的."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.525,
                0.738,
                0.605
            ],
            "angle": 0,
            "content": "概括来说，适当函数 \\(f\\) 的特点是“至少有一处取值不为正无穷”，以及“处处取值不为负无穷”。对最优化问题 \\(\\min_{x} f(x)\\)，适当函数可以帮助去掉一些我们不感兴趣的函数，从而在一个比较合理的函数类中考虑最优化问题。我们约定：在本书中若无特殊说明，定理中所讨论的函数均为适当函数。"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.608,
                0.45,
                0.626
            ],
            "angle": 0,
            "content": "对于适当函数 \\(f\\) ，规定其定义域"
        },
        {
            "type": "equation",
            "bbox": [
                0.349,
                0.642,
                0.559,
                0.661
            ],
            "angle": 0,
            "content": "\\[\n\\mathbf {d o m} f = \\{x \\mid f (x) <   + \\infty \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.677,
                0.737,
                0.714
            ],
            "angle": 0,
            "content": "正是因为适当函数的最小值不可能在函数值为无穷处取到，因此 \\(\\operatorname{dom} f\\) 的定义方式是自然的."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.742,
                0.295,
                0.76
            ],
            "angle": 0,
            "content": "2.3.2 闭函数"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.774,
                0.737,
                0.832
            ],
            "angle": 0,
            "content": "闭函数是另一类重要的广义实值函数，本书后面章节的许多定理都建立在闭函数之上。在数学分析课程中我们接触过连续函数，本小节介绍的闭函数可以看成是连续函数的一种推广。"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.836,
                0.559,
                0.853
            ],
            "angle": 0,
            "content": "在介绍闭函数之前，我们先引入一些基本概念。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "36"
        },
        {
            "type": "header",
            "bbox": [
                0.683,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.157,
                0.364,
                0.174
            ],
            "angle": 0,
            "content": "1. 下水平集"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.187,
                0.82,
                0.205
            ],
            "angle": 0,
            "content": "下水平集是描述实值函数取值情况的一个重要概念. 为此有如下定义:"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.216,
                0.703,
                0.234
            ],
            "angle": 0,
            "content": "定义2.8（\\(\\alpha\\)-下水平集）对于广义实值函数 \\(f \\colon \\mathbb{R}^n \\to \\overline{\\mathbb{R}}\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.466,
                0.247,
                0.618,
                0.267
            ],
            "angle": 0,
            "content": "\\[\nC _ {\\alpha} = \\{x \\mid f (x) \\leqslant \\alpha \\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.279,
                0.423,
                0.296
            ],
            "angle": 0,
            "content": "称为 \\(f\\) 的 \\(\\alpha\\)-下水平集"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.308,
                0.827,
                0.387
            ],
            "angle": 0,
            "content": "在最优化问题中，多数情况都要对函数 \\( f(x) \\) 求极小值，通过研究 \\( \\alpha \\)-下水平集可以知道具体在哪些点处 \\( f(x) \\) 的值不超过 \\( \\alpha \\)。若 \\( C_{\\alpha} \\) 非空，我们知道 \\( f(x) \\) 的全局极小点（若存在）一定落在 \\( C_{\\alpha} \\) 中，因此也就无需考虑 \\( C_{\\alpha} \\) 之外的点。"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.411,
                0.347,
                0.427
            ],
            "angle": 0,
            "content": "2. 上方图"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.441,
                0.82,
                0.459
            ],
            "angle": 0,
            "content": "上方图是从集合的角度来描述一个函数的具体性质。我们有如下定义："
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.47,
                0.67,
                0.488
            ],
            "angle": 0,
            "content": "定义2.9（上方图）对于广义实值函数 \\(f\\colon \\mathbb{R}^n\\to \\overline{\\mathbb{R}}\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.416,
                0.501,
                0.666,
                0.52
            ],
            "angle": 0,
            "content": "\\[\n\\mathbf {e p i} f = \\left\\{\\left(x, t\\right) \\in \\mathbb {R} ^ {n + 1} \\mid f (x) \\leqslant t \\right\\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.533,
                0.39,
                0.55
            ],
            "angle": 0,
            "content": "称为 \\(f\\) 的上方图"
        },
        {
            "type": "image",
            "bbox": [
                0.372,
                0.57,
                0.716,
                0.74
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.418,
                0.758,
                0.667,
                0.775
            ],
            "angle": 0,
            "content": "图2.2 函数 \\(f\\) 和其上方图epif"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "上方图的一个直观的例子如图 2.2 所示。上方图将函数和集合建立了联系，\\(f\\) 的很多性质都可以在 \\(\\mathbf{epi} f\\) 上得到体现。在后面的分析中将看到，我们可以通过 \\(\\mathbf{epi} f\\) 的一些性质来反推 \\(f\\) 的性质。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "2.3 广义实值函数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "37"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.157,
                0.36,
                0.174
            ],
            "angle": 0,
            "content": "3. 闭函数下半连续函数"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.187,
                0.737,
                0.224
            ],
            "angle": 0,
            "content": "基于前面介绍的一些基本概念，我们可以给出闭函数和下半连续函数的定义."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.235,
                0.737,
                0.273
            ],
            "angle": 0,
            "content": "定义2.10（闭函数）设 \\(f\\colon \\mathbb{R}^n\\to \\overline{\\mathbb{R}}\\) 为广义实值函数，若epif为闭集，则称 \\(f\\) 为闭函数"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.284,
                0.737,
                0.321
            ],
            "angle": 0,
            "content": "定义2.11（下半连续函数）设广义实值函数 \\(f\\colon \\mathbb{R}^n\\to \\overline{\\mathbb{R}}\\) ，若对任意的\\(x\\in \\mathbb{R}^n\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.378,
                0.325,
                0.529,
                0.349
            ],
            "angle": 0,
            "content": "\\[\n\\liminf_{y\\to x}f(y)\\geqslant f(x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.355,
                0.361,
                0.372
            ],
            "angle": 0,
            "content": "则 \\(f(x)\\) 为下半连续函数"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.383,
                0.537,
                0.4
            ],
            "angle": 0,
            "content": "如图2.3所示， \\(f(x)\\) 为 \\(\\mathbb{R}\\) 上的下半连续函数"
        },
        {
            "type": "image",
            "bbox": [
                0.312,
                0.409,
                0.599,
                0.549
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.35,
                0.561,
                0.557,
                0.578
            ],
            "angle": 0,
            "content": "图2.3 下半连续函数 \\( f(x) \\)"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.598,
                0.737,
                0.635
            ],
            "angle": 0,
            "content": "有趣的是，虽然表面上看这两种函数的定义方式截然不同，但闭函数和下半连续函数是等价的．实际上我们有如下定理："
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.647,
                0.754,
                0.683
            ],
            "angle": 0,
            "content": "定理2.2（闭函数和下半连续函数的等价性[12]）设广义实值函数 \\(f\\colon \\mathbb{R}^n\\to\\) \\(\\overline{\\mathbb{R}}\\) ，则以下命题等价："
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.695,
                0.47,
                0.712
            ],
            "angle": 0,
            "content": "(1) \\(f(x)\\) 的任意 \\(\\alpha\\)-下水平集都是闭集；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.724,
                0.367,
                0.741
            ],
            "angle": 0,
            "content": "(2) \\(f(x)\\) 是下半连续的；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.754,
                0.329,
                0.771
            ],
            "angle": 0,
            "content": "(3) \\(f(x)\\) 是闭函数."
        },
        {
            "type": "list",
            "bbox": [
                0.181,
                0.695,
                0.47,
                0.771
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.782,
                0.737,
                0.819
            ],
            "angle": 0,
            "content": "证明. (2) \\(\\Rightarrow\\) (3): 设 \\((x_{k},y_{k})\\in \\mathbf{epi}f\\) 且 \\(\\lim_{k\\to \\infty}(x_k,y_k) = (\\bar{x},\\bar{y})\\) ，根据下半连续性和极限定义，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.325,
                0.832,
                0.58,
                0.855
            ],
            "angle": 0,
            "content": "\\[\nf (\\bar {x}) \\leqslant \\lim  _ {k \\rightarrow \\infty} \\inf  f (x _ {k}) \\leqslant \\lim  _ {k \\rightarrow \\infty} y _ {k} = \\bar {y},\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "38"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.571,
                0.174
            ],
            "angle": 0,
            "content": "这等价于 \\((\\bar{x},\\bar{y})\\in \\mathbf{epi}f\\) ，即epif是闭集"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.177,
                0.825,
                0.236
            ],
            "angle": 0,
            "content": "(3) \\(\\Rightarrow\\) (1): 取 \\(\\alpha\\)-下水平集的元素 \\(x_{k} \\rightarrow \\bar{x}\\), 注意到 \\((x_{k}, \\alpha) \\in \\mathbf{epi} f\\) 且 \\((x_{k}, \\alpha) \\rightarrow (\\bar{x}, \\alpha)\\), 由 \\(\\mathbf{epi} f\\) 为闭集可知 \\((\\bar{x}, \\alpha) \\in \\mathbf{epi} f\\), 即 \\(f(\\bar{x}) \\leqslant \\alpha\\). 这说明了 \\(f(x)\\) 的任意 \\(\\alpha\\)-下水平集是闭集."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.239,
                0.825,
                0.283
            ],
            "angle": 0,
            "content": "(1) \\(\\Rightarrow\\) (2): 我们用反证法. 反设存在序列 \\(\\{x_{k}\\} \\to \\bar{x} (k\\to \\infty)\\) 但 \\(f(\\bar{x}) > \\lim_{k\\to \\infty}\\inf f(x_k)\\), 取 \\(t\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.448,
                0.288,
                0.632,
                0.311
            ],
            "angle": 0,
            "content": "\\[\nf(\\bar{x}) > t > \\liminf_{k\\to \\infty}f(x_{k}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.316,
                0.825,
                0.374
            ],
            "angle": 0,
            "content": "由下极限的定义，\\(\\{x_{k} \\mid f(x_{k}) \\leqslant t\\}\\) 中必定含有无穷多个 \\(x_{k}\\)，不妨设 \\(\\{x_{k}\\}\\) 中存在子列 \\(\\{x_{k_{l}}\\}\\) 使得 \\(f(x_{k_{l}}) \\leqslant t\\) 且 \\(\\lim_{l \\to \\infty} x_{k_{l}} = \\bar{x}\\)。这显然与 \\(t\\)-下水平集为闭集矛盾。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.387,
                0.825,
                0.466
            ],
            "angle": 0,
            "content": "以上等价性为我们之后证明定理提供了很大的方便。由于下半连续函数也具有某种连续性，第五章也会介绍其相应的最小值存在定理。在许多文献中闭函数和下半连续函数往往只出现一种定义，读者应当注意这个等价关系。"
        },
        {
            "type": "text",
            "bbox": [
                0.291,
                0.47,
                0.68,
                0.487
            ],
            "angle": 0,
            "content": "闭（下半连续）函数间的简单运算是保持原有性质："
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.497,
                0.825,
                0.555
            ],
            "angle": 0,
            "content": "(1) 加法: 若 \\(f\\) 与 \\(g\\) 均为适当的闭 (下半连续) 函数, 并且 \\(\\operatorname{dom} f \\cap \\operatorname{dom} g \\neq \\emptyset\\), 则 \\(f + g\\) 也是闭 (下半连续) 函数. 在这里添加适当函数的条件是为了避免出现未定式 \\((- \\infty) + (+ \\infty)\\) 的情况;"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.567,
                0.825,
                0.604
            ],
            "angle": 0,
            "content": "(2) 仿射映射的复合: 若 \\(f\\) 为闭 (下半连续) 函数, 则 \\(f(Ax + b)\\) 也为闭 (下半连续) 函数;"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.616,
                0.825,
                0.657
            ],
            "angle": 0,
            "content": "(3) 取上确界: 若每一个函数 \\(f_{\\alpha}\\) 均为闭（下半连续）函数, 则 \\(\\sup_{\\alpha} f_{\\alpha}(x)\\) 也为闭（下半连续）函数."
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.497,
                0.825,
                0.657
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.486,
                0.688,
                0.597,
                0.709
            ],
            "angle": 0,
            "content": "2.4 凸集"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.728,
                0.459,
                0.747
            ],
            "angle": 0,
            "content": "2.4.1 凸集的相关定义"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.759,
                0.559,
                0.776
            ],
            "angle": 0,
            "content": "对于 \\(\\mathbb{R}^n\\) 中的两个点 \\(x_{1}\\neq x_{2}\\) ，形如"
        },
        {
            "type": "equation",
            "bbox": [
                0.465,
                0.788,
                0.617,
                0.805
            ],
            "angle": 0,
            "content": "\\[\ny = \\theta x _ {1} + (1 - \\theta) x _ {2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "的点形成了过点 \\(x_{1}\\) 和 \\(x_{2}\\) 的直线。当 \\(0 \\leqslant \\theta \\leqslant 1\\) 时，这样的点形成了连接点 \\(x_{1}\\) 与 \\(x_{2}\\) 的线段。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "2.4 凸集"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "39"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.194
            ],
            "angle": 0,
            "content": "定义2.12 如果过集合 \\(C\\) 中任意两点的直线都在 \\(C\\) 内，则称 \\(C\\) 为仿射集，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.288,
                0.203,
                0.62,
                0.22
            ],
            "angle": 0,
            "content": "\\[\nx _ {1}, x _ {2} \\in C \\Longrightarrow \\theta x _ {1} + (1 - \\theta) x _ {2} \\in C, \\forall \\theta \\in \\mathbb {R}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.239,
                0.744,
                0.278
            ],
            "angle": 0,
            "content": "线性方程组 \\(Ax = b\\) 的解集是仿射集．反之，任何仿射集都可以表示成一个线性方程组的解集，读者可以自行验证."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.295,
                0.741,
                0.334
            ],
            "angle": 0,
            "content": "定义2.13 如果连接集合 \\(C\\) 中任意两点的线段都在 \\(C\\) 内，则称 \\(C\\) 为凸集，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.274,
                0.341,
                0.635,
                0.359
            ],
            "angle": 0,
            "content": "\\[\nx _ {1}, x _ {2} \\in C \\Longrightarrow \\theta x _ {1} + (1 - \\theta) x _ {2} \\in C, \\forall 0 \\leqslant \\theta \\leqslant 1.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.377,
                0.741,
                0.414
            ],
            "angle": 0,
            "content": "从仿射集的定义容易看出仿射集都是凸集. 下面给出一些凸集和非凸集的例子."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.433,
                0.741,
                0.472
            ],
            "angle": 0,
            "content": "例2.2 在图2.4中，(a)为凸集，(b)(c)为非凸集，其中(c)不含部分边界点"
        },
        {
            "type": "image",
            "bbox": [
                0.189,
                0.499,
                0.339,
                0.616
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.249,
                0.619,
                0.266,
                0.63
            ],
            "angle": 0,
            "content": "(a)"
        },
        {
            "type": "image",
            "bbox": [
                0.389,
                0.499,
                0.539,
                0.614
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.455,
                0.619,
                0.472,
                0.63
            ],
            "angle": 0,
            "content": "(b)"
        },
        {
            "type": "image",
            "bbox": [
                0.569,
                0.503,
                0.724,
                0.61
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.642,
                0.619,
                0.658,
                0.63
            ],
            "angle": 0,
            "content": "(c)"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.334,
                0.657,
                0.575,
                0.674
            ],
            "angle": 0,
            "content": "图2.4 一个凸集和两个非凸集"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.703,
                0.538,
                0.72
            ],
            "angle": 0,
            "content": "从凸集可以引出凸组合和凸包等概念．形如"
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.738,
                0.621,
                0.755
            ],
            "angle": 0,
            "content": "\\[\nx = \\theta_ {1} x _ {1} + \\theta_ {2} x _ {2} + \\dots + \\theta_ {k} x _ {k},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.29,
                0.76,
                0.62,
                0.778
            ],
            "angle": 0,
            "content": "\\[\n1 = \\theta_ {1} + \\theta_ {2} + \\dots + \\theta_ {k}, \\quad \\theta_ {i} \\geqslant 0, i = 1, 2, \\dots , k\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.795,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "的点称为 \\(x_{1}, x_{2}, \\cdots, x_{k}\\) 的凸组合。集合 \\(S\\) 中点所有可能的凸组合构成的集合称作 \\(S\\) 的凸包，记作 \\(\\operatorname{conv} S\\)。实际上，\\(\\operatorname{conv} S\\) 是包含 \\(S\\) 的最小的凸集。如图2.5所示，左边的为离散点集的凸包，右边的为扇形的凸包。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.131
            ],
            "angle": 0,
            "content": "40"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "image",
            "bbox": [
                0.325,
                0.183,
                0.507,
                0.322
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image",
            "bbox": [
                0.582,
                0.196,
                0.737,
                0.316
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.423,
                0.366,
                0.663,
                0.383
            ],
            "angle": 0,
            "content": "图2.5 离散点集和扇形的凸包"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.408,
                0.816,
                0.425
            ],
            "angle": 0,
            "content": "若在凸组合的定义中去掉 \\(\\theta_{i} \\geqslant 0\\) 的限制，我们可以得到仿射包的概念。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.437,
                0.797,
                0.453
            ],
            "angle": 0,
            "content": "定义2.14（仿射包）设 \\(S\\) 为 \\(\\mathbb{R}^n\\) 的子集，称如下集合为 \\(S\\) 的仿射包："
        },
        {
            "type": "equation",
            "bbox": [
                0.257,
                0.468,
                0.826,
                0.488
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{x \\mid x = \\theta_ {1} x _ {1} + \\theta_ {2} x _ {2} + \\dots + \\theta_ {k} x _ {k}, \\quad x _ {1}, x _ {2}, \\dots , x _ {k} \\in S, \\quad \\theta_ {1} + \\theta_ {2} + \\dots + \\theta_ {k} = 1 \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.5,
                0.364,
                0.515
            ],
            "angle": 0,
            "content": "记为 affine \\(S\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.291,
                0.528,
                0.691,
                0.546
            ],
            "angle": 0,
            "content": "图2.6展示了 \\(\\mathbb{R}^3\\) 中圆盘 \\(S\\) 的仿射包，其为一个平面"
        },
        {
            "type": "image",
            "bbox": [
                0.38,
                0.568,
                0.701,
                0.691
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.448,
                0.733,
                0.637,
                0.75
            ],
            "angle": 0,
            "content": "图2.6 集合 \\(S\\) 的仿射包"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.775,
                0.825,
                0.811
            ],
            "angle": 0,
            "content": "一般而言，一个集合的仿射包实际上是包含该集合的最小的仿射集，这个概念在之后我们讨论凸问题最优性条件的时候会用到。"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.817,
                0.333,
                0.831
            ],
            "angle": 0,
            "content": "形如"
        },
        {
            "type": "equation",
            "bbox": [
                0.42,
                0.837,
                0.663,
                0.853
            ],
            "angle": 0,
            "content": "\\[\nx = \\theta_ {1} x _ {1} + \\theta_ {2} x _ {2}, \\quad \\theta_ {1} > 0, \\theta_ {2} > 0\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "2.4 凸集"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.119,
                0.735,
                0.131
            ],
            "angle": 0,
            "content": "41"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.195
            ],
            "angle": 0,
            "content": "的点称为点 \\(x_{1}, x_{2}\\) 的锥组合。若集合 \\(S\\) 中任意点的锥组合都在 \\(S\\) 中，则称 \\(S\\) 为凸锥，如图2.7所示。"
        },
        {
            "type": "image",
            "bbox": [
                0.312,
                0.234,
                0.531,
                0.364
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.404,
                0.4,
                0.506,
                0.417
            ],
            "angle": 0,
            "content": "图2.7 凸锥"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.532,
                0.334,
                0.55
            ],
            "angle": 0,
            "content": "2.4.2 重要的凸集"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.592,
                0.695,
                0.609
            ],
            "angle": 0,
            "content": "下面将介绍一些重要的凸集．这些凸集在实际问题中常常会遇到."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.717,
                0.327,
                0.733
            ],
            "angle": 0,
            "content": "1. 超平面和半空间"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.774,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "任取非零向量 \\(a\\) ，形如 \\(\\{x|a^{\\mathrm{T}}x = b\\}\\) 的集合称为超平面，形如 \\(\\{x|a^{\\mathrm{T}}x\\leqslant b\\}\\) 的集合称为半空间（如图2.8）． \\(a\\) 是对应的超平面和半空间的法向量．一个超平面将 \\(\\mathbb{R}^n\\) 分为两个半空间．容易看出，超平面是仿射集和凸集，半空间是凸集但不是仿射集."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "42"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "image",
            "bbox": [
                0.302,
                0.193,
                0.504,
                0.316
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image",
            "bbox": [
                0.561,
                0.192,
                0.792,
                0.353
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.448,
                0.385,
                0.634,
                0.401
            ],
            "angle": 0,
            "content": "图2.8 超平面和半空间"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.438,
                0.396,
                0.455
            ],
            "angle": 0,
            "content": "2. 球、椭球、锥"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.47,
                0.825,
                0.507
            ],
            "angle": 0,
            "content": "球和椭球也是常见的凸集．球是空间中到某个点距离（或两者差的范数）小于某个常数的点的集合，并将"
        },
        {
            "type": "equation",
            "bbox": [
                0.345,
                0.525,
                0.737,
                0.544
            ],
            "angle": 0,
            "content": "\\[\nB \\left(x _ {c}, r\\right) = \\left\\{x \\mid \\| x - x _ {c} \\| _ {2} \\leqslant r \\right\\} = \\left\\{x _ {c} + r u \\mid \\| u \\| _ {2} \\leqslant 1 \\right\\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.561,
                0.646,
                0.578
            ],
            "angle": 0,
            "content": "称为中心为 \\(x_{c}\\) ，半径为 \\(r\\) 的(欧几里得)球．而形如"
        },
        {
            "type": "equation",
            "bbox": [
                0.427,
                0.595,
                0.656,
                0.615
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{x \\mid \\left(x - x _ {c}\\right) ^ {\\mathrm {T}} P ^ {- 1} \\left(x - x _ {c}\\right) \\leqslant 1 \\right\\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.632,
                0.825,
                0.67
            ],
            "angle": 0,
            "content": "的集合称为椭球，其中 \\(P \\in S_{++}^{n}\\) (即 \\(P\\) 对称正定). 椭球的另一种表示为 \\(\\{x_{c} + Au \\mid \\|u\\|_{2} \\leqslant 1\\}\\)，\\(A\\) 为非奇异的方阵."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.675,
                0.825,
                0.712
            ],
            "angle": 0,
            "content": "在定义一个球时，并不一定要使用欧几里得空间的距离．对于一般的范数，同样可以定义“球”．令 \\(\\| \\cdot \\|\\) 是任意一个范数，"
        },
        {
            "type": "equation",
            "bbox": [
                0.473,
                0.73,
                0.609,
                0.748
            ],
            "angle": 0,
            "content": "\\[\n\\{x \\mid \\| x - x _ {c} \\| \\leqslant r \\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.766,
                0.677,
                0.783
            ],
            "angle": 0,
            "content": "称为中心为 \\(x_{c}\\) ，半径为 \\(r\\) 的范数球．另外，我们称集合"
        },
        {
            "type": "equation",
            "bbox": [
                0.478,
                0.801,
                0.604,
                0.819
            ],
            "angle": 0,
            "content": "\\[\n\\{(x, t) | \\| x \\| \\leqslant t \\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.782,
                0.853
            ],
            "angle": 0,
            "content": "为范数锥. 欧几里得范数锥也称为二次锥. 范数球和范数锥都是凸集"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "2.4 凸集"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "43"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.157,
                0.258,
                0.174
            ],
            "angle": 0,
            "content": "3. 多面体"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.188,
                0.657,
                0.205
            ],
            "angle": 0,
            "content": "我们把满足线性等式和不等式组的点的集合称为多面体，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.362,
                0.221,
                0.546,
                0.24
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{x \\mid A x \\leqslant b, \\quad C x = d \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.256,
                0.739,
                0.294
            ],
            "angle": 0,
            "content": "其中 \\(A \\in \\mathbb{R}^{m \\times n}, C \\in \\mathbb{R}^{p \\times n}, x \\leqslant y\\) 表示向量 \\(x\\) 的每个分量均小于等于 \\(y\\) 的对应分量。多面体是有限个半空间和超平面的交集，因此是凸集。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.32,
                0.31,
                0.338
            ],
            "angle": 0,
            "content": "4. （半）正定锥"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.35,
                0.744,
                0.431
            ],
            "angle": 0,
            "content": "记 \\(\\mathcal{S}^n\\) 为 \\(n\\times n\\) 对称矩阵的集合， \\(S_{+}^{n} = \\left\\{X\\in S^{n}|X\\succeq 0\\right\\}\\) 为 \\(n\\times n\\) 半正定矩阵的集合， \\(S_{++}^{n} = \\{X\\in S^{n}|X\\succ 0\\}\\) 为 \\(n\\times n\\) 正定矩阵的集合．容易证明 \\(S_{+}^{n}\\) 是凸锥，因此 \\(S_{+}^{n}\\) 又称为半正定锥．图2.9展示了二维半正定锥的几何形状."
        },
        {
            "type": "image",
            "bbox": [
                0.206,
                0.451,
                0.778,
                0.62
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.529,
                0.629,
                0.725,
                0.648
            ],
            "angle": 0,
            "content": "图2.9 二维半正定锥 \\(S_{+}^{2}\\)"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.67,
                0.334,
                0.689
            ],
            "angle": 0,
            "content": "2.4.3 保凸的运算"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.702,
                0.739,
                0.739
            ],
            "angle": 0,
            "content": "下面介绍证明一个集合（设为 \\(C\\)）为凸集的两种方式。第一种是利用定义"
        },
        {
            "type": "equation",
            "bbox": [
                0.283,
                0.745,
                0.625,
                0.763
            ],
            "angle": 0,
            "content": "\\[\nx _ {1}, x _ {2} \\in C, 0 \\leqslant \\theta \\leqslant 1 \\Longrightarrow \\theta x _ {1} + (1 - \\theta) x _ {2} \\in C\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "来证明集合 \\(C\\) 是凸集．第二种方法是说明集合 \\(C\\) 可由简单的凸集(超平面、半空间、范数球等)经过保凸的运算后得到．为此，我们需要掌握一些常见的保凸运算．下面的两个定理分别说明了取交集和仿射变换这两种运算是保凸的."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "44"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.157,
                0.763,
                0.174
            ],
            "angle": 0,
            "content": "定理2.3 任意多个凸集的交为凸集，即若 \\(C_i, i \\in \\mathcal{I}\\) 是凸集，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.52,
                0.197,
                0.564,
                0.228
            ],
            "angle": 0,
            "content": "\\[\n\\bigcap_ {i \\in \\mathcal {I}} C _ {i}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.247,
                0.605,
                0.264
            ],
            "angle": 0,
            "content": "为凸集．这里 \\(\\mathcal{I}\\) 是任意指标集（不要求可列）."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.289,
                0.825,
                0.327
            ],
            "angle": 0,
            "content": "定理2.4 设 \\(f\\colon \\mathbb{R}^n\\to \\mathbb{R}^m\\) 是仿射变换 \\(\\left(f(x) = Ax + b,A\\in \\mathbb{R}^{m\\times n},b\\in \\mathbb{R}^m\\right)\\) ，则"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.353,
                0.484,
                0.37
            ],
            "angle": 0,
            "content": "(1) 凸集在 \\(f\\) 下的像是凸集："
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.391,
                0.752,
                0.412
            ],
            "angle": 0,
            "content": "\\[\nS \\subseteq \\mathbb {R} ^ {n}   \\text {是 凸 集} \\Longrightarrow f (S) \\stackrel {\\mathrm {d e f}} {=} \\{f (x) | x \\in S \\}   \\text {是 凸 集};\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.448,
                0.501,
                0.465
            ],
            "angle": 0,
            "content": "(2) 凸集在 \\(f\\) 下的原像是凸集："
        },
        {
            "type": "equation",
            "bbox": [
                0.328,
                0.487,
                0.782,
                0.507
            ],
            "angle": 0,
            "content": "\\[\nC \\subseteq \\mathbb {R} ^ {m}   \\text {是 凸 集} \\Longrightarrow f ^ {- 1} (C) \\stackrel {\\mathrm {d e f}} {=} \\left\\{x \\in \\mathbb {R} ^ {n} | f (x) \\in C \\right\\}   \\text {是 凸 集}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.544,
                0.825,
                0.624
            ],
            "angle": 0,
            "content": "注意到缩放、平移和投影变换都是仿射变换，因此凸集经过缩放、平移或投影的像仍是凸集。利用仿射变换保凸的性质，可以证明线性矩阵不等式的解集 \\(\\{x \\mid x_1A_1 + x_2A_2 + \\dots + x_mA_m \\preceq B\\}\\) 是凸集 \\((A_i, i = 1, 2, \\dots, m, B \\in S^p)\\)，双曲锥 \\(\\{x \\mid x^{\\mathrm{T}}Px \\leqslant (c^{\\mathrm{T}}x)^2, c^{\\mathrm{T}}x \\geqslant 0\\} (P \\in S_{+}^{n})\\) 是凸集。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.673,
                0.46,
                0.691
            ],
            "angle": 0,
            "content": "2.4.4 分离超平面定理"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.711,
                0.825,
                0.749
            ],
            "angle": 0,
            "content": "这里，我们介绍凸集的一个重要性质，即可以用超平面分离不相交的凸集．最基本的结果是分离超平面定理和支撑超平面定理."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.774,
                0.825,
                0.813
            ],
            "angle": 0,
            "content": "定理2.5(分离超平面定理[164]定理11.3）如果 \\(C\\) 和 \\(D\\) 是不相交的两个凸集，则存在非零向量 \\(a\\) 和常数 \\(b\\) ，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.835,
                0.702,
                0.853
            ],
            "angle": 0,
            "content": "\\[\na ^ {\\mathrm {T}} x \\leqslant b, \\quad \\forall x \\in C, \\quad \\text {且} a ^ {\\mathrm {T}} x \\geqslant b, \\quad \\forall x \\in D,\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "2.4 凸集"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "45"
        },
        {
            "type": "image",
            "bbox": [
                0.351,
                0.155,
                0.559,
                0.306
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.374,
                0.318,
                0.536,
                0.335
            ],
            "angle": 0,
            "content": "图2.10 分离超平面"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.362,
                0.553,
                0.38
            ],
            "angle": 0,
            "content": "即超平面 \\(\\{x|a^{\\mathrm{T}}x = b\\}\\) 分离了 \\(C\\) 和 \\(D\\) （如图2.10）"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.396,
                0.737,
                0.434
            ],
            "angle": 0,
            "content": "严格分离（即上式成立严格不等号）需要更强的假设。例如，当 \\(C\\) 是闭凸集，\\(D\\) 是单点集时，我们有如下严格分离定理。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.45,
                0.737,
                0.487
            ],
            "angle": 0,
            "content": "定理2.6（严格分离定理[31]例2.20）设 \\(C\\) 是闭凸集，点 \\(x_0 \\notin C\\) ，则存在非零向量 \\(a\\) 和常数 \\(b\\) ，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.328,
                0.504,
                0.579,
                0.522
            ],
            "angle": 0,
            "content": "\\[\na ^ {\\mathrm {T}} x <   b, \\forall x \\in C, \\quad \\text {且} \\quad a ^ {\\mathrm {T}} x _ {0} > b.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.542,
                0.737,
                0.579
            ],
            "angle": 0,
            "content": "上述严格分离定理要求点 \\(x_0 \\notin C\\)。当点 \\(x_0\\) 恰好在凸集 \\(C\\) 的边界上时，我们可以构造支撑超平面。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.596,
                0.737,
                0.634
            ],
            "angle": 0,
            "content": "定义2.15（支撑超平面）给定集合 \\(C\\) 及其边界上一点 \\(x_0\\) ，如果 \\(a \\neq 0\\) 满足 \\(a^{\\mathrm{T}}x \\leqslant a^{\\mathrm{T}}x_0, \\forall x \\in C\\) ，那么称集合"
        },
        {
            "type": "equation",
            "bbox": [
                0.391,
                0.65,
                0.515,
                0.669
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{x \\mid a ^ {\\mathrm {T}} x = a ^ {\\mathrm {T}} x _ {0} \\right\\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.687,
                0.432,
                0.703
            ],
            "angle": 0,
            "content": "为 \\(C\\) 在边界点 \\(x_0\\) 处的支撑超平面."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.72,
                0.737,
                0.776
            ],
            "angle": 0,
            "content": "因此，点 \\(x_0\\) 和集合 \\(C\\) 也被该超平面分开．从几何上来说，超平面 \\(\\{x|a^{\\mathrm{T}}x = a^{\\mathrm{T}}x_0\\}\\) 与集合 \\(C\\) 在点 \\(x_0\\) 处相切并且半空间 \\(\\{x|a^{\\mathrm{T}}x\\leqslant a^{\\mathrm{T}}x_0\\}\\) 包含 \\(C\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.783,
                0.644,
                0.8
            ],
            "angle": 0,
            "content": "根据凸集的分离超平面定理，我们有如下支撑超平面定理"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.815,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "定理2.7（支撑超平面定理[164]推论11.6.1）如果 \\(C\\) 是凸集，则在 \\(C\\) 的任意边界点处都存在支撑超平面。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "46"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.118,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.235
            ],
            "angle": 0,
            "content": "支撑超平面定理有非常强的几何直观：给定一个平面后，可把凸集边界上的任意一点当成支撑点将凸集放置在该平面上。其他形状的集合一般没有这个性质，例如图2.4的(b)，不可能以凹陷处为支撑点将其放置在水平面上。"
        },
        {
            "type": "title",
            "bbox": [
                0.474,
                0.267,
                0.61,
                0.287
            ],
            "angle": 0,
            "content": "2.5 凸函数"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.305,
                0.825,
                0.343
            ],
            "angle": 0,
            "content": "有了凸集的定义，我们来定义一类特殊的函数，即凸函数。因在实际问题中的广泛应用，凸函数的研究得到了人们大量的关注。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.368,
                0.44,
                0.385
            ],
            "angle": 0,
            "content": "2.5.1 凸函数的定义"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.399,
                0.797,
                0.416
            ],
            "angle": 0,
            "content": "定义2.16 (凸函数) 设函数 \\(f\\) 为适当函数，如果 \\(\\mathbf{dom}f\\) 是凸集，且"
        },
        {
            "type": "equation",
            "bbox": [
                0.388,
                0.428,
                0.694,
                0.447
            ],
            "angle": 0,
            "content": "\\[\nf (\\theta x + (1 - \\theta) y) \\leqslant \\theta f (x) + (1 - \\theta) f (y)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.458,
                0.69,
                0.475
            ],
            "angle": 0,
            "content": "对所有 \\(x, y \\in \\mathbf{dom} f, 0 \\leqslant \\theta \\leqslant 1\\) 都成立，则称 \\(f\\) 是凸函数。"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.486,
                0.825,
                0.522
            ],
            "angle": 0,
            "content": "直观地来看，连接凸函数的图像上任意两点的线段都在函数图像上方（如图2.11）."
        },
        {
            "type": "image",
            "bbox": [
                0.432,
                0.533,
                0.672,
                0.65
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.479,
                0.675,
                0.605,
                0.691
            ],
            "angle": 0,
            "content": "图2.11 凸函数"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.715,
                0.827,
                0.773
            ],
            "angle": 0,
            "content": "相应地，我们也可以定义凹函数：若 \\(-f\\) 是凸函数，则称 \\(f\\) 是凹函数。只要改变一下符号，很多凸函数的性质都可以直接应用到凹函数上。另外，如果 \\(\\operatorname{dom} f\\) 是凸集，且"
        },
        {
            "type": "equation",
            "bbox": [
                0.388,
                0.786,
                0.694,
                0.805
            ],
            "angle": 0,
            "content": "\\[\nf (\\theta x + (1 - \\theta) y) <   \\theta f (x) + (1 - \\theta) f (y)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "对所有的 \\(x, y \\in \\mathbf{dom} f, x \\neq y, 0 < \\theta < 1\\) 成立，则称 \\(f\\) 是严格凸函数。除了严格凸函数以外，还有另一类常用的凸函数：强凸函数。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.266,
                0.133
            ],
            "angle": 0,
            "content": "2.5 凸函数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "47"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.157,
                0.562,
                0.174
            ],
            "angle": 0,
            "content": "定义2.17 (强凸函数) 若存在常数 \\(m > 0\\) ，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.368,
                0.187,
                0.538,
                0.215
            ],
            "angle": 0,
            "content": "\\[\ng (x) = f (x) - \\frac {m}{2} \\| x \\| ^ {2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.228,
                0.736,
                0.265
            ],
            "angle": 0,
            "content": "为凸函数, 则称 \\(f(x)\\) 为强凸函数, 其中 \\(m\\) 为强凸参数. 为了方便我们也称 \\(f(x)\\) 为 \\(m\\)-强凸函数."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.277,
                0.737,
                0.319
            ],
            "angle": 0,
            "content": "通过直接对 \\(g(x) = f(x) - \\frac{m}{2}\\| x\\|^2\\) 应用凸函数的定义，我们可得到另一个常用的强凸函数定义."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.336,
                0.737,
                0.374
            ],
            "angle": 0,
            "content": "定义2.18（强凸函数的等价定义）若存在常数 \\(m > 0\\) ，使得对任意 \\(x,y\\in\\) \\(\\mathbf{dom}f\\) 以及 \\(\\theta \\in (0,1)\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.216,
                0.386,
                0.691,
                0.415
            ],
            "angle": 0,
            "content": "\\[\nf (\\theta x + (1 - \\theta y)) \\leqslant \\theta f (x) + (1 - \\theta) f (y) - \\frac {m}{2} \\theta (1 - \\theta) \\| x - y \\| ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.427,
                0.503,
                0.443
            ],
            "angle": 0,
            "content": "则称 \\(f(x)\\) 为强凸函数，其中 \\(m\\) 为强凸参数."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.46,
                0.738,
                0.56
            ],
            "angle": 0,
            "content": "强凸函数的两种定义侧重点不同：从定义2.17可以看出，强凸函数减去一个正定二次函数仍然是凸的；而从定义2.18可以看出，强凸函数一定是严格凸函数，当 \\(m = 0\\) 时退化成凸函数。无论从哪个定义出发，容易看出和凸函数相比，强凸函数有更好的性质。在后面很多算法的理论分析中，为了得到点列的收敛性以及更快的收敛速度，我们都要加上强凸这一条件。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.565,
                0.611,
                0.58
            ],
            "angle": 0,
            "content": "此外，根据强凸函数的等价定义容易得出下面的结论："
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.598,
                0.684,
                0.615
            ],
            "angle": 0,
            "content": "命题2.3 设 \\(f\\) 为强凸函数且存在最小值，则 \\(f\\) 的最小值点唯一。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.632,
                0.737,
                0.669
            ],
            "angle": 0,
            "content": "证明．采用反证法．设 \\(x\\neq y\\) 均为 \\(f\\) 的最小值点，根据强凸函数的等价定义，取 \\(\\theta \\in (0,1)\\) ，则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.223,
                0.683,
                0.685,
                0.762
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (\\theta x + (1 - \\theta) y) \\leqslant \\theta f (x) + (1 - \\theta) f (y) - \\frac {m}{2} \\theta (1 - \\theta) \\| x - y \\| ^ {2} \\\\ = f (x) - \\frac {m}{2} \\theta (1 - \\theta) \\| x - y \\| ^ {2} \\\\ <   f (x), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.777,
                0.737,
                0.795
            ],
            "angle": 0,
            "content": "其中严格不等号成立是因为 \\(x \\neq y\\). 这显然和 \\(f(x)\\) 为最小值矛盾, 得证. \\(\\square\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.816,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "注2.2 命题2.3中 \\(f\\) 存在最小值是前提．强凸函数 \\(f\\) 的全局极小点不一定存在，例如 \\(f(x) = x^{2}\\)，\\(\\mathbf{dom} f = (1, 2)\\)."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "48"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.461,
                0.174
            ],
            "angle": 0,
            "content": "2.5.2 凸函数判定定理"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.187,
                0.826,
                0.245
            ],
            "angle": 0,
            "content": "凸函数的一个最基本的判定方式是：先将其限制在任意直线上，然后判断对应的一维函数是否是凸的。如下面的定理所述，一个函数是凸函数当且仅当将函数限制在任意直线在定义域内的部分上时仍是凸的。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.255,
                0.825,
                0.291
            ],
            "angle": 0,
            "content": "定理2.8 \\(f(x)\\) 是凸函数当且仅当对任意的 \\(x \\in \\mathbf{dom} f, v \\in \\mathbb{R}^n, g: \\mathbb{R} \\to \\mathbb{R}\\),"
        },
        {
            "type": "equation",
            "bbox": [
                0.351,
                0.296,
                0.73,
                0.315
            ],
            "angle": 0,
            "content": "\\[\ng (t) = f (x + t v), \\quad \\mathbf {d o m} g = \\{t | x + t v \\in \\mathbf {d o m} f \\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.322,
                0.337,
                0.337
            ],
            "angle": 0,
            "content": "是凸函数."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.348,
                0.825,
                0.386
            ],
            "angle": 0,
            "content": "证明．先证必要性．设 \\(f(x)\\) 是凸函数，要证 \\(g(t) = f(x + tv)\\) 是凸函数.先说明 \\(\\mathbf{dom}g\\) 是凸集．对任意的 \\(t_1,t_2\\in \\mathbf{dom}g\\) 以及 \\(\\theta \\in (0,1)\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.475,
                0.397,
                0.609,
                0.443
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} x + t _ {1} v \\in \\mathbf {d o m} f, \\\\ x + t _ {2} v \\in \\mathbf {d o m} f, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.451,
                0.424,
                0.467
            ],
            "angle": 0,
            "content": "由 \\(\\mathbf{dom}f\\) 是凸集可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.421,
                0.479,
                0.662,
                0.497
            ],
            "angle": 0,
            "content": "\\[\nx + (\\theta t _ {1} + (1 - \\theta) t _ {2}) v \\in \\mathbf {d o m} f,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.507,
                0.757,
                0.525
            ],
            "angle": 0,
            "content": "这说明 \\(\\theta t_{1} + (1 - \\theta)t_{2}\\in \\mathbf{dom}g\\) ，即 \\(\\mathbf{dom}g\\) 是凸集．此外，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.34,
                0.533,
                0.741,
                0.626
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} g (\\theta t _ {1} + (1 - \\theta) t _ {2}) = f (x + (\\theta t _ {1} + (1 - \\theta) t _ {2}) v) \\\\ = f \\left(\\theta \\left(x + t _ {1} v\\right) + (1 - \\theta) \\left(x + t _ {2} v\\right)\\right) \\\\ \\leqslant \\theta f (x + t _ {1} v) + (1 - \\theta) f (x + t _ {2} v) \\\\ = \\theta g (t _ {1}) + (1 - \\theta) g (t _ {2}). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.633,
                0.547,
                0.65
            ],
            "angle": 0,
            "content": "结合以上两点得到函数 \\(g(t)\\) 是凸函数"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.654,
                0.825,
                0.754
            ],
            "angle": 0,
            "content": "再证充分性．设对任意的 \\(x\\in \\mathbf{dom}f,v\\in \\mathbb{R}^n\\) ， \\(g(t) = f(x + tv)\\) 为凸函数，对任意的 \\(x,y\\in \\mathbf{dom}f\\) 以及 \\(\\theta \\in (0,1)\\) ，现在要说明 \\(\\mathbf{dom}f\\) 是凸集以及估计 \\(f(\\theta x + (1 - \\theta)y)\\) 的上界．取 \\(v = y - x\\) ，以及 \\(t_1 = 0,t_2 = 1\\) ，由 \\(\\mathbf{dom}g\\) 是凸集可知 \\(\\theta \\cdot 0 + (1 - \\theta)\\cdot 1\\in \\mathbf{dom}g\\) ，即 \\(\\theta x + (1 - \\theta)y\\in \\mathbf{dom}f\\) ，这说明\\(\\mathbf{dom}f\\) 是凸集．再根据 \\(g(t) = f(x + tv)\\) 的凸性，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.417,
                0.763,
                0.663,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} g (1 - \\theta) = g (\\theta t _ {1} + (1 - \\theta) t _ {2}) \\\\ \\leqslant \\theta g (t _ {1}) + (1 - \\theta) g (t _ {2}) \\\\ = \\theta g (0) + (1 - \\theta) g (1) \\\\ = \\theta f (x) + (1 - \\theta) f (y). \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.266,
                0.133
            ],
            "angle": 0,
            "content": "2.5 凸函数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.119,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "49"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.157,
                0.279,
                0.174
            ],
            "angle": 0,
            "content": "而等式左边有"
        },
        {
            "type": "equation",
            "bbox": [
                0.25,
                0.19,
                0.657,
                0.21
            ],
            "angle": 0,
            "content": "\\[\ng (1 - \\theta) = f (x + (1 - \\theta) (y - x)) = f (\\theta x + (1 - \\theta) y),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.226,
                0.343,
                0.243
            ],
            "angle": 0,
            "content": "这说明 \\(f(x)\\) 是凸函数"
        },
        {
            "type": "image",
            "bbox": [
                0.72,
                0.227,
                0.738,
                0.24
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.26,
                0.557,
                0.277
            ],
            "angle": 0,
            "content": "这里给出实际中经常遇到的一些凸（凹）函数"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.293,
                0.376,
                0.309
            ],
            "angle": 0,
            "content": "例2.4 凸函数的例子："
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.324,
                0.736,
                0.362
            ],
            "angle": 0,
            "content": "(1) 仿射函数: \\(a^{\\mathrm{T}}x + b\\), 其中 \\(a, x \\in \\mathbb{R}^n\\) 是向量; \\(\\langle A, X \\rangle\\), 其中 \\(A, X \\in \\mathbb{R}^{m \\times n}\\) 是矩阵;"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.378,
                0.462,
                0.394
            ],
            "angle": 0,
            "content": "(2) 指数函数: \\(e^{ax}, a, x \\in \\mathbb{R}\\) 是凸函数;"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.411,
                0.592,
                0.427
            ],
            "angle": 0,
            "content": "(3) 幂函数: \\(x^{\\alpha} (x > 0)\\), 当 \\(\\alpha \\geqslant 1\\) 或 \\(\\alpha \\leqslant 0\\) 时为凸函数;"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.443,
                0.436,
                0.46
            ],
            "angle": 0,
            "content": "(4) 负熵: \\(x \\ln x (x > 0)\\) 是凸函数;"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.475,
                0.737,
                0.493
            ],
            "angle": 0,
            "content": "(5) 所有范数都是凸函数 (向量和矩阵版本), 这是由于范数有三角不等式."
        },
        {
            "type": "list",
            "bbox": [
                0.181,
                0.324,
                0.737,
                0.493
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.508,
                0.69,
                0.525
            ],
            "angle": 0,
            "content": "下面的例子说明如何利用定理2.8来判断一个函数是否为凸函数"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.54,
                0.633,
                0.558
            ],
            "angle": 0,
            "content": "例2.5 \\(f(X) = -\\ln \\operatorname{det} X\\) 是凸函数，其中 \\(\\mathbf{dom} f = S_{++}^{n}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.562,
                0.737,
                0.599
            ],
            "angle": 0,
            "content": "事实上，任取 \\(X\\succ 0\\) 以及方向 \\(V\\in \\mathcal{S}^n\\) ，将 \\(f\\) 限制在直线 \\(X + tV\\) （ \\(t\\) 满足 \\(X + tV\\succ 0\\) ）上，考虑函数 \\(g(t) = -\\mathrm{Ind}\\operatorname *{det}(X + tV)\\) .那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.281,
                0.613,
                0.626,
                0.674
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} g (t) = - \\ln \\det X - \\ln \\det \\left(I + t X ^ {- 1 / 2} V X ^ {- 1 / 2}\\right) \\\\ = - \\ln \\det X - \\sum_ {i = 1} ^ {n} \\ln (1 + t \\lambda_ {i}), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.687,
                0.735,
                0.725
            ],
            "angle": 0,
            "content": "其中 \\(\\lambda_{i}\\) 是 \\(X^{-1 / 2}VX^{-1 / 2}\\) 的第 \\(i\\) 个特征值．对每个 \\(X\\succ 0\\) 以及方向 \\(V\\) ， \\(g\\) 关于 \\(t\\) 是凸的．因此 \\(f\\) 是凸的."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.741,
                0.737,
                0.779
            ],
            "angle": 0,
            "content": "对于可微函数，除了将其限制在直线上之外，还可以利用其导数信息来判断它的凸性。具体来说，有如下的一阶条件："
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.794,
                0.737,
                0.83
            ],
            "angle": 0,
            "content": "定理2.9（一阶条件）对于定义在凸集上的可微函数 \\(f\\) ，\\(f\\) 是凸函数当且仅当"
        },
        {
            "type": "equation",
            "bbox": [
                0.273,
                0.835,
                0.634,
                0.855
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x) + \\nabla f (x) ^ {\\mathrm {T}} (y - x), \\quad \\forall x, y \\in \\mathbf {d o m} f.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "50"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.194
            ],
            "angle": 0,
            "content": "证明．先证必要性．设 \\(f\\) 是凸函数，则对于任意的 \\(x,y\\in \\mathbf{dom}f\\) 以及 \\(t\\in (0,1)\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.198,
                0.687,
                0.217
            ],
            "angle": 0,
            "content": "\\[\nt f (y) + (1 - t) f (x) \\geqslant f (x + t (y - x)).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.226,
                0.608,
                0.243
            ],
            "angle": 0,
            "content": "将上式移项，两边同时除以 \\(t\\) ，注意 \\(t > 0\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.393,
                0.253,
                0.69,
                0.286
            ],
            "angle": 0,
            "content": "\\[\nf (y) - f (x) \\geqslant \\frac {f (x + t (y - x)) - f (x)}{t}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.296,
                0.482,
                0.312
            ],
            "angle": 0,
            "content": "令 \\(t \\to 0\\)，由极限保号性可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.313,
                0.322,
                0.769,
                0.355
            ],
            "angle": 0,
            "content": "\\[\nf (y) - f (x) \\geqslant \\lim  _ {t \\rightarrow 0} \\frac {f (x + t (y - x)) - f (x)}{t} = \\nabla f (x) ^ {\\mathrm {T}} (y - x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.365,
                0.611,
                0.381
            ],
            "angle": 0,
            "content": "这里最后一个等式成立是由于方向导数的性质"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.386,
                0.825,
                0.424
            ],
            "angle": 0,
            "content": "再证充分性．对任意的 \\(x,y\\in \\mathbf{dom}f\\) 以及任意的 \\(t\\in (0,1)\\) ，定义 \\(z = tx + (1 - t)y\\) ，应用两次一阶条件我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.426,
                0.437,
                0.655,
                0.48
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (x) \\geqslant f (z) + \\nabla f (z) ^ {\\mathrm {T}} (x - z), \\\\ f (y) \\geqslant f (z) + \\nabla f (z) ^ {\\mathrm {T}} (y - z). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.493,
                0.825,
                0.51
            ],
            "angle": 0,
            "content": "将上述第一个不等式两边同时乘 \\(t\\) ，第二个不等式两边同时乘 \\(1 - t\\) ，相加得"
        },
        {
            "type": "equation",
            "bbox": [
                0.421,
                0.526,
                0.662,
                0.544
            ],
            "angle": 0,
            "content": "\\[\nt f (x) + (1 - t) f (y) \\geqslant f (z) + 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.56,
                0.56,
                0.576
            ],
            "angle": 0,
            "content": "这正是凸函数的定义，因此充分性成立"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.59,
                0.825,
                0.649
            ],
            "angle": 0,
            "content": "定理2.9说明可微凸函数 \\(f\\) 的图形始终在其任一点处切线的上方，见图2.12．因此，用可微凸函数 \\(f\\) 在任意一点处的一阶近似可以得到 \\(f\\) 的一个全局下界．另一个常用的一阶条件是梯度单调性."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.662,
                0.825,
                0.701
            ],
            "angle": 0,
            "content": "定理2.10（梯度单调性）设 \\(f\\) 为可微函数，则 \\(f\\) 为凸函数当且仅当 \\(\\operatorname{dom} f\\) 为凸集且 \\(\\nabla f\\) 为单调映射，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.358,
                0.715,
                0.725,
                0.735
            ],
            "angle": 0,
            "content": "\\[\n\\left(\\nabla f (x) - \\nabla f (y)\\right) ^ {\\mathrm {T}} (x - y) \\geqslant 0, \\quad \\forall x, y \\in \\mathbf {d o m} f.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.75,
                0.775,
                0.767
            ],
            "angle": 0,
            "content": "证明. 先证必要性. 若 \\(f\\) 可微且为凸函数, 根据一阶条件, 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.424,
                0.78,
                0.656,
                0.824
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (y) \\geqslant f (x) + \\nabla f (x) ^ {\\mathrm {T}} (y - x), \\\\ f (x) \\geqslant f (y) + \\nabla f (y) ^ {\\mathrm {T}} (x - y). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.578,
                0.853
            ],
            "angle": 0,
            "content": "将两式不等号左右两边相加即可得到结论"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.266,
                0.133
            ],
            "angle": 0,
            "content": "2.5 凸函数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.735,
                0.131
            ],
            "angle": 0,
            "content": "51"
        },
        {
            "type": "image",
            "bbox": [
                0.254,
                0.153,
                0.637,
                0.332
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.348,
                0.346,
                0.56,
                0.363
            ],
            "angle": 0,
            "content": "图2.12 凸函数的全局下界"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.389,
                0.605,
                0.406
            ],
            "angle": 0,
            "content": "再证充分性. 若 \\(\\nabla f\\) 为单调映射, 构造一元辅助函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.23,
                0.418,
                0.675,
                0.438
            ],
            "angle": 0,
            "content": "\\[\ng (t) = f (x + t (y - x)), \\quad g ^ {\\prime} (t) = \\nabla f (x + t (y - x)) ^ {T} (y - x)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.45,
                0.538,
                0.468
            ],
            "angle": 0,
            "content": "由 \\(\\nabla f\\) 的单调性可知 \\(g^{\\prime}(t)\\geqslant g^{\\prime}(0),\\forall t\\geqslant 0.\\) 因此"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.477,
                0.625,
                0.534
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (y) = g (1) = g (0) + \\int_ {0} ^ {1} g ^ {\\prime} (t) d t \\\\ \\geqslant g (0) + g ^ {\\prime} (0) = f (x) + \\nabla f (x) ^ {\\mathrm {T}} (y - x). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.547,
                0.643,
                0.563
            ],
            "angle": 0,
            "content": "和凸函数类似，严格凸函数和强凸函数都有对应的单调性。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.576,
                0.58,
                0.594
            ],
            "angle": 0,
            "content": "推论2.2 设 \\(f\\) 为可微函数，且 \\(\\mathbf{dom} f\\) 是凸集，则"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.605,
                0.401,
                0.621
            ],
            "angle": 0,
            "content": "(1) \\(f\\) 是严格凸函数当且仅当"
        },
        {
            "type": "equation",
            "bbox": [
                0.29,
                0.635,
                0.659,
                0.655
            ],
            "angle": 0,
            "content": "\\[\n\\left(\\nabla f (x) - \\nabla f (y)\\right) ^ {\\mathrm {T}} (x - y) > 0, \\quad \\forall x, y \\in \\mathbf {d o m} f;\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.672,
                0.409,
                0.688
            ],
            "angle": 0,
            "content": "(2) \\(f\\) 是 \\(m\\)-强凸函数当且仅当"
        },
        {
            "type": "equation",
            "bbox": [
                0.256,
                0.702,
                0.692,
                0.722
            ],
            "angle": 0,
            "content": "\\[\n\\left(\\nabla f (x) - \\nabla f (y)\\right) ^ {\\mathrm {T}} (x - y) \\geqslant m \\| x - y \\| ^ {2}, \\quad \\forall x, y \\in \\mathbf {d o m} f.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.739,
                0.715,
                0.756
            ],
            "angle": 0,
            "content": "进一步地，如果函数二阶连续可微，我们可以得到下面的二阶条件："
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.768,
                0.739,
                0.804
            ],
            "angle": 0,
            "content": "定理2.11（二阶条件）设 \\(f\\) 为定义在凸集上的二阶连续可微函数，则 \\(f\\) 是凸函数当且仅当"
        },
        {
            "type": "equation",
            "bbox": [
                0.35,
                0.808,
                0.557,
                0.827
            ],
            "angle": 0,
            "content": "\\[\n\\nabla^ {2} f (x) \\succeq 0, \\quad \\forall x \\in \\mathbf {d o m} f.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.56,
                0.854
            ],
            "angle": 0,
            "content": "如果 \\(\\nabla^2 f(x) \\succ 0, \\forall x \\in \\mathbf{dom}f\\)，则 \\(f\\) 是严格凸函数。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "52"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.826,
                0.214
            ],
            "angle": 0,
            "content": "证明．先证必要性．反设 \\(f(x)\\) 在点 \\(x\\) 处的海瑟矩阵 \\(\\nabla^2 f(x) \\neq 0\\) ，即存在非零向量 \\(v \\in \\mathbb{R}^n\\) 使得 \\(v^{\\mathrm{T}} \\nabla^2 f(x)v < 0\\) . 根据佩亚诺（Peano）余项的泰勒展开，"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.215,
                0.745,
                0.245
            ],
            "angle": 0,
            "content": "\\[\nf (x + t v) = f (x) + t \\nabla f (x) ^ {\\mathrm {T}} v + \\frac {t ^ {2}}{2} v ^ {\\mathrm {T}} \\nabla^ {2} f (x) v + o (t ^ {2}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.247,
                0.477,
                0.264
            ],
            "angle": 0,
            "content": "移项后等式两边同时除以 \\(t^2\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.341,
                0.27,
                0.741,
                0.303
            ],
            "angle": 0,
            "content": "\\[\n\\frac {f (x + t v) - f (x) - t \\nabla f (x) ^ {\\mathrm {T}} v}{t ^ {2}} = \\frac {1}{2} v ^ {\\mathrm {T}} \\nabla^ {2} f (x) v + o (1).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.31,
                0.371,
                0.326
            ],
            "angle": 0,
            "content": "当 \\(t\\) 充分小时，"
        },
        {
            "type": "equation",
            "bbox": [
                0.41,
                0.323,
                0.675,
                0.356
            ],
            "angle": 0,
            "content": "\\[\n\\frac {f (x + t v) - f (x) - t \\nabla f (x) ^ {\\mathrm {T}} v}{t ^ {2}} <   0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.36,
                0.749,
                0.377
            ],
            "angle": 0,
            "content": "这显然和一阶条件（定理2.9）矛盾，因此必有 \\(\\nabla^2 f(x) \\succeq 0\\) 成立。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.381,
                0.826,
                0.418
            ],
            "angle": 0,
            "content": "再证充分性. 设 \\(f(x)\\) 满足二阶条件 \\(\\nabla^2 f(x) \\succeq 0\\), 对任意 \\(x, y \\in \\mathbf{dom} f\\), 根据泰勒展开（定理 2.1）,"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.424,
                0.804,
                0.455
            ],
            "angle": 0,
            "content": "\\[\nf (y) = f (x) + \\nabla f (x) ^ {\\mathrm {T}} (y - x) + \\frac {1}{2} (y - x) ^ {\\mathrm {T}} \\nabla^ {2} f (x + t (y - x)) (y - x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.461,
                0.825,
                0.479
            ],
            "angle": 0,
            "content": "其中 \\(t \\in (0,1)\\) 是和 \\(x, y\\) 有关的常数. 由半正定性可知对任意 \\(x, y \\in \\mathbf{dom} f\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.424,
                0.489,
                0.657,
                0.509
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x) + \\nabla f (x) ^ {\\mathrm {T}} (y - x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.519,
                0.826,
                0.578
            ],
            "angle": 0,
            "content": "这是凸函数判定的一阶条件, 由定理2.9知 \\(f\\) 为凸函数. 进一步, 若 \\(\\nabla^2 f(x) > 0\\), 上式中不等号严格成立 \\((x \\neq y)\\). 利用定理2.9的充分性的证明过程可得 \\(f(x)\\) 为严格凸函数. \\(\\square\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.591,
                0.825,
                0.629
            ],
            "angle": 0,
            "content": "当函数二阶连续可微时，利用二阶条件判断凸性通常更为方便。下面给出两个用二阶条件判断凸性的例子。"
        },
        {
            "type": "title",
            "bbox": [
                0.293,
                0.64,
                0.342,
                0.655
            ],
            "angle": 0,
            "content": "例2.6"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.658,
                0.825,
                0.704
            ],
            "angle": 0,
            "content": "(1) 考虑二次函数 \\( f(x) = \\frac{1}{2} x^{\\mathrm{T}}Px + q^{\\mathrm{T}}x + r (P \\in S^{n}) \\)，容易计算出其梯度与海瑟矩阵分别为"
        },
        {
            "type": "equation",
            "bbox": [
                0.44,
                0.715,
                0.684,
                0.734
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x) = P x + q, \\quad \\nabla^ {2} f (x) = P.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.746,
                0.559,
                0.763
            ],
            "angle": 0,
            "content": "那么， \\(f\\) 是凸函数当且仅当 \\(P\\succeq 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.769,
                0.797,
                0.801
            ],
            "angle": 0,
            "content": "(2) 考虑最小二乘函数 \\( f(x) = \\frac{1}{2} \\| Ax - b \\|_2^2 \\)，其梯度与海瑟矩阵分别为"
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.806,
                0.715,
                0.825
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x) = A ^ {\\mathrm {T}} (A x - b), \\quad \\nabla^ {2} f (x) = A ^ {\\mathrm {T}} A.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.836,
                0.796,
                0.853
            ],
            "angle": 0,
            "content": "注意到 \\(A^{\\mathrm{T}}A\\) 恒为半正定矩阵，因此，对任意的 \\(A\\) ，\\(f\\) 都是凸函数."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.266,
                0.133
            ],
            "angle": 0,
            "content": "2.5 凸函数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "53"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.737,
                0.195
            ],
            "angle": 0,
            "content": "除了上述结果之外，还可以使用上方图epif来判断 \\(f\\) 的凸性．实际上我们有如下定理："
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.205,
                0.677,
                0.222
            ],
            "angle": 0,
            "content": "定理2.12 函数 \\(f(x)\\) 为凸函数当且仅当其上方图epi \\(f\\) 是凸集"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.231,
                0.443,
                0.248
            ],
            "angle": 0,
            "content": "定理2.12的证明留给读者完成"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.273,
                0.334,
                0.291
            ],
            "angle": 0,
            "content": "2.5.3 保凸的运算"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.304,
                0.741,
                0.406
            ],
            "angle": 0,
            "content": "要验证一个函数 \\(f\\) 是凸函数，前面已经介绍了三种方法：一是用定义去验证凸性，通常将函数限制在一条直线上；二是利用一阶条件、二阶条件证明函数的凸性；三是直接研究 \\(f\\) 的上方图epif．而接下来要介绍的方法说明 \\(f\\) 可由简单的凸函数通过一些保凸的运算得到．下面的定理说明非负加权和、与仿射函数的复合、逐点取最大值等运算，是不改变函数的凸性的."
        },
        {
            "type": "title",
            "bbox": [
                0.205,
                0.414,
                0.28,
                0.43
            ],
            "angle": 0,
            "content": "定理2.13"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.441,
                0.551,
                0.457
            ],
            "angle": 0,
            "content": "(1) 若 \\(f\\) 是凸函数，则 \\(\\alpha f\\) 是凸函数，其中 \\(\\alpha \\geqslant 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.469,
                0.523,
                0.487
            ],
            "angle": 0,
            "content": "(2) 若 \\( f_{1} \\)，\\( f_{2} \\) 是凸函数，则 \\( f_{1} + f_{2} \\) 是凸函数."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.498,
                0.512,
                0.515
            ],
            "angle": 0,
            "content": "(3) 若 \\( f \\) 是凸函数，则 \\( f(Ax + b) \\) 是凸函数"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.526,
                0.734,
                0.562
            ],
            "angle": 0,
            "content": "(4) 若 \\(f_{1}, f_{2}, \\cdots, f_{m}\\) 是凸函数，则 \\(f(x) = \\max \\{f_{1}(x), f_{2}(x), \\cdots, f_{m}(x)\\}\\) 是凸函数."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.576,
                0.557,
                0.593
            ],
            "angle": 0,
            "content": "(5) 若对每个 \\(y \\in \\mathcal{A}\\), \\(f(x, y)\\) 关于 \\(x\\) 是凸函数, 则"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.441,
                0.734,
                0.593
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.604,
                0.544,
                0.633
            ],
            "angle": 0,
            "content": "\\[\ng(x) = \\sup_{y\\in \\mathcal{A}}f(x,y)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.64,
                0.29,
                0.655
            ],
            "angle": 0,
            "content": "是凸函数."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.668,
                0.737,
                0.726
            ],
            "angle": 0,
            "content": "(6) 给定函数 \\(g: \\mathbb{R}^n \\to \\mathbb{R}\\) 和 \\(h: \\mathbb{R} \\to \\mathbb{R}\\), 令 \\(f(x) = h(g(x))\\). 若 \\(g\\) 是凸函数, \\(h\\) 是凸函数且单调不减, 那么 \\(f\\) 是凸函数; 若 \\(g\\) 是凹函数, \\(h\\) 是凸函数且单调不增, 那么 \\(f\\) 是凸函数."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.738,
                0.482,
                0.755
            ],
            "angle": 0,
            "content": "(7) 给定函数 \\(g\\colon \\mathbb{R}^n\\to \\mathbb{R}^k\\) ， \\(h\\colon \\mathbb{R}^k\\to \\mathbb{R},\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.668,
                0.737,
                0.755
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.306,
                0.767,
                0.639,
                0.786
            ],
            "angle": 0,
            "content": "\\[\nf (x) = h (g (x)) = h \\left(g _ {1} (x), g _ {2} (x), \\dots , g _ {k} (x)\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.795,
                0.737,
                0.853
            ],
            "angle": 0,
            "content": "若 \\(g_{i}\\) 是凸函数, \\(h\\) 是凸函数且关于每个分量单调不减, 那么 \\(f\\) 是凸函数; 若 \\(g_{i}\\) 是凹函数, \\(h\\) 是凸函数且关于每个分量单调不增, 那么 \\(f\\) 是凸函数."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "54"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.157,
                0.684,
                0.174
            ],
            "angle": 0,
            "content": "(8) 若 \\( f(x, y) \\) 关于 \\( (x, y) \\) 整体是凸函数，\\( C \\) 是凸集，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.494,
                0.189,
                0.629,
                0.214
            ],
            "angle": 0,
            "content": "\\[\ng (x) = \\inf  _ {y \\in C} f (x, y)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.226,
                0.378,
                0.241
            ],
            "angle": 0,
            "content": "是凸函数."
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.256,
                0.677,
                0.273
            ],
            "angle": 0,
            "content": "(9) 定义函数 \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) 的透视函数 \\(g: \\mathbb{R}^n \\times \\mathbb{R} \\to \\mathbb{R}\\),"
        },
        {
            "type": "equation",
            "bbox": [
                0.35,
                0.283,
                0.772,
                0.312
            ],
            "angle": 0,
            "content": "\\[\ng (x, t) = t f \\left(\\frac {x}{t}\\right), \\quad \\mathbf {d o m} g = \\left\\{\\left(x, t\\right) \\mid \\frac {x}{t} \\in \\mathbf {d o m} f, t > 0 \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.322,
                0.534,
                0.339
            ],
            "angle": 0,
            "content": "若 \\(f\\) 是凸函数，则 \\(g\\) 是凸函数"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.351,
                0.779,
                0.369
            ],
            "angle": 0,
            "content": "证明．我们只对其中的(4)(5)(8)进行证明，剩下的读者可自行验证"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.385,
                0.719,
                0.402
            ],
            "angle": 0,
            "content": "(4) 我们只对 \\(m = 2\\) 的情况验证, 一般情况下同理可证. 设"
        },
        {
            "type": "equation",
            "bbox": [
                0.46,
                0.417,
                0.663,
                0.436
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\max  \\left\\{f _ {1} (x), f _ {2} (x) \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.45,
                0.634,
                0.467
            ],
            "angle": 0,
            "content": "对任意的 \\(0 \\leqslant \\theta \\leqslant 1\\) 和 \\(x, y \\in \\mathbf{dom} f\\)，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.48,
                0.825,
                0.549
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (\\theta x + (1 - \\theta) y) = \\max  \\left\\{f _ {1} (\\theta x + (1 - \\theta) y), f _ {2} (\\theta x + (1 - \\theta) y) \\right\\} \\\\ \\leqslant \\max  \\left\\{\\theta f _ {1} (x) + (1 - \\theta) f _ {1} (y), \\theta f _ {2} (x) + (1 - \\theta) f _ {2} (y) \\right\\} \\\\ \\leqslant \\theta f (x) + (1 - \\theta) f (y), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.561,
                0.824,
                0.599
            ],
            "angle": 0,
            "content": "其中第一个不等式是 \\( f_{1} \\) 和 \\( f_{2} \\) 的凸性，第二个不等式是将 \\( f_{1}(x) \\) 和 \\( f_{2}(x) \\) 放大为 \\( f(x) \\)。所以 \\( f \\) 是凸函数。"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.612,
                0.824,
                0.629
            ],
            "angle": 0,
            "content": "(5) 可以直接仿照 (4) 的证明进行验证. 也可利用上方图的性质. 不难看出"
        },
        {
            "type": "equation",
            "bbox": [
                0.477,
                0.643,
                0.646,
                0.675
            ],
            "angle": 0,
            "content": "\\[\n\\mathbf {e p i} g = \\bigcap_ {y \\in \\mathcal {A}} \\mathbf {e p i} f (\\cdot , y).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.687,
                0.825,
                0.725
            ],
            "angle": 0,
            "content": "由于任意多个凸集的交集还是凸集，所以epig是凸集，根据上方图的性质容易推出 \\(g\\) 是凸函数."
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.738,
                0.731,
                0.755
            ],
            "angle": 0,
            "content": "(8) 仍然根据定义进行验证. 任取 \\(\\theta \\in (0,1)\\) 以及 \\(x_{1}, x_{2}\\), 要证"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.77,
                0.727,
                0.789
            ],
            "angle": 0,
            "content": "\\[\ng \\left(\\theta x _ {1} + (1 - \\theta) x _ {2}\\right) \\leqslant \\theta g \\left(x _ {1}\\right) + (1 - \\theta) g \\left(x _ {2}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.804,
                0.678,
                0.821
            ],
            "angle": 0,
            "content": "由 \\(g\\) 的定义知对任意 \\(\\varepsilon > 0\\)，存在 \\(y_1, y_2 \\in C\\)，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.446,
                0.836,
                0.677,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x _ {i}, y _ {i}\\right) <   g \\left(x _ {i}\\right) + \\varepsilon , \\quad i = 1, 2.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.266,
                0.133
            ],
            "angle": 0,
            "content": "2.5 凸函数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "55"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.157,
                0.251,
                0.174
            ],
            "angle": 0,
            "content": "因此"
        },
        {
            "type": "equation",
            "bbox": [
                0.261,
                0.185,
                0.684,
                0.284
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} g \\left(\\theta x _ {1} + (1 - \\theta) x _ {2}\\right) = \\inf  _ {y \\in C} f \\left(\\theta x _ {1} + (1 - \\theta) x _ {2}, y\\right) \\\\ \\leqslant f \\left(\\theta x _ {1} + (1 - \\theta) x _ {2}, \\theta y _ {1} + (1 - \\theta) y _ {2}\\right) \\\\ \\leqslant \\theta f \\left(x _ {1}, y _ {1}\\right) + (1 - \\theta) f \\left(x _ {2}, y _ {2}\\right) \\\\ \\leqslant \\theta g \\left(x _ {1}\\right) + (1 - \\theta) g \\left(x _ {2}\\right) + \\varepsilon , \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.209,
                0.295,
                0.737,
                0.332
            ],
            "angle": 0,
            "content": "其中第一个不等号是利用了 \\(C\\) 的凸性, 第二个不等号利用了 \\(f(x, y)\\) 的凸性. 最后令 \\(\\varepsilon\\) 趋于 0 可以得到最终结论."
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.347,
                0.592,
                0.364
            ],
            "angle": 0,
            "content": "下面是一些利用保凸运算证明函数是凸函数的例子"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.377,
                0.599,
                0.393
            ],
            "angle": 0,
            "content": "例2.7 利用与仿射函数的复合函数保凸，可以证明："
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.406,
                0.428,
                0.422
            ],
            "angle": 0,
            "content": "(1) 线性不等式的对数障碍函数："
        },
        {
            "type": "equation",
            "bbox": [
                0.227,
                0.432,
                0.717,
                0.468
            ],
            "angle": 0,
            "content": "\\[\nf (x) = - \\sum_ {i = 1} ^ {m} \\ln \\left(b _ {i} - a _ {i} ^ {\\mathrm {T}} x\\right), \\quad \\mathbf {d o m} f = \\left\\{x \\mid a _ {i} ^ {\\mathrm {T}} x <   b _ {i}, i = 1, 2, \\dots , m \\right\\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.478,
                0.29,
                0.493
            ],
            "angle": 0,
            "content": "是凸函数."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.513,
                0.643,
                0.53
            ],
            "angle": 0,
            "content": "(2) 仿射函数的（任意）范数：\\(f(x) = \\|Ax + b\\|\\) 都是凸函数."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.543,
                0.531,
                0.559
            ],
            "angle": 0,
            "content": "例 2.8 利用逐点取最大值保凸，可以证明："
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.571,
                0.599,
                0.597
            ],
            "angle": 0,
            "content": "(1) 分段线性函数: \\( f(x) = \\max_{i=1,2,\\dots,m} (a_i^{\\mathrm{T}}x + b_i) \\) 是凸函数."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.607,
                0.446,
                0.623
            ],
            "angle": 0,
            "content": "(2) \\(x \\in \\mathbb{R}^n\\) 的前 \\(r\\) 个最大分量之和："
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.571,
                0.599,
                0.623
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.366,
                0.638,
                0.58,
                0.657
            ],
            "angle": 0,
            "content": "\\[\nf (x) = x _ {[ 1 ]} + x _ {[ 2 ]} + \\dots + x _ {[ r ]}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.209,
                0.67,
                0.737,
                0.707
            ],
            "angle": 0,
            "content": "是凸函数 \\((x_{[i]}\\) 为 \\(x\\) 的从大到小排列的第 \\(i\\) 个分量). 事实上, \\(f(x)\\) 可以写成如下多个线性函数取最大值的形式:"
        },
        {
            "type": "equation",
            "bbox": [
                0.251,
                0.721,
                0.695,
                0.74
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\max  \\left\\{x _ {i _ {1}} + x _ {i _ {2}} + \\dots + x _ {i _ {r}} \\mid 1 \\leqslant i _ {1} <   i _ {2} <   \\dots <   i _ {r} \\leqslant n \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.758,
                0.531,
                0.775
            ],
            "angle": 0,
            "content": "例 2.9 利用逐点取上确界保凸, 可以证明:"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.787,
                0.573,
                0.816
            ],
            "angle": 0,
            "content": "(1) 集合 \\(C\\) 的支撑函数: \\(S_{C}(x) = \\sup_{y \\in C} y^{\\mathrm{T}} x\\) 是凸函数."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.826,
                0.717,
                0.855
            ],
            "angle": 0,
            "content": "(2) 集合 \\(C\\) 的点到给定点 \\(x\\) 的最远距离: \\(f(x) = \\sup_{y \\in C} \\| x - y \\|\\) 是凸函数."
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.787,
                0.717,
                0.855
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "56"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.157,
                0.484,
                0.174
            ],
            "angle": 0,
            "content": "(3) 对称矩阵的最大特征值："
        },
        {
            "type": "equation",
            "bbox": [
                0.439,
                0.188,
                0.686,
                0.22
            ],
            "angle": 0,
            "content": "\\[\n\\lambda_{\\max}(X) = \\sup_{\\| y\\|_{2} = 1}y^{\\mathrm{T}}Xy, \\quad X\\in \\mathcal{S}^{n}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.23,
                0.38,
                0.246
            ],
            "angle": 0,
            "content": "是凸函数."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.26,
                0.644,
                0.277
            ],
            "angle": 0,
            "content": "例2.10 利用复合函数的保凸性质，可以证明："
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.291,
                0.644,
                0.309
            ],
            "angle": 0,
            "content": "(1) 如果 \\( g(x) \\) 是凸函数, 则 \\( \\exp(g(x)) \\) 是凸函数;"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.318,
                0.617,
                0.354
            ],
            "angle": 0,
            "content": "(2) 如果 \\( g \\) 是正值凹函数, 则 \\( \\frac{1}{g(x)} \\) 是凸函数;"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.364,
                0.672,
                0.401
            ],
            "angle": 0,
            "content": "(3) 如果 \\(g_{i}\\) 是正值凹函数，则 \\(\\sum_{i=1}^{m} \\ln(g_{i}(x))\\) 是凹函数；"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.412,
                0.663,
                0.449
            ],
            "angle": 0,
            "content": "(4) 如果 \\(g_{i}\\) 是凸函数, 则 \\(\\ln \\sum_{i=1}^{m} \\exp(g_{i}(x))\\) 是凸函数."
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.291,
                0.672,
                0.449
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.456,
                0.594,
                0.473
            ],
            "angle": 0,
            "content": "例2.11 利用取下确界保凸，可以证明："
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.485,
                0.825,
                0.523
            ],
            "angle": 0,
            "content": "(1) 考虑函数 \\( f(x, y) = x^{\\mathrm{T}}Ax + 2x^{\\mathrm{T}}By + y^{\\mathrm{T}}Cy \\)，其中 \\( A \\in S^{m}, B \\in \\mathbb{R}^{m \\times n}, C \\in S^{n} \\)。其海瑟矩阵若满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.477,
                0.533,
                0.652,
                0.579
            ],
            "angle": 0,
            "content": "\\[\n\\left[ \\begin{array}{c c} A & B \\\\ B ^ {\\mathrm {T}} & C \\end{array} \\right] \\succeq 0, \\quad C \\succ 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.588,
                0.587,
                0.606
            ],
            "angle": 0,
            "content": "则 \\(f(x,y)\\) 为凸函数．对 \\(y\\) 求最小值得"
        },
        {
            "type": "equation",
            "bbox": [
                0.414,
                0.619,
                0.71,
                0.645
            ],
            "angle": 0,
            "content": "\\[\ng (x) = \\inf  _ {y} f (x, y) = x ^ {\\mathrm {T}} \\left(A - B C ^ {- 1} B ^ {\\mathrm {T}}\\right) x,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.657,
                0.825,
                0.695
            ],
            "angle": 0,
            "content": "因此 \\(g\\) 是凸函数．进一步地，根据凸函数判定的二阶条件可以得到\\(A - BC^{-1}B^{\\mathrm{T}}\\succeq 0\\) ，这也称为 \\(A\\) 的Schur补（见附录B.1.9)."
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.71,
                0.721,
                0.734
            ],
            "angle": 0,
            "content": "(2) 点 \\(x\\) 到凸集 \\(S\\) 的距离: \\(\\operatorname{dist}(x, S) = \\inf_{y \\in S} \\| x - y \\|\\) 是凸函数."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.755,
                0.44,
                0.773
            ],
            "angle": 0,
            "content": "2.5.4 凸函数的性质"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.786,
                0.347,
                0.803
            ],
            "angle": 0,
            "content": "1. 连续性"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "凸函数不一定是连续函数，但下面这个定理说明凸函数在定义域中内点处是连续的."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.266,
                0.133
            ],
            "angle": 0,
            "content": "2.5 凸函数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "57"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.738,
                0.195
            ],
            "angle": 0,
            "content": "定理2.14 设 \\(f: \\mathbb{R}^n \\to (-\\infty, +\\infty]\\) 为凸函数。对任意点 \\(x_0 \\in \\operatorname{intdom} f\\)，有 \\(f\\) 在点 \\(x_0\\) 处连续。这里 \\(\\operatorname{intdom} f\\) 表示定义域 \\(\\operatorname{dom} f\\) 的内点。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.206,
                0.737,
                0.244
            ],
            "angle": 0,
            "content": "定理2.14的证明见[175]定理1.3.12．此定理表明凸函数“差不多”是连续的，它的一个直接推论为："
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.256,
                0.737,
                0.293
            ],
            "angle": 0,
            "content": "推论2.3 设 \\(f(x)\\) 是凸函数，且 \\(\\operatorname{dom} f\\) 是开集，则 \\(f(x)\\) 在 \\(\\operatorname{dom} f\\) 上是连续的。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.306,
                0.738,
                0.323
            ],
            "angle": 0,
            "content": "证明. 由于开集中所有的点都为内点, 利用定理 2.14 可直接得到结论. \\(\\square\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.336,
                0.595,
                0.353
            ],
            "angle": 0,
            "content": "凸函数在定义域的边界上可能不连续．一个例子为："
        },
        {
            "type": "equation",
            "bbox": [
                0.375,
                0.361,
                0.53,
                0.412
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\left\\{ \\begin{array}{l l} 0, & x <   0, \\\\ 1, & x = 0. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.421,
                0.737,
                0.438
            ],
            "angle": 0,
            "content": "其中 \\(\\mathbf{dom}f = (-\\infty ,0]\\) .容易证明 \\(f(x)\\) 是凸函数，但其在点 \\(x = 0\\) 处不连续"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.462,
                0.292,
                0.478
            ],
            "angle": 0,
            "content": "2. 凸下水平集"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.492,
                0.579,
                0.509
            ],
            "angle": 0,
            "content": "凸函数的所有下水平集都为凸集，即有如下结果："
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.521,
                0.711,
                0.567
            ],
            "angle": 0,
            "content": "命题2.4设 \\(f(x)\\) 是凸函数，则 \\(f(x)\\) 所有的 \\(\\alpha\\) -下水平集 \\(C_{\\alpha}\\) 为凸集证明．任取 \\(x_{1},x_{2}\\in C_{\\alpha}\\) ，对任意的 \\(\\theta \\in (0,1)\\) ，根据 \\(f(x)\\) 的凸性我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.288,
                0.578,
                0.617,
                0.62
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f \\left(\\theta x _ {1} + (1 - \\theta) x _ {2}\\right) \\leqslant \\theta f \\left(x _ {1}\\right) + (1 - \\theta) f \\left(x _ {2}\\right) \\\\ \\leqslant \\theta \\alpha + (1 - \\theta) \\alpha = \\alpha . \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.632,
                0.309,
                0.648
            ],
            "angle": 0,
            "content": "这说明 \\(C_{\\alpha}\\) 是凸集"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.662,
                0.737,
                0.699
            ],
            "angle": 0,
            "content": "需要注意的是，上述命题的逆命题不成立，即任意下水平集为凸集的函数不一定是凸函数。读者可自行举出反例。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.723,
                0.274,
                0.739
            ],
            "angle": 0,
            "content": "3. 二次下界"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.753,
                0.438,
                0.77
            ],
            "angle": 0,
            "content": "强凸函数具有二次下界的性质."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.782,
                0.737,
                0.819
            ],
            "angle": 0,
            "content": "引理2.2（二次下界）设 \\(f(x)\\) 是参数为 \\(m\\) 的可微强凸函数，则如下不等式成立："
        },
        {
            "type": "equation",
            "bbox": [
                0.195,
                0.828,
                0.737,
                0.857
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x) + \\nabla f (x) ^ {\\mathrm {T}} (y - x) + \\frac {m}{2} \\| y - x \\| ^ {2}, \\quad \\forall x, y \\in \\mathbf {d o m} f. \\tag {2.5.1}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "58"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.155,
                0.826,
                0.197
            ],
            "angle": 0,
            "content": "证明．由强凸函数的定义， \\(g(x) = f(x) - \\frac{m}{2}\\| x\\|^2\\) 是凸函数，根据凸函数的一阶条件可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.425,
                0.2,
                0.657,
                0.219
            ],
            "angle": 0,
            "content": "\\[\ng (y) \\geqslant g (x) + \\nabla g (x) ^ {\\mathrm {T}} (y - x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.228,
                0.28,
                0.243
            ],
            "angle": 0,
            "content": "即"
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.251,
                0.825,
                0.311
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (y) \\geqslant f (x) - \\frac {m}{2} \\| x \\| ^ {2} + \\frac {m}{2} \\| y \\| ^ {2} + (\\nabla f (x) - m x) ^ {\\mathrm {T}} (y - x) \\\\ = f (x) + \\nabla f (x) ^ {\\mathrm {T}} (y - x) + \\frac {m}{2} \\| y - x \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.317,
                0.825,
                0.355
            ],
            "angle": 0,
            "content": "利用二次下界容易推出可微强凸函数的下水平集都是有界的，证明留给读者完成."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.366,
                0.757,
                0.382
            ],
            "angle": 0,
            "content": "推论2.4 设 \\(f\\) 为可微强凸函数，则 \\(f\\) 的所有 \\(\\alpha\\)-下水平集有界。"
        },
        {
            "type": "title",
            "bbox": [
                0.462,
                0.413,
                0.621,
                0.433
            ],
            "angle": 0,
            "content": "2.6 共轭函数"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.453,
                0.518,
                0.47
            ],
            "angle": 0,
            "content": "2.6.1 共轭函数的定义和例子"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.484,
                0.825,
                0.522
            ],
            "angle": 0,
            "content": "共轭函数是凸分析中的一个重要概念，其在凸优化问题的理论与算法中扮演着重要角色."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.533,
                0.722,
                0.549
            ],
            "angle": 0,
            "content": "定义2.19（共轭函数）任一适当函数 \\(f\\) 的共轭函数定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.43,
                0.561,
                0.826,
                0.591
            ],
            "angle": 0,
            "content": "\\[\nf ^ {*} (y) = \\sup  _ {x \\in \\operatorname {d o m} f} \\left\\{y ^ {\\mathrm {T}} x - f (x) \\right\\}. \\tag {2.6.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.599,
                0.827,
                0.7
            ],
            "angle": 0,
            "content": "设 \\(f\\) 为 \\(\\mathbb{R}\\) 上的适当函数，图2.13展示了对固定的 \\(y\\) ， \\(f^{*}(y)\\) 的几何意义在这里注意共轭函数是广义实值函数， \\(f^{*}(y)\\) 可以是正无穷．自然地，我们规定其定义域 \\(\\operatorname {dom}f^{*}\\) 为使得 \\(f^{*}(y)\\) 有限的 \\(y\\) 组成的集合．对任意函数 \\(f\\) 都可以定义共轭函数（不要求 \\(f\\) 是凸的），根据定理2.13的(5)，共轭函数 \\(f^{*}\\) 恒为凸函数．由共轭函数的定义，有如下的重要不等式："
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.71,
                0.489,
                0.727
            ],
            "angle": 0,
            "content": "命题2.5(Fenchel不等式)"
        },
        {
            "type": "equation",
            "bbox": [
                0.464,
                0.738,
                0.826,
                0.758
            ],
            "angle": 0,
            "content": "\\[\nf (x) + f ^ {*} (y) \\geqslant x ^ {\\mathrm {T}} y. \\tag {2.6.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.77,
                0.623,
                0.787
            ],
            "angle": 0,
            "content": "证明. 由定义立即得出，对任意的 \\(x \\in \\mathbf{dom} f\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.379,
                0.798,
                0.704,
                0.829
            ],
            "angle": 0,
            "content": "\\[\nf ^ {*} (y) = \\sup  _ {x \\in \\mathbf {d o m} f} \\left\\{y ^ {\\mathrm {T}} x - f (x) \\right\\} \\geqslant y ^ {\\mathrm {T}} x - f (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.408,
                0.853
            ],
            "angle": 0,
            "content": "整理即得(2.6.2)式"
        },
        {
            "type": "text",
            "bbox": [
                0.808,
                0.838,
                0.826,
                0.851
            ],
            "angle": 0,
            "content": "□"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "2.6 共轭函数"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "59"
        },
        {
            "type": "image",
            "bbox": [
                0.284,
                0.152,
                0.633,
                0.343
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.382,
                0.353,
                0.527,
                0.37
            ],
            "angle": 0,
            "content": "图2.13 共轭函数"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.398,
                0.506,
                0.415
            ],
            "angle": 0,
            "content": "以下我们给出一些常见函数的共轭函数"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.429,
                0.462,
                0.446
            ],
            "angle": 0,
            "content": "例2.12（二次函数） 考虑二次函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.355,
                0.456,
                0.551,
                0.487
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\frac {1}{2} x ^ {\\mathrm {T}} A x + b ^ {\\mathrm {T}} x + c.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.498,
                0.363,
                0.515
            ],
            "angle": 0,
            "content": "(1) 强凸情形 \\((A\\succ 0)\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.525,
                0.605,
                0.556
            ],
            "angle": 0,
            "content": "\\[\nf ^ {*} (y) = \\frac {1}{2} (y - b) ^ {\\mathrm {T}} A ^ {- 1} (y - b) - c;\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.572,
                0.381,
                0.589
            ],
            "angle": 0,
            "content": "(2) 一般凸情形 \\((A\\succeq 0)\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.264,
                0.599,
                0.685,
                0.632
            ],
            "angle": 0,
            "content": "\\[\nf ^ {*} (y) = \\frac {1}{2} (y - b) ^ {\\mathrm {T}} A ^ {\\dagger} (y - b) - c, \\quad \\mathbf {d o m} f ^ {*} = \\mathcal {R} (A) + b\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.642,
                0.416,
                0.658
            ],
            "angle": 0,
            "content": "这里 \\(\\mathcal{R}(A)\\) 为 \\(A\\) 的像空间."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.673,
                0.614,
                0.69
            ],
            "angle": 0,
            "content": "例 2.13 (凸集的示性函数) 给定凸集 \\(C\\)，其示性函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.363,
                0.701,
                0.545,
                0.752
            ],
            "angle": 0,
            "content": "\\[\nI _ {C} (x) = \\left\\{ \\begin{array}{l l} 0, & x \\in C, \\\\ + \\infty , & x \\notin C. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.765,
                0.347,
                0.78
            ],
            "angle": 0,
            "content": "可知对应的共轭函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.312,
                0.796,
                0.597,
                0.825
            ],
            "angle": 0,
            "content": "\\[\nI _ {C} ^ {*} (y) = \\sup  _ {x} \\left\\{y ^ {T} x - I _ {C} (x) \\right\\} = \\sup  _ {x \\in C} y ^ {T} x.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.456,
                0.854
            ],
            "angle": 0,
            "content": "这里 \\(I_{C}^{*}(y)\\) 又称为凸集 \\(C\\) 的支撑函数"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "60"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.194
            ],
            "angle": 0,
            "content": "例2.14（范数）范数的共轭函数为其单位对偶范数球的示性函数，即若 \\( f(x) = \\| x\\| \\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.429,
                0.195,
                0.652,
                0.245
            ],
            "angle": 0,
            "content": "\\[\nf ^ {*} (y) = \\left\\{ \\begin{array}{l l} 0, & \\| y \\| _ {*} \\leqslant 1, \\\\ + \\infty , & \\| y \\| _ {*} > 1. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.255,
                0.669,
                0.285
            ],
            "angle": 0,
            "content": "证明．对偶范数定义为： \\(\\| y\\|_{*} = \\sup_{\\| x\\| \\leqslant 1}x^{\\mathrm{T}}y.\\) 为了计算"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.297,
                0.649,
                0.327
            ],
            "angle": 0,
            "content": "\\[\nf ^ {*} (y) = \\sup  _ {x \\in \\mathbf {d o m} f} \\left\\{y ^ {\\mathrm {T}} x - \\| x \\| \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.339,
                0.424,
                0.355
            ],
            "angle": 0,
            "content": "我们分两种情形讨论："
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.369,
                0.825,
                0.419
            ],
            "angle": 0,
            "content": "(1) 若 \\(\\|y\\|_{*} \\leqslant 1\\)，则 \\(y^{\\mathrm{T}}x \\leqslant \\|x\\|\\) 对任一 \\(x\\) 成立，且当 \\(x = 0\\) 时等号成立，从而 \\(\\sup_{x \\in \\mathbf{dom}f} \\{y^{\\mathrm{T}}x - \\|x\\|\\} = 0\\)；"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.43,
                0.826,
                0.466
            ],
            "angle": 0,
            "content": "(2) 若 \\(\\|y\\|_{*} > 1\\)，则至少存在一个 \\(x\\)，使得 \\(\\|x\\| \\leqslant 1\\) 且 \\(x^{\\mathrm{T}}y > 1\\)，从而对 \\(t > 0\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.369,
                0.826,
                0.466
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.415,
                0.47,
                0.709,
                0.49
            ],
            "angle": 0,
            "content": "\\[\nf ^ {*} (y) \\geqslant y ^ {\\mathrm {T}} (t x) - \\| t x \\| = t \\left(y ^ {\\mathrm {T}} x - \\| x \\|\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.501,
                0.585,
                0.517
            ],
            "angle": 0,
            "content": "而不等式右端当 \\(t\\to +\\infty\\) 时趋于无穷"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.545,
                0.441,
                0.563
            ],
            "angle": 0,
            "content": "2.6.2 二次共轭函数"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.576,
                0.756,
                0.593
            ],
            "angle": 0,
            "content": "定义2.20（二次共轭函数）任一函数 \\(f\\) 的二次共轭函数定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.421,
                0.608,
                0.66,
                0.638
            ],
            "angle": 0,
            "content": "\\[\nf ^ {* *} (x) = \\sup  _ {y \\in \\mathbf {d o m} f ^ {*}} \\left\\{x ^ {\\mathrm {T}} y - f ^ {*} (y) \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.65,
                0.684,
                0.667
            ],
            "angle": 0,
            "content": "显然 \\(f^{**}\\) 恒为闭凸函数，且由Fenchel不等式(2.6.2)可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.462,
                0.684,
                0.621,
                0.702
            ],
            "angle": 0,
            "content": "\\[\nf ^ {* *} (x) \\leqslant f (x), \\quad \\forall x,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.717,
                0.825,
                0.755
            ],
            "angle": 0,
            "content": "或等价地，\\(\\mathbf{epi}f \\subseteq \\mathbf{epi}f^{**}\\)。对于凸函数 \\(f\\)，下面的定理描述了 \\(f\\) 的二次共轭函数与其自身的关系[164]推论12.2.1。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.77,
                0.534,
                0.787
            ],
            "angle": 0,
            "content": "定理2.15 若 \\(f\\) 为闭凸函数，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.462,
                0.803,
                0.621,
                0.821
            ],
            "angle": 0,
            "content": "\\[\nf ^ {* *} (x) = f (x), \\quad \\forall x,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.465,
                0.854
            ],
            "angle": 0,
            "content": "或等价地， \\(\\mathbf{epi}f = \\mathbf{epi}f^{**}\\)"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.266,
                0.133
            ],
            "angle": 0,
            "content": "2.7 次梯度"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.119,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "61"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.192
            ],
            "angle": 0,
            "content": "证明．我们采用反证法．若 \\((x,f^{**}(x))\\notin \\mathbf{epi}f\\) ，则存在一个严格分割超平面："
        },
        {
            "type": "equation",
            "bbox": [
                0.291,
                0.192,
                0.737,
                0.239
            ],
            "angle": 0,
            "content": "\\[\n\\left[ \\begin{array}{l} a \\\\ b \\end{array} \\right] ^ {\\mathrm {T}} \\left[ \\begin{array}{c} z - x \\\\ s - f ^ {* *} (x) \\end{array} \\right] \\leqslant c <   0, \\quad \\forall (z, s) \\in \\mathbf {e p i} f. \\tag {2.6.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.242,
                0.692,
                0.259
            ],
            "angle": 0,
            "content": "其中 \\(a\\in \\mathbb{R}^n,b,c\\in \\mathbb{R}\\) 且 \\(b\\leqslant 0\\) （若 \\(b > 0\\) ，则取 \\(s\\to +\\infty\\) 可推出矛盾)."
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.264,
                0.52,
                0.28
            ],
            "angle": 0,
            "content": "若 \\(b < 0\\), 在(2.6.3)式中取 \\(s = f(z)\\), 则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.329,
                0.291,
                0.578,
                0.311
            ],
            "angle": 0,
            "content": "\\[\na ^ {\\mathrm {T}} z + b f (z) - a ^ {\\mathrm {T}} x - b f ^ {* *} (x) \\leqslant c.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.317,
                0.512,
                0.347
            ],
            "angle": 0,
            "content": "记 \\(y = -\\frac{a}{b}\\) ，并将上式左边关于 \\(z\\) 极大化得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.352,
                0.576,
                0.381
            ],
            "angle": 0,
            "content": "\\[\nf ^ {*} (y) - y ^ {T} x + f ^ {* *} (x) \\leqslant - \\frac {c}{b} <   0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.388,
                0.414,
                0.403
            ],
            "angle": 0,
            "content": "与Fenchel不等式(2.6.2)相违背"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.404,
                0.734,
                0.446
            ],
            "angle": 0,
            "content": "若 \\(b = 0\\) ，取 \\(\\hat{y}\\in \\mathbf{dom}f^{*}\\) 并给 \\(\\left[ \\begin{array}{l}a\\\\ b \\end{array} \\right]\\) 加上一个 \\(\\left[ \\begin{array}{c}\\hat{y}\\\\ -1 \\end{array} \\right]\\) 的 \\(\\varepsilon (>0)\\) 倍，则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.214,
                0.454,
                0.737,
                0.502
            ],
            "angle": 0,
            "content": "\\[\n\\left[ \\begin{array}{c} a + \\varepsilon \\hat {y} \\\\ - \\varepsilon \\end{array} \\right] ^ {\\mathrm {T}} \\left[ \\begin{array}{c} z - x \\\\ s - f ^ {* *} (x) \\end{array} \\right] \\leqslant c + \\varepsilon \\left(f ^ {*} (\\hat {y}) - x ^ {\\mathrm {T}} \\hat {y} + f ^ {* *} (x)\\right) <   0, \\tag {2.6.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.508,
                0.416,
                0.525
            ],
            "angle": 0,
            "content": "即化为 \\(b < 0\\) 的情况，推出矛盾"
        },
        {
            "type": "title",
            "bbox": [
                0.386,
                0.555,
                0.521,
                0.576
            ],
            "angle": 0,
            "content": "2.7 次梯度"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.596,
                0.351,
                0.614
            ],
            "angle": 0,
            "content": "2.7.1 次梯度的定义"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.627,
                0.738,
                0.685
            ],
            "angle": 0,
            "content": "前面介绍了可微函数的梯度。但是对于一般的函数，之前定义的梯度不一定存在。对于凸函数，类比梯度的一阶性质，我们可以引入次梯度的概念，其在凸优化算法设计与理论分析中扮演着重要角色。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.696,
                0.738,
                0.734
            ],
            "angle": 0,
            "content": "定义2.21（次梯度）设 \\(f\\) 为适当凸函数，\\(x\\) 为定义域 \\(\\mathbf{dom} f\\) 中的一点。若向量 \\(g \\in \\mathbb{R}^n\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.3,
                0.746,
                0.737,
                0.765
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x) + g ^ {\\mathrm {T}} (y - x), \\quad \\forall y \\in \\mathbf {d o m} f, \\tag {2.7.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.777,
                0.627,
                0.793
            ],
            "angle": 0,
            "content": "则称 \\(g\\) 为函数 \\(f\\) 在点 \\(x\\) 处的一个次梯度．进一步地，称集合"
        },
        {
            "type": "equation",
            "bbox": [
                0.208,
                0.805,
                0.737,
                0.826
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (x) = \\left\\{g \\mid g \\in \\mathbb {R} ^ {n}, f (y) \\geqslant f (x) + g ^ {\\mathrm {T}} (y - x), \\forall y \\in \\mathbf {d o m} f \\right\\} \\tag {2.7.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.836,
                0.355,
                0.853
            ],
            "angle": 0,
            "content": "为 \\(f\\) 在点 \\(x\\) 处的次微分."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "62"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.825,
                0.195
            ],
            "angle": 0,
            "content": "如图2.14所示，对适当凸函数 \\( f(x) \\)，\\( g_{1} \\) 为点 \\( x_{1} \\) 处的唯一次梯度，而 \\( g_{2}, g_{3} \\) 为点 \\( x_{2} \\) 处的两个不同的次梯度。"
        },
        {
            "type": "image",
            "bbox": [
                0.258,
                0.203,
                0.826,
                0.412
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.43,
                0.448,
                0.655,
                0.466
            ],
            "angle": 0,
            "content": "图2.14 函数 \\(f(x)\\) 的次梯度"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.486,
                0.825,
                0.543
            ],
            "angle": 0,
            "content": "从定义2.21可以看出，次梯度实际上借鉴了凸函数判定定理的一阶条件（定理2.9）．定义次梯度的初衷之一也是希望它具有类似于梯度的一些性质."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.548,
                0.814,
                0.566
            ],
            "angle": 0,
            "content": "从次梯度的定义可直接推出，若 \\(g\\) 是 \\(f(x)\\) 在 \\(x_0\\) 处的次梯度，则函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.436,
                0.574,
                0.646,
                0.596
            ],
            "angle": 0,
            "content": "\\[\nl (x) \\stackrel {\\text {d e f}} {=} f \\left(x _ {0}\\right) + g ^ {\\mathrm {T}} \\left(x - x _ {0}\\right)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.606,
                0.825,
                0.666
            ],
            "angle": 0,
            "content": "为凸函数 \\(f(x)\\) 的一个全局下界．此外，次梯度 \\(g\\) 可以诱导出上方图epif在点 \\((x,f(x))\\) 处的一个支撑超平面，因为容易验证，对epif中的任意点\\((y,t)\\)，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.364,
                0.672,
                0.722,
                0.72
            ],
            "angle": 0,
            "content": "\\[\n\\left[ \\begin{array}{c} g \\\\ - 1 \\end{array} \\right] ^ {\\mathrm {T}} \\left(\\left[ \\begin{array}{c} y \\\\ t \\end{array} \\right] - \\left[ \\begin{array}{c} x \\\\ f (x) \\end{array} \\right]\\right) \\leqslant 0, \\quad \\forall (y, t) \\in \\mathbf {e p i} f.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.726,
                0.825,
                0.786
            ],
            "angle": 0,
            "content": "接下来的一个问题自然就是：次梯度在什么条件下是存在的？实际上对一般凸函数 \\(f\\) 而言，\\(f\\) 未必在所有的点处都存在次梯度。但对于定义域中的内点，\\(f\\) 在其上的次梯度总是存在的。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "定理2.16（次梯度存在性）设 \\(f\\) 为凸函数，\\(\\operatorname{dom} f\\) 为其定义域．如果 \\(x \\in \\operatorname{intdom} f\\) ，则 \\(\\partial f(x)\\) 是非空的，其中 \\(\\operatorname{intdom} f\\) 的含义是集合 \\(\\operatorname{dom} f\\) 的所有内点."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.266,
                0.133
            ],
            "angle": 0,
            "content": "2.7 次梯度"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "63"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.737,
                0.195
            ],
            "angle": 0,
            "content": "证明．考虑 \\(f(x)\\) 的上方图epif．由于 \\((x,f(x))\\) 是epif边界上的点，且epif为凸集，根据支撑超平面定理，存在 \\(a\\in \\mathbb{R}^n\\) ， \\(b\\in \\mathbb{R}\\) 使得："
        },
        {
            "type": "equation",
            "bbox": [
                0.283,
                0.207,
                0.627,
                0.256
            ],
            "angle": 0,
            "content": "\\[\n\\left[ \\begin{array}{l} a \\\\ b \\end{array} \\right] ^ {\\mathrm {T}} \\left(\\left[ \\begin{array}{l} y \\\\ t \\end{array} \\right] - \\left[ \\begin{array}{l} x \\\\ f (x) \\end{array} \\right]\\right) \\leqslant 0, \\quad \\forall (y, t) \\in \\mathbf {e p i} f.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.267,
                0.194,
                0.283
            ],
            "angle": 0,
            "content": "即"
        },
        {
            "type": "equation",
            "bbox": [
                0.359,
                0.288,
                0.737,
                0.308
            ],
            "angle": 0,
            "content": "\\[\na ^ {\\mathrm {T}} (y - x) \\leqslant b (f (x) - t). \\tag {2.7.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.321,
                0.739,
                0.4
            ],
            "angle": 0,
            "content": "我们断言 \\(b < 0\\). 这是因为根据 \\(t\\) 的任意性, 在(2.7.3)式中令 \\(t \\to +\\infty\\), 可以得知(2.7.3)式成立的必要条件是 \\(b \\leqslant 0\\); 同时由于 \\(x\\) 是内点, 因此当取 \\(y = x + \\varepsilon a \\in \\mathbf{dom} f, \\varepsilon > 0\\) 时, \\(b = 0\\) 不能使得(2.7.3)式成立. 于是令 \\(g = -\\frac{a}{b}\\), 则对任意 \\(y \\in \\mathbf{dom} f\\), 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.294,
                0.412,
                0.614,
                0.445
            ],
            "angle": 0,
            "content": "\\[\ng ^ {\\mathrm {T}} (y - x) = \\frac {a ^ {\\mathrm {T}} (y - x)}{- b} \\leqslant - (f (x) - f (y)),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.457,
                0.228,
                0.473
            ],
            "angle": 0,
            "content": "整理得"
        },
        {
            "type": "equation",
            "bbox": [
                0.356,
                0.478,
                0.55,
                0.499
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x) + g ^ {\\mathrm {T}} (y - x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.511,
                0.424,
                0.528
            ],
            "angle": 0,
            "content": "这说明 \\(g\\) 是 \\(f\\) 在点 \\(x\\) 处的次梯度"
        },
        {
            "type": "image",
            "bbox": [
                0.72,
                0.512,
                0.738,
                0.525
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.548,
                0.729,
                0.566
            ],
            "angle": 0,
            "content": "根据定义可以计算一些简单函数的次微分，在这里我们给出一个例子。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.581,
                0.737,
                0.639
            ],
            "angle": 0,
            "content": "例2.15 \\((\\ell_{2}\\) 范数的次微分）设 \\(f(x) = \\| x\\| _2\\) ，则 \\(f(x)\\) 在点 \\(x = 0\\) 处不可微，我们求其在该点处的次梯度．注意到对任意的 \\(g\\) 且 \\(\\| g\\| _2\\leqslant 1\\) ，根据柯西不等式，"
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.645,
                0.584,
                0.665
            ],
            "angle": 0,
            "content": "\\[\ng ^ {\\mathrm {T}} (x - 0) \\leqslant \\| g \\| _ {2} \\| x \\| _ {2} \\leqslant \\| x \\| _ {2} - 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.678,
                0.21,
                0.693
            ],
            "angle": 0,
            "content": "因此"
        },
        {
            "type": "equation",
            "bbox": [
                0.363,
                0.7,
                0.544,
                0.719
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{g \\mid \\| g \\| _ {2} \\leqslant 1 \\right\\} \\subseteq \\partial f (0).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.731,
                0.692,
                0.75
            ],
            "angle": 0,
            "content": "接下来说明若 \\(\\| g\\| _2 > 1\\) ，则 \\(g\\notin \\partial f(0)\\) .取 \\(x = g\\) ，若 \\(g\\) 为次梯度，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.314,
                0.765,
                0.595,
                0.785
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| g \\right\\| _ {2} - 0 \\geqslant g ^ {\\mathrm {T}} (g - 0) = \\left\\| g \\right\\| _ {2} ^ {2} > \\left\\| g \\right\\| _ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.802,
                0.416,
                0.817
            ],
            "angle": 0,
            "content": "这显然是矛盾的。综上，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.362,
                0.836,
                0.545,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (0) = \\left\\{g \\mid \\| g \\| _ {2} \\leqslant 1 \\right\\}.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "64"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.44,
                0.174
            ],
            "angle": 0,
            "content": "2.7.2 次梯度的性质"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.187,
                0.825,
                0.225
            ],
            "angle": 0,
            "content": "凸函数 \\(f(x)\\) 的次梯度和次微分有许多有用的性质．下面的定理说明次微分 \\(\\partial f(x)\\) 在一定条件下分别为闭凸集和非空有界集."
        },
        {
            "type": "text",
            "bbox": [
                0.291,
                0.238,
                0.663,
                0.255
            ],
            "angle": 0,
            "content": "定理2.17 设 \\(f\\) 是凸函数，则 \\(\\partial f(x)\\) 有如下性质："
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.267,
                0.724,
                0.285
            ],
            "angle": 0,
            "content": "(1) 对任何 \\(x \\in \\mathbf{dom} f\\) ，\\(\\partial f(x)\\) 是一个闭凸集（可能为空集）；"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.298,
                0.624,
                0.315
            ],
            "angle": 0,
            "content": "(2) 如果 \\(x \\in \\mathbf{intdom} f\\)，则 \\(\\partial f(x)\\) 非空有界集."
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.267,
                0.724,
                0.315
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.327,
                0.763,
                0.345
            ],
            "angle": 0,
            "content": "证明．设 \\(g_{1}, g_{2} \\in \\partial f(x)\\)，并设 \\(\\lambda \\in (0,1)\\)，由次梯度的定义我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.386,
                0.358,
                0.694,
                0.379
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x) + g _ {1} ^ {\\mathrm {T}} (y - x), \\quad \\forall y \\in \\mathbf {d o m} f,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.382,
                0.694,
                0.404
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x) + g _ {2} ^ {\\mathrm {T}} (y - x), \\quad \\forall y \\in \\mathbf {d o m} f.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.417,
                0.825,
                0.455
            ],
            "angle": 0,
            "content": "由上面第一式的 \\(\\lambda\\) 倍加上第二式的 \\((1 - \\lambda)\\) 倍，我们得到 \\(\\lambda g_{1} + (1 - \\lambda)g_{2}\\in\\) \\(\\partial f(x)\\) ，从而 \\(\\partial f(x)\\) 是凸集．此外令 \\(g_{k}\\in \\partial f(x)\\) 为次梯度且 \\(g_{k}\\to g\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.388,
                0.468,
                0.694,
                0.488
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x) + g _ {k} ^ {\\mathrm {T}} (y - x), \\quad \\forall y \\in \\mathbf {d o m} f,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.502,
                0.726,
                0.519
            ],
            "angle": 0,
            "content": "在上述不等式中取极限，并注意到极限的保号性，最终我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.388,
                0.533,
                0.692,
                0.553
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x) + g ^ {\\mathrm {T}} (y - x), \\quad \\forall y \\in \\mathbf {d o m} f.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.566,
                0.424,
                0.584
            ],
            "angle": 0,
            "content": "这说明 \\(\\partial f(x)\\) 为闭集"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.587,
                0.826,
                0.667
            ],
            "angle": 0,
            "content": "下设 \\(x \\in \\mathbf{intdom} f\\)，我们来证明 \\(\\partial f(x)\\) 是非空有界的。首先，\\(\\partial f(x)\\) 非空是定理 2.16 的直接结果，因此我们只需要证明有界性。对 \\(i = 1, 2, \\dots, n\\)，定义 \\(e_i = (0, \\dots, 1, \\dots, 0)\\)（第 \\(i\\) 个分量为 1，其余分量均为 0），易知 \\(\\{e_i\\}_{i=1}^n\\) 为 \\(\\mathbb{R}^n\\) 的一组标准正交基。取定充分小的正数 \\(r\\)，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.396,
                0.681,
                0.687,
                0.699
            ],
            "angle": 0,
            "content": "\\[\nB = \\{x \\pm r e _ {i} \\mid i = 1, 2, \\dots , n \\} \\subset \\mathbf {d o m} f.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.713,
                0.663,
                0.731
            ],
            "angle": 0,
            "content": "对任意 \\(g \\in \\partial f(x)\\)，不妨设 \\(g\\) 不为 0。存在 \\(y \\in B\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.383,
                0.744,
                0.699,
                0.764
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x) + g ^ {\\mathrm {T}} (y - x) = f (x) + r \\| g \\| _ {\\infty}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.778,
                0.334,
                0.794
            ],
            "angle": 0,
            "content": "由此得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.417,
                0.791,
                0.667,
                0.832
            ],
            "angle": 0,
            "content": "\\[\n\\| g \\| _ {\\infty} \\leqslant \\frac {\\max  _ {y \\in B} f (y) - f (x)}{r} <   + \\infty ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.371,
                0.854
            ],
            "angle": 0,
            "content": "即 \\(\\partial f(x)\\) 有界"
        },
        {
            "type": "text",
            "bbox": [
                0.808,
                0.838,
                0.826,
                0.851
            ],
            "angle": 0,
            "content": "□"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.266,
                0.133
            ],
            "angle": 0,
            "content": "2.7 次梯度"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "65"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.194
            ],
            "angle": 0,
            "content": "当凸函数 \\(f(x)\\) 在某点处可微时，\\(\\nabla f(x)\\) 就是 \\(f(x)\\) 在该点处唯一的次梯度."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.209,
                0.562,
                0.226
            ],
            "angle": 0,
            "content": "命题2.6 设 \\(f(x)\\) 在 \\(x_0 \\in \\mathbf{intdom} f\\) 处可微，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.242,
                0.531,
                0.261
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (x _ {0}) = \\{\\nabla f (x _ {0}) \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.276,
                0.737,
                0.335
            ],
            "angle": 0,
            "content": "证明. 根据可微凸函数的一阶条件 (定理 2.9) 可知梯度 \\(\\nabla f(x_0)\\) 为次梯度. 下证 \\(f(x)\\) 在点 \\(x_0\\) 处不可能有其他次梯度. 设 \\(g \\in \\partial f(x_0)\\), 根据次梯度的定义, 对任意的非零 \\(v \\in \\mathbb{R}^n\\) 且 \\(x_0 + tv \\in \\operatorname{dom} f, t > 0\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.351,
                0.35,
                0.554,
                0.369
            ],
            "angle": 0,
            "content": "\\[\nf (x _ {0} + t v) \\geqslant f (x _ {0}) + t g ^ {\\mathrm {T}} v.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.385,
                0.576,
                0.403
            ],
            "angle": 0,
            "content": "若 \\(g \\neq \\nabla f(x_0)\\)，取 \\(v = g - \\nabla f(x_0) \\neq 0\\)，上式变形为"
        },
        {
            "type": "equation",
            "bbox": [
                0.232,
                0.412,
                0.675,
                0.449
            ],
            "angle": 0,
            "content": "\\[\n\\frac {f \\left(x _ {0} + t v\\right) - f \\left(x _ {0}\\right) - t \\nabla f \\left(x _ {0}\\right) ^ {\\mathrm {T}} v}{t \\| v \\|} \\geqslant \\frac {(g - \\nabla f \\left(x _ {0}\\right)) ^ {\\mathrm {T}} v}{\\| v \\|} = \\| v \\|.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.46,
                0.737,
                0.497
            ],
            "angle": 0,
            "content": "不等式两边令 \\(t \\to 0\\)，根据 Fréchet 可微的定义，左边趋于 0，而右边是非零正数，可得到矛盾. □"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.513,
                0.737,
                0.551
            ],
            "angle": 0,
            "content": "和梯度类似，凸函数的次梯度也具有某种单调性．这一性质在很多和次梯度有关的算法的收敛性分析中起到了关键的作用."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.565,
                0.737,
                0.601
            ],
            "angle": 0,
            "content": "定理2.18（次梯度的单调性）设 \\(f:\\mathbb{R}^n\\to \\mathbb{R}\\) 为凸函数， \\(x,y\\in \\operatorname {dom}f\\) 则"
        },
        {
            "type": "equation",
            "bbox": [
                0.375,
                0.605,
                0.532,
                0.624
            ],
            "angle": 0,
            "content": "\\[\n(u - v) ^ {\\mathrm {T}} (x - y) \\geqslant 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.635,
                0.38,
                0.652
            ],
            "angle": 0,
            "content": "其中 \\(u\\in \\partial f(x)\\) ， \\(v\\in \\partial f(y)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.667,
                0.367,
                0.683
            ],
            "angle": 0,
            "content": "证明．由次梯度的定义，"
        },
        {
            "type": "equation",
            "bbox": [
                0.356,
                0.696,
                0.55,
                0.715
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x) + u ^ {\\mathrm {T}} (y - x),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.358,
                0.72,
                0.548,
                0.74
            ],
            "angle": 0,
            "content": "\\[\nf (x) \\geqslant f (y) + v ^ {\\mathrm {T}} (x - y).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.753,
                0.42,
                0.769
            ],
            "angle": 0,
            "content": "将以上两个不等式相加即得结论"
        },
        {
            "type": "text",
            "bbox": [
                0.72,
                0.754,
                0.737,
                0.766
            ],
            "angle": 0,
            "content": "□"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.785,
                0.685,
                0.801
            ],
            "angle": 0,
            "content": "对于闭凸函数（即凸下半连续函数），次梯度还具有某种连续性。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.737,
                0.855
            ],
            "angle": 0,
            "content": "定理2.19 设 \\(f(x)\\) 是闭凸函数且 \\(\\partial f\\) 在点 \\(\\bar{x}\\) 附近存在且非空．若序列 \\(x^{k} \\to \\bar{x}\\)，\\(g^{k} \\in \\partial f(x^{k})\\) 为 \\(f(x)\\) 在点 \\(x^{k}\\) 处的次梯度，且 \\(g^{k} \\to \\bar{g}\\)，则 \\(\\bar{g} \\in \\partial f(\\bar{x})\\)"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "66"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.623,
                0.174
            ],
            "angle": 0,
            "content": "证明．对任意 \\(y\\in \\mathbf{dom}f\\) ，根据次梯度的定义，"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.189,
                0.649,
                0.211
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f \\left(x ^ {k}\\right) + \\left\\langle g ^ {k}, y - x ^ {k} \\right\\rangle .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.227,
                0.538,
                0.243
            ],
            "angle": 0,
            "content": "对上述不等式两边取下极限，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.256,
                0.674,
                0.307
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (y) \\geqslant \\lim  _ {k \\rightarrow \\infty} \\inf  [ f (x ^ {k}) + \\langle g ^ {k}, y - x ^ {k} \\rangle ] \\\\ \\geqslant f (\\bar {x}) + \\langle \\bar {g}, y - \\bar {x} \\rangle , \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.321,
                0.827,
                0.361
            ],
            "angle": 0,
            "content": "其中第二个不等式利用了 \\(f(x)\\) 的下半连续性以及 \\(g^{k}\\to \\bar{g}\\) ，由此可推出 \\(\\bar{g}\\in\\) \\(\\partial f(\\bar{x})\\) □"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.378,
                0.825,
                0.436
            ],
            "angle": 0,
            "content": "在这里注意，定理2.19不完全是 \\(\\partial f(x)\\) 的连续性，它额外要求 \\(g^{k}\\) 本身是收敛的．这个性质等价于 \\(\\partial f(x)\\) 的图像 \\(\\{(x,g)\\mid g\\in \\partial f(x),x\\in \\mathbf{dom}f\\}\\) 是闭集."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.467,
                0.48,
                0.485
            ],
            "angle": 0,
            "content": "2.7.3 凸函数的方向导数"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.499,
                0.825,
                0.537
            ],
            "angle": 0,
            "content": "在数学分析中我们接触过方向导数的概念. 设 \\(f\\) 为适当函数, 给定点 \\(x_0\\) 以及方向 \\(d \\in \\mathbb{R}^n\\), 方向导数 (若存在) 定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.411,
                0.548,
                0.825,
                0.582
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {t \\downarrow 0} \\phi (t) = \\lim  _ {t \\downarrow 0} \\frac {f \\left(x _ {0} + t d\\right) - f \\left(x _ {0}\\right)}{t}, \\tag {2.7.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.595,
                0.825,
                0.653
            ],
            "angle": 0,
            "content": "其中 \\(t \\downarrow 0\\) 表示 \\(t\\) 单调下降趋于 0。对于凸函数 \\(f(x)\\)，易知 \\(\\phi(t)\\) 在 \\((0, +\\infty)\\) 上是单调不减的，(2.7.4) 式中的极限号 \\(\\lim\\) 可以替换为下确界 \\(\\inf\\)。上述此时极限总是存在（可以为无穷），进而凸函数总是可以定义方向导数。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.669,
                0.825,
                0.707
            ],
            "angle": 0,
            "content": "定义2.22（方向导数）对于凸函数 \\(f\\) ，给定点 \\(x_0\\in \\mathbf{dom}f\\) 以及方向\\(d\\in \\mathbb{R}^n\\) ，其方向导数定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.717,
                0.675,
                0.751
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (x _ {0}; d) = \\inf  _ {t > 0} \\frac {f (x _ {0} + t d) - f (x _ {0})}{t}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.762,
                0.825,
                0.8
            ],
            "angle": 0,
            "content": "方向导数可能是正负无穷，但在定义域的内点处方向导数 \\(\\partial f(x_0;d)\\) 是有限的."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.815,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "命题2.7 设 \\(f(x)\\) 为凸函数, \\(x_0 \\in \\mathbf{intdom} f\\), 则对任意 \\(d \\in \\mathbb{R}^n\\), \\(\\partial f(x_0; d)\\) 有限."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.266,
                0.133
            ],
            "angle": 0,
            "content": "2.7 次梯度"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "67"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.737,
                0.195
            ],
            "angle": 0,
            "content": "证明．首先 \\(\\partial f(x_0;d)\\) 不为正无穷是显然的．由于 \\(x_0\\in \\mathbf{intdom}f\\) ，根据定理2.16可知 \\(f(x)\\) 在点 \\(x_0\\) 处存在次梯度 \\(g\\) ．根据方向导数的定义，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.328,
                0.205,
                0.58,
                0.27
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\partial f (x _ {0}; d) = \\inf  _ {t > 0} \\frac {f (x _ {0} + t d) - f (x _ {0})}{t} \\\\ \\geqslant \\inf  _ {t > 0} \\frac {t g ^ {\\mathrm {T}} d}{t} = g ^ {\\mathrm {T}} d. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.28,
                0.666,
                0.298
            ],
            "angle": 0,
            "content": "其中的不等式利用了次梯度的定义．这说明 \\(\\partial f(x_0;d)\\) 不为负无穷."
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.31,
                0.737,
                0.368
            ],
            "angle": 0,
            "content": "凸函数的方向导数和次梯度之间有很强的联系。以下结果表明，凸函数 \\( f(x) \\) 关于 \\( d \\) 的方向导数 \\( \\partial f(x;d) \\) 正是 \\( f \\) 在点 \\( x \\) 处的所有次梯度与 \\( d \\) 的内积的最大值。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.381,
                0.737,
                0.417
            ],
            "angle": 0,
            "content": "定理2.20 设 \\(f: \\mathbb{R}^n \\to (-\\infty, +\\infty]\\) 为凸函数，点 \\(x_0 \\in \\operatorname{intdom} f\\) ，\\(d\\) 为 \\(\\mathbb{R}^n\\) 中任一方向，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.367,
                0.422,
                0.736,
                0.448
            ],
            "angle": 0,
            "content": "\\[\n\\partial f \\left(x _ {0}; d\\right) = \\max  _ {g \\in \\partial f \\left(x _ {0}\\right)} g ^ {\\mathrm {T}} d. \\tag {2.7.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.456,
                0.737,
                0.495
            ],
            "angle": 0,
            "content": "证明．为了方便，对任意 \\(v\\in \\mathbb{R}^m\\) ，我们定义 \\(q(v) = \\partial f(x_0;v)\\) ．根据命题2.7的证明过程可直接得出对任意 \\(g\\in \\partial f(x_0)\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.364,
                0.507,
                0.542,
                0.526
            ],
            "angle": 0,
            "content": "\\[\nq (d) = \\partial f (x _ {0}; d) \\geqslant g ^ {\\mathrm {T}} d.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.539,
                0.666,
                0.556
            ],
            "angle": 0,
            "content": "这说明 \\(\\partial f(x_0;d)\\) 是 \\(g^{\\mathrm{T}}d\\) 的一个上界，接下来说明该上界为上确界."
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.561,
                0.279,
                0.575
            ],
            "angle": 0,
            "content": "构造函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.327,
                0.575,
                0.58,
                0.603
            ],
            "angle": 0,
            "content": "\\[\nh (v, t) = t \\left(f \\left(x _ {0} + \\frac {v}{t}\\right) - f (x _ {0})\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.608,
                0.737,
                0.645
            ],
            "angle": 0,
            "content": "可知 \\(h(v,t)\\) 为 \\(\\tilde{f} (v) = f(x_0 + v) - f(x_0)\\) 的透视函数（见定理2.13的(9))，并且"
        },
        {
            "type": "equation",
            "bbox": [
                0.275,
                0.645,
                0.632,
                0.676
            ],
            "angle": 0,
            "content": "\\[\nq (v) = \\inf  _ {t ^ {\\prime} > 0} \\frac {f \\left(x _ {0} + t ^ {\\prime} v\\right) - f \\left(x _ {0}\\right)}{t ^ {\\prime}} \\xlongequal {t = 1 / t ^ {\\prime}} \\inf  _ {t > 0} h (v, t).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.68,
                0.737,
                0.759
            ],
            "angle": 0,
            "content": "根据定理 2.13 的 (9) 知透视函数 \\(h(v, t)\\) 为凸函数，又根据 (8) 知取下确界仍为凸函数，因此 \\(q(v)\\) 关于 \\(v\\) 是凸函数。由命题 2.7 直接可以得出 \\(\\mathbf{dom} q = \\mathbb{R}^n\\)，因此 \\(q(v)\\) 在全空间任意一点次梯度存在。对方向 \\(d\\)，设 \\(\\hat{g} \\in \\partial q(d)\\)，则对任意 \\(v \\in \\mathbb{R}^n\\) 以及 \\(\\lambda \\geqslant 0\\)，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.316,
                0.772,
                0.591,
                0.792
            ],
            "angle": 0,
            "content": "\\[\n\\lambda q (v) = q (\\lambda v) \\geqslant q (d) + \\hat {g} ^ {\\mathrm {T}} (\\lambda v - d).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.804,
                0.565,
                0.822
            ],
            "angle": 0,
            "content": "令 \\(\\lambda = 0\\) ，我们有 \\(q(d)\\leqslant \\hat{g}^{\\mathrm{T}}d\\) ；令 \\(\\lambda \\to +\\infty\\) ，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.835,
                0.501,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nq (v) \\geqslant \\hat {g} ^ {\\mathrm {T}} v,\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "68"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.331,
                0.173
            ],
            "angle": 0,
            "content": "进而推出"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.176,
                0.684,
                0.196
            ],
            "angle": 0,
            "content": "\\[\nf (x + v) \\geqslant f (x) + q (v) \\geqslant f (x) + \\hat {g} ^ {\\mathrm {T}} v.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.204,
                0.826,
                0.243
            ],
            "angle": 0,
            "content": "这说明 \\(\\hat{g} \\in \\partial f(x)\\) 且 \\(\\hat{g}^{\\mathrm{T}}d \\geqslant q(d)\\). 即 \\(q(d)\\) 为 \\(g^{\\mathrm{T}}d\\) 的上确界，且当 \\(g = \\hat{g}\\) 时上确界达到. □"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.255,
                0.79,
                0.273
            ],
            "angle": 0,
            "content": "定理2.20可对一般的 \\(x\\in \\mathbf{dom}f\\) 作如下推广，证明见[166]引理2.75."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.286,
                0.825,
                0.322
            ],
            "angle": 0,
            "content": "定理2.21 设 \\(f\\) 为适当凸函数，且在 \\(x_0\\) 处次微分不为空集，则对任意 \\(d \\in \\mathbb{R}^n\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.454,
                0.326,
                0.629,
                0.357
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (x _ {0}; d) = \\sup  _ {g \\in \\partial f (x _ {0})} g ^ {\\mathrm {T}} d,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.363,
                0.599,
                0.38
            ],
            "angle": 0,
            "content": "且当 \\(\\partial f(x_0;d)\\) 不为无穷时，上确界可以取到."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.405,
                0.48,
                0.423
            ],
            "angle": 0,
            "content": "2.7.4 次梯度的计算规则"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.436,
                0.825,
                0.495
            ],
            "angle": 0,
            "content": "如何计算一个不可微凸函数的次梯度在优化算法设计中是很重要的问题。根据定义来计算次梯度一般来说比较繁琐，我们来介绍一些次梯度的计算规则。本小节讨论的计算规则都默认 \\(x \\in \\operatorname{intdom} f\\)。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.518,
                0.362,
                0.535
            ],
            "angle": 0,
            "content": "1. 基本规则"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.548,
                0.784,
                0.565
            ],
            "angle": 0,
            "content": "我们首先不加证明地给出一些计算次梯度（次微分）的基本规则"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.577,
                0.825,
                0.595
            ],
            "angle": 0,
            "content": "(1) 可微凸函数: 设 \\(f\\) 为凸函数, 若 \\(f\\) 在点 \\(x\\) 处可微, 则 \\(\\partial f(x) = \\{\\nabla f(x)\\}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.608,
                0.691,
                0.625
            ],
            "angle": 0,
            "content": "(2) 凸函数的非负线性组合: 设 \\(f_{1}, f_{2}\\) 为凸函数, 且满足"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.577,
                0.825,
                0.625
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.464,
                0.64,
                0.661,
                0.657
            ],
            "angle": 0,
            "content": "\\[\n\\mathbf {i n t d o m} f _ {1} \\cap \\mathbf {d o m} f _ {2} \\neq \\emptyset ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.672,
                0.516,
                0.689
            ],
            "angle": 0,
            "content": "而 \\(x\\in \\mathbf{dom}f_1\\cap \\mathbf{dom}f_2\\) 若"
        },
        {
            "type": "equation",
            "bbox": [
                0.417,
                0.704,
                0.707,
                0.722
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\alpha_ {1} f _ {1} (x) + \\alpha_ {2} f _ {2} (x), \\quad \\alpha_ {1}, \\alpha_ {2} \\geqslant 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.736,
                0.432,
                0.753
            ],
            "angle": 0,
            "content": "则 \\(f(x)\\) 的次微分"
        },
        {
            "type": "equation",
            "bbox": [
                0.449,
                0.767,
                0.673,
                0.786
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (x) = \\alpha_ {1} \\partial f _ {1} (x) + \\alpha_ {2} \\partial f _ {2} (x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.805,
                0.701,
                0.822
            ],
            "angle": 0,
            "content": "(3) 线性变量替换: 设 \\(h\\) 为适当凸函数, 并且函数 \\(f\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.446,
                0.836,
                0.678,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nf (x) = h (A x + b), \\quad \\forall x \\in \\mathbb {R} ^ {m},\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.266,
                0.133
            ],
            "angle": 0,
            "content": "2.7 次梯度"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.119,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "69"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.156,
                0.737,
                0.193
            ],
            "angle": 0,
            "content": "其中 \\(A \\in \\mathbb{R}^{n \\times m}, b \\in \\mathbb{R}^n\\)。若存在 \\(x^\\sharp \\in \\mathbb{R}^m\\)，使得 \\(Ax^\\sharp + b \\in \\text{intdom} h\\) 则"
        },
        {
            "type": "equation",
            "bbox": [
                0.312,
                0.198,
                0.637,
                0.217
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (x) = A ^ {\\mathrm {T}} \\partial h (A x + b), \\quad \\forall x \\in \\mathbf {i n t d o m} f.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.23,
                0.741,
                0.267
            ],
            "angle": 0,
            "content": "注2.3 第一个结论就是推论2.6；第二个结论是定理2.22的简单推论；第三个结论见 \\([164]^{\\text{定理} 23.9}\\)。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.29,
                0.379,
                0.308
            ],
            "angle": 0,
            "content": "2. 两个函数之和的次梯度"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.32,
                0.737,
                0.357
            ],
            "angle": 0,
            "content": "以下的Moreau-Rockafellar定理给出两个凸函数之和的次微分的计算方法."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.369,
                0.737,
                0.407
            ],
            "angle": 0,
            "content": "定理2.22 (Moreau-Rockafellar[164]定理23.8) 设 \\(f_{1}, f_{2} : \\mathbb{R}^{n} \\to (-\\infty, +\\infty]\\) 是两个凸函数，则对任意的 \\(x_{0} \\in \\mathbb{R}^{n}\\)，"
        },
        {
            "type": "equation",
            "bbox": [
                0.32,
                0.42,
                0.737,
                0.439
            ],
            "angle": 0,
            "content": "\\[\n\\partial f _ {1} \\left(x _ {0}\\right) + \\partial f _ {2} \\left(x _ {0}\\right) \\subseteq \\partial \\left(f _ {1} + f _ {2}\\right) \\left(x _ {0}\\right). \\tag {2.7.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.451,
                0.638,
                0.468
            ],
            "angle": 0,
            "content": "进一步地，若int \\(\\mathbf{dom}f_1\\cap \\mathbf{dom}f_2\\neq \\emptyset\\) ，则对任意的 \\(x_0\\in \\mathbb{R}^n\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.32,
                0.481,
                0.737,
                0.5
            ],
            "angle": 0,
            "content": "\\[\n\\partial \\left(f _ {1} + f _ {2}\\right) \\left(x _ {0}\\right) = \\partial f _ {1} \\left(x _ {0}\\right) + \\partial f _ {2} \\left(x _ {0}\\right). \\tag {2.7.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.512,
                0.738,
                0.572
            ],
            "angle": 0,
            "content": "证明．第一个结论由次梯度的定义是显而易见的．以下我们证第二个结论.对于任意给定的 \\(x_0\\) ，设 \\(g\\in \\partial (f_1 + f_2)(x_0)\\) ．如果 \\(f_{1}(x_{0}) = +\\infty\\) ，则 \\((f_{1} +\\) \\(f_{2})(x_{0}) = +\\infty\\) ．由次梯度的定义，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.299,
                0.582,
                0.609,
                0.602
            ],
            "angle": 0,
            "content": "\\[\n\\left(f _ {1} + f _ {2}\\right) (x) \\geqslant \\left(f _ {1} + f _ {2}\\right) \\left(x _ {0}\\right) + g ^ {\\mathrm {T}} \\left(x - x _ {0}\\right)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.614,
                0.738,
                0.653
            ],
            "angle": 0,
            "content": "对任意 \\(x \\in \\mathbb{R}^n\\) 成立，故 \\(f_1 + f_2 \\equiv +\\infty\\)。这与 \\(\\operatorname{int} \\operatorname{dom} f_1 \\cap \\operatorname{dom} f_2 \\neq \\emptyset\\) 矛盾，因此以下我们假设 \\(f_1(x_0), f_2(x_0) < +\\infty\\)。定义如下两个集合："
        },
        {
            "type": "equation",
            "bbox": [
                0.219,
                0.664,
                0.687,
                0.685
            ],
            "angle": 0,
            "content": "\\[\nS _ {1} = \\left\\{\\left(x - x _ {0}, y\\right) \\in \\mathbb {R} ^ {n} \\times \\mathbb {R} \\mid y > f _ {1} (x) - f _ {1} \\left(x _ {0}\\right) - g ^ {\\mathrm {T}} \\left(x - x _ {0}\\right) \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.222,
                0.689,
                0.589,
                0.709
            ],
            "angle": 0,
            "content": "\\[\nS _ {2} = \\left\\{\\left(x - x _ {0}, y\\right) \\in \\mathbb {R} ^ {n} \\times \\mathbb {R} \\mid y \\leqslant f _ {2} \\left(x _ {0}\\right) - f _ {2} (x) \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.721,
                0.616,
                0.738
            ],
            "angle": 0,
            "content": "容易验证 \\(S_{1}, S_{2}\\) 均为非空凸集．设 \\((x - x_{0}, y) \\in S_{1} \\cap S_{2}\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.75,
                0.577,
                0.77
            ],
            "angle": 0,
            "content": "\\[\ny > f _ {1} (x) - f _ {1} (x _ {0}) - g ^ {\\mathrm {T}} (x - x _ {0}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.332,
                0.775,
                0.477,
                0.794
            ],
            "angle": 0,
            "content": "\\[\ny \\leqslant f _ {2} (x _ {0}) - f _ {2} (x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.806,
                0.296,
                0.822
            ],
            "angle": 0,
            "content": "上两式相减即得"
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.835,
                0.612,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\left(f _ {1} + f _ {2}\\right) (x) <   \\left(f _ {1} + f _ {2}\\right) \\left(x _ {0}\\right) + g ^ {\\mathrm {T}} \\left(x - x _ {0}\\right),\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "70"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.195
            ],
            "angle": 0,
            "content": "这与 \\(g \\in \\partial (f_1 + f_2)(x_0)\\) 矛盾．因此 \\(S_{1} \\cap S_{2} = \\emptyset\\) ，根据凸集分离定理，存在非零的 \\((a,b) \\in \\mathbb{R}^n \\times \\mathbb{R}\\) 和另一个实数 \\(c \\in \\mathbb{R}\\)，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.393,
                0.211,
                0.824,
                0.227
            ],
            "angle": 0,
            "content": "\\[\na ^ {\\mathrm {T}} \\left(x - x _ {0}\\right) + b y \\leqslant c, \\quad \\forall (x - x _ {0}, y) \\in S _ {1}, \\tag {2.7.8}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.235,
                0.824,
                0.252
            ],
            "angle": 0,
            "content": "\\[\na ^ {\\mathrm {T}} \\left(x - x _ {0}\\right) + b y \\geqslant c, \\quad \\forall (x - x _ {0}, y) \\in S _ {2}. \\tag {2.7.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.267,
                0.826,
                0.346
            ],
            "angle": 0,
            "content": "注意到 \\((0,0) \\in S_{2}\\), 故 \\(c \\leqslant 0\\). 此外还有 \\((0,\\varepsilon) \\in S_{1}\\) 对任何 \\(\\varepsilon > 0\\) 成立, 由此容易得到 \\(c = 0\\) 以及 \\(b \\leqslant 0\\). 如果 \\(b = 0\\), 则由上两式即得 \\(a^{\\mathrm{T}}(x - x_0) = 0\\) 对任何 \\(x \\in \\mathbf{dom}f_{1} \\cap \\mathbf{dom}f_{2}\\) 成立. 现在取 \\(\\hat{x} \\in \\operatorname{int} \\mathbf{dom}f_{1} \\cap \\mathbf{dom}f_{2}\\), 并设 \\(\\delta > 0\\) 使得点 \\(\\hat{x}\\) 处的邻域 \\(N_{\\delta}(\\hat{x}) \\subset \\operatorname{int} \\mathbf{dom}f_{1} \\cap \\mathbf{dom}f_{2}\\), 则"
        },
        {
            "type": "equation",
            "bbox": [
                0.46,
                0.36,
                0.623,
                0.379
            ],
            "angle": 0,
            "content": "\\[\na ^ {\\mathrm {T}} u = a ^ {\\mathrm {T}} \\left(\\hat {x} + u - x _ {0}\\right)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.39,
                0.825,
                0.449
            ],
            "angle": 0,
            "content": "对任何 \\(u \\in \\mathbb{R}^n\\) 成立. 此时再令 \\(u = \\frac{\\delta a}{2\\|a\\|_2}\\) 即得 \\(a = 0\\). 但这与 \\((a, b)\\) 非零矛盾, 故 \\(b\\) 不可能为 0. 现将 (2.7.8) 式除以 \\(-b\\), 并令 \\(\\hat{a} = -\\frac{a}{b}\\), 就得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.413,
                0.462,
                0.672,
                0.48
            ],
            "angle": 0,
            "content": "\\[\n\\hat {a} ^ {\\mathrm {T}} \\left(x - x _ {0}\\right) \\leqslant y, \\quad \\forall (x - x _ {0}, y) \\in S _ {1},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.413,
                0.486,
                0.672,
                0.504
            ],
            "angle": 0,
            "content": "\\[\n\\hat {a} ^ {\\mathrm {T}} (x - x _ {0}) \\geqslant y, \\quad \\forall (x - x _ {0}, y) \\in S _ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.519,
                0.825,
                0.558
            ],
            "angle": 0,
            "content": "利用上面两个式子和 \\(S_{1}\\) 和 \\(S_{2}\\) 的定义可以分别得到 \\(g + \\hat{a} \\in \\partial f_{1}(x_{0})\\) 和 \\(-\\hat{a} \\in \\partial f_{2}(x_{0})\\)。因此 \\(g = (g + \\hat{a}) + (-\\hat{a}) \\in \\partial f_{1}(x_{0}) + \\partial f_{2}(x_{0})\\) □"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.581,
                0.414,
                0.597
            ],
            "angle": 0,
            "content": "3. 函数族的上确界"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.61,
                0.82,
                0.627
            ],
            "angle": 0,
            "content": "容易验证一族凸函数的上确界函数仍是凸函数．我们有如下重要结果："
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.641,
                0.826,
                0.678
            ],
            "angle": 0,
            "content": "定理2.23 (Dubovitskii-Milyutin [62]) 设 \\(f_{1}, f_{2}, \\dots, f_{m}: \\mathbb{R}^{n} \\to (-\\infty, +\\infty]\\) 均为凸函数，令"
        },
        {
            "type": "equation",
            "bbox": [
                0.359,
                0.693,
                0.722,
                0.712
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\max  \\left\\{f _ {1} (x), f _ {2} (x), \\dots , f _ {m} (x) \\right\\}, \\quad \\forall x \\in \\mathbb {R} ^ {n}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.722,
                0.714,
                0.761
            ],
            "angle": 0,
            "content": "对 \\(x_0 \\in \\bigcap_{i=1}^{m} \\mathbf{int} \\mathbf{dom} f_i\\)，定义 \\(I(x_0) = \\{i \\mid f_i(x_0) = f(x_0)\\}\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.437,
                0.773,
                0.824,
                0.805
            ],
            "angle": 0,
            "content": "\\[\n\\partial f \\left(x _ {0}\\right) = \\mathbf {c o n v} \\bigcup_ {i \\in I \\left(x _ {0}\\right)} \\partial f _ {i} \\left(x _ {0}\\right). \\tag {2.7.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "证明．若 \\(f(x_0) = +\\infty\\) ，则 \\(f_{i}(x_{0}) = +\\infty ,i\\in I(x_{0})\\) ，于是(2.7.10)式两端均为 \\(\\varnothing\\) .下设 \\(f(x_0) <   + \\infty\\) ： \\(\\forall i\\in I(x_0)\\) ，容易验证 \\(\\partial f_i(x_0)\\subseteq \\partial f(x_0)\\) .再由定"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.266,
                0.133
            ],
            "angle": 0,
            "content": "2.7 次梯度"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "71"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.267,
                0.173
            ],
            "angle": 0,
            "content": "理2.17可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.178,
                0.56,
                0.212
            ],
            "angle": 0,
            "content": "\\[\n\\mathbf {c o n v} \\bigcup_ {i \\in I (x _ {0})} \\partial f _ {i} (x _ {0}) \\subseteq \\partial f (x _ {0}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.222,
                0.739,
                0.304
            ],
            "angle": 0,
            "content": "另一方面，设 \\(g \\in \\partial f(x_0)\\)。假设 \\(g \\notin \\mathbf{conv} \\bigcup_{i \\in I(x_0)} \\partial f_i(x_0)\\)，由严格分离定理（注意到 \\(\\mathbf{conv} \\bigcup_{i \\in I(x_0)} \\partial f_i(x_0)\\) 和 \\(\\{g\\}\\) 均为闭凸集）和定理2.20，存在 \\(a \\in \\mathbb{R}^n\\) 和 \\(b \\in \\mathbb{R}\\)，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.288,
                0.32,
                0.619,
                0.351
            ],
            "angle": 0,
            "content": "\\[\na ^ {\\mathrm {T}} g > b \\geqslant \\max  _ {i \\in I (x _ {0})} \\sup  _ {\\xi \\in \\partial f _ {i} (x _ {0})} a ^ {\\mathrm {T}} \\xi = \\max  _ {i \\in I (x _ {0})} \\partial f _ {i} (x _ {0}; a).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.364,
                0.211,
                0.38
            ],
            "angle": 0,
            "content": "因为"
        },
        {
            "type": "equation",
            "bbox": [
                0.303,
                0.393,
                0.605,
                0.492
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\partial f (x _ {0}; a) = \\lim  _ {t \\rightarrow 0 ^ {+}} \\frac {f (x _ {0} + t a) - f (x _ {0})}{t} \\\\ = \\max  _ {i \\in I (x _ {0})} \\lim  _ {t \\rightarrow 0 ^ {+}} \\frac {f _ {i} \\left(x _ {0} + t a\\right) - f _ {i} \\left(x _ {0}\\right)}{t} \\\\ = \\max  _ {i \\in I (x _ {0})} \\partial f _ {i} \\left(x _ {0}; a\\right). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.506,
                0.739,
                0.56
            ],
            "angle": 0,
            "content": "故 \\(a^{\\mathrm{T}}g > \\partial f(x_0;a)\\) ．但由于 \\(g\\in \\partial f(x_0)\\) ，我们有 \\(f(x_{0} + ta)\\geqslant f(x_{0}) + tg^{\\mathrm{T}}a\\) 因而 \\(\\partial f(x_0;a)\\geqslant a^{\\mathrm{T}}g\\) ，这就导致矛盾．故 \\(g\\in \\mathbf{conv}\\bigcup_{i\\in I(x_0)}\\partial f_i(x_0).\\) □"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.575,
                0.714,
                0.592
            ],
            "angle": 0,
            "content": "有了前面的结论，我们可以非常简单地得到一些基本函数的次梯度"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.608,
                0.739,
                0.626
            ],
            "angle": 0,
            "content": "例2.16 设 \\( f_{1}, f_{2} \\) 为凸可微函数(如图2.15)，令 \\( f(x) = \\max \\{f_{1}(x), f_{2}(x)\\} \\)."
        },
        {
            "type": "image",
            "bbox": [
                0.256,
                0.645,
                0.655,
                0.81
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.333,
                0.83,
                0.576,
                0.848
            ],
            "angle": 0,
            "content": "图2.15 例2.16图（一维情形）"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.281,
                0.131
            ],
            "angle": 0,
            "content": "72"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.156,
                0.826,
                0.194
            ],
            "angle": 0,
            "content": "(1) 若 \\( f_{1}(x) = f_{2}(x) \\)，则 \\( \\partial f(x) = \\{v \\mid v = t\\nabla f_{1}(x) + (1 - t)\\nabla f_{2}(x), 0 \\leqslant t \\leqslant 1\\} \\)；"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.21,
                0.599,
                0.229
            ],
            "angle": 0,
            "content": "(2) 若 \\(f_{1}(x) > f_{2}(x)\\), 则 \\(\\partial f(x) = \\{\\nabla f_{1}(x)\\}\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.242,
                0.597,
                0.262
            ],
            "angle": 0,
            "content": "(3) 若 \\(f_{2}(x) > f_{1}(x)\\), 则 \\(\\partial f(x) = \\{\\nabla f_{2}(x)\\}\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.156,
                0.826,
                0.262
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.279,
                0.498,
                0.296
            ],
            "angle": 0,
            "content": "例 2.17 (分段线性函数) 令"
        },
        {
            "type": "equation",
            "bbox": [
                0.442,
                0.312,
                0.641,
                0.337
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\max  _ {i = 1, 2, \\dots , m} \\left\\{a _ {i} ^ {\\mathrm {T}} x + b _ {i} \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.35,
                0.69,
                0.366
            ],
            "angle": 0,
            "content": "其中 \\(x, a_{i} \\in \\mathbb{R}^{n}, b_{i} \\in \\mathbb{R}, i = 1,2,\\dots,m\\)，如图2.16所示，则"
        },
        {
            "type": "image",
            "bbox": [
                0.371,
                0.406,
                0.715,
                0.564
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.422,
                0.636,
                0.663,
                0.653
            ],
            "angle": 0,
            "content": "图2.16 例2.17图（一维情形）"
        },
        {
            "type": "equation",
            "bbox": [
                0.433,
                0.702,
                0.651,
                0.72
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (x) = \\mathbf {c o n v} \\left\\{a _ {i} \\mid i \\in I (x) \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.732,
                0.297,
                0.748
            ],
            "angle": 0,
            "content": "其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.753,
                0.65,
                0.773
            ],
            "angle": 0,
            "content": "\\[\nI (x) = \\left\\{i \\mid a _ {i} ^ {\\mathrm {T}} x + b _ {i} = f (x) \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.787,
                0.825,
                0.824
            ],
            "angle": 0,
            "content": "例2.18 \\((\\ell_{1}\\) 范数）定义 \\(f:\\mathbb{R}^n\\to \\mathbb{R}\\) 为 \\(\\ell_1\\) 范数，则对 \\(x = (x_{1},x_{2},\\dots ,x_{n})\\in\\) \\(\\mathbb{R}^n\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.438,
                0.828,
                0.644,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\| x \\| _ {1} = \\max  _ {s \\in \\{- 1, 1 \\} ^ {n}} s ^ {\\mathrm {T}} x.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.266,
                0.133
            ],
            "angle": 0,
            "content": "2.7 次梯度"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "73"
        },
        {
            "type": "image",
            "bbox": [
                0.168,
                0.154,
                0.352,
                0.256
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.169,
                0.267,
                0.35,
                0.282
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (0, 0) = [ - 1, 1 ] \\times [ - 1, 1 ]\n\\]"
        },
        {
            "type": "image",
            "bbox": [
                0.387,
                0.157,
                0.53,
                0.255
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.387,
                0.267,
                0.545,
                0.281
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (1, 0) = \\{1 \\} \\times [ - 1, 1 ]\n\\]"
        },
        {
            "type": "image",
            "bbox": [
                0.611,
                0.158,
                0.738,
                0.254
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.61,
                0.268,
                0.734,
                0.282
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (1, 1) = \\{(1, 1) \\}\n\\]"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.323,
                0.3,
                0.585,
                0.317
            ],
            "angle": 0,
            "content": "图2.17 \\(\\ell_{1}\\) 范数的次微分（ \\(n = 2\\) ）"
        },
        {
            "type": "text",
            "bbox": [
                0.206,
                0.342,
                0.243,
                0.357
            ],
            "angle": 0,
            "content": "于是"
        },
        {
            "type": "equation",
            "bbox": [
                0.256,
                0.367,
                0.65,
                0.436
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (x) = J _ {1} \\times J _ {2} \\times \\dots \\times J _ {n}, \\quad J _ {k} = \\left\\{ \\begin{array}{c l} [ - 1, 1 ], & x _ {k} = 0, \\\\ \\{1 \\}, & x _ {k} > 0, \\\\ \\{- 1 \\}, & x _ {k} <   0. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.446,
                0.283,
                0.46
            ],
            "angle": 0,
            "content": "如图2.17所示，"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.474,
                0.497,
                0.489
            ],
            "angle": 0,
            "content": "定理2.23 可进一步推广为下面的结果："
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.502,
                0.651,
                0.52
            ],
            "angle": 0,
            "content": "定理2.24 设 \\(\\{f_{\\alpha} \\mid \\mathbb{R}^{n} \\to (-\\infty, +\\infty]\\}_{\\alpha \\in A}\\) 是一族凸函数，令"
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.532,
                0.521,
                0.559
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\sup  _ {\\alpha \\in \\mathcal {A}} f _ {\\alpha} (x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.567,
                0.687,
                0.586
            ],
            "angle": 0,
            "content": "对 \\(x_0 \\in \\cap_{\\alpha \\in \\mathcal{A}} \\operatorname{int} \\mathbf{dom} f_\\alpha\\)，定义 \\(I(x_0) = \\{\\alpha \\in \\mathcal{A} \\mid f_\\alpha(x_0) = f(x_0)\\}\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.343,
                0.596,
                0.562,
                0.629
            ],
            "angle": 0,
            "content": "\\[\n\\mathbf {c o n v} \\bigcup_ {\\alpha \\in I (x _ {0})} \\partial f _ {\\alpha} (x _ {0}) \\subseteq \\partial f (x _ {0}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.638,
                0.48,
                0.655
            ],
            "angle": 0,
            "content": "如果还有 \\(\\mathcal{A}\\) 是紧集且 \\(f_{\\alpha}\\) 关于 \\(\\alpha\\) 连续，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.343,
                0.667,
                0.564,
                0.7
            ],
            "angle": 0,
            "content": "\\[\n\\mathbf {c o n v} \\bigcup_ {\\alpha \\in I (x _ {0})} \\partial f _ {\\alpha} (x _ {0}) = \\partial f (x _ {0}).\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.719,
                0.378,
                0.735
            ],
            "angle": 0,
            "content": "4. 固定分量的函数极小值"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.746,
                0.753,
                0.79
            ],
            "angle": 0,
            "content": "设 \\(h: \\mathbb{R}^n \\times \\mathbb{R}^m \\to (-\\infty, +\\infty]\\) 是关于 \\((x,y)\\) 的凸函数, 则 \\(f(x) \\stackrel{\\mathrm{def}}{=} \\inf_{y} h(x,y)\\) 是关于 \\(x \\in \\mathbb{R}^n\\) 的凸函数. 以下结果可以用于求解 \\(f\\) 在点 \\(x\\) 处的一个次梯度."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.801,
                0.36,
                0.817
            ],
            "angle": 0,
            "content": "定理2.25 考虑函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.83,
                0.525,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\inf  _ {y} h (x, y),\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.281,
                0.131
            ],
            "angle": 0,
            "content": "74"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.157,
                0.297,
                0.173
            ],
            "angle": 0,
            "content": "其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.442,
                0.179,
                0.64,
                0.198
            ],
            "angle": 0,
            "content": "\\[\nh: \\mathbb {R} ^ {n} \\times \\mathbb {R} ^ {m} \\rightarrow (- \\infty , + \\infty ]\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.21,
                0.825,
                0.249
            ],
            "angle": 0,
            "content": "是关于 \\((x,y)\\) 的凸函数．对 \\(\\hat{x}\\in \\mathbb{R}^n\\) ，设 \\(\\hat{y}\\in \\mathbb{R}^{m}\\) 满足 \\(h(\\hat{x},\\hat{y}) = f(\\hat{x})\\) ，且存在\\(g\\in \\mathbb{R}^n\\) 使得 \\((g,0)\\in \\partial h(\\hat{x},\\hat{y})\\) ，则 \\(g\\in \\partial f(\\hat{x})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.264,
                0.682,
                0.281
            ],
            "angle": 0,
            "content": "证明．由次梯度的定义知，对任意 \\(x\\in \\mathbb{R}^n,y\\in \\mathbb{R}^m\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.386,
                0.295,
                0.697,
                0.34
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} h (x, y) \\geqslant h (\\hat {x}, \\hat {y}) + g ^ {\\mathrm {T}} (x - \\hat {x}) + 0 ^ {\\mathrm {T}} (y - \\hat {y}) \\\\ = f (\\hat {x}) + g ^ {\\mathrm {T}} (x - \\hat {x}). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.353,
                0.299,
                0.37
            ],
            "angle": 0,
            "content": "于是"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.375,
                0.686,
                0.4
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\inf  _ {y} h (x, y) \\geqslant f (\\hat {x}) + g ^ {\\mathrm {T}} (x - \\hat {x}).\n\\]"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.378,
                0.826,
                0.391
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.416,
                0.75,
                0.433
            ],
            "angle": 0,
            "content": "有了上面的结果，我们可以推导如下距离函数的部分次梯度："
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.449,
                0.569,
                0.466
            ],
            "angle": 0,
            "content": "例2.19 设 \\(C\\) 是 \\(\\mathbb{R}^n\\) 中一闭凸集，令"
        },
        {
            "type": "equation",
            "bbox": [
                0.463,
                0.483,
                0.619,
                0.509
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\inf  _ {y \\in C} \\| x - y \\| _ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.521,
                0.6,
                0.538
            ],
            "angle": 0,
            "content": "令 \\(\\hat{x} \\in \\mathbb{R}^n\\)，我们来求 \\(f\\) 在 \\(\\hat{x}\\) 处的一个次梯度"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.554,
                0.608,
                0.573
            ],
            "angle": 0,
            "content": "(1) 若 \\( f(\\hat{x}) = 0 \\)，则容易验证 \\( g = 0 \\in \\partial f(\\hat{x}) \\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.588,
                0.757,
                0.606
            ],
            "angle": 0,
            "content": "(2) 若 \\( f(\\hat{x}) > 0 \\)，由 \\( C \\) 是闭凸集，可取 \\( \\hat{y} \\) 为 \\( \\hat{x} \\) 在 \\( C \\) 上的投影，即"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.554,
                0.757,
                0.606
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.44,
                0.619,
                0.682,
                0.652
            ],
            "angle": 0,
            "content": "\\[\n\\hat{y} = \\mathcal{P}_{c}(\\hat{x})\\stackrel {\\text{def}}{=}\\operatorname *{arg  min}_{y\\in C}\\| \\hat{x} -y\\|_{2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.664,
                0.477,
                0.68
            ],
            "angle": 0,
            "content": "利用 \\(\\hat{y}\\) 的定义可以验证"
        },
        {
            "type": "equation",
            "bbox": [
                0.372,
                0.693,
                0.752,
                0.728
            ],
            "angle": 0,
            "content": "\\[\ng = \\frac {1}{\\| \\hat {x} - \\hat {y} \\| _ {2}} (\\hat {x} - \\hat {y}) = \\frac {1}{\\| \\hat {x} - \\mathcal {P} _ {c} (\\hat {x}) \\| _ {2}} (\\hat {x} - \\mathcal {P} _ {c} (\\hat {x})),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.739,
                0.579,
                0.757
            ],
            "angle": 0,
            "content": "满足定理2.25的条件．故 \\(g\\in \\partial f(\\hat{x})\\)"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.785,
                0.362,
                0.801
            ],
            "angle": 0,
            "content": "5. 复合函数"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "对于复合函数的次梯度，我们有如下链式法则（注意比较其与可微情形下链式法则的异同）："
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "2.8 总结"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "75"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.737,
                0.195
            ],
            "angle": 0,
            "content": "定理2.26 设 \\(f_{1}, f_{2}, \\dots, f_{m}: \\mathbb{R}^{n} \\to (-\\infty, +\\infty]\\) 为 \\(m\\) 个凸函数，\\(h: \\mathbb{R}^{m} \\to (-\\infty, +\\infty]\\) 为关于各分量单调递增的凸函数，令"
        },
        {
            "type": "equation",
            "bbox": [
                0.327,
                0.214,
                0.579,
                0.233
            ],
            "angle": 0,
            "content": "\\[\nf (x) = h \\left(f _ {1} (x), f _ {2} (x), \\dots , f _ {m} (x)\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.251,
                0.701,
                0.269
            ],
            "angle": 0,
            "content": "设 \\(z = (z_{1},z_{2},\\dots ,z_{m})\\in \\partial h(f_{1}(\\hat{x}),f_{2}(\\hat{x}),\\dots ,f_{m}(\\hat{x}))\\) 以及 \\(g_{i}\\in \\partial f_{i}(\\hat{x})\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.304,
                0.284,
                0.603,
                0.307
            ],
            "angle": 0,
            "content": "\\[\ng \\stackrel {\\text {d e f}} {=} z _ {1} g _ {1} + z _ {2} g _ {2} + \\dots + z _ {m} g _ {m} \\in \\partial f (\\hat {x}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.327,
                0.449,
                0.344
            ],
            "angle": 0,
            "content": "证明．易知 \\(f\\) 也是凸函数．我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.186,
                0.36,
                0.717,
                0.447
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (x) \\geqslant h \\left(f _ {1} (\\hat {x}) + g _ {1} ^ {\\mathrm {T}} (x - \\hat {x}), f _ {2} (\\hat {x}) + g _ {2} ^ {\\mathrm {T}} (x - \\hat {x}), \\dots , f _ {m} (\\hat {x}) + g _ {m} ^ {\\mathrm {T}} (x - \\hat {x})\\right) \\\\ \\geqslant h \\left(f _ {1} (\\hat {x}), f _ {2} (\\hat {x}), \\dots , f _ {m} (\\hat {x})\\right) + \\sum_ {i = 1} ^ {m} z _ {i} g _ {i} ^ {T} (x - \\hat {x}) \\\\ = f (\\hat {x}) + g ^ {\\mathrm {T}} (x - \\hat {x}), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.462,
                0.441,
                0.479
            ],
            "angle": 0,
            "content": "因此 \\(g\\) 为 \\(f\\) 在点 \\(\\hat{x}\\) 处的一个次梯度"
        },
        {
            "type": "image",
            "bbox": [
                0.72,
                0.463,
                0.738,
                0.476
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.399,
                0.52,
                0.51,
                0.542
            ],
            "angle": 0,
            "content": "2.8 总结"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.562,
                0.738,
                0.621
            ],
            "angle": 0,
            "content": "本章介绍了本书一些必要的预备知识。主要内容包括范数、导数、凸分析等方面的内容。其中凸分析方面的内容编写参考了[31]和Lieven Vandenbergh教授的课件。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.626,
                0.738,
                0.685
            ],
            "angle": 0,
            "content": "在优化算法实现当中，经常会涉及数值代数方面的内容：例如求解线性方程组、正交分解、特征值（奇异值）分解等运算。这些内容可以参考本书的附录B.2。有关数值代数的详细内容，我们推荐读者阅读[56, 216-217]。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.69,
                0.738,
                0.748
            ],
            "angle": 0,
            "content": "导数相关内容涉及梯度、海瑟矩阵的基本概念、矩阵变量函数的求导方法。关于矩阵变量函数的导数的更多内容，可以参考[150]。对于Wirtinger导数，可以参考[39]的第VI节。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.753,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "我们在凸分析部分详细介绍了凸集、凸函数、次梯度的知识，对于共轭函数部分的内容涉及较少，后面章节需要的时候会继续展开讨论。本章不加证明地给出了凸集、凸函数的很多定理和性质，其中很多证明可以在[31]中找到，该书中有对凸集、凸函数进一步的介绍和更多的例子。比较严格化的凸分析内容，我们建议感兴趣的读者阅读[164]。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.281,
                0.131
            ],
            "angle": 0,
            "content": "76"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "title",
            "bbox": [
                0.507,
                0.155,
                0.58,
                0.176
            ],
            "angle": 0,
            "content": "习题2"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.194,
                0.826,
                0.252
            ],
            "angle": 0,
            "content": "2.1 说明矩阵 \\(F\\) 范数不是算子范数（即它不可能被任何一种向量范数所诱导）。提示：算子范数需要满足某些必要条件，只需找到一个 \\(F\\) 范数不满足的必要条件即可。"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.267,
                0.654,
                0.285
            ],
            "angle": 0,
            "content": "2.2 证明：矩阵 \\(A\\) 的2范数等于其最大奇异值，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.478,
                0.3,
                0.644,
                0.327
            ],
            "angle": 0,
            "content": "\\[\n\\sigma_ {1} (A) = \\max  _ {\\| x \\| _ {2} = 1} \\| A x \\| _ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.344,
                0.553,
                0.361
            ],
            "angle": 0,
            "content": "2.3 证明如下有关矩阵范数的不等式："
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.375,
                0.497,
                0.393
            ],
            "angle": 0,
            "content": "(a) \\(\\| AB\\| _F\\leqslant \\| A\\| _2\\| B\\| _F\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.4,
                0.501,
                0.419
            ],
            "angle": 0,
            "content": "(b) \\(|\\langle A,B\\rangle |\\leqslant \\| A\\| _2\\| B\\| _*.\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.305,
                0.375,
                0.501,
                0.419
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.433,
                0.396,
                0.449
            ],
            "angle": 0,
            "content": "2.4 设矩阵 \\(A\\) 为"
        },
        {
            "type": "equation",
            "bbox": [
                0.505,
                0.448,
                0.62,
                0.492
            ],
            "angle": 0,
            "content": "\\[\nA = \\left[ \\begin{array}{c c} I & B \\\\ B ^ {\\mathrm {T}} & I \\end{array} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.498,
                0.664,
                0.515
            ],
            "angle": 0,
            "content": "其中 \\(\\|B\\|_2 < 1\\)，\\(I\\) 为单位矩阵，证明：\\(A\\) 可逆且"
        },
        {
            "type": "equation",
            "bbox": [
                0.465,
                0.526,
                0.66,
                0.562
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| A \\right\\| _ {2} \\left\\| A ^ {- 1} \\right\\| _ {2} = \\frac {1 + \\left\\| B \\right\\| _ {2}}{1 - \\left\\| B \\right\\| _ {2}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.578,
                0.825,
                0.615
            ],
            "angle": 0,
            "content": "2.5 假设 \\(A\\) 和 \\(B\\) 均为半正定矩阵，求证：\\(\\langle A, B \\rangle \\geqslant 0\\)。提示：利用对称矩阵的特征值分解。"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.63,
                0.532,
                0.647
            ],
            "angle": 0,
            "content": "2.6 计算下列矩阵变量函数的导数"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.661,
                0.807,
                0.678
            ],
            "angle": 0,
            "content": "(a) \\(f(X) = a^{\\mathrm{T}}Xb\\) ，这里 \\(X\\in \\mathbb{R}^{m\\times n}\\) ， \\(a\\in \\mathbb{R}^m,b\\in \\mathbb{R}^n\\) 为给定的向量；"
        },
        {
            "type": "text",
            "bbox": [
                0.304,
                0.687,
                0.825,
                0.725
            ],
            "angle": 0,
            "content": "(b) \\(f(X) = \\operatorname{Tr}(X^{\\mathrm{T}}AX)\\), 其中 \\(X \\in \\mathbb{R}^{m \\times n}\\) 是长方形矩阵, \\(A\\) 是方阵 (但不一定对称);"
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.733,
                0.825,
                0.772
            ],
            "angle": 0,
            "content": "(c) \\(f(X) = \\operatorname{Ind}\\det(X)\\)，其中 \\(X \\in \\mathbb{R}^{n \\times n}\\)，定义域为 \\(\\{X \\mid \\det(X) > 0\\}\\)（注意这个习题和例2.1的(3)的区别)."
        },
        {
            "type": "list",
            "bbox": [
                0.304,
                0.661,
                0.825,
                0.772
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.786,
                0.425,
                0.802
            ],
            "angle": 0,
            "content": "2.7 考虑二次不等式"
        },
        {
            "type": "equation",
            "bbox": [
                0.484,
                0.806,
                0.641,
                0.824
            ],
            "angle": 0,
            "content": "\\[\nx ^ {T} A x + b ^ {T} x + c \\leqslant 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.836,
                0.696,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(A\\) 为 \\(n\\) 阶对称矩阵，设 \\(C\\) 为上述不等式的解集"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.223,
                0.132
            ],
            "angle": 0,
            "content": "习题2"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "77"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.157,
                0.486,
                0.174
            ],
            "angle": 0,
            "content": "(a) 证明: 当 \\(A\\) 正定时, \\(C\\) 为凸集;"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.183,
                0.737,
                0.222
            ],
            "angle": 0,
            "content": "(b) 设 \\(C'\\) 是 \\(C\\) 和超平面 \\(g^{\\mathrm{T}}x + h = 0\\) 的交集 \\((g \\neq 0)\\)，若存在 \\(\\lambda \\in \\mathbb{R}\\)，使得 \\(A + \\lambda gg^{\\mathrm{T}}\\) 半正定，证明：\\(C'\\) 为凸集."
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.157,
                0.737,
                0.222
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.237,
                0.737,
                0.296
            ],
            "angle": 0,
            "content": "2.8 (鞍点问题) 设函数 \\(f: \\mathbb{R}^n \\times \\mathbb{R}^m \\to \\mathbb{R}\\) 满足如下性质：当固定 \\(z \\in \\mathbb{R}^m\\) 时，\\(f(x, z)\\) 关于 \\(x\\) 为凸函数；当固定 \\(x \\in \\mathbb{R}^n\\) 时，\\(f(x, z)\\) 关于 \\(z\\) 是凹函数，则称 \\(f\\) 为凸-凹函数。"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.311,
                0.736,
                0.349
            ],
            "angle": 0,
            "content": "(a) 设 \\(f\\) 二阶可导, 试利用海瑟矩阵 \\(\\nabla^2 f\\) 给出 \\(f\\) 为凸-凹函数的一个二阶条件;"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.358,
                0.736,
                0.396
            ],
            "angle": 0,
            "content": "(b) 设 \\(f\\) 为凸-凹函数且可微，且在点 \\((\\bar{x}, \\bar{z})\\) 处满足 \\(\\nabla f(\\bar{x}, \\bar{z}) = 0\\)，求证：对任意 \\(x\\) 和 \\(z\\)，如下鞍点性质成立："
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.311,
                0.736,
                0.396
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.392,
                0.413,
                0.591,
                0.432
            ],
            "angle": 0,
            "content": "\\[\nf (\\bar {x}, z) \\leqslant f (\\bar {x}, \\bar {z}) \\leqslant f (x, \\bar {z}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.246,
                0.448,
                0.517,
                0.464
            ],
            "angle": 0,
            "content": "进一步证明 \\(f\\) 满足极小-极大性质："
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.482,
                0.609,
                0.509
            ],
            "angle": 0,
            "content": "\\[\n\\sup  _ {z} \\inf  _ {x} f (x, z) = \\inf  _ {x} \\sup  _ {z} f (x, z).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.218,
                0.524,
                0.729,
                0.541
            ],
            "angle": 0,
            "content": "(c) 设 \\(f\\) 可微但不一定是凸-凹函数, 且在点 \\((\\bar{x}, \\bar{z})\\) 处满足鞍点性质"
        },
        {
            "type": "equation",
            "bbox": [
                0.363,
                0.557,
                0.62,
                0.576
            ],
            "angle": 0,
            "content": "\\[\nf (\\bar {x}, z) \\leqslant f (\\bar {x}, \\bar {z}) \\leqslant f (x, \\bar {z}), \\quad \\forall x, z\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.246,
                0.591,
                0.402,
                0.609
            ],
            "angle": 0,
            "content": "求证： \\(\\nabla f(\\bar{x},\\bar{z}) = 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.625,
                0.718,
                0.641
            ],
            "angle": 0,
            "content": "注：这个题目的结论和之后我们要学习的拉格朗日函数有密切联系."
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.657,
                0.482,
                0.674
            ],
            "angle": 0,
            "content": "2.9 利用凸函数二阶条件证明如下结论："
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.685,
                0.614,
                0.72
            ],
            "angle": 0,
            "content": "(a) \\(\\ln\\)-sum-exp 函数: \\( f(x) = \\ln \\sum_{k=1}^{n} \\exp x_k \\) 是凸函数;"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.727,
                0.625,
                0.763
            ],
            "angle": 0,
            "content": "(b) 几何平均: \\( f(x) = \\left( \\prod_{k=1}^{n} x_k \\right)^{1/n} (x \\in \\mathbb{R}_{++}^n) \\) 是凹函数;"
        },
        {
            "type": "text",
            "bbox": [
                0.218,
                0.769,
                0.735,
                0.82
            ],
            "angle": 0,
            "content": "(c) 设 \\( f(x) = \\left(\\sum_{i=1}^{n} x_i^p\\right)^{1/p} \\)，其中 \\( p \\in (0,1) \\)，定义域为 \\( x > 0 \\)，则 \\( f(x) \\) 是凹函数."
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.685,
                0.735,
                0.82
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.836,
                0.323,
                0.852
            ],
            "angle": 0,
            "content": "2.10 证明定理 2.12."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "78"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第二章 基础知识"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.157,
                0.586,
                0.174
            ],
            "angle": 0,
            "content": "2.11 考虑如下带有半正定约束的优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.483,
                0.182,
                0.642,
                0.261
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\min  & \\operatorname {T r} (X), \\\\ \\text {s . t .} & \\left[ \\begin{array}{c c} A & B \\\\ B ^ {\\mathrm {T}} & X \\end{array} \\right] \\succeq 0, \\\\ & X \\in \\mathcal {S} ^ {n}, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.269,
                0.452,
                0.285
            ],
            "angle": 0,
            "content": "其中 \\(A\\) 是正定矩阵"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.297,
                0.825,
                0.333
            ],
            "angle": 0,
            "content": "(a) 利用附录B.1.9中的Schur补的结论证明此优化问题的解为 \\(X = B^{\\mathrm{T}}A^{-1}B\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.34,
                0.825,
                0.381
            ],
            "angle": 0,
            "content": "(b) 利用定理2.13的 (8) 证明: 函数 \\(f(A, B) = \\operatorname{Tr}(B^{\\mathrm{T}}A^{-1}B)\\) 关于 \\((A, B)\\) 是凸函数, 其中 \\(f(A, B)\\) 的定义域 \\(\\mathbf{dom} f = \\mathcal{S}_{++}^{m} \\times \\mathbb{R}^{m \\times n}\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.305,
                0.297,
                0.825,
                0.381
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.391,
                0.484,
                0.407
            ],
            "angle": 0,
            "content": "2.12 求下列函数的共轭函数："
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.414,
                0.465,
                0.45
            ],
            "angle": 0,
            "content": "(a) 负熵: \\(\\sum_{i=1}^{n} x_{i} \\ln x_{i}\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.451,
                0.575,
                0.469
            ],
            "angle": 0,
            "content": "(b) 矩阵对数: \\( f(x) = -\\ln \\det(X) \\);"
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.475,
                0.555,
                0.491
            ],
            "angle": 0,
            "content": "(c) 最大值函数: \\( f(x) = \\max x_{i} \\);"
        },
        {
            "type": "text",
            "bbox": [
                0.304,
                0.5,
                0.822,
                0.539
            ],
            "angle": 0,
            "content": "(d) 二次锥上的对数函数: \\( f(x, t) = -\\ln (t^2 - x^{\\mathrm{T}}x) \\), 注意这里 \\( f \\) 的自变量是 \\((x, t)\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.304,
                0.414,
                0.822,
                0.539
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.55,
                0.501,
                0.567
            ],
            "angle": 0,
            "content": "2.13 求下列函数的一个次梯度："
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.578,
                0.536,
                0.596
            ],
            "angle": 0,
            "content": "(a) \\(f(x) = \\| Ax - b\\| _2 + \\| x\\| _2;\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.304,
                0.601,
                0.825,
                0.643
            ],
            "angle": 0,
            "content": "(b) \\( f(x) = \\inf_{y} \\| Ay - x \\|_{\\infty} \\)，这里可以假设能够取到 \\( \\hat{y} \\)，使得 \\( \\| A \\hat{y} - x \\|_{\\infty} = f(x) \\)."
        },
        {
            "type": "list",
            "bbox": [
                0.304,
                0.578,
                0.825,
                0.643
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.259,
                0.654,
                0.827,
                0.691
            ],
            "angle": 0,
            "content": "2.14 利用定理2.24来求出最大特征值函数 \\( f(x) = \\lambda_1(A(x)) \\) 的次微分 \\( \\partial f(x) \\)，其中 \\( A(x) \\) 是关于 \\( x \\) 的线性函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.697,
                0.731,
                0.734
            ],
            "angle": 0,
            "content": "\\[\nA (x) = A _ {0} + \\sum_ {i = 1} ^ {n} x _ {i} A _ {i}, \\quad A _ {i} \\in \\mathcal {S} ^ {m}, i = 0, \\dots , n.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.74,
                0.506,
                0.757
            ],
            "angle": 0,
            "content": "说明 \\(f(x)\\) 何时是可微函数"
        },
        {
            "type": "title",
            "bbox": [
                0.259,
                0.768,
                0.743,
                0.786
            ],
            "angle": 0,
            "content": "2.15 设 \\( f(x) \\) 为 \\( m \\)-强凸函数，求证：对于任意的 \\( x \\in \\mathbf{intdom}f \\)，"
        },
        {
            "type": "equation",
            "bbox": [
                0.41,
                0.791,
                0.713,
                0.823
            ],
            "angle": 0,
            "content": "\\[\nf (x) - \\inf  _ {y \\in \\mathbf {d o m} f} f (y) \\leqslant \\frac {1}{2 m} \\operatorname {d i s t} ^ {2} (0, \\partial f (x)),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.829,
                0.678,
                0.847
            ],
            "angle": 0,
            "content": "其中 \\(\\mathrm{dist}(z,S)\\) 表示点 \\(z\\) 到集合 \\(S\\) 的欧几里得距离"
        }
    ],
    [
        {
            "type": "title",
            "bbox": [
                0.308,
                0.254,
                0.601,
                0.285
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.337,
                0.744,
                0.438
            ],
            "angle": 0,
            "content": "优化模型是一类重要的数学模型，它是利用数学的方式来刻画一个真实问题。我们以运输问题为例来说明优化建模的过程。假如有若干仓库和市场，仓库需要向市场供应一定量的货物，而每条供应线路都需要一定费用。那么这里的优化模型就是在满足供货的情况下选择一种花费最低的方案，其中目标函数就是运输的总费用，约束就是库存和市场需求等方面的限制。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.442,
                0.744,
                0.729
            ],
            "angle": 0,
            "content": "建模的过程通常涉及以下步骤：定义目标，查找相关文献，建立模型并收集数据，初始测试，验证模型。上面的步骤中，有的可能需要重复执行。具体地，给定一个实际问题，我们需要确定模型的目标，并查找文献中是否存在类似的模型。接着，我们需要收集数据。大部分情况下，收集数据是十分耗时并且容易出错的，但有时可以很容易地找到数据集。之后，我们需要选择一个合适的模型，除了查找相关文献外，还需要结合问题的实际背景。我们往往会局限于已知的建模工具，因此建立的模型并不一定是最合适的。对于建立的模型，我们需要对其最简化的版本用实际数据来进行测试，并逐步提高复杂度来判断模型表达的正确性。验证模型是通过模型在已知情形的表现是否一致来检查模型的合理性。通过这些步骤之后，我们建立了一个合理的模型。为了让这个模型具有更高的可信度以及采用度，我们需要比较解决同一个实际问题已有的其他模型，在相同的数据集上至少给出精度差不多的解，并且某些数据集上的表现要比已有模型好。如果该实际问题没有其他模型，那么建立的模型在已知情形的预测结果需要和我们的认知一致。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.733,
                0.744,
                0.854
            ],
            "angle": 0,
            "content": "优化这一数学分支与实际应用关系密切。在研究一个优化问题的性质以及求解方式之前，了解问题本身的来源是至关重要的，这有助于明确优化方法的研究方向。另一方面，面对一个实际问题，使用数学的手段将其转化成一个可以求解的优化问题也是一项重要技能。本章将从常用的建模技巧开始，接着介绍统计学、信号处理、图像处理以及机器学习中常见的优化模型。我们将侧重于解释优化模型背后的思想和实际含义，以便帮助读者理解"
        },
        {
            "type": "page_number",
            "bbox": [
                0.444,
                0.87,
                0.466,
                0.882
            ],
            "angle": 0,
            "content": "79"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "80"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.439,
                0.174
            ],
            "angle": 0,
            "content": "模型中每一部分的作用."
        },
        {
            "type": "title",
            "bbox": [
                0.462,
                0.208,
                0.621,
                0.229
            ],
            "angle": 0,
            "content": "3.1 建模技术"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.248,
                0.826,
                0.306
            ],
            "angle": 0,
            "content": "优化建模关注的是对一个实际问题建立合适的优化模型，即我们要确定优化问题的目标函数和决策变量所在的可行域。下面分别对目标函数和约束的设计来介绍常见的建模技术。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.336,
                0.459,
                0.354
            ],
            "angle": 0,
            "content": "3.1.1 目标函数的设计"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.369,
                0.381,
                0.384
            ],
            "angle": 0,
            "content": "1. 最小二乘法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.399,
                0.825,
                0.436
            ],
            "angle": 0,
            "content": "最小二乘法建模常见于线性（非线性）方程问题中．设 \\(\\phi_i(x)\\colon \\mathbb{R}^n\\to\\) \\(\\mathbb{R},i = 1,2,\\dots ,m\\) 为 \\(n\\) 元函数，且有如下方程组："
        },
        {
            "type": "equation",
            "bbox": [
                0.437,
                0.454,
                0.825,
                0.471
            ],
            "angle": 0,
            "content": "\\[\nb _ {i} = \\phi_ {i} (x), \\quad i = 1, 2, \\dots , m, \\tag {3.1.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.488,
                0.825,
                0.588
            ],
            "angle": 0,
            "content": "其中 \\(b_{i} \\in \\mathbb{R}\\) 是已知的实数。方程组 (3.1.1) 的求解在实际中应用广泛，但这个问题并不总是可解的。首先，方程组的个数 \\(m\\) 可以超过自变量个数 \\(n\\)，因此方程组的解可能不存在；其次，由于测量误差等因素，方程组 (3.1.1) 的等式关系可能不是精确成立的。为了能在这种实际情况下求解出 \\(x\\)，最小二乘法的思想是极小化误差的 \\(\\ell_{2}\\) 范数平方，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.454,
                0.601,
                0.825,
                0.637
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\sum_ {i = 1} ^ {m} \\left(b _ {i} - \\phi_ {i} (x)\\right) ^ {2}. \\tag {3.1.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.649,
                0.825,
                0.686
            ],
            "angle": 0,
            "content": "特别地, 当 \\(\\phi_{i}\\) 均为线性函数时, 我们称问题 (3.1.2) 为线性最小二乘问题, 否则称其为非线性最小二乘问题."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.691,
                0.826,
                0.811
            ],
            "angle": 0,
            "content": "最小二乘的思想非常直观：若方程 (3.1.1) 存在解，则求解问题 (3.1.2) 的全局最优解就相当于求出了方程的解；当方程的解不存在时，问题 (3.1.2) 实际给出了某种程度上误差最小的解。读者可能注意到，最小二乘法使用了 \\(\\ell_{2}\\) 范数来度量误差大小，其主要优点有两个：第一，\\(\\ell_{2}\\) 范数平方是光滑可微的，它会给目标函数带来较好的性质；第二，\\(\\ell_{2}\\) 范数对于某种误差的处理有最优性，这一点我们在第 3.2 节中会进一步给出解答。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "最小二乘并不总是最合理的。除了构造最小二乘问题外，根据实际问题的需要，我们还经常建立最小一乘模型以及最小最大模型，其思想是使用不"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "3.1 建模技术"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.119,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "81"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.737,
                0.174
            ],
            "angle": 0,
            "content": "同的范数代替 \\(\\ell_{2}\\) 范数．如果要保证偏差的绝对值之和最小，相应的模型为："
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.183,
                0.737,
                0.22
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\sum_ {i = 1} ^ {m} | b _ {i} - \\phi_ {i} (x) |; \\tag {3.1.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.231,
                0.56,
                0.247
            ],
            "angle": 0,
            "content": "如果想要保证最大偏差最小化，对应的优化模型为："
        },
        {
            "type": "equation",
            "bbox": [
                0.363,
                0.262,
                0.737,
                0.286
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\max  _ {i} | b _ {i} - \\phi_ {i} (x) |. \\tag {3.1.4}\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.304,
                0.258,
                0.319
            ],
            "angle": 0,
            "content": "2. 正则化"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.333,
                0.738,
                0.432
            ],
            "angle": 0,
            "content": "在建模的时候，我们往往需要借助于想要得到的解的性质。比如，在最小二乘问题(3.1.2)中，当 \\(m < n\\) 时，最优解很可能不止一个，但不一定所有的解都是我们想要的。为了让解具有某种光滑性以及克服问题的病态性质（比如当 \\(\\phi\\) 为线性函数且 \\(m < n\\) 时，问题(3.1.2)的海瑟矩阵是奇异的），那么改进的模型为"
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.432,
                0.737,
                0.467
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\sum_ {i = 1} ^ {m} \\left(b _ {i} - \\phi_ {i} (x)\\right) ^ {2} + \\mu \\| x \\| _ {2} ^ {2}, \\tag {3.1.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.473,
                0.737,
                0.51
            ],
            "angle": 0,
            "content": "其中 \\(\\mu > 0\\) 为一个平衡参数. 如果想要得到的一个稀疏的解, 可以借助 \\(\\ell_0\\) 范数并构造如下模型:"
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.52,
                0.737,
                0.557
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\sum_ {i = 1} ^ {m} \\left(b _ {i} - \\phi_ {i} (x)\\right) ^ {2} + \\mu \\| x \\| _ {0}, \\tag {3.1.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.567,
                0.737,
                0.604
            ],
            "angle": 0,
            "content": "其中 \\(\\mu > 0\\) 用来控制解的稀疏度。由于 \\(\\ell_0\\) 范数在实际中难以处理，我们往往使用 \\(\\ell_1\\) 范数来代替 \\(\\ell_0\\) 范数来保证稀疏性，即构造模型"
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.614,
                0.737,
                0.65
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\sum_ {i = 1} ^ {m} \\left(b _ {i} - \\phi_ {i} (x)\\right) ^ {2} + \\mu \\| x \\| _ {1}. \\tag {3.1.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.661,
                0.737,
                0.697
            ],
            "angle": 0,
            "content": "在图像处理中, \\(x\\) 本身可能不是稀疏的, 但是其在变换域中是稀疏的. 相应的模型为"
        },
        {
            "type": "equation",
            "bbox": [
                0.318,
                0.697,
                0.737,
                0.733
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\sum_ {i = 1} ^ {m} \\left(b _ {i} - \\phi_ {i} (x)\\right) ^ {2} + \\mu \\| W (x) \\| _ {0}, \\tag {3.1.8}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.739,
                0.211,
                0.754
            ],
            "angle": 0,
            "content": "以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.318,
                0.753,
                0.737,
                0.791
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\sum_ {i = 1} ^ {m} \\left(b _ {i} - \\phi_ {i} (x)\\right) ^ {2} + \\mu \\| W (x) \\| _ {1}, \\tag {3.1.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.795,
                0.664,
                0.811
            ],
            "angle": 0,
            "content": "其中 \\(W: \\mathbb{R}^n \\to \\mathbb{R}^p\\) 表示某种变换，常用的有全变差以及小波变换。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.816,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "正则项在目标函数中的意义是明显的。例如在(3.1.5)式中，目标函数可分为两项，正则项为 \\(\\mu \\| x\\| _2^2\\) ，其含义是我们需要寻找一个 \\(x\\) ，使其同时满足"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "82"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.118,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.256
            ],
            "angle": 0,
            "content": "“误差尽量小”以及“欧几里得长度尽量短”。参数 \\(\\mu\\) 的作用是调整这两项的权重，当 \\(\\mu\\) 较大时该模型更侧重于拒绝 \\(\\ell_{2}\\) 范数较大的解。在这里我们看到 \\(\\ell_{2}\\) 范数有“收缩”的作用，即限制解向量的长度。同理，在 (3.1.6) 式中，正则项对解向量 \\(x\\) 的非零元个数进行惩罚，其含义为寻找同时满足“误差尽量小”和“非零元素尽量少”的解。通过选取参数 \\(\\mu\\) 来调节最终解 \\(x\\) 的稀疏性。"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.262,
                0.826,
                0.381
            ],
            "angle": 0,
            "content": "在第一章的低秩矩阵补全问题中，我们要求最后的解矩阵是低秩的，即在目标函数中还会有保证低秩的项．对于一个矩阵 \\(X\\) ，令 \\(\\operatorname{rank}(X)\\) 表示其秩（奇异值组成的向量的 \\(\\ell_0\\) 范数）．为了保证解的低秩，我们一般会在原优化模型的基础上添加关于 \\(\\operatorname{rank}(X)\\) 的罚项．由于 \\(\\ell_0\\) 范数与 \\(\\ell_1\\) 范数在某种程度上的等价性，实际中我们往往惩罚矩阵的核范数 \\(\\| X\\|_{*}\\) （所有奇异值的和）.这种方式的另外一个好处来自于 \\(\\| X\\|_{*}\\) 的凸性."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.413,
                0.396,
                0.429
            ],
            "angle": 0,
            "content": "3. 最大似然估计"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.445,
                0.825,
                0.544
            ],
            "angle": 0,
            "content": "在实际问题中有很多数据来自未知的概率分布，从数据反推分布的具体形式是非常重要的课题。最大似然估计就是统计中常用的一种估计概率分布的方法，其通过最大化似然函数，使得观测数据尽可能地服从假定的模型。因为最大似然估计的直观性，其在实际中非常流行。本小节简要介绍最大似然估计的思想，相关的概率基础可参见附录B.3。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.549,
                0.825,
                0.648
            ],
            "angle": 0,
            "content": "这里考虑一种简单的情况。假设已经知道数据来自某种特定的分布，但不知道该分布具体的参数。为了方便起见，令 \\(p(a;x)\\) 是其分布律或概率密度函数，其中 \\(x\\) 为未知参数。为了估计 \\(x\\) ，我们选取一列独立同分布的样本点 \\(a_1,a_2,\\dots ,a_n\\) 。似然函数定义为在参数 \\(x\\) 下的数据集 \\(\\{a_i,i = 1,2,\\dots ,n\\}\\) 发生的概率，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.47,
                0.651,
                0.612,
                0.687
            ],
            "angle": 0,
            "content": "\\[\nL (x) = \\prod_ {i = 1} ^ {n} p \\left(a _ {i}; x\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.697,
                0.825,
                0.734
            ],
            "angle": 0,
            "content": "我们看到 \\( L(x) \\) 实际上就是这 \\( n \\) 个点的联合概率（联合密度），但此时的自变量变成了参数 \\( x \\)."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.74,
                0.702,
                0.757
            ],
            "angle": 0,
            "content": "有了似然函数 \\(L(x)\\) 之后，参数的最大似然估计定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.472,
                0.775,
                0.611,
                0.802
            ],
            "angle": 0,
            "content": "\\[\n\\hat{x}\\in \\operatorname *{arg  max}_{x\\in \\mathcal{X}}L(x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{X}\\) 为参数空间．假设最大似然估计存在，则求解最大似然估计本质上是在一族分布中寻找最有可能产生该样本的参数．在实际中，似然函数的对"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "3.1 建模技术"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "83"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.538,
                0.174
            ],
            "angle": 0,
            "content": "数的最大值往往更容易求解，即考虑最大化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.186,
                0.737,
                0.214
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {x \\in \\mathcal {X}} \\quad \\ell (x) \\stackrel {\\text {d e f}} {=} \\ln L (x). \\tag {3.1.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.224,
                0.739,
                0.304
            ],
            "angle": 0,
            "content": "因为 \\(\\ln (x)\\) 是严格单调递增的，因此上面问题的最优解也为 \\(\\hat{x}\\)。但由于取对数可将乘法变为加法，实际更易操作，所以在统计中更倾向于使用对数似然函数 \\(\\ell(x)\\)。对于很多模型来说，似然函数的表达式是已知的，但是其最优解的显式表达式是未知的，因此我们需要利用数值优化算法求其全局最优解。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.308,
                0.738,
                0.367
            ],
            "angle": 0,
            "content": "这里，我们考虑一个利用最大似然估计来计算均值和协方差矩阵的例子．如果假设数据服从高斯分布，即 \\(a_{i}\\sim \\mathcal{N}(\\mu ,\\Sigma)\\) ，其中均值 \\(\\mu \\in \\mathbb{R}^p\\) 和协方差矩阵 \\(\\Sigma \\in S_{++}^{p}\\) 是未知的，那么有"
        },
        {
            "type": "equation",
            "bbox": [
                0.19,
                0.377,
                0.717,
                0.415
            ],
            "angle": 0,
            "content": "\\[\np (a; x) = \\frac {1}{\\sqrt {(2 \\pi) ^ {p} \\det (\\Sigma)}} \\exp \\left[ - \\frac {1}{2} (a - \\mu) ^ {\\mathrm {T}} \\Sigma^ {- 1} (a - \\mu) \\right], \\quad x = \\{\\mu , \\Sigma \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.425,
                0.738,
                0.464
            ],
            "angle": 0,
            "content": "通过构造并求解相应的优化问题 (3.1.10)，我们可以得到均值和协方差矩阵的估计：\\(\\hat{\\mu}\\) 和 \\(\\hat{\\Sigma}\\)."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.487,
                0.379,
                0.504
            ],
            "angle": 0,
            "content": "4. 代价、损失、收益函数"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.517,
                0.74,
                0.68
            ],
            "angle": 0,
            "content": "运筹学中的很多问题就是极小化代价（损失）并极大化收益的过程。比如，我们在玩俄罗斯方块的时候，每走一步都会有相应的分数奖励，我们期望的自然是最后的得分越高越好。在旅行的时候，我们一般会提前规划好想去的城市并确定行程。此时，我们希望在游览所有城市的情况下路程最短或者差旅费最少。在超市物品定价的时候，我们一般会根据价格与可能销售数量之间的关系来确定一个能带来最大利润的定价。这些实际问题，都可以写成优化问题的形式，其目标函数或者是极小化代价（损失），或者是极大化收益，或者是两者兼顾。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.704,
                0.291,
                0.719
            ],
            "angle": 0,
            "content": "5. 泛函、变分"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.733,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "物理、化学中的很多问题都可以表述为能量极小化的形式。比如，在电子结构计算中，我们通过极小化原子和电子之间的相互作用能量来计算稳定态。在玻色－爱因斯坦凝聚问题中，我们也是通过极小化能量泛函来找到玻色子的物质状态。一般来说，能量泛函是定义在函数空间上的，即相应优化问题的自变量是无穷维空间中的函数。我们可以通过变分来得到其相应的最优性条件等。实际中常用的另一种方式，是利用合适的离散化，将能量泛"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "84"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.214
            ],
            "angle": 0,
            "content": "函的极小化问题从无穷维空间中拉回到有限维空间中，从而得到相应问题的离散解。注意，不同的离散化一般对应于不同的目标函数及不同的优化问题。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.24,
                0.362,
                0.256
            ],
            "angle": 0,
            "content": "6. 松弛问题"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.269,
                0.826,
                0.431
            ],
            "angle": 0,
            "content": "当原始问题不容易求解时，优化中一个常用的技巧是松弛。这一技巧的基本思想是：在保留原问题部分性质的条件下，使用简单的项替代目标函数中难以处理的项，进而使得问题更易求解。例如，\\(\\ell_0\\) 范数是不可微且非凸的。由于 \\(\\ell_1\\) 范数是 \\(\\ell_0\\) 范数在某种程度上的近似，在实际中往往用 \\(\\ell_1\\) 范数代替目标函数中的 \\(\\ell_0\\) 范数。因为 \\(\\ell_1\\) 范数是凸的，相应模型的理论分析以及算法设计会更简单。对于低秩优化问题，秩对应矩阵奇异值中非零元的个数，其也是非凸且不可微的。常用方式是将其用矩阵的核范数（矩阵奇异值组成的向量的 \\(\\ell_1\\) 范数）代替，因而得到一个更易处理的凸松弛项。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.435,
                0.825,
                0.473
            ],
            "angle": 0,
            "content": "另一种松弛的策略是在问题 (1.1.1) 中使用目标函数的一个下界 \\( f_{R}(x) \\) 来替换 \\( f(x) \\) 进行求解，其中 \\( f_{R}(x) \\) 应该满足："
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.485,
                0.478,
                0.503
            ],
            "angle": 0,
            "content": "(1) \\(f_{R}(x)\\leqslant f(x),\\forall x\\in \\mathcal{X}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.516,
                0.457,
                0.534
            ],
            "angle": 0,
            "content": "(2) \\(f_{R}(x)\\) 具有简单结构"
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.485,
                0.478,
                0.534
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.547,
                0.825,
                0.584
            ],
            "angle": 0,
            "content": "例如我们将在第五章中介绍的拉格朗日函数 (5.4.2)，它实际上就可以看成是原问题目标函数的一种松弛。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.588,
                0.825,
                0.646
            ],
            "angle": 0,
            "content": "在这里注意，松弛之后的问题和原问题未必等价。之前我们反复提到 \\(\\ell_0\\) 范数可替换为 \\(\\ell_1\\) 范数均是在一定条件下进行的，而 \\(\\ell_2\\) 范数一般不能作为 \\(\\ell_0\\) 范数的松弛。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.672,
                0.42,
                0.69
            ],
            "angle": 0,
            "content": "3.1.2 约束的设计"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.703,
                0.448,
                0.72
            ],
            "angle": 0,
            "content": "1. 问题本身的物理性质"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.733,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "根据实际问题的具体形式，优化问题的决策变量需要满足各种各样的约束。比如，在电子结构计算中，我们假设轨道函数之间是相互正交的。在化学反应过程当中，各成分的浓度随着时间的变化对应于一个微分方程组。在最优设计中，我们有时需要优化物体的形状。如在飞机机翼设计中，受机翼周围的气流影响，机翼形状的改变对应于一个微分方程。在很多情况下，我们还需要非负约束，比如图像的像素值，物体的浓度，拥有的资源数量，"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "3.1 建模技术"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "85"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.737,
                0.195
            ],
            "angle": 0,
            "content": "等等. 当线性或者一般的等式观测带有噪声或者需要更多的鲁棒性时, 我们也将等式约束放宽为不等式约束."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.219,
                0.275,
                0.235
            ],
            "angle": 0,
            "content": "2. 等价转换"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.249,
                0.657,
                0.266
            ],
            "angle": 0,
            "content": "对于一个优化问题，如果其目标函数是复合函数的形式，如"
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.28,
                0.522,
                0.304
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} f (A x + b),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.313,
                0.722,
                0.332
            ],
            "angle": 0,
            "content": "我们经常引入变量 \\(y\\) 和等式约束 \\(y = Ax + b\\) 而考虑其带约束的等价问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.345,
                0.574,
                0.369
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {y} f (y), \\quad \\text {s . t .} \\quad y = A x + b.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.38,
                0.737,
                0.417
            ],
            "angle": 0,
            "content": "对于优化问题 \\(\\min_{x} f(x) = h(x) + r(x)\\), 我们往往引入 \\(y\\) 以及约束 \\(x = y\\), 将其转化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.329,
                0.422,
                0.576,
                0.445
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad h (x) + r (y), \\quad \\text {s . t .} \\quad x = y.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.449,
                0.739,
                0.507
            ],
            "angle": 0,
            "content": "通过这种方式, 将目标函数进行拆分. 对于不等式约束, 我们也可以通过引入松弛变量将其转变为等式约束和简单的非负或非正约束. 如对约束 \\(c(x) \\leqslant 0\\), 引入 \\(y \\geqslant 0\\), 可将其等价转化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.523,
                0.537,
                0.541
            ],
            "angle": 0,
            "content": "\\[\nc (x) + y = 0, \\quad y \\geqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.555,
                0.692,
                0.594
            ],
            "angle": 0,
            "content": "另外，我们还可以利用上方图来得到问题的等价形式。对于优化问题 \\(\\min_{x} f(x)\\)，根据上方图的定义，可知其等价于优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.358,
                0.607,
                0.548,
                0.631
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, t} \\quad t, \\quad \\text {s . t .} \\quad f (x) \\leqslant t.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.642,
                0.738,
                0.701
            ],
            "angle": 0,
            "content": "将优化问题的约束进行等价转换有助于让我们更方便地研究该问题的数学性质. 表面上看起来不相似的问题经过等价转化之后可能会变成类型相同的优化问题, 并最终会使得我们能够找到统一的算法来求解这些问题."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.724,
                0.241,
                0.74
            ],
            "angle": 0,
            "content": "3. 松弛"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.753,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "当原始优化模型的约束过于复杂时，我们同样可以采用松弛的技巧将难处理的约束替换为容易处理的约束。概括来说，设原始问题的可行域为 \\(\\mathcal{X}\\)，松弛之后的可行域为 \\(\\mathcal{X}_R\\)，则 \\(\\mathcal{X}_R\\) 需要满足 \\(\\mathcal{X}_R \\supset \\mathcal{X}\\)，即松弛后问题的可行域会放大。例如可以使用盒约束 \\(x \\in [0,1]\\) 来代替整数约束 \\(x \\in \\{0,1\\}\\)，或者使用不等式约束 \\(c(x) \\geqslant 0\\) 来代替等式约束 \\(c(x) = 0\\)。放大可行域要遵守两"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "86"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.118,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "点基本原则：一是将原有约束进行简化，即松弛后的问题更易处理；二是不宜将可行域放得过大，过分放大可行域会丢失原问题的关键信息，进而使得求解松弛问题变得没有意义。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.219,
                0.825,
                0.277
            ],
            "angle": 0,
            "content": "我们以球约束 \\(\\| x\\| _2 = 1\\) 为例来给出它的松弛．这是一个非凸约束，可以令 \\(X = xx^{\\mathrm{T}}\\) 并添加 \\(\\operatorname {Tr}(X) = 1\\) 将其等价刻画．注意到 \\(X = xx^{\\mathrm{T}}\\) 仍然是非凸约束，我们将其替换为半正定约束 \\(X\\succeq 0\\) ，实际上我们做了如下松弛："
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.292,
                0.688,
                0.309
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| x \\right\\| _ {2} = 1 \\rightarrow X \\in \\mathcal {S} ^ {n}, \\operatorname {T r} (X) = 1, X \\succeq 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.324,
                0.825,
                0.382
            ],
            "angle": 0,
            "content": "可以看到，当 \\(X\\) 的秩等于1时，松弛后的约束和球约束是等价的；当 \\(X\\) 的秩大于1时，我们无法找到与松弛问题变量 \\(X\\) 对应的原问题变量 \\(x\\) ，但可以通过一些技巧构造近似解."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.386,
                0.826,
                0.464
            ],
            "angle": 0,
            "content": "既然松弛问题的可行域变大，一个自然的问题就是：松弛问题的解和原问题的解具有什么联系？在一般情况下，松弛问题和原问题不等价。但在一定条件下可以证明松弛问题的解就是原问题的解，比较著名的例子可参考[40]。"
        },
        {
            "type": "title",
            "bbox": [
                0.462,
                0.495,
                0.621,
                0.516
            ],
            "angle": 0,
            "content": "3.2 回归分析"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.534,
                0.826,
                0.633
            ],
            "angle": 0,
            "content": "在现实生活中我们常常需要根据已有的信息或知识来对未知事物做出预测。在机器学习中，监督学习的任务是根据给定包含输入信息的数据集，学习一个模型，使得模型能够对新的输入数据做出好的预测。根据输出变量的类型是连续的还是离散的，经典的监督学习包括回归和分类两类问题。本节介绍如何建立回归问题的模型。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.66,
                0.362,
                0.677
            ],
            "angle": 0,
            "content": "3.2.1 概述"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.69,
                0.564,
                0.707
            ],
            "angle": 0,
            "content": "一般的回归模型可以写成如下形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.491,
                0.722,
                0.825,
                0.74
            ],
            "angle": 0,
            "content": "\\[\nb = f (a) + \\varepsilon , \\tag {3.2.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.753,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(a \\in \\mathbb{R}^d\\) 为自变量，\\(b \\in \\mathbb{R}\\) 为响应变量，\\(\\varepsilon \\in \\mathbb{R}\\) 是模型的误差（或噪声）。模型 (3.2.1) 的含义为响应变量 \\(b\\) 与自变量 \\(a\\) 通过函数 \\(f\\) 联系在一起。在实际问题中，我们一般只能知道 \\(a\\) 和 \\(b\\) 的观测值，而误差 \\(\\varepsilon\\) 是未知的。建立回归模型的最终任务是利用 \\(m\\) 个观测值 \\((a_i, b_i)\\) 来求解出 \\(f\\) 的具体形式，然后可以利用新观测的自变量对响应变量做出预测。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "3.2 回归分析"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "87"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.744,
                0.362
            ],
            "angle": 0,
            "content": "在回归模型的建立中，\\(f\\) 的选取范围是非常重要的，它实际决定了我们需要使用什么类别的模型来拟合观测到的数据。给定观测数据 \\((a_i, b_i)\\)，我们总能利用插值的方式构造出 \\(f\\)，使得 \\(f(a_i) = b_i, i = 1, 2, \\dots, m\\)。这种拟合的方式误差为零，但它是否是一个好的模型呢？一个好的模型需要有比较优秀的预测能力（或称为泛化能力），即我们需要将 \\(f\\) 作用在测试集数据上，计算其预测误差。虽然比较复杂的 \\(f\\) 可以几乎完美地拟合观测到的数据，但其预测能力可能比较差，这也就是“过拟合”现象。反之，若 \\(f\\) 形式过于简单，求解之后其并不能完全解释 \\(a\\) 和 \\(b\\) 之间的依赖关系，在已经观测的数据和预测数据上都有较大误差，这是所谓“欠拟合”现象。一个好的模型需要兼顾两方面，它应该在观测的数据上有比较小的误差，同时又具有简单的形式。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.366,
                0.741,
                0.403
            ],
            "angle": 0,
            "content": "函数 \\(f\\) 取值于函数空间中，为了缩小 \\(f\\) 的范围，一般会将其进行参数化，即模型(3.2.1)变为"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.414,
                0.515,
                0.432
            ],
            "angle": 0,
            "content": "\\[\nb = f (a; x) + \\varepsilon ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.448,
                0.741,
                0.507
            ],
            "angle": 0,
            "content": "其中 \\(f(a;x)\\) 的含义是 \\(f\\) 以 \\(x \\in \\mathbb{R}^n\\) 为参数，通过选取不同的 \\(x\\) 得到不同的 \\(f\\). 参数化的重要意义在于其将 \\(f\\) 选取的范围缩小到了有限维空间 \\(\\mathbb{R}^n\\) 中，求解函数 \\(f\\) 的过程实际上就是求解参数 \\(x\\) 的过程."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.545,
                0.355,
                0.563
            ],
            "angle": 0,
            "content": "3.2.2 线性回归模型"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.581,
                0.741,
                0.619
            ],
            "angle": 0,
            "content": "在带参数的回归模型中，最简单的模型是线性回归模型．设 \\((w_{i},b_{i})\\) 为观测到的自变量和响应变量，且不同数据点相互独立，则对每个数据点，"
        },
        {
            "type": "equation",
            "bbox": [
                0.216,
                0.639,
                0.694,
                0.657
            ],
            "angle": 0,
            "content": "\\[\nb _ {i} = w _ {i 1} x _ {1} + w _ {i 2} x _ {2} + \\dots + w _ {i, n - 1} x _ {n - 1} + x _ {n} + \\varepsilon_ {i}, \\quad i = 1, 2, \\dots , m,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.676,
                0.747,
                0.735
            ],
            "angle": 0,
            "content": "其中 \\(x_{i}\\) 是需要确定的参数， \\(\\varepsilon_{i}\\) 是某种噪声且不同数据点之间相互独立.将训练集中的输入特征加上常数项1写成 \\(a_{i} = \\left(w_{i}^{\\mathrm{T}}\\quad 1\\right)^{\\mathrm{T}}\\) ，令 \\(x = (x_{1},x_{2},\\dots ,x_{n})^{\\mathrm{T}}\\in\\) \\(\\mathbb{R}^n\\) ，则线性回归模型可以简写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.401,
                0.754,
                0.737,
                0.773
            ],
            "angle": 0,
            "content": "\\[\nb _ {i} = a _ {i} ^ {\\mathrm {T}} x + \\varepsilon_ {i}. \\tag {3.2.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.793,
                0.738,
                0.831
            ],
            "angle": 0,
            "content": "在这一小节里线性回归模型的常数项1对应元素 \\(x_{n}\\) ：下面我们在不同条件下构造相应的优化模型."
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.836,
                0.741,
                0.853
            ],
            "angle": 0,
            "content": "将训练集中的输入特征写成一个 \\(m \\times n\\) 矩阵 \\(A\\)，将标签 \\(b_{i}\\) 和噪声 \\(\\varepsilon_{i}\\) 写"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "88"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.157,
                0.383,
                0.174
            ],
            "angle": 0,
            "content": "成向量形式，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.401,
                0.181,
                0.686,
                0.274
            ],
            "angle": 0,
            "content": "\\[\nA = \\left[ \\begin{array}{c} a _ {1} ^ {\\mathrm {T}} \\\\ a _ {2} ^ {\\mathrm {T}} \\\\ \\vdots \\\\ a _ {m} ^ {\\mathrm {T}} \\end{array} \\right], \\quad b = \\left[ \\begin{array}{c} b _ {1} \\\\ b _ {2} \\\\ \\vdots \\\\ b _ {m} \\end{array} \\right], \\quad \\varepsilon = \\left[ \\begin{array}{c} \\varepsilon_ {1} \\\\ \\varepsilon_ {2} \\\\ \\vdots \\\\ \\varepsilon_ {m} \\end{array} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.281,
                0.482,
                0.298
            ],
            "angle": 0,
            "content": "则得到模型(3.2.2)的矩阵形式"
        },
        {
            "type": "equation",
            "bbox": [
                0.496,
                0.313,
                0.825,
                0.329
            ],
            "angle": 0,
            "content": "\\[\nb = A x + \\varepsilon . \\tag {3.2.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.343,
                0.697,
                0.361
            ],
            "angle": 0,
            "content": "假设 \\(\\varepsilon_{i}\\) 是高斯白噪声，即 \\(\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})\\)。那么我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.379,
                0.369,
                0.705,
                0.406
            ],
            "angle": 0,
            "content": "\\[\np (b _ {i} \\mid a _ {i}; x) = \\frac {1}{\\sqrt {2 \\pi \\sigma^ {2}}} \\exp \\left(- \\frac {(b _ {i} - a _ {i} ^ {\\mathrm {T}} x) ^ {2}}{2 \\sigma^ {2}}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.414,
                0.401,
                0.43
            ],
            "angle": 0,
            "content": "则对数似然函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.302,
                0.438,
                0.783,
                0.477
            ],
            "angle": 0,
            "content": "\\[\n\\ell (x) = \\ln \\prod_ {i = 1} ^ {m} p (b _ {i} \\mid a _ {i}; x) = - \\frac {m}{2} \\ln (2 \\pi) - m \\ln \\sigma - \\sum_ {i = 1} ^ {m} \\frac {(b _ {i} - a _ {i} ^ {\\mathrm {T}} x) ^ {2}}{2 \\sigma^ {2}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.485,
                0.825,
                0.52
            ],
            "angle": 0,
            "content": "最大似然估计则是极大化对数似然函数，去除掉常数项之后我们得到了如下最小二乘问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.466,
                0.519,
                0.825,
                0.551
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2}. \\tag {3.2.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.555,
                0.825,
                0.613
            ],
            "angle": 0,
            "content": "注意，在构建最大似然估计时不需要知道 \\(\\varepsilon_{i}\\) 的方差 \\(\\sigma^{2}\\)。以上变形的最重要的意义在于它建立了最小二乘法和回归分析的联系：当假设误差是高斯白噪声时，最小二乘解就是线性回归模型的最大似然解。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.618,
                0.827,
                0.676
            ],
            "angle": 0,
            "content": "当 \\(\\varepsilon_{i}\\) 不是高斯白噪声时，求解线性回归模型(3.2.3)和最小二乘模型(3.2.4)并不等价，因此我们需要借助似然函数来构造其他噪声所对应的目标函数．例如，在某些噪声下构造出的模型实际上为最小一乘问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.474,
                0.69,
                0.825,
                0.713
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| A x - b \\| _ {1}. \\tag {3.2.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.722,
                0.694,
                0.739
            ],
            "angle": 0,
            "content": "在习题3.7中也给出了在其他噪声假设下相应的目标函数"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.763,
                0.5,
                0.782
            ],
            "angle": 0,
            "content": "3.2.3 正则化线性回归模型"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "在第3.1节中我们介绍了建模中正则化的技巧．正则化在回归模型中的应用非常广泛，例如当数据集的特征数量大于样本总数时，问题(3.2.4)的解不唯一，这时需要借助正则项来选出性质不同的解."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "3.2 回归分析"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.119,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "89"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.157,
                0.338,
                0.173
            ],
            "angle": 0,
            "content": "1. Tikhonov 正则化"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.187,
                0.739,
                0.245
            ],
            "angle": 0,
            "content": "为了平衡模型的拟合性质和解的光滑性，Tikhonov正则化或岭回归（ridge regression）添加 \\(\\ell_2\\) 范数平方为正则项．假设 \\(\\varepsilon_{i}\\) 是高斯白噪声，则带\\(\\ell_2\\) 范数平方正则项的线性回归模型实际上是在求解如下问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.253,
                0.737,
                0.285
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2} + \\mu \\| x \\| _ {2} ^ {2}. \\tag {3.2.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.294,
                0.741,
                0.352
            ],
            "angle": 0,
            "content": "由于正则项的存在，该问题的目标函数是强凸函数，解的性质得到改善。在岭回归问题(3.2.6)中，正则项 \\(\\| x\\| _2^2\\) 的作用实际上是对范数较大的 \\(x\\) 进行惩罚，因此另一种常见的变形是给定参数 \\(\\sigma >0\\) ，求解："
        },
        {
            "type": "equation",
            "bbox": [
                0.312,
                0.36,
                0.737,
                0.392
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2}, \\quad \\text {s . t .} \\quad \\| x \\| _ {2} \\leqslant \\sigma . \\tag {3.2.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.402,
                0.738,
                0.44
            ],
            "angle": 0,
            "content": "由于问题(3.2.6)和问题(3.2.7)的最优性条件类似，当参数 \\(\\mu\\) 和 \\(\\sigma\\) 满足一定关系时，它们的解可以是相同的。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.463,
                0.373,
                0.48
            ],
            "angle": 0,
            "content": "2. LASSO问题及其变形"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.493,
                0.738,
                0.531
            ],
            "angle": 0,
            "content": "如果希望得到的解 \\(x\\) 是稀疏的，那么可以考虑添加 \\(\\ell_1\\) 范数为正则项，对应的正则化问题为第一章中提到的LASSO问题(1.2.5)："
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.54,
                0.564,
                0.572
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2} + \\mu \\| x \\| _ {1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.58,
                0.739,
                0.659
            ],
            "angle": 0,
            "content": "其中 \\(\\mu > 0\\) 为已知的实数, \\(x\\) 是待估计的参数. LASSO 问题通过惩罚参数的 \\(\\ell_{1}\\) 范数来控制解的稀疏性, 如果 \\(x\\) 是稀疏的, 那么预测值 \\(b_{i}\\) 只和 \\(a_{i}\\) 的部分元素相关. 因此, 数据点原有的 \\(n\\) 个特征中, 对预测起作用的特征对应于 \\(x\\) 的分量不为 0 , 从而 LASSO 模型起到了特征提取的功能."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.663,
                0.484,
                0.68
            ],
            "angle": 0,
            "content": "类似于问题 (3.2.7)，也可以考虑问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.312,
                0.689,
                0.737,
                0.721
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2}, \\quad \\text {s . t .} \\quad \\| x \\| _ {1} \\leqslant \\sigma . \\tag {3.2.8}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.73,
                0.558,
                0.747
            ],
            "angle": 0,
            "content": "考虑到噪声 \\(\\varepsilon\\) 的存在，还可以给定 \\(\\nu > 0\\) 考虑模型："
        },
        {
            "type": "equation",
            "bbox": [
                0.32,
                0.761,
                0.737,
                0.786
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| x \\| _ {1}, \\quad \\text {s . t .} \\quad \\| A x - b \\| _ {2} \\leqslant v. \\tag {3.2.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.795,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "优化模型 (3.2.8) 和 (3.2.9) 本质思想是相似的, 即 “在控制误差的条件下使得 \\(x\\) 的 \\(\\ell_{1}\\) 范数尽量小”. 但它们所属的优化问题种类实际上是不一样的, 我们将在第四章中进一步说明."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "90"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.157,
                0.794,
                0.174
            ],
            "angle": 0,
            "content": "如果 \\(\\varepsilon\\) 不是高斯白噪声，则需要根据具体类型选择损失函数，比如"
        },
        {
            "type": "equation",
            "bbox": [
                0.422,
                0.189,
                0.825,
                0.213
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| A x - b \\| _ {2} + \\mu \\| x \\| _ {1}, \\tag {3.2.10}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.425,
                0.215,
                0.825,
                0.237
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| A x - b \\| _ {1} + \\mu \\| x \\| _ {1}. \\tag {3.2.11}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.247,
                0.825,
                0.305
            ],
            "angle": 0,
            "content": "上述两个模型和问题 (1.2.5) 的差别在于对损失函数选择的范数不同，它们的性能可能很不一样。当然损失函数项还有很多变化形式，如同时考虑 \\(\\ell_{2}\\) 范数和 \\(\\ell_{1}\\) 范数的组合，或选择 Student- \\(t\\) 分布等。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.309,
                0.825,
                0.388
            ],
            "angle": 0,
            "content": "传统 LASSO 问题要求 \\( x \\) 是一个稀疏解，但实际问题的稀疏性可能有很多种表达。如果参数 \\( x \\) 具有分组稀疏性，即 \\( x \\) 的分量可分为 \\( G \\) 个组，每个组内的参数必须同时为零或同时非零，则传统 LASSO 问题的解无法满足这样的需求。为此人们提出了分组 LASSO 模型："
        },
        {
            "type": "equation",
            "bbox": [
                0.4,
                0.398,
                0.825,
                0.437
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2} + \\mu \\sum_ {\\ell = 1} ^ {G} \\sqrt {n _ {\\ell}} \\| x _ {\\mathcal {I} _ {\\ell}} \\| _ {2}, \\tag {3.2.12}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.448,
                0.545,
                0.464
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{I}_{\\ell}\\) 是属于第 \\(\\ell\\) 组变量的指标集且"
        },
        {
            "type": "equation",
            "bbox": [
                0.457,
                0.474,
                0.627,
                0.514
            ],
            "angle": 0,
            "content": "\\[\n\\left| \\mathcal {I} _ {\\ell} \\right| = n _ {\\ell}, \\quad \\sum_ {\\ell = 1} ^ {G} n _ {\\ell} = n.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.524,
                0.825,
                0.603
            ],
            "angle": 0,
            "content": "当 \\(n_{\\ell} = 1, \\ell = 1,2,\\dots,G\\) 时，问题(3.2.12)就退化成传统的LASSO问题．从分组LASSO问题正则项的设计可以看出，该正则项实际上是 \\(\\| x_{\\mathcal{I}_\\ell}\\| _2\\) 的 \\(\\ell_1\\) 范数，因此只有少数 \\(\\| x_{\\mathcal{I}_\\ell}\\| _2\\) 不为零．分组LASSO问题把稀疏性从单个特征提升到了组的级别上，但不要求组内的稀疏性."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.607,
                0.825,
                0.644
            ],
            "angle": 0,
            "content": "如果需要同时保证分组以及单个特征的稀疏性，我们可以考虑将两种正则项结合起来，即有稀疏分组 LASSO 模型："
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.655,
                0.825,
                0.693
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2} + \\mu_ {1} \\sum_ {\\ell = 1} ^ {G} \\sqrt {n _ {\\ell}} \\| x _ {\\mathcal {I} _ {\\ell}} \\| _ {2} + \\mu_ {2} \\| x \\| _ {1}, \\tag {3.2.13}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.703,
                0.825,
                0.773
            ],
            "angle": 0,
            "content": "其中 \\(\\mu_1, \\mu_2 > 0\\) 为给定的常数。稀疏分组 LASSO 模型的正则项由两部分组成：\\(\\|x\\|_1\\) 是为了保证 \\(x\\) 本身的稀疏性，而 \\(\\sum_{\\ell=1}^{G} \\sqrt{n_\\ell} \\|x_{\\mathcal{I}_\\ell}\\|_2\\) 是为了保证 \\(x\\) 的分组稀疏性。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.778,
                0.825,
                0.816
            ],
            "angle": 0,
            "content": "对于某些实际问题，特征 \\(x\\) 本身不是稀疏的，但其在某种变换下是稀疏的，因此我们也需要相应调整正则项．一般的问题形式可以是："
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.825,
                0.825,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2} + \\mu \\| F x \\| _ {1}, \\tag {3.2.14}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "3.3 逻辑回归"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "91"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.737,
                0.195
            ],
            "angle": 0,
            "content": "在这里为了方便起见，我们假设 \\(x\\) 在某线性变换下是稀疏的，其中 \\(F \\in \\mathbb{R}^{p \\times n}\\) 是该线性变换对应的矩阵．例如当 \\(F\\) 取为"
        },
        {
            "type": "equation",
            "bbox": [
                0.322,
                0.205,
                0.583,
                0.314
            ],
            "angle": 0,
            "content": "\\[\nF = \\left[ \\begin{array}{c c c c c c} 1 & - 1 & & & & \\\\ & 1 & - 1 & & & \\\\ & & \\ddots & \\ddots & & \\\\ & & & 1 & - 1 & \\\\ & & & & 1 & - 1 \\end{array} \\right]\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.324,
                0.744,
                0.385
            ],
            "angle": 0,
            "content": "时，它实际上要求 \\(x\\) 相邻点之间的变化是稀疏的．实际上 \\(\\|Fx\\|_1\\) 还可以与 \\(\\|x\\|_1\\) 结合起来，这表示同时对 \\(Fx\\) 和 \\(x\\) 提出稀疏性的要求．例如融合 LASSO 模型（fused-LASSO）可表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.247,
                0.391,
                0.741,
                0.43
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2} + \\mu_ {1} \\| x \\| _ {1} + \\mu_ {2} \\sum_ {i = 2} ^ {n} | x _ {i} - x _ {i - 1} |, \\tag {3.2.15}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.441,
                0.741,
                0.493
            ],
            "angle": 0,
            "content": "其中 \\(\\mu_{2}\\sum_{i = 2}^{n}|x_{i} - x_{i - 1}|\\) 用来控制相邻系数之间的平稳度.通过求解模型(3.2.15)可以获得线性回归模型的一个稀疏解，且 \\(x\\) 的分量之间的变化比较平缓"
        },
        {
            "type": "title",
            "bbox": [
                0.374,
                0.522,
                0.534,
                0.544
            ],
            "angle": 0,
            "content": "3.3 逻辑回归"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.56,
                0.741,
                0.64
            ],
            "angle": 0,
            "content": "在分类问题中，输出变量取值于离散空间．对于二分类问题，预测变量只有两个取值，即-1,1.我们简要介绍机器学习中的一种经典分类模型：逻辑回归（logistic regression）模型．给定特征 \\(a\\) ，逻辑回归假设这个样本属于类别1的概率"
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.653,
                0.587,
                0.673
            ],
            "angle": 0,
            "content": "\\[\np (1 | a; x) = P (t = 1 | a; x) = \\theta (a ^ {\\mathrm {T}} x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.687,
                0.318,
                0.704
            ],
            "angle": 0,
            "content": "其中Sigmoid函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.373,
                0.702,
                0.738,
                0.737
            ],
            "angle": 0,
            "content": "\\[\n\\theta (z) = \\frac {1}{1 + \\exp (- z)}, \\tag {3.3.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.741,
                0.363,
                0.758
            ],
            "angle": 0,
            "content": "那么属于类别-1的概率"
        },
        {
            "type": "equation",
            "bbox": [
                0.306,
                0.771,
                0.602,
                0.791
            ],
            "angle": 0,
            "content": "\\[\np (- 1 | a; x) = 1 - p (1 | a; x) = \\theta (- a ^ {\\mathrm {T}} x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.804,
                0.542,
                0.822
            ],
            "angle": 0,
            "content": "因此对于 \\(b \\in \\{-1, 1\\}\\)，上述概率可以简洁地写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.368,
                0.835,
                0.54,
                0.855
            ],
            "angle": 0,
            "content": "\\[\np (b \\mid a; x) = \\theta (b \\cdot a ^ {\\mathrm {T}} x).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "92"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.836,
                0.195
            ],
            "angle": 0,
            "content": "假设数据对 \\(\\{a_i, b_i\\}, i = 1,2,\\dots,m\\) 之间独立同分布，则在给定 \\(a_1,a_2,\\dots,a_m\\) 情况下，\\(b_1,b_2,\\dots,b_m\\) 的联合概率密度是"
        },
        {
            "type": "equation",
            "bbox": [
                0.314,
                0.203,
                0.768,
                0.281
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} p \\left(b _ {1}, b _ {2}, \\dots , b _ {m} \\mid a _ {1}, a _ {2}, \\dots , a _ {m}; x\\right) = \\prod_ {i = 1} ^ {m} p \\left(b _ {i} \\mid a _ {i}; x\\right) \\\\ = \\frac {1}{\\prod_ {i = 1} ^ {m} (1 + \\exp (- b _ {i} \\cdot a _ {i} ^ {\\mathrm {T}} x))}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.287,
                0.597,
                0.304
            ],
            "angle": 0,
            "content": "那么，最大似然估计是求解如下最优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.418,
                0.313,
                0.825,
                0.351
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\sum_ {i = 1} ^ {m} \\ln \\left(1 + \\exp \\left(- b _ {i} \\cdot a _ {i} ^ {\\mathrm {T}} x\\right)\\right). \\tag {3.3.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.36,
                0.825,
                0.397
            ],
            "angle": 0,
            "content": "与稀疏优化类似，常用模型还需要考虑参数 \\(x\\) 的性质，加上正则项，如Tikhonov正则化模型"
        },
        {
            "type": "equation",
            "bbox": [
                0.387,
                0.407,
                0.826,
                0.445
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\sum_ {i = 1} ^ {m} \\ln \\left(1 + \\exp \\left(- b _ {i} \\cdot a _ {i} ^ {\\mathrm {T}} x\\right)\\right) + \\lambda \\| x \\| _ {2} ^ {2} \\tag {3.3.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.453,
                0.432,
                0.47
            ],
            "angle": 0,
            "content": "和 \\(\\ell_1\\) 范数正则化模型："
        },
        {
            "type": "equation",
            "bbox": [
                0.385,
                0.48,
                0.826,
                0.517
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\sum_ {i = 1} ^ {m} \\ln \\left(1 + \\exp \\left(- b _ {i} \\cdot a _ {i} ^ {\\mathrm {T}} x\\right)\\right) + \\lambda \\| x \\| _ {1}. \\tag {3.3.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.526,
                0.825,
                0.564
            ],
            "angle": 0,
            "content": "假设数据对 \\(\\{a_i, b_i\\}\\) 是由随机变量对 \\(\\{\\alpha, \\beta\\}\\) 产生的，那么损失函数可以写成均值形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.437,
                0.566,
                0.644,
                0.587
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ \\ln \\left(1 + \\exp \\left(- \\beta \\cdot \\alpha^ {\\mathrm {T}} x\\right)\\right) \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.595,
                0.741,
                0.612
            ],
            "angle": 0,
            "content": "而问题(3.3.3)和(3.3.4)可以写成下面随机优化问题的抽样形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.382,
                0.625,
                0.7,
                0.65
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\mathbb {E} \\left[ \\ln \\left(1 + \\exp (- \\beta \\cdot \\alpha^ {\\mathrm {T}} x)\\right) \\right] + \\lambda r (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.659,
                0.825,
                0.696
            ],
            "angle": 0,
            "content": "其中 \\(r(x)\\) 为正则项，\\(\\lambda\\) 为正则参数。很多机器学习问题可以写成更一般的随机优化问题和它的离散版本："
        },
        {
            "type": "equation",
            "bbox": [
                0.464,
                0.711,
                0.826,
                0.734
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x) + \\lambda r (x), \\tag {3.3.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.743,
                0.299,
                0.759
            ],
            "angle": 0,
            "content": "其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.277,
                0.769,
                0.805,
                0.806
            ],
            "angle": 0,
            "content": "\\[\nf (x) \\stackrel {\\mathrm {d e f}} {=} \\mathbb {E} [ F (x, \\xi) ] = \\int_ {\\Omega} F (x, \\xi (\\omega))   \\mathrm {d} P (\\omega), \\quad \\text {或} \\quad f (x) \\stackrel {\\mathrm {d e f}} {=} \\frac {1}{m} \\sum_ {i = 1} ^ {m} f _ {i} (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.815,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "\\(\\xi : \\Omega \\to W\\) 是定义在给定概率空间 \\((\\Omega, \\mathcal{F}, P)\\) 上的随机变量, \\(W\\) 是可测空间, \\(F: \\mathbb{R}^n \\times W \\to \\mathbb{R}\\) 以及 \\(f_i: \\mathbb{R}^n \\to \\mathbb{R}, i = 1, 2, \\dots, m\\) 对应某个损失函数."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.303,
                0.133
            ],
            "angle": 0,
            "content": "3.4 支持向量机"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "93"
        },
        {
            "type": "title",
            "bbox": [
                0.361,
                0.155,
                0.548,
                0.177
            ],
            "angle": 0,
            "content": "3.4 支持向量机"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.198,
                0.744,
                0.3
            ],
            "angle": 0,
            "content": "支持向量机（support vector machine, SVM）是另一种被应用广泛的二分类模型。给定训练数据集 \\(D\\) 中的样本点 \\((a_{i}, b_{i})\\) 且 \\(a_{i} \\in \\mathbb{R}^{n}\\)，\\(b_{i} \\in \\{-1,1\\}\\)，SVM 的基本思想是找到一个超平面将 \\(\\mathbb{R}^{n}\\) 中的样本点划分成两类。我们先考虑一种简单情形：假定训练数据集是线性可分的，即正负两类刚好被划分到超平面 \\(x^{\\mathrm{T}}w + y = 0\\) 两侧，如图 3.1 所示。"
        },
        {
            "type": "image",
            "bbox": [
                0.286,
                0.324,
                0.627,
                0.555
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.352,
                0.575,
                0.557,
                0.593
            ],
            "angle": 0,
            "content": "图3.1 线性可分的数据集"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.618,
                0.741,
                0.699
            ],
            "angle": 0,
            "content": "可以看出，给定线性可分的数据集后，满足“划分要求”的超平面一般不是唯一的。比较理想的超平面应该具有下面的特点：数据点距此平面的距离都比较远。使用这样的超平面建立的二分类模型会有比较好的鲁棒性。几何学的知识告诉我们，样本空间中一点 \\( w \\) 到超平面 \\( x^{\\mathrm{T}}w + y = 0 \\) 的距离"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.711,
                0.514,
                0.749
            ],
            "angle": 0,
            "content": "\\[\nd = \\frac {| x ^ {\\mathrm {T}} w + y |}{| | x | | _ {2}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.762,
                0.524,
                0.779
            ],
            "angle": 0,
            "content": "如果这个超平面对样本点 \\((a_{i}, b_{i})\\) 分类正确，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.329,
                0.798,
                0.579,
                0.817
            ],
            "angle": 0,
            "content": "\\[\nb _ {i} \\left(a _ {i} ^ {\\mathrm {T}} x + y\\right) > 0, \\quad i = 1, 2, \\dots , m.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "为了寻找理想的超平面来分离数据点，我们要求两类数据中的点到该超平"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "94"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.156,
                0.747,
                0.174
            ],
            "angle": 0,
            "content": "面 \\(x^{\\mathrm{T}}w + y = 0\\) 最小距离尽量的大．于是可建立如下的原始模型："
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.193,
                0.826,
                0.226
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {x, y, \\gamma} \\quad \\begin{array}{l} \\gamma , \\\\ b _ {i} (a _ {i} ^ {\\mathrm {T}} x + u) \\end{array} \\tag {3.4.1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.407,
                0.226,
                0.689,
                0.248
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & \\frac {\\left. x _ {i} + y _ {i} \\right|}{\\left\\| x \\right\\| _ {2}} \\geqslant \\gamma \\quad i = 1, 2, \\dots , m. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.262,
                0.827,
                0.32
            ],
            "angle": 0,
            "content": "问题 (3.4.1) 中 \\(\\gamma\\) 的含义是明显的: 它表示所有样本点到超平面 \\(x^{\\mathrm{T}} w + y = 0\\) 距离的最小值, 而目标是将其最大化. 接下来我们对问题 (3.4.1) 进行等价转化, 使得其形式更加自然. 注意到问题 (3.4.1) 中的约束等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.399,
                0.337,
                0.684,
                0.355
            ],
            "angle": 0,
            "content": "\\[\nb _ {i} \\left(a _ {i} ^ {\\mathrm {T}} x + y\\right) \\geqslant \\gamma \\| x \\| _ {2}, \\quad i = 1, 2, \\dots , m,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.373,
                0.825,
                0.436
            ],
            "angle": 0,
            "content": "而以相同的正倍数对 \\(x\\) 和 \\(y\\) 进行放缩不会影响问题 (3.4.1) 的约束和目标函数。根据这个特点将问题 (3.4.1) 的可行域缩小，强制取 \\(\\| x \\|_2 = \\frac{1}{\\gamma}\\)，则该问题等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.375,
                0.443,
                0.446,
                0.464
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {x, y, \\gamma} \\quad \\gamma ,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.388,
                0.466,
                0.707,
                0.484
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & b _ {i} \\left(a _ {i} ^ {\\mathrm {T}} x + y\\right) \\geqslant \\gamma \\| x \\| _ {2}, \\quad i = 1, 2, \\dots , m, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.484,
                0.505,
                0.516
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| x \\right\\| _ {2} = \\frac {1}{\\gamma}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.525,
                0.679,
                0.558
            ],
            "angle": 0,
            "content": "最终我们消去变量 \\(\\gamma\\) 以及约束 \\(\\|x\\|_2 = \\frac{1}{\\gamma}\\) 得到优化问题:"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.57,
                0.825,
                0.607
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, y} \\frac {1}{2} \\| x \\| _ {2} ^ {2}, \\tag {3.4.2}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.405,
                0.609,
                0.688,
                0.627
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & b _ {i} \\left(a _ {i} ^ {\\mathrm {T}} x + y\\right) \\geqslant 1, \\quad i = 1, 2, \\dots , m. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.643,
                0.827,
                0.701
            ],
            "angle": 0,
            "content": "对问题 (3.4.2) 求解得到 \\(\\hat{x}, \\hat{y}\\)，我们称使得 \\(b_{i}(a_{i}^{\\mathrm{T}}\\hat{x} + \\hat{y}) = 1\\) 成立的 \\(a_{i}\\) 为支持向量。不难发现，超平面的参数 \\(\\hat{x}, \\hat{y}\\) 完全由支持向量所决定。换句话说，去掉非支持向量的数据点不会影响 (3.4.2) 的解。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.706,
                0.827,
                0.743
            ],
            "angle": 0,
            "content": "当线性可分的假设不成立时，我们对每个数据点引入非负松弛变量 \\(\\xi_{i}\\) 允许有误分点，则(3.4.1)中的约束变为"
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.755,
                0.731,
                0.791
            ],
            "angle": 0,
            "content": "\\[\n\\frac {b _ {i} \\left(a _ {i} ^ {\\mathrm {T}} x + y\\right)}{\\| x \\| _ {2}} \\geqslant \\gamma (1 - \\xi_ {i}), \\quad \\xi_ {i} \\geqslant 0, \\quad i = 1, 2, \\dots , m.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.803,
                0.826,
                0.856
            ],
            "angle": 0,
            "content": "这里使用相对形式 \\(\\gamma (1 - \\xi_{i})\\) 来表示误分点的“距离”。显然，误分点不宜太多，我们通过松弛变量的函数 \\(\\sum_{i = 1}^{m}\\xi_{i}\\) 来控制误分的程度。经过与前面类似的"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "3.5 概率图模型"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "95"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.366,
                0.174
            ],
            "angle": 0,
            "content": "等价转化，最终得到问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.184,
                0.512,
                0.218
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, y, \\xi} \\frac {1}{2} \\| x \\| _ {2} ^ {2} + \\mu \\sum_ {i = 1} ^ {m} \\xi_ {i},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.353,
                0.218,
                0.736,
                0.242
            ],
            "angle": 0,
            "content": "\\[\n\\text {s . t .} \\quad b _ {i} \\left(a _ {i} ^ {\\mathrm {T}} x + y\\right) \\geqslant 1 - \\xi_ {i}, \\tag {3.4.3}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.393,
                0.251,
                0.565,
                0.266
            ],
            "angle": 0,
            "content": "\\[\n\\xi_ {i} \\geqslant 0, \\quad i = 1, 2, \\dots , m,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.277,
                0.737,
                0.315
            ],
            "angle": 0,
            "content": "其中 \\(\\mu > 0\\) 是惩罚系数。增大 \\(\\mu\\) 值将增大对误分类的惩罚，减小 \\(\\mu\\) 值将减小误分类的惩罚。上述两个优化问题 (3.4.2) 和 (3.4.3) 是二次规划的特殊形式。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.319,
                0.486,
                0.335
            ],
            "angle": 0,
            "content": "模型(3.4.3)也等价于无约束优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.288,
                0.344,
                0.737,
                0.38
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, y} \\quad \\frac {1}{2} \\| x \\| _ {2} ^ {2} + \\mu \\sum_ {i = 1} ^ {m} \\max  \\left\\{1 - b _ {i} \\left(a _ {i} ^ {\\mathrm {T}} x + y\\right), 0 \\right\\}. \\tag {3.4.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.389,
                0.739,
                0.51
            ],
            "angle": 0,
            "content": "注意到 \\(\\max \\{1 - b_{i}(a_{i}^{\\mathrm{T}}x + y),0\\}\\) 是对不满足不等式 \\(b_{i}(a_{i}^{\\mathrm{T}}x + y)\\geqslant 1\\) 的量的惩罚，因此问题(3.4.4)也可以看成求解问题(3.4.2)的罚函数法（有关罚函数法在第7.1节中会进一步讨论）．虽然 \\(\\max \\{z,0\\}\\) 不可微，但是问题(3.4.4)的简单形式也给算法设计提供了很多可能．也可以看出引入不同罚函数能构造出不同的SVM模型．此外，当训练数据中含有冗余特征时，人们也考虑将\\(\\| x\\| _2^2\\) 项替换成 \\(\\ell_1\\) 范数求解："
        },
        {
            "type": "equation",
            "bbox": [
                0.293,
                0.52,
                0.737,
                0.555
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| x \\| _ {1} + \\mu \\sum_ {i = 1} ^ {m} \\max  \\left\\{1 - b _ {i} \\left(a _ {i} ^ {\\mathrm {T}} x + y\\right), 0 \\right\\}. \\tag {3.4.5}\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.361,
                0.574,
                0.547,
                0.596
            ],
            "angle": 0,
            "content": "3.5 概率图模型"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.613,
                0.737,
                0.672
            ],
            "angle": 0,
            "content": "概率图模型是概率论中一个重要的概念。它是一种利用图结构来描述多元随机变量之间条件独立关系的概率模型，对于高维空间中的概率模型的研究具有重要作用。本节涉及的概率论知识可以在附录B.3中找到。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.675,
                0.737,
                0.713
            ],
            "angle": 0,
            "content": "给定 \\(n\\) 维空间中的一个随机向量 \\(X = (X_{1}, X_{2}, \\dots, X_{n})\\)，其对应的联合概率为 \\(n\\) 元函数。根据条件概率有"
        },
        {
            "type": "equation",
            "bbox": [
                0.174,
                0.725,
                0.792,
                0.806
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} P (X = x) \\\\ = P \\left(X _ {1} = x _ {1}\\right) P \\left(X _ {2} = x _ {2} \\mid X _ {1} = x _ {1}\\right) \\dots P \\left(X _ {n} = x _ {n} \\mid X _ {1} = x _ {1}, X _ {2} = x _ {2}, \\dots , X _ {n - 1} = x _ {n - 1}\\right) \\\\ = \\prod_ {k = 1} ^ {n} P \\left(X _ {k} = x _ {k} \\mid X _ {1} = x _ {1}, X _ {2} = x _ {2}, \\dots , X _ {k - 1} = x _ {k - 1}\\right). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "假设每个变量 \\(X_{i}, i = 1,2,\\dots ,n\\) 为离散的，并且只有 \\(m\\) 个取值．在没有任何独立性假设的情况下，我们需要 \\((m^{n} - 1)\\) 个参数才能确定其概率分布.如果"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "96"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.826,
                0.235
            ],
            "angle": 0,
            "content": "变量 \\(X_{i}\\) 之间有条件独立性, 相应的概率分布可以用少量的参数来确定。例如: 对于 3 个二值变量 \\(X_{1}, X_{2}, X_{3}\\), 在不知道其相互间的依赖关系时, 一共需要 \\(2^{3} - 1 = 7\\) 个参数来确定其联合分布。如果假设已知 \\(X_{2}\\) 时, \\(X_{1}\\) 和 \\(X_{3}\\) 独立, 则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.345,
                0.249,
                0.742,
                0.265
            ],
            "angle": 0,
            "content": "\\[\nP \\left(X _ {1} = x _ {1} \\mid X _ {2} = x _ {2}, X _ {3} = x _ {3}\\right) = P \\left(X _ {1} = x _ {1} \\mid X _ {2} = x _ {2}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.345,
                0.273,
                0.741,
                0.289
            ],
            "angle": 0,
            "content": "\\[\nP \\left(X _ {3} = x _ {3} \\mid X _ {1} = x _ {1}, X _ {2} = x _ {2}\\right) = P \\left(X _ {3} = x _ {3} \\mid X _ {2} = x _ {2}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.301,
                0.4,
                0.316
            ],
            "angle": 0,
            "content": "进一步地，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.309,
                0.331,
                0.775,
                0.394
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} P (X = x) \\\\ = P \\left(X _ {1} = x _ {1}\\right) P \\left(X _ {2} = x _ {2} \\mid X _ {1} = x _ {1}\\right) P \\left(X _ {3} = x _ {3} \\mid X _ {1} = x _ {1}, X _ {2} = x _ {2}\\right) \\\\ = P \\left(X _ {1} = x _ {1}\\right) P \\left(X _ {2} = x _ {2} \\mid X _ {1} = x _ {1}\\right) P \\left(X _ {3} = x _ {3} \\mid X _ {2} = x _ {2}\\right). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.406,
                0.7,
                0.423
            ],
            "angle": 0,
            "content": "因此，只需要 \\(1 + 2 + 2 = 5\\) 个参数就可以确定该联合分布"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.427,
                0.827,
                0.505
            ],
            "angle": 0,
            "content": "当概率模型中变量比较多时，其相应的依赖关系也会比较复杂。这时，图模型可以帮助我们更加直观地了解随机变量之间的条件独立关系。我们这里介绍无向图模型，也称为马尔可夫（Markov）随机场或马尔可夫网络，其利用无向图来描述一组具有马尔可夫性质的随机变量的联合分布。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.518,
                0.828,
                0.596
            ],
            "angle": 0,
            "content": "定义3.1（马尔可夫随机场）对于随机向量 \\(X = (X_{1},X_{2},\\dots ,X_{n})\\) 和有 \\(n\\) 个节点的无向图 \\(G = (V,E)\\) ，其中 \\(V\\) 表示节点集合且 \\(V = \\{X_{1},X_{2},\\dots ,X_{n}\\}\\) \\(E\\) 表示节点之间边的集合．如果 \\((G,X)\\) 满足局部马尔可夫性质，即给定变量\\(X_{k}\\) 的邻居的取值，其与所有其他的变量独立，"
        },
        {
            "type": "equation",
            "bbox": [
                0.396,
                0.61,
                0.686,
                0.629
            ],
            "angle": 0,
            "content": "\\[\nP \\left(X _ {k} = x _ {k} \\mid X _ {- k}\\right) = P \\left(X _ {k} = x _ {k} \\mid X _ {N (k)}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.642,
                0.826,
                0.7
            ],
            "angle": 0,
            "content": "其中 \\(X_{-k}\\) 表示除了 \\(X_{k}\\) 外其他随机变量的集合, \\(X_{N(k)}\\) 表示 \\(X_{k}\\) 的邻居集合,即和 \\(X_{k}\\) 有边直接相连的随机变量的集合, 那么, 我们称 \\((G,X)\\) 为一个马尔可夫随机场."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.711,
                0.825,
                0.77
            ],
            "angle": 0,
            "content": "假设概率图模型中的随机向量服从多元高斯分布 \\(\\mathcal{N}(\\mu, \\Sigma)\\). 令 \\(\\Theta = \\Sigma^{-1}\\)为协方差矩阵 \\(\\Sigma\\) 的逆矩阵，并称之为精度矩阵. 根据 [73]练习 17.3，我们有如下命题："
        },
        {
            "type": "text",
            "bbox": [
                0.344,
                0.785,
                0.737,
                0.802
            ],
            "angle": 0,
            "content": "\\(\\theta_{ij} = 0 \\Leftrightarrow\\) 给定 \\(X_{k}, k \\neq i, j\\) 的取值, \\(X_{i}\\) 和 \\(X_{j}\\) 是独立的."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.815,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "因此，精度矩阵中的元素 \\(\\theta_{ij}\\) 为0则表示无向图中节点 \\(X_{i}\\) 和 \\(X_{j}\\) 之间不存在直接相连的边，即在给定邻居信息的情况下，\\(X_{i}\\) 与 \\(X_{j}\\) 条件独立．如果"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "3.5 概率图模型"
        },
        {
            "type": "page_number",
            "bbox": [
                0.716,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "97"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.157,
                0.739,
                0.235
            ],
            "angle": 0,
            "content": "\\(\\theta_{ij} \\neq 0\\), 则表示 \\(X_{i}\\) 和 \\(X_{j}\\) 之间存在直接相连的边, 是彼此相关的. 因此, 精度矩阵给出了无向图的结构信息以及分布的参数信息. 在实际中, 我们关心如何从数据中学出精度矩阵. 利用精度矩阵的似然函数, 我们可以得到一个凸优化问题."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.24,
                0.741,
                0.279
            ],
            "angle": 0,
            "content": "具体地，给定 \\(n\\) 维高斯随机向量 \\(Y = (Y_{1},Y_{2},\\dots ,Y_{n})\\sim \\mathcal{N}(\\mu ,\\Sigma),\\mu \\in \\mathbb{R}^{n},\\Sigma \\in S_{++}^{n}\\) 的一组实际取值 \\(\\{y^1,y^2,\\dots ,y^m\\}\\) ，其经验协方差矩阵为"
        },
        {
            "type": "equation",
            "bbox": [
                0.351,
                0.29,
                0.559,
                0.328
            ],
            "angle": 0,
            "content": "\\[\nS = \\frac {1}{m} \\sum_ {i = 1} ^ {m} (y ^ {i} - \\bar {y}) (y ^ {i} - \\bar {y}) ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.34,
                0.67,
                0.378
            ],
            "angle": 0,
            "content": "其中 \\(\\bar{y} = \\frac{1}{m}\\sum_{i = 1}^{m}y^{i}\\) 为样本均值．关于精度矩阵 \\(X\\) 的对数似然函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.314,
                0.388,
                0.597,
                0.407
            ],
            "angle": 0,
            "content": "\\[\n\\ell (X) = \\ln \\det  (X) - \\operatorname {T r} (X S), \\quad X \\succ 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.423,
                0.733,
                0.44
            ],
            "angle": 0,
            "content": "其中 \\(X > 0\\) 表示自变量 \\(X\\) 在正定矩阵空间取值. 通过最大化对数似然函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.405,
                0.457,
                0.737,
                0.48
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {X \\succ 0} \\ell (X), \\tag {3.5.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.49,
                0.741,
                0.571
            ],
            "angle": 0,
            "content": "我们可以得到精度矩阵 \\(X\\) 的估计。这种方式得到的解往往不是稀疏的，也就意味着相应的概率图是全连接的（任意两个节点之间都存在直接相连的边），随机变量之间的相关性过密，解释性可能很差。假设概率图中的边不是全连接的，我们建立如下的改进模型："
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.587,
                0.737,
                0.61
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {X \\succ 0} \\quad \\ell (X) - \\lambda \\| X \\| _ {1}, \\tag {3.5.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.621,
                0.738,
                0.658
            ],
            "angle": 0,
            "content": "其中 \\(\\lambda > 0\\) 是用来控制稀疏度的参数。上述模型得到的稀疏解可以用来估计高维随机变量之间的条件独立性。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.663,
                0.741,
                0.761
            ],
            "angle": 0,
            "content": "除了利用似然函数进行建模以外，我们还可以直接从损失函数加正则项的思想出发来直接设计优化问题。根据精度矩阵的定义，真实的精度矩阵 \\(\\Theta = \\Sigma^{-1}\\)。通过抽样的方式可以得到 \\(\\Sigma\\) 的一个估计 \\(S\\)，即上文提到的经验协方差矩阵。我们的目标是要估计 \\(\\Sigma^{-1}\\)，且使其具有稀疏结构，因此可设计如下优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.765,
                0.737,
                0.79
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X} \\| S X - I \\| + \\lambda \\| X \\| _ {1}, \\tag {3.5.3}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.791,
                0.455,
                0.805
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & X \\succeq 0, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.815,
                0.744,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(\\|\\cdot\\|\\) 可以是任意一种范数（比较常用的为 \\(F\\) 范数或 \\(\\ell_1\\) 范数），\\(X\\succeq 0\\) 表示 \\(X\\) 在半正定矩阵空间中取值．问题 (3.5.3) 中每一项的含义是明显的："
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.281,
                0.13
            ],
            "angle": 0,
            "content": "98"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.826,
                0.195
            ],
            "angle": 0,
            "content": "\\(\\| SX - I\\|\\) 是要求 \\(X\\) 尽量为 \\(S\\) 的逆矩阵， \\(\\| X\\| _1\\) 是要求 \\(X\\) 本身稀疏， \\(X\\succeq 0\\) 保证了求得的精度矩阵是半正定的．我们也可给出问题(3.5.3)的一个变形："
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.215,
                0.825,
                0.256
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {X} \\| X \\| _ {1}, \\tag {3.5.4} \\\\ \\begin{array}{l l} \\text {s . t .} & \\| S X - I \\| \\leqslant \\sigma , X \\succeq 0. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.276,
                0.825,
                0.334
            ],
            "angle": 0,
            "content": "这个优化模型的建立过程和问题 (3.2.9) 比较相似, 即在满足一定误差范围内的 \\(X\\) 中寻找 \\(\\ell_{1}\\) 范数最小的解。当参数 \\(\\sigma\\) 比较小时, 问题 (3.5.4) 的可行域可能是空集。"
        },
        {
            "type": "title",
            "bbox": [
                0.462,
                0.383,
                0.621,
                0.404
            ],
            "angle": 0,
            "content": "3.6 相位恢复"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.429,
                0.825,
                0.548
            ],
            "angle": 0,
            "content": "相位恢复是信号处理中的一个重要问题，它是从信号在某个变换域的幅度测量值来恢复该信号。其问题背景如下：将待测物体（信号）放置在指定位置，用透射光照射，经过衍射成像，可以由探测器得到其振幅分布。我们需要从该振幅分布中恢复出原始信号的信息。由Fraunhofer衍射方程可知，探测器处的光场可以被观测物体的傅里叶变换很好地逼近。但是因为实际中的探测器只能测量光的强度，因此我们只能得到振幅信息。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.556,
                0.825,
                0.633
            ],
            "angle": 0,
            "content": "信号的相位通常包含丰富的信息。图3.2的第一列给出了两个图片 \\(Y\\) 和 \\(S\\)。对它们分别做二维离散傅里叶变换 \\(\\mathcal{F}\\) 得到 \\(\\mathcal{F}(Y)\\) 和 \\(\\mathcal{F}(S)\\)。由于变换后的图片 \\(\\mathcal{F}(Y)\\) 是复数矩阵，它可以由模长 \\(|\\mathcal{F}(Y)|\\) 和相位 phase \\((\\mathcal{F}(Y))\\) 来表示，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.418,
                0.647,
                0.666,
                0.667
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {F} (Y) = | \\mathcal {F} (Y) | \\odot \\operatorname {p h a s e} (\\mathcal {F} (Y)),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.685,
                0.825,
                0.723
            ],
            "angle": 0,
            "content": "其中 \\(|\\mathcal{F}(\\Upsilon)|\\) 表示对每个元素取模长，运算 \\(\\odot\\) 表示矩阵对应元素相乘。现在交换 \\(Y\\) 和 \\(S\\) 的相位，但保留模长，然后做傅里叶逆变换 \\(\\mathcal{F}^{-1}\\) 得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.74,
                0.682,
                0.797
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\hat {S} \\stackrel {{\\text {d e f}}} {{=}} \\mathcal {F} ^ {- 1} \\Big (| \\mathcal {F} (Y) | \\odot \\operatorname {p h a s e} (\\mathcal {F} (S)) \\Big), \\\\ \\hat {Y} \\stackrel {{\\text {d e f}}} {{=}} \\mathcal {F} ^ {- 1} \\Big (| \\mathcal {F} (S) | \\odot \\operatorname {p h a s e} (\\mathcal {F} (Y)) \\Big). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.815,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "它们分别显示在图3.2的(d)和(h). 我们可以看出 \\(\\hat{S}\\) 基本上是 \\(S\\) 的形状, 而 \\(\\hat{Y}\\) 基本上是 \\(Y\\) 的形状. 这个试验告诉我们相位信息可能比模长信息更重要."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.282,
                0.133
            ],
            "angle": 0,
            "content": "3.6 相位恢复"
        },
        {
            "type": "page_number",
            "bbox": [
                0.717,
                0.118,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "99"
        },
        {
            "type": "image",
            "bbox": [
                0.177,
                0.161,
                0.298,
                0.266
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.221,
                0.276,
                0.253,
                0.287
            ],
            "angle": 0,
            "content": "(a) \\(Y\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.321,
                0.161,
                0.443,
                0.266
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.351,
                0.276,
                0.411,
                0.288
            ],
            "angle": 0,
            "content": "(b) \\(|\\mathcal{F}(Y)|\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.466,
                0.161,
                0.588,
                0.266
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.48,
                0.275,
                0.573,
                0.289
            ],
            "angle": 0,
            "content": "(c) phase \\((\\mathcal{F}(Y))\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.611,
                0.161,
                0.733,
                0.266
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.657,
                0.275,
                0.688,
                0.287
            ],
            "angle": 0,
            "content": "(d) \\(\\hat{S}\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.177,
                0.3,
                0.298,
                0.404
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.222,
                0.415,
                0.252,
                0.427
            ],
            "angle": 0,
            "content": "(e) \\(S\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.321,
                0.3,
                0.443,
                0.404
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.353,
                0.414,
                0.41,
                0.428
            ],
            "angle": 0,
            "content": "(f) \\(|\\mathcal{F}(S)|\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.466,
                0.3,
                0.588,
                0.404
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.48,
                0.414,
                0.573,
                0.428
            ],
            "angle": 0,
            "content": "(g) phase \\((\\mathcal{F}(S))\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.612,
                0.3,
                0.733,
                0.404
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.656,
                0.414,
                0.688,
                0.427
            ],
            "angle": 0,
            "content": "(h) \\(\\hat{Y}\\)"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.37,
                0.447,
                0.538,
                0.464
            ],
            "angle": 0,
            "content": "图3.2 相位的重要性"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.498,
                0.737,
                0.556
            ],
            "angle": 0,
            "content": "在实际应用中，我们不一定使用傅里叶变换对原始信号进行采样处理。给定复信号 \\(x = (x_0, x_1, x_2, \\dots, x_{n-1})^{\\mathrm{T}} \\in \\mathbb{C}^n\\) 以及采样数 \\(m\\)，我们可以逐分量定义如下线性变换："
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.579,
                0.586,
                0.596
            ],
            "angle": 0,
            "content": "\\[\n(\\mathcal {A} (x)) _ {k} = \\langle a _ {k}, x \\rangle , \\quad k = 1, 2, \\dots , m,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.617,
                0.737,
                0.656
            ],
            "angle": 0,
            "content": "其中 \\(a_{k} \\in \\mathbb{C}^{n}\\) 为已知复向量。容易验证当该线性变换 \\(\\mathcal{A}\\) 为离散傅里叶变换时，\\(a_{k}\\) 有如下形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.673,
                0.582,
                0.698
            ],
            "angle": 0,
            "content": "\\[\na _ {k} = \\left(\\mathrm {e} ^ {2 \\pi i \\frac {k - 1}{n} t}\\right) _ {t = 0} ^ {n - 1}, \\quad k = 1, 2, \\dots , n.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.717,
                0.737,
                0.755
            ],
            "angle": 0,
            "content": "针对一般形式的 \\(a_{k}\\)，如果将其对应的振幅观测记为 \\(b_{k}\\)，那么相位恢复问题本质上是求解如下的二次方程组："
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.775,
                0.737,
                0.794
            ],
            "angle": 0,
            "content": "\\[\nb _ {k} ^ {2} = \\left| \\langle a _ {k}, x \\rangle \\right| ^ {2}, \\quad k = 1, 2, \\dots , m. \\tag {3.6.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "虽然求解线性方程组很简单，但是求解二次方程组问题(3.6.1)却是NP难的。下面我们介绍两种将问题(3.6.1)转化为可解优化模型的做法。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "100"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.157,
                0.398,
                0.173
            ],
            "angle": 0,
            "content": "1. 最小二乘模型"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.187,
                0.756,
                0.204
            ],
            "angle": 0,
            "content": "比较常见的模型是将问题(3.6.1)转化为非线性最小二乘问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.44,
                0.213,
                0.826,
                0.25
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {C} ^ {n}} \\sum_ {i = 1} ^ {m} \\left(| \\langle a _ {i}, x \\rangle | ^ {2} - b _ {i} ^ {2}\\right) ^ {2}. \\tag {3.6.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.261,
                0.825,
                0.318
            ],
            "angle": 0,
            "content": "这个模型的目标函数是可微（Wirtinger 导数）的四次函数，是非凸优化问题。相较于问题 (3.6.1)，模型(3.6.2) 能够更好地处理观测中带有的噪声。在实际中，我们也常常构造以下非光滑模型："
        },
        {
            "type": "equation",
            "bbox": [
                0.445,
                0.329,
                0.826,
                0.365
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {C} ^ {n}} \\sum_ {i = 1} ^ {m} \\left(\\left| \\langle a _ {i}, x \\rangle \\right| - b _ {i}\\right) ^ {2}. \\tag {3.6.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.375,
                0.741,
                0.392
            ],
            "angle": 0,
            "content": "假设 \\(a_{i}\\) 和 \\(x\\) 均为实的，我们可以得到相应的实数情况下的模型："
        },
        {
            "type": "equation",
            "bbox": [
                0.44,
                0.401,
                0.826,
                0.439
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\sum_ {i = 1} ^ {m} \\left(\\left| \\langle a _ {i}, x \\rangle \\right| ^ {2} - b _ {i} ^ {2}\\right) ^ {2}, \\tag {3.6.4}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.445,
                0.45,
                0.826,
                0.486
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\sum_ {i = 1} ^ {m} \\left(\\left| \\langle a _ {i}, x \\rangle \\right| - b _ {i}\\right) ^ {2}. \\tag {3.6.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.492,
                0.825,
                0.528
            ],
            "angle": 0,
            "content": "因为相位恢复问题在实际中有重要应用，如何寻找模型 (3.6.2)-(3.6.5) 的全局最优解引起了人们的广泛关注。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.553,
                0.362,
                0.57
            ],
            "angle": 0,
            "content": "2. 相位提升"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.583,
                0.825,
                0.62
            ],
            "angle": 0,
            "content": "相位提升（phase lift）是求解相位恢复问题的另一种方法。前面提到，相位恢复问题本质的困难在于处理二次方程组(3.6.1)。注意到"
        },
        {
            "type": "equation",
            "bbox": [
                0.412,
                0.634,
                0.672,
                0.654
            ],
            "angle": 0,
            "content": "\\[\n\\left| \\left\\langle a _ {i}, x \\right\\rangle \\right| ^ {2} = \\bar {a} _ {i} ^ {\\mathrm {T}} x \\bar {x} ^ {\\mathrm {T}} a _ {i} = \\operatorname {T r} \\left(x \\bar {x} ^ {\\mathrm {T}} a _ {i} \\bar {a} _ {i} ^ {\\mathrm {T}}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.667,
                0.557,
                0.685
            ],
            "angle": 0,
            "content": "令 \\(X = x\\bar{x}^{\\mathrm{T}}\\) ，方程组(3.6.1)可以转化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.304,
                0.698,
                0.825,
                0.719
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {T r} \\left(X a _ {i} \\bar {a} _ {i} ^ {\\mathrm {T}}\\right) = b _ {i} ^ {2}, \\quad i = 1, 2, \\dots , m, \\quad X \\succeq 0, \\quad \\operatorname {r a n k} (X) = 1. \\tag {3.6.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.732,
                0.825,
                0.769
            ],
            "angle": 0,
            "content": "如果方程组 (3.6.1) 的解 \\(x\\) 存在，那么 \\(X = x\\bar{x}^{\\mathrm{T}}\\) 就为方程组 (3.6.6) 的解．对于方程组 (3.6.6)，我们考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.396,
                0.782,
                0.518,
                0.806
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X} \\operatorname {r a n k} (X),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.81,
                0.825,
                0.829
            ],
            "angle": 0,
            "content": "\\[\n\\text {s . t .} \\quad \\operatorname {T r} \\left(X a _ {i} \\bar {a} _ {i} ^ {\\mathrm {T}}\\right) = b _ {i} ^ {2}, \\quad i = 1, 2, \\dots , m, \\tag {3.6.7}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.446,
                0.837,
                0.498,
                0.852
            ],
            "angle": 0,
            "content": "\\[\nX \\succeq 0.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "3.7 主成分分析"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "101"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.157,
                0.739,
                0.277
            ],
            "angle": 0,
            "content": "因为秩一解存在, 所以问题 (3.6.7) 的最优解的秩最多是 1 . 记上述问题的最优解为 \\(X\\), 对其作秩一分解 \\(X = x \\bar{x}^{\\mathrm{T}}\\). 那么 \\(c x, c \\in \\mathbb{C}\\) 且 \\(|c| = 1\\) 就为方程(3.6.1)的解. 以上的论述说明了问题 (3.6.7) 和相位恢复问题 (3.6.1) 是等价的. 形式上的区别在于问题 (3.6.7) 的自变量为矩阵 \\(X\\), 把向量变量转化为矩阵变量的操作又被称为 “提升”. 使用 “提升” 的目的是将约束从关于向量 \\(x\\) 的二次函数转化为关于矩阵 \\(X\\) 的线性函数."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.283,
                0.741,
                0.319
            ],
            "angle": 0,
            "content": "因为秩优化的计算复杂性，我们采用核范数对其进行松弛，得到如下优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.314,
                0.324,
                0.418,
                0.347
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X} \\operatorname {T r} (X),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.325,
                0.352,
                0.737,
                0.372
            ],
            "angle": 0,
            "content": "\\[\n\\mathrm {s . t .} \\quad \\operatorname {T r} \\left(X a _ {i} \\bar {a} _ {i} ^ {\\mathrm {T}}\\right) = b _ {i} ^ {2}, i = 1, 2, \\dots , m, \\tag {3.6.8}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.379,
                0.416,
                0.395
            ],
            "angle": 0,
            "content": "\\[\nX \\succeq 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.407,
                0.741,
                0.507
            ],
            "angle": 0,
            "content": "其中 \\(\\operatorname{Tr}(X) = \\| X\\|_{*}\\) 来自于矩阵 \\(X\\) 的半正定性．注意，问题(3.6.8)的目标函数变成了线性函数，它唯一非线性的部分是半正定约束 \\(X\\succeq 0\\) ：当问题(3.6.8)存在秩一解时，原始相位恢复问题的解可以通过秩一分解得到．文章[40]证明了当 \\(m\\geqslant c_0n\\ln n\\) （ \\(c_{0}\\) 为一个问题相关的常数）时，问题(3.6.8)的解在高概率下是秩一的."
        },
        {
            "type": "title",
            "bbox": [
                0.361,
                0.546,
                0.546,
                0.567
            ],
            "angle": 0,
            "content": "3.7 主成分分析"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.587,
                0.74,
                0.77
            ],
            "angle": 0,
            "content": "主成分分析是数据处理和降维中的一个重要技巧，它提供了一种将高维空间中的点在低维子空间中表达的方法。给定数据 \\(a_{i} \\in \\mathbb{R}^{p}, i = 1,2,\\dots,n\\)，其中 \\(n\\) 表示样本数，定义 \\(A = [a_{1},a_{2},\\dots,a_{n}]\\)。不失一般性，我们假设 \\(A\\) 的行和为0（否则可以逐元素减去该行元素的平均值。这不会改变数据的相对结构，只是在 \\(\\mathbb{R}^p\\) 空间沿着坐标轴进行了平移）。主成分分析的思想是寻找样本点方差最大的若干方向构成的子空间，之后将数据点投影到该子空间内来实现降维。图3.3给出了 \\(\\mathbb{R}^2\\) 中的一组数据点，可以看出数据点沿着方向 \\(x_{1}\\) 的变化最大。在这个例子中，主成分分析方法就是确定黑色实线，然后将数据点投影到 \\(x_{1}\\) 来进行降维。下面介绍其对应的最优化问题。"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.774,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "假设我们想要将 \\(\\mathbb{R}^p\\) 中的数据点集 \\(\\{a_i\\}_{i=1}^n\\) 投影到 \\(\\mathbb{R}^p\\) 的一个 \\(d\\) 维子空间 \\((d < p)\\) 中，记 \\(X \\in \\mathbb{R}^{p \\times d}\\) 为该子空间的标准正交基形成的列正交矩阵。易知，数据点 \\(a_i\\) 在 \\(X\\) 张成的子空间的投影为 \\(\\mathcal{P}_X(a_i) \\stackrel{\\mathrm{def}}{=} XX^{\\mathrm{T}} a_i\\)。根据主成分分析的基本思想，我们需要寻找最优的 \\(X\\)，使得投影后的数据点集 \\(\\{\\mathcal{P}_X(a_i)\\}\\)"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "102"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "image",
            "bbox": [
                0.379,
                0.17,
                0.701,
                0.42
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.466,
                0.444,
                0.618,
                0.46
            ],
            "angle": 0,
            "content": "图3.3 主成分分析"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.49,
                0.744,
                0.506
            ],
            "angle": 0,
            "content": "的方差最大。根据零均值假设，投影后数据点集的协方差矩阵为"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.518,
                0.695,
                0.555
            ],
            "angle": 0,
            "content": "\\[\n\\frac {1}{n} \\sum_ {i = 1} ^ {n} X X ^ {\\mathrm {T}} a _ {i} (X X ^ {\\mathrm {T}} a _ {i}) ^ {\\mathrm {T}} = \\frac {1}{n} X X ^ {\\mathrm {T}} A A ^ {\\mathrm {T}} X X ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.568,
                0.814,
                0.585
            ],
            "angle": 0,
            "content": "而多元分布的方差大小可由协方差矩阵的迹来刻画，因此，得到优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.601,
                0.826,
                0.62
            ],
            "angle": 0,
            "content": "\\[\n\\max  \\quad \\operatorname {T r} \\left(X ^ {\\mathrm {T}} A A ^ {\\mathrm {T}} X\\right), \\quad \\text {s . t .} \\quad X ^ {\\mathrm {T}} X = I, \\tag {3.7.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.638,
                0.447,
                0.654
            ],
            "angle": 0,
            "content": "其中利用了 \\(\\operatorname{Tr}(\\cdot)\\) 的性质"
        },
        {
            "type": "equation",
            "bbox": [
                0.311,
                0.668,
                0.772,
                0.689
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {T r} \\left(X X ^ {\\mathrm {T}} A A ^ {\\mathrm {T}} X X ^ {\\mathrm {T}}\\right) = \\operatorname {T r} \\left(X ^ {\\mathrm {T}} A A ^ {\\mathrm {T}} X X ^ {\\mathrm {T}} X\\right) \\xlongequal {X ^ {\\mathrm {T}} X = I} \\operatorname {T r} \\left(X ^ {\\mathrm {T}} A A ^ {\\mathrm {T}} X\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.706,
                0.825,
                0.744
            ],
            "angle": 0,
            "content": "可以证明，上述问题等价于求解 \\(AA^{\\mathrm{T}}\\in \\mathbb{R}^{p\\times p}\\) 从大到小排列的前 \\(d\\) 个的特征值对应的特征向量."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.749,
                0.826,
                0.807
            ],
            "angle": 0,
            "content": "下面从重构误差的角度来理解主成分分析模型。我们使用数据点到 \\(X\\) 张成的子空间的投影 \\(\\mathcal{P}_X(a_i)\\) 来表示 \\(a_{i}\\)，则该点的重构误差为 \\(\\| XX^{\\mathrm{T}}a_{i} - a_{i}\\|_{2}\\)。定义所有点的重构误差平方和为"
        },
        {
            "type": "equation",
            "bbox": [
                0.294,
                0.819,
                0.789,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i = 1} ^ {n} \\left\\| X X ^ {\\mathrm {T}} a _ {i} - a _ {i} \\right\\| _ {2} ^ {2} = \\left\\| X X ^ {\\mathrm {T}} A - A \\right\\| _ {F} ^ {2} = - \\operatorname {T r} \\left(X ^ {\\mathrm {T}} A A ^ {\\mathrm {T}} X\\right) + \\operatorname {T r} \\left(A ^ {\\mathrm {T}} A\\right),\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "3.8 矩阵分离问题"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "103"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.692,
                0.174
            ],
            "angle": 0,
            "content": "我们需要寻找最优的 \\(X\\)，使得重构误差平方和最小，即求解优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.259,
                0.19,
                0.65,
                0.209
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{r l} \\min  & - \\operatorname {T r} \\left(X ^ {\\mathrm {T}} A A ^ {\\mathrm {T}} X\\right) + \\operatorname {T r} \\left(A ^ {\\mathrm {T}} A\\right), \\quad \\text {s . t .} \\quad X ^ {\\mathrm {T}} X = I, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.226,
                0.739,
                0.264
            ],
            "angle": 0,
            "content": "此形式和(3.7.1)是等价的．因此我们得到结论：主成分分析寻找方差最大的子空间投影，实际上是极小化投影点的重构误差."
        },
        {
            "type": "title",
            "bbox": [
                0.349,
                0.297,
                0.56,
                0.319
            ],
            "angle": 0,
            "content": "3.8 矩阵分离问题"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.337,
                0.744,
                0.417
            ],
            "angle": 0,
            "content": "矩阵分离问题也是一类重要的低秩矩阵计算问题. 给定矩阵 \\(M \\in \\mathbb{R}^{m \\times n}\\), 我们希望将它分解成低秩矩阵 \\(X\\) 和稀疏矩阵 \\(S\\), 使得 \\(X + S = M\\), 同时尽量使得矩阵 \\(X\\) 的秩和矩阵 \\(S\\) 的 \\(\\ell_0\\) 范数都比较小, 其中 \\(\\ell_0\\) 范数 \\(\\|S\\|_0\\) 指 \\(S\\) 所有非零元素的个数. 因此得到如下模型:"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.43,
                0.737,
                0.462
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X, S \\in \\mathbb {R} ^ {m \\times n}} \\operatorname {r a n k} (X) + \\mu \\| S \\| _ {0}, \\tag {3.8.1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.463,
                0.515,
                0.477
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & X + S = M. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.493,
                0.738,
                0.532
            ],
            "angle": 0,
            "content": "由于模型中含矩阵的秩和 \\(\\ell_0\\) 范数，难以直接求解，所以我们用核范数代替秩，用最小化 \\(\\ell_1\\) 范数限制噪声矩阵的稀疏性，得到如下凸优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.359,
                0.545,
                0.737,
                0.577
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X, S \\in \\mathbb {R} ^ {m \\times n}} \\| X \\| _ {*} + \\mu \\| S \\| _ {1}, \\tag {3.8.2}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.578,
                0.509,
                0.592
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & X + S = M. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.608,
                0.738,
                0.666
            ],
            "angle": 0,
            "content": "矩阵分离问题有时候也称为鲁棒主成分分析（robust PCA），目标是在图像处理中最大程度地去除原有数据中的噪声，寻找数据在低维空间上的最佳投影。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.671,
                0.74,
                0.854
            ],
            "angle": 0,
            "content": "现在考虑视频分割的问题. 它是指把人们感兴趣的对象从视频场景中提取出来, 例如分割出一段视频中的静止部分. 视频的每一帧实际上是一个静态图片, 虽然每幅图片中的静止对象可能受到光照变化、遮挡、平移、噪声等影响, 造成不同图片之间有细微差别, 但是不可否认的是它们彼此之间具有高度的相似性. 如果把所有图片中的静止部分表示成一个矩阵, 显然它们是相似的, 并且由于静止对象具有一定的内部结构, 由静止对象构成的矩阵一定是低秩的 (各行或各列线性相关). 类似地, 视频中的动态部分以及其他背景因素可以看作噪声. 那么我们的任务就变成将视频含有的信息矩阵分解为含有内部结构的低秩矩阵和稀疏噪声矩阵之和."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "104"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.827,
                0.343
            ],
            "angle": 0,
            "content": "我们选用实际的视频数据来具体说明。假设视频共有 \\( n \\) 帧，每帧（每幅图片）共有 \\( m \\) 个像素，我们将每幅图片表示成一个列向量，这些列向量放在一起就形成了给定的矩阵 \\( M \\)。图3.4的第一列给出了该视频中三幅不同图片。可以看出，它们是一个酒店大堂的信息。由于大堂的背景，如前台、地板等是相对固定的，在每一帧里基本上是一样的，它们对应于矩阵 \\( M \\) 的低秩部分。而大堂里的人群相对稀少，他们对应于矩阵 \\( M \\) 的稀疏部分。令 \\( \\mu = \\frac{1}{2\\sqrt{m}} \\)，对模型(3.8.2)进行求解，得到的矩阵 \\( X \\) 和 \\( S \\) 中对应的三列分别显示在图3.4的第二列和第三列。可以看出这两列比较完美地实现了分离效果。"
        },
        {
            "type": "image",
            "bbox": [
                0.315,
                0.357,
                0.765,
                0.446
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image",
            "bbox": [
                0.315,
                0.463,
                0.765,
                0.555
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image",
            "bbox": [
                0.315,
                0.57,
                0.765,
                0.661
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.458,
                0.676,
                0.629,
                0.693
            ],
            "angle": 0,
            "content": "图3.4 矩阵分离问题"
        },
        {
            "type": "title",
            "bbox": [
                0.462,
                0.742,
                0.621,
                0.765
            ],
            "angle": 0,
            "content": "3.9 字典学习"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.782,
                0.828,
                0.82
            ],
            "angle": 0,
            "content": "正如各种各样的知识都可以用一本字典里的字通过排列组合来表达一样，字典学习的目的就是将已有的（超）大规模的数据集进行压缩，找到"
        },
        {
            "type": "page_footnote",
            "bbox": [
                0.284,
                0.839,
                0.46,
                0.852
            ],
            "angle": 0,
            "content": "1详见交替方向乘子法的(8.6.31)"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.281,
                0.133
            ],
            "angle": 0,
            "content": "3.9 字典学习"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "105"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.219
            ],
            "angle": 0,
            "content": "蕴藏在这些数据点背后的最基本的原理。考虑一个 \\(m\\) 维空间中的数据集 \\(\\{a_{i}\\}_{i=1}^{n}, a_{i} \\in \\mathbb{R}^{m}\\)，假定每个 \\(a_{i}\\) 都是由同一个字典生成的，且生成之后的数据带有噪声，因此字典学习的线性模型可以表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.407,
                0.235,
                0.501,
                0.252
            ],
            "angle": 0,
            "content": "\\[\na = D x + e,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.27,
                0.744,
                0.435
            ],
            "angle": 0,
            "content": "这里 \\(D \\in \\mathbb{R}^{m \\times k}\\) 是某个未知的字典，它的每一列 \\(d_i\\) 是字典的一个基向量；\\(x\\) 是字典中基的系数，同样是未知的；\\(e\\) 是某种噪声．字典学习模型不同于多元线性回归模型，原因是我们需要在字典学习模型中同时解出字典 \\(D\\) 和系数 \\(x\\)．一般来说，数据的维数 \\(m\\) 和字典里基向量的数量 \\(k\\) 是远小于观测数 \\(n\\) 的，例如观测的数据是 \\(10 \\times 10\\) 的图像，则 \\(m = 100\\)，但采集的数据量 \\(n\\) 可以非常大（例如 \\(n \\geqslant 100000\\)）．如果 \\(k < m\\)，我们称字典 \\(D\\) 是不完备的；如果 \\(k > m\\)，我们称字典 \\(D\\) 是超完备的；\\(k = m\\) 对应的字典不能对表示带来任何的提高，因此实际中不予考虑．当 \\(e\\) 是高斯白噪声时，可以定义损失函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.351,
                0.445,
                0.558,
                0.479
            ],
            "angle": 0,
            "content": "\\[\nf (D, X) = \\frac {1}{2 n} \\| D X - A \\| _ {F} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.488,
                0.741,
                0.528
            ],
            "angle": 0,
            "content": "其中 \\(A = [a_{1}, a_{2}, \\dots, a_{n}] \\in \\mathbb{R}^{m \\times n}\\) 为所有观测数据全体，\\(X = [x_{1}, x_{2}, \\dots, x_{n}] \\in \\mathbb{R}^{k \\times n}\\) 是所有基系数全体."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.531,
                0.744,
                0.633
            ],
            "angle": 0,
            "content": "在实际计算中，我们并不要求 \\(D\\) 的列是正交的，因此一个样本点 \\(a_{i}\\) 可能存在着多种不同的表示。这种冗余性给表示引入了稀疏性，这意味着字典 \\(D\\) 是超完备的 \\((k > m)\\)。稀疏性还可以帮助我们快速确定样本点是由哪几个基向量（而不是所有基向量）表示的，进而提高计算速度。具体地，在 \\(e\\) 为高斯白噪声的条件下，我们定义稀疏编码损失函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.316,
                0.644,
                0.591,
                0.677
            ],
            "angle": 0,
            "content": "\\[\nf (D, X) = \\frac {1}{2 n} \\| D X - A \\| _ {F} ^ {2} + \\lambda \\| X \\| _ {1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.687,
                0.741,
                0.792
            ],
            "angle": 0,
            "content": "这里 \\(\\lambda\\) 为正则化参数，其大小用来控制 \\(X\\) 的稀疏度．我们注意到在 \\(f(D,X)\\) 中有乘积项 \\(DX\\) ，显然 \\(f\\) 的极小值点处必有 \\(\\| X\\| _1\\to 0\\) （因为假设 \\((D,X)\\) 为问题的最小值点，那么 \\(f(cD,\\frac{1}{c} X) <   f(D,X),\\forall c > 1)\\) ．因此，这里的保稀疏的正则项并没有意义．一个改进的做法是要求字典中的基向量模长不能太大，即 \\(\\| D\\| _F\\leqslant 1\\) ，最终得到的优化问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.803,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {D, X} \\frac {1}{2 n} \\| D X - A \\| _ {F} ^ {2} + \\lambda \\| X \\| _ {1}, \\tag {3.9.1} \\\\ \\begin{array}{l l} \\text {s . t .} & \\| D \\| _ {F} \\leqslant 1. \\end{array} \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "106"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "title",
            "bbox": [
                0.442,
                0.155,
                0.642,
                0.177
            ],
            "angle": 0,
            "content": "3.10 K-均值聚类"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.194,
                0.827,
                0.295
            ],
            "angle": 0,
            "content": "聚类分析是统计学中的一个基本问题，其在机器学习、数据挖掘、模式识别和图像分析中有着重要应用。聚类(clustering)不同于分类(classification)，在聚类问题中我们仅仅知道数据点本身，而不知道每个数据点具体的标签。聚类分析的任务就是将一些无标签的数据点按照某种相似度来进行归类，进而从数据点本身来学习其内蕴的类别特征。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.298,
                0.825,
                0.398
            ],
            "angle": 0,
            "content": "给定 \\( p \\) 维空间中 \\( n \\) 个数据点 \\( a_1, a_2, \\dots, a_n \\)，假定两个数据点之间的相似度可以通过其欧几里得距离来衡量。我们的目标是将相似的点归为一类，同时将不相似的点区分开。为了简单起见我们假设类的个数为已知的，不妨记为 \\( k \\)，且同一个数据点只属于一个类。因此聚类问题就是要寻找 \\( k \\) 个不相交的非空集合 \\( S_1, S_2, \\dots, S_k \\)，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.41,
                0.414,
                0.673,
                0.432
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{a _ {1}, a _ {2}, \\dots , a _ {n} \\right\\} = S _ {1} \\cup S _ {2} \\cup \\dots \\cup S _ {k},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.447,
                0.825,
                0.485
            ],
            "angle": 0,
            "content": "且同类点之间的距离要足够近。为了在数学上描述“同类点之间的距离足够近”，我们定义组内距离平方和为"
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.497,
                0.825,
                0.535
            ],
            "angle": 0,
            "content": "\\[\nW \\left(S _ {1}, S _ {2}, \\dots , S _ {k}\\right) = \\sum_ {i = 1} ^ {k} \\sum_ {a \\in S _ {i}} \\| a - c _ {i} \\| ^ {2}, \\tag {3.10.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.548,
                0.538,
                0.565
            ],
            "angle": 0,
            "content": "这里 \\(c_{i}\\) 为第 \\(i\\) 类数据点的中心点，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.492,
                0.575,
                0.591,
                0.612
            ],
            "angle": 0,
            "content": "\\[\nc _ {i} = \\frac {1}{n _ {i}} \\sum_ {a \\in S _ {i}} a,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.624,
                0.825,
                0.682
            ],
            "angle": 0,
            "content": "其中 \\(n_i\\) 表示集合 \\(S_i\\) 的元素个数。注意在问题中假设了 \\(S_i\\) 非空，因此有 \\(n_i \\neq 0\\)。定义好聚类标准之后，就可以建立优化模型了。我们想要找到一个聚类方式，使得组内距离平方和最小，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.695,
                0.57,
                0.732
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {S _ {1}, S _ {2}, \\dots , S _ {k}} \\quad \\sum_ {i = 1} ^ {k} \\sum_ {a \\in S _ {i}} \\| a - c _ {i} \\| ^ {2},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.731,
                0.824,
                0.757
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\text {s . t .} \\quad S _ {1} \\cup S _ {2} \\cup \\dots \\cup S _ {k} = \\left\\{a _ {1}, a _ {2}, \\dots , a _ {n} \\right\\}, \\end{array} \\tag {3.10.2}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.448,
                0.765,
                0.605,
                0.782
            ],
            "angle": 0,
            "content": "\\[\nS _ {i} \\cap S _ {j} = \\emptyset , \\quad \\forall i \\neq j,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "问题 (3.10.2) 的自变量是数据点集合的分割方式，看起来比较难处理，因此有必要将问题 (3.10.2) 写成我们熟悉的形式。接下来给出问题 (3.10.2) 的两种矩阵表达形式，它们之间都是等价的。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.31,
                0.133
            ],
            "angle": 0,
            "content": "3.10 K-均值聚类"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "107"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.157,
                0.38,
                0.174
            ],
            "angle": 0,
            "content": "1. K-均值聚类等价表述一"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.188,
                0.741,
                0.268
            ],
            "angle": 0,
            "content": "在原始聚类问题中，组内距离平方和定义为(3.10.1)式，即需要计算 \\(S_{i}\\) 的点到它们中心点 \\(c_{i}\\) 的平方和．实际上，选取中心点 \\(c_{i}\\) 作为参考点不是必须的，我们完全可以选取其他点 \\(h_{i}\\) 作为参照来计算组内距离．因此组内距离平方和可以推广为"
        },
        {
            "type": "equation",
            "bbox": [
                0.308,
                0.278,
                0.602,
                0.318
            ],
            "angle": 0,
            "content": "\\[\n\\widehat {W} (S _ {1}, S _ {2}, \\dots , S _ {k}, H) = \\sum_ {i = 1} ^ {k} \\sum_ {a \\in S _ {i}} \\| a - h _ {i} \\| ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.331,
                0.741,
                0.39
            ],
            "angle": 0,
            "content": "其中 \\(H \\in \\mathbb{R}^{k \\times p}\\) 且第 \\(i\\) 行的向量为 \\(h_i^{\\mathrm{T}}\\). 为了表示聚类方式 \\(S_1, S_2, \\dots, S_k\\), 一个很自然的想法是使用一个向量 \\(\\phi_i \\in \\mathbb{R}^k\\) 来表示点 \\(a_i\\) 所处的类别. 具体地, 定义 \\(\\phi_i\\) 为"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.391,
                0.537,
                0.443
            ],
            "angle": 0,
            "content": "\\[\n(\\phi_ {i}) _ {j} = \\left\\{ \\begin{array}{l l} 1, & a _ {i} \\in S _ {j}, \\\\ 0, & a _ {i} \\notin S _ {j}, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.45,
                0.383,
                0.466
            ],
            "angle": 0,
            "content": "则聚类问题可以等价描述为"
        },
        {
            "type": "equation",
            "bbox": [
                0.226,
                0.48,
                0.375,
                0.504
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {\\Phi , H} \\| A - \\Phi H \\| _ {F} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.235,
                0.504,
                0.737,
                0.522
            ],
            "angle": 0,
            "content": "s.t. \\(\\Phi \\in \\mathbb{R}^{n\\times k}\\) ,每一行只有一个元素为1，其余为0, (3.10.3)"
        },
        {
            "type": "equation",
            "bbox": [
                0.275,
                0.526,
                0.356,
                0.542
            ],
            "angle": 0,
            "content": "\\[\nH \\in \\mathbb {R} ^ {k \\times p}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.559,
                0.429,
                0.578
            ],
            "angle": 0,
            "content": "在这里 \\(\\varPhi\\) 的第 \\(i\\) 行的向量就是 \\(\\phi_i^{\\mathrm{T}}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.581,
                0.739,
                0.639
            ],
            "angle": 0,
            "content": "接下来说明问题 (3.10.3) 和原问题 (3.10.2) 是等价的。为此只需要说明参考点集 \\(H\\) 的取法实际上就是每一类的中心点。当固定聚类方式 \\(\\Phi\\) 时，第 \\(i\\) 类点的组内距离平方和为"
        },
        {
            "type": "equation",
            "bbox": [
                0.301,
                0.653,
                0.607,
                0.685
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {a \\in S _ {i}} \\| a - h _ {i} \\| ^ {2} = \\sum_ {a \\in S _ {i}} \\| a \\| ^ {2} + \\| h _ {i} \\| ^ {2} - 2 a ^ {\\mathrm {T}} h _ {i}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.697,
                0.367,
                0.714
            ],
            "angle": 0,
            "content": "根据二次函数的性质，当"
        },
        {
            "type": "equation",
            "bbox": [
                0.389,
                0.724,
                0.52,
                0.762
            ],
            "angle": 0,
            "content": "\\[\nh _ {i} = \\frac {1}{n _ {i}} \\sum_ {a \\in S _ {i}} a = c _ {i}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.604,
                0.791
            ],
            "angle": 0,
            "content": "时组内距离平方和最小. 因此该问题和问题 (3.10.2) 等价."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.795,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "我们引入问题 (3.10.3) 的理由有两个：第一是其形式简洁，且将不易处理的自变量“分割方式”转化为了矩阵；第二是其可以看成是一个矩阵分解问题，便于我们设计算法，具体可参考本书的第8.4节。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.291,
                0.13
            ],
            "angle": 0,
            "content": "108"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.157,
                0.473,
                0.174
            ],
            "angle": 0,
            "content": "2. K-均值聚类等价表述二"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.187,
                0.826,
                0.245
            ],
            "angle": 0,
            "content": "K-均值聚类的第二种等价表述利用了列正交矩阵的性质，这种表达方式和问题(3.10.3)相比更为简洁．为此，首先定义 \\(\\mathbf{1}_{S_t}, 1 \\leqslant t \\leqslant k\\) 为 \\(n\\) 维空间中每个分量取值0或1的向量，且"
        },
        {
            "type": "equation",
            "bbox": [
                0.457,
                0.253,
                0.624,
                0.305
            ],
            "angle": 0,
            "content": "\\[\n\\mathbf {1} _ {S _ {t}} (i) = \\left\\{ \\begin{array}{l l} 1, & a _ {i} \\in S _ {t}, \\\\ 0, & a _ {i} \\notin S _ {t}. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.313,
                0.825,
                0.397
            ],
            "angle": 0,
            "content": "可以证明（见习题3.14），第 \\(t\\) 类 \\(S_{t}\\) 中每个点到其中心点的距离平方和可以写成 \\(\\frac{1}{2n_t}\\mathrm{Tr}(D\\mathbf{1}_{S_t}\\mathbf{1}_{S_t}^{\\mathrm{T}})\\)，其中 \\(D\\in \\mathbb{R}^{n\\times n}\\) 的元素为 \\(D_{ij} = \\| a_i - a_j\\|^2\\)。这说明 \\(S_{t}\\) 中每个点到中心点的距离平方和与 \\(S_{t}\\) 中所有点两两之间距离平方和有一定的联系。因此，我们将问题(3.10.2)转化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.404,
                0.531,
                0.437
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {S _ {1}, S _ {2}, \\dots , S _ {k}} \\quad \\frac {1}{2} \\operatorname {T r} (D X),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.44,
                0.825,
                0.478
            ],
            "angle": 0,
            "content": "\\[\n\\text {s . t .} \\quad X = \\sum_ {t = 1} ^ {k} \\frac {1}{n _ {t}} \\mathbf {1} _ {S _ {t}} \\mathbf {1} _ {S _ {t}} ^ {\\mathrm {T}}, \\tag {3.10.4}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.446,
                0.484,
                0.71,
                0.502
            ],
            "angle": 0,
            "content": "\\[\nS _ {1} \\cup S _ {2} \\cup \\dots \\cup S _ {k} = \\left\\{a _ {1}, a _ {2}, \\dots , a _ {n} \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.446,
                0.509,
                0.605,
                0.528
            ],
            "angle": 0,
            "content": "\\[\nS _ {i} \\cap S _ {j} = \\emptyset , \\quad \\forall i \\neq j.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.539,
                0.825,
                0.577
            ],
            "angle": 0,
            "content": "对半定矩阵 \\(X\\) 进行分解 \\(X = YY^{\\mathrm{T}}, Y \\in \\mathbb{R}^{n \\times k}\\), 我们可以进一步得到如下矩阵优化问题 (这里 1 是 \\(n\\) 维向量且分量全为 1):"
        },
        {
            "type": "equation",
            "bbox": [
                0.448,
                0.588,
                0.593,
                0.613
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {Y \\in \\mathbb {R} ^ {n \\times k}} \\operatorname {T r} (Y ^ {\\mathrm {T}} D Y),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.617,
                0.825,
                0.636
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad Y Y ^ {\\mathrm {T}} \\mathbf {1} = \\mathbf {1}, \\tag {3.10.5}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.508,
                0.643,
                0.634,
                0.661
            ],
            "angle": 0,
            "content": "\\[\nY ^ {T} Y = I _ {k}, Y \\geqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.673,
                0.825,
                0.711
            ],
            "angle": 0,
            "content": "读者可以自行验证这两个问题的等价性(见习题3.14).如果求得问题(3.10.5)的解 \\(Y\\) ，则 \\(YY^{\\mathrm{T}}\\) 就对应于问题(3.10.4)的解"
        },
        {
            "type": "title",
            "bbox": [
                0.369,
                0.741,
                0.713,
                0.763
            ],
            "angle": 0,
            "content": "3.11 图像处理中的全变差模型"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.78,
                0.826,
                0.818
            ],
            "angle": 0,
            "content": "本书简要介绍基于全变差（TV）的图像处理模型．对于定义在区域\\(\\Omega \\subset \\mathbb{R}^2\\) 的函数 \\(u(x,y)\\) ，其全变差"
        },
        {
            "type": "equation",
            "bbox": [
                0.461,
                0.826,
                0.825,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\| u \\| _ {T V} = \\int_ {\\Omega} \\| \\mathcal {D} u \\| d x, \\tag {3.11.1}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.411,
                0.133
            ],
            "angle": 0,
            "content": "3.11 图像处理中的全变差模型"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "109"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.343,
                0.174
            ],
            "angle": 0,
            "content": "其中梯度算子 \\(\\mathcal{D}\\) 满足："
        },
        {
            "type": "equation",
            "bbox": [
                0.382,
                0.171,
                0.526,
                0.21
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {D} u = \\left(\\frac {\\partial u}{\\partial x}, \\frac {\\partial u}{\\partial y}\\right) ^ {\\mathrm {T}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.213,
                0.436,
                0.23
            ],
            "angle": 0,
            "content": "这里， \\(\\| \\mathcal{D}u\\|\\) 可以采用 \\(\\ell_1\\) 范数，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.367,
                0.237,
                0.543,
                0.274
            ],
            "angle": 0,
            "content": "\\[\n\\| \\mathcal {D} u \\| _ {1} = \\left| \\frac {\\partial u}{\\partial x} \\right| + \\left| \\frac {\\partial u}{\\partial y} \\right|,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.28,
                0.578,
                0.298
            ],
            "angle": 0,
            "content": "称对应的全变差是各向异性的．如果采用 \\(\\ell_2\\) 范数，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.339,
                0.305,
                0.572,
                0.348
            ],
            "angle": 0,
            "content": "\\[\n\\| \\mathcal {D} u \\| _ {2} = \\sqrt {\\left(\\frac {\\partial u}{\\partial x}\\right) ^ {2} + \\left(\\frac {\\partial u}{\\partial y}\\right) ^ {2}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.355,
                0.405,
                0.372
            ],
            "angle": 0,
            "content": "称对应的全变差是各向同性的"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.376,
                0.738,
                0.414
            ],
            "angle": 0,
            "content": "令 \\(b(x,y)\\) 是观测到的带噪声的图像， \\(\\mathcal{A}\\) 是线性算子．在经典的Rudin-Osher-Fatemi（ROF）模型下，图像去噪和去模糊问题可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.342,
                0.425,
                0.738,
                0.449
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {u} \\| \\mathcal {A} u - b \\| _ {L _ {2}} ^ {2} + \\lambda \\| u \\| _ {T V}, \\tag {3.11.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.457,
                0.52,
                0.475
            ],
            "angle": 0,
            "content": "这里，定义域为 \\(\\Omega\\) 的函数 \\(f\\) 的 \\(L_{2}\\) 范数定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.482,
                0.543,
                0.52
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| f \\right\\| _ {L _ {2}} = \\left(\\int_ {\\Omega} f ^ {2} d x\\right) ^ {1 / 2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.527,
                0.738,
                0.606
            ],
            "angle": 0,
            "content": "如果 \\(\\mathcal{A}\\) 是单位算子或模糊算子, 则上述模型分别对应图像去噪和去模糊. 目标函数中的第一项是数据保真项, 即重构出的图片要与已有的采集信息相容. 第二项是正则项, 用来保证重构出的图像的阶跃是稀疏的, 或者说使得重构出的图像类似于一个分片常数函数."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.61,
                0.738,
                0.698
            ],
            "angle": 0,
            "content": "下面给出连续模型(3.11.2)的离散格式．为简单起见，假设区域 \\(\\Omega = [0,1]\\times [0,1]\\) 并且将它离散为 \\(n\\times n\\) 网格，则格点 \\(\\left(\\frac{i}{n},\\frac{j}{n}\\right)\\) 对应指标 \\((i,j)\\) 我们将图像 \\(u\\) 表示为矩阵 \\(U\\in \\mathbb{R}^{n\\times n}\\) ，其元素 \\(u_{i,j}\\) 对应指标 \\((i,j)\\) ．运用前向差分离散梯度算子D得到 \\((\\mathrm{DU})_{i,j} = ((\\mathrm{D}_1U)_{i,j},(\\mathrm{D}_2U)_{i,j})^{\\mathrm{T}}\\) ，且有"
        },
        {
            "type": "equation",
            "bbox": [
                0.201,
                0.705,
                0.708,
                0.757
            ],
            "angle": 0,
            "content": "\\[\n(\\mathrm {D} _ {1} U) _ {i, j} = \\left\\{ \\begin{array}{l l} u _ {i + 1, j} - u _ {i, j}, & i <   n, \\\\ 0, & i = n, \\end{array} \\right. (\\mathrm {D} _ {2} U) _ {i, j} = \\left\\{ \\begin{array}{l l} u _ {i, j + 1} - u _ {i, j}, & j <   n, \\\\ 0, & j = n. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.766,
                0.738,
                0.813
            ],
            "angle": 0,
            "content": "这里对于 \\(i,j = n\\) 的点适用诺伊曼（Neumann）边界条件 \\(\\frac{\\partial u}{\\partial n} = 0\\) 且有 \\(DU \\in \\mathbb{R}^{n \\times n \\times 2}\\)。那么离散全变差可以定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.823,
                0.738,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\| U \\| _ {T V} = \\sum_ {1 \\leqslant i, j \\leqslant n} \\| (D U) _ {i, j} \\|, \\tag {3.11.3}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.291,
                0.13
            ],
            "angle": 0,
            "content": "110"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.826,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.544,
                0.174
            ],
            "angle": 0,
            "content": "其中 \\(\\| \\cdot \\|\\) 可以是 \\(\\ell_1\\) 范数或者 \\(\\ell_2\\) 范数"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.177,
                0.615,
                0.195
            ],
            "angle": 0,
            "content": "对于任意的 \\(U, V \\in \\mathbb{R}^{n \\times n \\times 2}\\)，我们定义内积"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.204,
                0.652,
                0.236
            ],
            "angle": 0,
            "content": "\\[\n\\langle U, V \\rangle = \\sum_ {1 \\leqslant i, j \\leqslant n, 1 \\leqslant k \\leqslant 2} u _ {i, j, k} v _ {i, j, k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.243,
                0.585,
                0.259
            ],
            "angle": 0,
            "content": "那么根据定义，离散的散度算子 \\(G\\) 需满足："
        },
        {
            "type": "equation",
            "bbox": [
                0.362,
                0.27,
                0.72,
                0.289
            ],
            "angle": 0,
            "content": "\\[\n\\langle U, G V \\rangle = - \\langle D U, V \\rangle , \\forall U \\in \\mathbb {R} ^ {n \\times n}, V \\in \\mathbb {R} ^ {n \\times n \\times 2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.298,
                0.68,
                0.319
            ],
            "angle": 0,
            "content": "记 \\(w_{ij} = (w_{i,j,1},w_{i,j,2})^{\\mathrm{T}}\\) ， \\(W = (w_{ij})_{i,j = 1}^{n}\\in \\mathbb{R}^{n\\times n\\times 2}\\) ，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.457,
                0.327,
                0.825,
                0.346
            ],
            "angle": 0,
            "content": "\\[\n(G W) _ {i j} = \\Delta_ {i, j, 1} + \\Delta_ {i, j, 2}, \\tag {3.11.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.356,
                0.299,
                0.372
            ],
            "angle": 0,
            "content": "其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.262,
                0.378,
                0.82,
                0.454
            ],
            "angle": 0,
            "content": "\\[\n\\Delta_ {i, j, 1} = \\left\\{ \\begin{array}{l l} w _ {i, j, 1} - w _ {i - 1, j, 1}, & 1 <   i <   n, \\\\ w _ {i, j, 1}, & i = 1, \\\\ - w _ {i, j, 1}, & i = n, \\end{array} \\right. \\quad \\Delta_ {i, j, 2} = \\left\\{ \\begin{array}{l l} w _ {i, j, 2} - w _ {i, j - 1, 2}, & 1 <   j <   n, \\\\ w _ {i, j, 2}, & j = 1, \\\\ - w _ {i, j, 2}, & j = n. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.461,
                0.825,
                0.519
            ],
            "angle": 0,
            "content": "运用合适的离散格式处理后，我们可得到离散的线性算子 \\(\\mathcal{A}\\) 和图像 \\(B\\)（这里沿用了连续情形的记号，但 \\(\\mathcal{A}\\) 的含义完全不同）。因此由连续问题(3.11.2)得到离散问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.421,
                0.529,
                0.825,
                0.555
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {U \\in \\mathbb {R} ^ {n \\times n}} \\| \\mathcal {A} U - B \\| _ {F} ^ {2} + \\lambda \\| U \\| _ {T V}. \\tag {3.11.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.56,
                0.825,
                0.598
            ],
            "angle": 0,
            "content": "图3.5给出了图像去噪的一个例子：(a)是原始图像，(b)是加了噪声的图像，(c)是算法恢复的结果."
        },
        {
            "type": "image",
            "bbox": [
                0.275,
                0.607,
                0.453,
                0.741
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image",
            "bbox": [
                0.453,
                0.607,
                0.631,
                0.741
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image",
            "bbox": [
                0.631,
                0.607,
                0.81,
                0.741
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.474,
                0.754,
                0.61,
                0.771
            ],
            "angle": 0,
            "content": "图3.5 图像去噪"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.79,
                0.825,
                0.828
            ],
            "angle": 0,
            "content": "在实际中，除了考虑ROF模型外，我们还考虑其一个变形，\\(\\mathrm{TV - L}^1\\) 模型．离散格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.421,
                0.832,
                0.825,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {U \\in \\mathbb {R} ^ {n \\times n}} \\| \\mathcal {A} U - B \\| _ {1} + \\lambda \\| U \\| _ {T V}. \\tag {3.11.6}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.291,
                0.133
            ],
            "angle": 0,
            "content": "3.12 小波模型"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "111"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.194
            ],
            "angle": 0,
            "content": "上述模型的一个好处是可以更好地处理非高斯噪声的情形，比如椒盐噪声等."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.206,
                0.737,
                0.264
            ],
            "angle": 0,
            "content": "图像处理中还有大量其他问题，如图像识别，图像分割，图像匹配，生物医学图像（CT，MRI，fMRI）处理，计算机图像和视觉，遥感图像，等等。它们中间产生了丰富的优化问题。"
        },
        {
            "type": "title",
            "bbox": [
                0.368,
                0.336,
                0.54,
                0.357
            ],
            "angle": 0,
            "content": "3.12 小波模型"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.388,
                0.739,
                0.509
            ],
            "angle": 0,
            "content": "小波分析是图像重构的另外一种方法. 它通过不同尺度变化对失真图像进行多尺度分析, 进而保留想要的尺度信息, 去掉噪声等对应的干扰信息. 小波分析的一个最重要的概念是小波框架, 它是空间中基函数的推广. 具体地, 将图像理解成一个向量 \\(x \\in \\mathbb{R}^{n}\\), 令 \\(W \\in \\mathbb{R}^{m \\times n}\\) 为小波框架. 需要注意的是, 这里 \\(m\\) 可以比 \\(n\\) 大, 但是有 \\(\\operatorname{rank}(W) = n\\), 也就意味着 \\(W\\) 带有一些冗余信息. 在小波框架下, 可以对图像 \\(x\\) 做分解得到小波系数 \\(\\alpha \\in \\mathbb{R}^{m}\\), 即"
        },
        {
            "type": "equation",
            "bbox": [
                0.419,
                0.54,
                0.487,
                0.554
            ],
            "angle": 0,
            "content": "\\[\n\\alpha = W x.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.586,
                0.482,
                0.602
            ],
            "angle": 0,
            "content": "反之，给定小波系数 \\(\\alpha\\) ，可以重构出图像"
        },
        {
            "type": "equation",
            "bbox": [
                0.415,
                0.632,
                0.492,
                0.649
            ],
            "angle": 0,
            "content": "\\[\nx = W ^ {T} \\alpha .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.68,
                0.434,
                0.696
            ],
            "angle": 0,
            "content": "为了保证重构的完整性，我们要求"
        },
        {
            "type": "equation",
            "bbox": [
                0.413,
                0.726,
                0.495,
                0.742
            ],
            "angle": 0,
            "content": "\\[\nW ^ {\\mathrm {T}} W = I.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.739,
                0.853
            ],
            "angle": 0,
            "content": "因为冗余性，所以 \\(WW^{\\mathrm{T}} \\neq I\\)。我们从图中3.6可以看到小波方法的重构原理。图3.6中给出了博雅塔（图（a））在一组给定小波基下的分解，得到的小波系数见图（b），其像素值的直方图如（c）所示，可以看出小波系数的稀疏性。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "112"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "image",
            "bbox": [
                0.264,
                0.163,
                0.443,
                0.318
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.329,
                0.327,
                0.376,
                0.339
            ],
            "angle": 0,
            "content": "(a) 原图"
        },
        {
            "type": "image",
            "bbox": [
                0.451,
                0.163,
                0.631,
                0.317
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.504,
                0.327,
                0.576,
                0.339
            ],
            "angle": 0,
            "content": "(b) 小波系数"
        },
        {
            "type": "image",
            "bbox": [
                0.636,
                0.162,
                0.821,
                0.317
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.686,
                0.327,
                0.769,
                0.339
            ],
            "angle": 0,
            "content": "(c) 稀疏的系数"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.475,
                0.359,
                0.61,
                0.376
            ],
            "angle": 0,
            "content": "图3.6 小波分解"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.403,
                0.825,
                0.461
            ],
            "angle": 0,
            "content": "从图3.6中可以看出，图像的失真部分对应的小波系数很小，只有少数的小波系数对原始图像起到决定作用。我们考虑基于小波框架的重构模型。常用的有"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.478,
                0.825,
                0.515
            ],
            "angle": 0,
            "content": "- 分解模型：直接求解重构图像，其通过惩罚图像的小波系数的 \\(\\ell_{1}\\) 范数来去除图像中不必要的噪声信息。问题形式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.427,
                0.527,
                0.825,
                0.559
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| \\lambda \\odot (W x) \\| _ {1} + \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2}, \\tag {3.12.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.297,
                0.57,
                0.825,
                0.607
            ],
            "angle": 0,
            "content": "其中 \\(b\\) 为实际观测的图像数据, \\(\\lambda \\in \\mathbb{R}^{m}\\) 是给定的非负向量, \\(\\odot\\) 表示逐个分量相乘."
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.625,
                0.825,
                0.662
            ],
            "angle": 0,
            "content": "- 合成模型：求解图像对应的小波系数来重构图像，其通过小波系数的 \\(\\ell_{1}\\) 范数来去除图像中不必要的噪声信息。问题形式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.43,
                0.674,
                0.825,
                0.707
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {\\alpha \\in \\mathbb {R} ^ {m}} \\| \\lambda \\odot \\alpha \\| _ {1} + \\frac {1}{2} \\| A W ^ {\\mathrm {T}} \\alpha - b \\| _ {2} ^ {2}. \\tag {3.12.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.724,
                0.825,
                0.782
            ],
            "angle": 0,
            "content": "- 平衡模型：求解图像对应的小波系数来重构图像。在合成模型中，\\(\\alpha\\)不一定对应于真实图像的小波系数。因此，平衡模型添加 \\((I - WW^{\\mathrm{T}})\\alpha\\) 的二次罚项来保证 \\(\\alpha\\) 更接近真实图像的小波系数。问题形式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.326,
                0.794,
                0.825,
                0.826
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {\\alpha \\in \\mathbb {R} ^ {m}} \\| \\lambda \\odot \\alpha \\| _ {1} + \\frac {1}{2} \\| A W ^ {\\mathrm {T}} \\alpha - b \\| _ {2} ^ {2} + \\frac {\\kappa}{2} \\| (I - W W ^ {\\mathrm {T}}) \\alpha \\| _ {2} ^ {2}, \\tag {3.12.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.836,
                0.446,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(\\kappa\\) 为给定常数"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.29,
                0.133
            ],
            "angle": 0,
            "content": "3.13 强化学习"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "113"
        },
        {
            "type": "image",
            "bbox": [
                0.205,
                0.166,
                0.689,
                0.307
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.315,
                0.33,
                0.595,
                0.348
            ],
            "angle": 0,
            "content": "图3.7 强化学习智能体-环境交互图"
        },
        {
            "type": "title",
            "bbox": [
                0.367,
                0.372,
                0.538,
                0.394
            ],
            "angle": 0,
            "content": "3.13 强化学习"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.411,
                0.739,
                0.532
            ],
            "angle": 0,
            "content": "人工智能的一个标志性事件是2016年3月DeepMind开发的人工智能程序“AlphaGo”以五局四胜打败韩国围棋大师李世石九段。这是计算机围棋程序第一次战胜人类职业棋手。2017年，DeepMind的新一代程序AlphaGo Zero以100:0的成绩打败了其前辈AlphaGo。这些程序成功的关键在于采用了强化学习（reinforcement learning）算法。AlphaGo Zero只需要掌握围棋的基本规则，不需要任何的人类经验，就能从零开始学习。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.536,
                0.74,
                0.697
            ],
            "angle": 0,
            "content": "虽然强化学习处理的实际问题千差万别，但是它们一般可以抽象出智能体（agent）和环境（environment）两个概念。智能体持续地与环境互动并从环境中学到经验和规则，如图3.7所示。智能体在状态 \\( s_t \\) 下执行动作 \\( a_t \\) 后，环境根据其内在的规则回应，到达新的状态 \\( s_{t+1} \\)，奖励 \\( r_{t+1} \\)。这个系统持续不断地重复这个过程，直到系统中止。想象一个机器人想要从点A走到点B，为了实现这个目标，它尝试了很多不同的移动方式，既从成功的动作中学到了经验，又从失败的摔倒中学到了教训，最终找到最有效、最快捷的行走方式。这种反复试错也是强化学习所使用的思想。"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.701,
                0.546,
                0.718
            ],
            "angle": 0,
            "content": "强化学习跟其他机器学习相比有如下不同点："
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.732,
                0.737,
                0.77
            ],
            "angle": 0,
            "content": "- 这个过程是无监督的。没有标签告诉智能体做什么动作是最好的，只有之前动作所获得的奖励会让智能体更偏向于执行某一类动作。"
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.784,
                0.737,
                0.822
            ],
            "angle": 0,
            "content": "- 环境给智能体动作的反馈是有延迟的。当前动作的效果也许不会立刻体现，但是它可能影响许多步后的奖励。"
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.836,
                0.737,
                0.853
            ],
            "angle": 0,
            "content": "- 时间顺序在强化学习中是非常重要的。所做决策的顺序将会决定最终"
        },
        {
            "type": "list",
            "bbox": [
                0.193,
                0.732,
                0.737,
                0.853
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "114"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.157,
                0.36,
                0.174
            ],
            "angle": 0,
            "content": "的结果."
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.188,
                0.827,
                0.247
            ],
            "angle": 0,
            "content": "- 智能体所做的动作会影响观察到的环境状态。在这个学习过程中观察到的环境状态或接收到的反馈不是独立的，它们是智能体动作的函数。这一点与监督学习中的样本独立性假设有很大差别。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.26,
                0.827,
                0.319
            ],
            "angle": 0,
            "content": "强化学习经常可以用马尔可夫决策过程（Markov decision process, MDP）来描述。在马尔可夫决策过程中，环境状态转移的概率只取决于当前的状态和动作，而与所有历史信息无关，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.335,
                0.744,
                0.353
            ],
            "angle": 0,
            "content": "\\[\nP \\left(s _ {t + 1} = s ^ {\\prime} \\mid s _ {t}, a _ {t}, s _ {t - 1}, a _ {t - 1}, \\dots , s _ {0}, a _ {0}\\right) = P \\left(s _ {t + 1} = s ^ {\\prime} \\mid s _ {t}, a _ {t}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.368,
                0.825,
                0.405
            ],
            "angle": 0,
            "content": "其次，环境所反馈的奖励的期望也只依赖于当前的状态和动作，所以该期望可以表示成"
        },
        {
            "type": "equation",
            "bbox": [
                0.298,
                0.423,
                0.769,
                0.442
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ r _ {t + 1} | s _ {t}, a _ {t}, s _ {t - 1}, a _ {t - 1}, \\dots , s _ {0}, a _ {0} \\right] = \\mathbb {E} \\left[ r _ {t + 1} | s _ {t} = s, a _ {t} = a \\right] = r (s, a).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.455,
                0.827,
                0.658
            ],
            "angle": 0,
            "content": "智能体要做的是通过在环境中不断尝试学习得到一个策略（policy）\\(\\pi\\)，根据这个策略，智能体可以知道在状态 \\(s\\) 下应执行什么动作。假设 \\(S\\) 和 \\(A\\) 为正整数，令 \\(\\mathcal{S} = \\{1,2,\\dots,S\\}\\) 和 \\(\\mathcal{A} = \\{1,2,\\dots,A\\}\\) 分别为状态空间和决策空间。用 \\(\\Delta_X\\) 表示在集合 \\(X\\) 上的概率分布构成的集合，策略 \\(\\pi: S \\to \\Delta_A\\) 是定义在状态空间上的一个映射。我们用 \\(\\pi(a|s)\\) 表示使用策略 \\(\\pi\\) 时，在当前状态 \\(s\\) 下选择决策 \\(a\\) 的概率。状态转移算子 \\(P: S \\times \\mathcal{A} \\to \\Delta_S\\) 为从状态空间和决策空间的乘积空间到状态空间上的概率分布的映射，用 \\(P_a(i,j)\\) 表示采用决策 \\(a\\) 从状态 \\(i\\) 跳到状态 \\(j\\) 的概率。令单步奖励函数 \\(r: S \\times \\mathcal{A} \\to \\mathbb{R}\\) 为状态和决策乘积空间上的实值函数，用 \\(r(s,a)\\) 表示在状态 \\(s\\) 下采取决策 \\(a\\) 得到的奖励的期望。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.662,
                0.827,
                0.743
            ],
            "angle": 0,
            "content": "由于在某一个状态下最优的动作选择并不一定产生全局最优的策略，比较理想的方式应该能最大化某种形式的累积奖励，例如有限时间的累积奖励，或是有限时间的有折扣奖励之和，抑或是无限时间的累积奖励或平均奖励。一条轨道 \\(\\tau\\) 是一系列的状态和动作的集合："
        },
        {
            "type": "equation",
            "bbox": [
                0.46,
                0.758,
                0.624,
                0.775
            ],
            "angle": 0,
            "content": "\\[\n\\tau = \\left\\{s _ {0}, a _ {0}, s _ {1}, a _ {1}, \\dots \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.791,
                0.663,
                0.808
            ],
            "angle": 0,
            "content": "给定折扣因子 \\(\\gamma \\in [0,1)\\), 有折扣累积奖励可以表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.482,
                0.819,
                0.603,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nR (\\tau) = \\sum_ {t = 0} ^ {\\infty} \\gamma^ {t} r _ {t},\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.258,
                0.133
            ],
            "angle": 0,
            "content": "3.14 总结"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "115"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.666,
                0.174
            ],
            "angle": 0,
            "content": "其中 \\(r_t = r(s_t, a_t)\\)。那么最优的策略是能最大化MDP收益的策略："
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.193,
                0.737,
                0.217
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {\\pi} \\quad \\mathbb {E} _ {\\tau \\sim \\pi} [ R (\\tau) ], \\tag {3.13.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.231,
                0.737,
                0.268
            ],
            "angle": 0,
            "content": "其中 \\(\\tau \\sim \\pi\\) 表示轨道 \\(\\tau\\) 是按照策略 \\(\\pi\\) 生成的。令 \\(V(i)\\) 为最优策略在任意时刻从状态 \\(i\\) 出发得到的期望奖励，那么问题 (3.13.1) 也等价于求解"
        },
        {
            "type": "equation",
            "bbox": [
                0.292,
                0.282,
                0.737,
                0.326
            ],
            "angle": 0,
            "content": "\\[\nV (i) = \\max  _ {a} \\left\\{\\sum_ {j} P _ {a} (i, j) (r (i, a) + \\gamma V (j)) \\right\\}. \\tag {3.13.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.341,
                0.7,
                0.358
            ],
            "angle": 0,
            "content": "该方程称为 Bellman 方程. 在任意时刻, 状态 \\(i\\) 处的决策 \\(a(i)\\) 应满足:"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.372,
                0.627,
                0.415
            ],
            "angle": 0,
            "content": "\\[\na (i) = \\underset {a} {\\arg \\max } \\left\\{\\sum_ {j} P _ {a} (i, j) \\left(r (i, a) + \\gamma V (j)\\right) \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.43,
                0.737,
                0.488
            ],
            "angle": 0,
            "content": "在这里指出，\\(V(i)\\) 与 \\(a(i)\\) 都不依赖于时间 \\(t\\)，其根本原因是下一步的状态仅依赖于当前状态和动作（马尔可夫性），且转移概率和奖励函数也与时间无关（时齐性）."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.494,
                0.739,
                0.593
            ],
            "angle": 0,
            "content": "很多时候，解决实际问题的困难往往在于状态集合或者动作集合中元素的数量非常多，甚至是天文数字。常用的一个方式是将策略用神经网络或者深度神经网络来表达，比如策略的输出是可计算的函数，它依赖于一系列参数，如神经网络的权重。为简单起见，人们经常将 \\(\\pi\\) 写为 \\(\\pi_{\\theta}\\)（\\(\\theta\\) 是神经网络的参数），问题(3.13.1)则表达为："
        },
        {
            "type": "equation",
            "bbox": [
                0.382,
                0.614,
                0.737,
                0.637
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {\\theta} \\quad \\mathbb {E} _ {\\tau \\sim \\pi_ {\\theta}} [ R (\\tau) ]. \\tag {3.13.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.652,
                0.738,
                0.711
            ],
            "angle": 0,
            "content": "对于不同的 \\(\\pi\\) ，目标函数的计算往往需要重新抽样，即在智能体-环境中重新交互得到．而随机优化问题(3.3.5)中的样本往往是事先全都给定的．当然强化学习的模型还有很多类型，我们这里不再仔细阐述."
        },
        {
            "type": "title",
            "bbox": [
                0.392,
                0.752,
                0.517,
                0.774
            ],
            "angle": 0,
            "content": "3.14 总结"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.795,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "本章介绍了常见的建模技术，也回顾了统计学习、机器学习、图像和信号处理中一些常见的优化问题。表3.1总结了本章各节使用过的建模技巧（其中“—”表示未涉及该方面的技巧）。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "116"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "table_caption",
            "bbox": [
                0.459,
                0.159,
                0.626,
                0.176
            ],
            "angle": 0,
            "content": "表 3.1 建模技巧总结"
        },
        {
            "type": "table",
            "bbox": [
                0.26,
                0.188,
                0.818,
                0.655
            ],
            "angle": 0,
            "content": "<table><tr><td>实例</td><td>目标函数设计</td><td>约束设计</td></tr><tr><td>回归分析</td><td>最小二乘\n最大似然估计\n正则化</td><td>等价转换</td></tr><tr><td>逻辑回归</td><td>最大似然估计\n正则化</td><td>-</td></tr><tr><td>支持向量机</td><td>损失函数（最小距离）</td><td>等价转换（最小距离）</td></tr><tr><td>概率图模型</td><td>最大似然估计\n正则化\n损失函数（逆矩阵）</td><td>问题本身性质（正定性）</td></tr><tr><td>相位恢复</td><td>最小二乘，松弛</td><td>松弛（相位提升）</td></tr><tr><td>主成分分析</td><td>损失函数（投影方差）</td><td>问题本身性质（正交性）</td></tr><tr><td>矩阵分离问题</td><td>正则化（稀疏，低秩）\n松弛（凸松弛）</td><td>问题本身性质（重构）</td></tr><tr><td>字典学习</td><td>最小二乘，正则化</td><td>消除解的不唯一性</td></tr><tr><td>K-均值聚类</td><td>损失函数（组内距离平方和）</td><td>问题本身性质\n等价转换</td></tr><tr><td>全变差模型</td><td>损失函数（保真项）\n正则化</td><td>-</td></tr><tr><td>小波模型</td><td colspan=\"2\">技巧同全变差模型，区分别为模型是在小波基下考虑的</td></tr><tr><td>强化学习</td><td>损失函数</td><td>问题本身性质（MDP）\n松弛（神经网络近似）</td></tr></table>"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.733,
                0.828,
                0.854
            ],
            "angle": 0,
            "content": "统计学习中的模型远远不止这些，更多关于监督和无监督学习的模型可以参考 [73]. 除了重构问题外，图像处理中的基本任务还包括图像分割以及图像配准问题，相关内容可以参考 [9]. 对于强化学习问题，我们仅介绍了马尔可夫决策过程，更多丰富精彩的内容可以在 [177] 中找到. 除了本章介绍的凸松弛模型外，实际中我们还常常构造非凸松弛模型以更好地逼近原始问题，这方面的内容可以参考 [195]."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.223,
                0.132
            ],
            "angle": 0,
            "content": "习题3"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "117"
        },
        {
            "type": "title",
            "bbox": [
                0.42,
                0.155,
                0.491,
                0.176
            ],
            "angle": 0,
            "content": "习题3"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.194,
                0.494,
                0.21
            ],
            "angle": 0,
            "content": "3.1 证明：方程组(3.6.1)的解不是唯一的"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.224,
                0.739,
                0.324
            ],
            "angle": 0,
            "content": "3.2 设有一片 \\(9 \\times 9\\) 的空地，每一小块空地可以改成池塘或者稻田。由于稻田需要经常灌溉，因此设计的时候每一块稻田至少要与一块池塘相邻（前、后、左、右四个方向视为相邻）。我们的最终目标是让稻田的数量达到最大。试将这个实际问题转化为优化问题，该优化问题中的目标函数和约束是如何设计的？"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.332,
                0.74,
                0.415
            ],
            "angle": 0,
            "content": "3.3 给定正交矩阵 \\(U = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{2} & -\\frac{\\sqrt{3}}{2} \\\\ 0 & \\frac{\\sqrt{3}}{2} & \\frac{1}{2} \\end{bmatrix}\\) 及矩阵 \\(A = U\\mathrm{Diag}(10^{-6},2,3)U^{\\mathrm{T}}\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.18,
                0.194,
                0.74,
                0.415
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.416,
                0.739,
                0.454
            ],
            "angle": 0,
            "content": "分别计算 \\(b = (0,0,0)^{\\mathrm{T}}\\) 和 \\(b = (10^{-4},0,0)^{\\mathrm{T}}\\) 的情形下模型(3.2.4)和(3.2.6)的解，其中参数 \\(\\mu\\) 待定，并分析得到的结果"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.467,
                0.738,
                0.526
            ],
            "angle": 0,
            "content": "3.4 在主成分分析中，我们需要计算高维空间中的数据点到低维空间中的投影．试给出 \\(a \\in \\mathbb{R}^n\\) 在由一般矩阵 \\(X \\in \\mathbb{R}^{n \\times p}(p < n)\\) 的列向量张成的空间中的投影，这里 \\(X\\) 可能不是列正交矩阵，也可能秩小于 \\(p\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.539,
                0.738,
                0.577
            ],
            "angle": 0,
            "content": "3.5 假设 \\(A = I\\)，请分别计算优化问题(3.2.6)和(3.2.7)的解．进一步地，当 \\(\\lambda\\) 和 \\(\\sigma\\) 满足何种关系时，两个问题的解是一样的？"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.591,
                0.664,
                0.608
            ],
            "angle": 0,
            "content": "3.6 给定向量 \\(a, b \\in \\mathbb{R}^n\\)，分别考虑取 \\(\\ell_1, \\ell_2, \\ell_{\\infty}\\) 范数时，优化问题"
        },
        {
            "type": "list",
            "bbox": [
                0.18,
                0.467,
                0.738,
                0.608
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.413,
                0.624,
                0.534,
                0.647
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R}} \\| x a - b \\|\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.658,
                0.255,
                0.674
            ],
            "angle": 0,
            "content": "的解."
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.688,
                0.355,
                0.705
            ],
            "angle": 0,
            "content": "3.7 考虑线性观测模型"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.72,
                0.588,
                0.739
            ],
            "angle": 0,
            "content": "\\[\nb _ {i} = a _ {i} ^ {\\mathrm {T}} x + \\varepsilon_ {i}, \\quad i = 1, 2, \\dots , m,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.755,
                0.738,
                0.813
            ],
            "angle": 0,
            "content": "其中 \\(a_{i}, b_{i}\\) 为观测数据，\\(\\varepsilon_{i}\\) 为独立同分布的噪声，\\(x\\) 是要估计的参数。在下面的假设下，请利用最大似然估计方法构造相应的优化问题来估计参数 \\(x\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.821,
                0.714,
                0.857
            ],
            "angle": 0,
            "content": "(a) 噪声 \\(\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})\\), 其密度函数为 \\(p(z) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{z^{2}}{2\\sigma^{2}}\\right)\\);"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "118"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        },
        {
            "type": "text",
            "bbox": [
                0.304,
                0.157,
                0.825,
                0.205
            ],
            "angle": 0,
            "content": "(b) 噪声 \\(\\varepsilon_{i}\\) 服从拉普拉斯（Laplace）分布，其密度函数为 \\(p(z) = \\frac{1}{2a} \\exp \\left(-\\frac{|z|}{a}\\right), a > 0\\)；"
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.206,
                0.825,
                0.252
            ],
            "angle": 0,
            "content": "(c) 噪声 \\(\\varepsilon_{i}\\) 为 \\([-a, a] (a > 0)\\) 上的均匀分布，其密度函数为 \\(p(z) = \\frac{1}{2a}, z \\in [-a, a]\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.304,
                0.157,
                0.825,
                0.252
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.259,
                0.668,
                0.275
            ],
            "angle": 0,
            "content": "3.8 在逻辑回归中，如果把Sigmoid函数(3.3.1)换成"
        },
        {
            "type": "equation",
            "bbox": [
                0.474,
                0.283,
                0.65,
                0.317
            ],
            "angle": 0,
            "content": "\\[\n\\theta (z) = \\frac {1}{2} + \\frac {z}{2 (1 + | z |)},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.324,
                0.825,
                0.361
            ],
            "angle": 0,
            "content": "试利用最大似然估计建立分类模型。该模型得到的优化问题是否是凸的？"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.375,
                0.483,
                0.39
            ],
            "angle": 0,
            "content": "3.9 给定以下带标签的数据："
        },
        {
            "type": "table",
            "bbox": [
                0.301,
                0.394,
                0.508,
                0.46
            ],
            "angle": 0,
            "content": "<table><tr><td>标签</td><td>数据点</td></tr><tr><td>-1</td><td>(1,5,1),(9,5,1)</td></tr><tr><td>1</td><td>(8,13,13),(5,1,9)</td></tr></table>"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.463,
                0.668,
                0.479
            ],
            "angle": 0,
            "content": "请建立原始的支持向量机模型并计算分割超平面"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.492,
                0.825,
                0.549
            ],
            "angle": 0,
            "content": "3.10 用超平面（如 \\(a^{\\mathrm{T}}x + b = 0\\)）来分类的模型称为线性分类模型。证明逻辑回归是线性分类模型。与支持向量机相比，逻辑回归的优缺点是什么？"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.563,
                0.685,
                0.579
            ],
            "angle": 0,
            "content": "3.11 请分析如何将支持向量机方法应用到多分类问题中"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.592,
                0.713,
                0.609
            ],
            "angle": 0,
            "content": "3.12 考虑三个随机变量 \\(X, Y, Z\\)，取值集合均为 \\(\\{1, 2, \\dots, n\\}\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.26,
                0.563,
                0.713,
                0.609
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.622,
                0.825,
                0.659
            ],
            "angle": 0,
            "content": "(a) 在没有独立性假设的条件下，为了表示随机向量 \\((X,Y,Z)\\) 的联合概率质量函数 \\(p(x,y,z)\\)，我们至少需要多少个参数？"
        },
        {
            "type": "text",
            "bbox": [
                0.304,
                0.666,
                0.825,
                0.704
            ],
            "angle": 0,
            "content": "(b) 如果在给定 \\(X\\) 的情况下, \\(Y\\) 和 \\(Z\\) 独立, 为了表示 \\(p(x, y, z)\\), 至少需要多少个参数?"
        },
        {
            "type": "list",
            "bbox": [
                0.304,
                0.622,
                0.825,
                0.704
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.717,
                0.825,
                0.754
            ],
            "angle": 0,
            "content": "3.13 给定 \\( n \\) 维高斯随机变量的一组实际取值: \\( y^{1}, y^{2}, \\dots, y^{m} \\). 试利用最大似然方法给出其精度矩阵的估计."
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.767,
                0.588,
                0.784
            ],
            "angle": 0,
            "content": "3.14 试证明如下和 K-均值聚类相关的结论"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.797,
                0.473,
                0.813
            ],
            "angle": 0,
            "content": "(a) 设 \\(S_{i}\\) 非空，证明："
        },
        {
            "type": "equation",
            "bbox": [
                0.452,
                0.824,
                0.707,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n2 n _ {i} \\sum_ {a \\in S _ {i}} \\| a - c _ {i} \\| ^ {2} = \\sum_ {a, a ^ {\\prime} \\in S _ {i}} \\| a - a ^ {\\prime} \\| ^ {2},\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.223,
                0.133
            ],
            "angle": 0,
            "content": "习题3"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "119"
        },
        {
            "type": "text",
            "bbox": [
                0.245,
                0.157,
                0.667,
                0.174
            ],
            "angle": 0,
            "content": "其中 \\(n_i\\) 为 \\(S_{i}\\) 中元素个数， \\(c_{i}\\) 为 \\(S_{i}\\) 所有数据点的中心点"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.183,
                0.554,
                0.2
            ],
            "angle": 0,
            "content": "(b) 证明: 问题 (3.10.4) 和问题 (3.10.5) 等价."
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.213,
                0.436,
                0.23
            ],
            "angle": 0,
            "content": "3.15 在 \\(\\mathbb{R}^2\\) 空间中，定义小波框架"
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.241,
                0.563,
                0.351
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} w _ {1} = \\sqrt {\\frac {2}{3}} (0, 1) ^ {\\mathrm {T}}, \\\\ w _ {2} = \\sqrt {\\frac {2}{3}} \\left(- \\frac {\\sqrt {3}}{2}, - \\frac {1}{2}\\right) ^ {\\mathrm {T}}, \\\\ w _ {3} = \\sqrt {\\frac {2}{3}} \\left(\\frac {\\sqrt {3}}{2}, - \\frac {1}{2}\\right) ^ {\\mathrm {T}}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.362,
                0.647,
                0.38
            ],
            "angle": 0,
            "content": "对于向量 \\(x = (1,3)^{\\mathrm{T}}\\) ，试给出其在小波框架下的稀疏表示"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "120"
        },
        {
            "type": "header",
            "bbox": [
                0.684,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第三章 优化建模"
        }
    ],
    [
        {
            "type": "title",
            "bbox": [
                0.272,
                0.253,
                0.636,
                0.285
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.336,
                0.744,
                0.478
            ],
            "angle": 0,
            "content": "在实际中优化问题的形式多种多样。对于不同种类的优化问题，我们需要根据问题的具体形式，来分析其理论性质以及设计最有效的算法。本章将会列举一些重要的优化问题，并根据第三章优化建模进一步给出相关的应用背景和应用举例。在这里我们指出，对于同一个实际问题，使用不同的建模手段可能获得形式不同的优化问题。这些问题的求解难度以及解的性质可能有非常大的差别，因此将实际问题转化为何种优化问题是优化建模中需要重点考虑的。"
        },
        {
            "type": "title",
            "bbox": [
                0.374,
                0.513,
                0.534,
                0.534
            ],
            "angle": 0,
            "content": "4.1 线性规划"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.556,
                0.411,
                0.574
            ],
            "angle": 0,
            "content": "4.1.1 基本形式和应用背景"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.589,
                0.441,
                0.606
            ],
            "angle": 0,
            "content": "线性规划问题的一般形式如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.62,
                0.482,
                0.645
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} c ^ {\\mathrm {T}} x,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.649,
                0.737,
                0.667
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A x = b, \\end{array} \\tag {4.1.1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.447,
                0.676,
                0.507,
                0.691
            ],
            "angle": 0,
            "content": "\\[\nG x \\leqslant e,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.708,
                0.741,
                0.767
            ],
            "angle": 0,
            "content": "其中 \\(c \\in \\mathbb{R}^n, A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, G \\in \\mathbb{R}^{p \\times n}\\) 和 \\(e \\in \\mathbb{R}^p\\) 是给定的矩阵和向量，\\(x \\in \\mathbb{R}^n\\) 是决策变量。在实际中，我们考虑问题 (4.1.1) 的两种特殊形式（其他形式都可以转化成这两种形式）：标准形（等式约束和决策变量非负）"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.781,
                0.482,
                0.806
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} c ^ {\\mathrm {T}} x,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.81,
                0.737,
                0.827
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad A x = b, \\tag {4.1.2}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.447,
                0.838,
                0.495,
                0.852
            ],
            "angle": 0,
            "content": "\\[\nx \\geqslant 0,\n\\]"
        },
        {
            "type": "page_number",
            "bbox": [
                0.44,
                0.869,
                0.468,
                0.882
            ],
            "angle": 0,
            "content": "121"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "122"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.157,
                0.495,
                0.174
            ],
            "angle": 0,
            "content": "以及不等式形（没有等式约束）"
        },
        {
            "type": "equation",
            "bbox": [
                0.486,
                0.19,
                0.826,
                0.222
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} c ^ {\\mathrm {T}} x, \\tag {4.1.3}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.497,
                0.223,
                0.596,
                0.236
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A x \\leqslant b. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.257,
                0.827,
                0.44
            ],
            "angle": 0,
            "content": "线性规划最先在第二次世界大战时被提出，用于最大化资源的利用效率。其中的“规划”也是一个军事词汇，指按照既定的时刻表去执行任务或者用最佳方式做人员部署。线性规划问题的研究很快得到了大家的关注。二战之后，美国空军启动了一项关于军事规划和分配模型的项目。在1947年，著名的单纯形方法被提出，使得线性规划问题可以被有效地求解。之后，线性规划用到了更多其他领域当中，如农业、石油、钢铁、运输、通信和运筹学等。线性规划的有效应用节省了大量的人力、物力和财力。随着计算机以及求解算法的快速发展，我们可以求解更大规模的线性规划问题，保证了线性规划问题的应用前景。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.477,
                0.402,
                0.495
            ],
            "angle": 0,
            "content": "4.1.2 应用举例"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.512,
                0.363,
                0.528
            ],
            "angle": 0,
            "content": "1. 运输问题"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.546,
                0.825,
                0.661
            ],
            "angle": 0,
            "content": "有 \\(I\\) 个港口 \\(P_{1}, P_{2}, \\dots, P_{I}\\), 提供某种商品. 有 \\(J\\) 个市场 \\(M_{1}, M_{2}, \\dots, M_{J}\\) 需要这种商品. 假设港口 \\(P_{i}\\) 有 \\(s_{i}\\) 单位的这种商品 \\((i = 1, 2, \\dots, I)\\), 市场 \\(M_{j}\\) 需要 \\(r_{j}\\) 单位的这种商品, 且总供应与总需求相等, 即 \\(\\sum_{i=1}^{I} s_{i} = \\sum_{j=1}^{J} r_{j}\\). 令 \\(b_{ij}\\) 为从港口 \\(P_{i}\\) 运输单位数量商品到市场 \\(M_{j}\\) 的成本. 运输问题是在满足市场需求下使得运输成本最低."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.667,
                0.772,
                0.684
            ],
            "angle": 0,
            "content": "令 \\(x_{ij}\\) 为从港口 \\(P_{i}\\) 运输到市场 \\(M_{j}\\) 的商品数量，总的运输代价为"
        },
        {
            "type": "equation",
            "bbox": [
                0.5,
                0.7,
                0.826,
                0.74
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i = 1} ^ {I} \\sum_ {j = 1} ^ {J} x _ {i j} b _ {i j}. \\tag {4.1.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.758,
                0.763,
                0.798
            ],
            "angle": 0,
            "content": "港口 \\(P_{i}\\) 总输出量为 \\(\\sum_{j = 1}^{J}x_{ij}\\) ，因为港口 \\(P_{i}\\) 存有的商量总量为 \\(s_i\\) ，所以"
        },
        {
            "type": "equation",
            "bbox": [
                0.442,
                0.814,
                0.826,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {j = 1} ^ {J} x _ {i j} = s _ {i}, \\quad i = 1, 2, \\dots , I. \\tag {4.1.5}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.282,
                0.132
            ],
            "angle": 0,
            "content": "4.1 线性规划"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "123"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.156,
                0.638,
                0.194
            ],
            "angle": 0,
            "content": "市场 \\(M_{j}\\) 总输入量为 \\(\\sum_{i = 1}^{I}x_{ij}\\) ，因为市场 \\(M_{j}\\) 的需求量为 \\(r_j\\) ，所以"
        },
        {
            "type": "equation",
            "bbox": [
                0.352,
                0.201,
                0.737,
                0.239
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i = 1} ^ {I} x _ {i j} = r _ {j}, \\quad j = 1, 2, \\dots , J. \\tag {4.1.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.245,
                0.382,
                0.261
            ],
            "angle": 0,
            "content": "因为运输量是非负的，所以"
        },
        {
            "type": "equation",
            "bbox": [
                0.306,
                0.273,
                0.737,
                0.291
            ],
            "angle": 0,
            "content": "\\[\nx _ {i j} \\geqslant 0, \\quad i = 1, 2, \\dots , I, \\quad j = 1, 2, \\dots , J. \\tag {4.1.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.301,
                0.737,
                0.339
            ],
            "angle": 0,
            "content": "因此，我们想要在约束 (4.1.5)-(4.1.7) 成立的情况下极小化 (4.1.4) 式．针对决策变量的 \\(I \\times J\\) 矩阵 \\(\\left(x_{ij}\\right)\\)，我们可以得到如下线性规划问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.284,
                0.346,
                0.623,
                0.498
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x} \\quad \\sum_ {i = 1} ^ {I} \\sum_ {j = 1} ^ {J} x _ {i j} b _ {i j}, \\\\ \\begin{array}{l} \\text {s . t .} \\quad \\sum_ {j = 1} ^ {J} x _ {i j} = s _ {i}, \\quad i = 1, 2, \\dots , I, \\end{array} \\\\ \\sum_ {i = 1} ^ {I} x _ {i j} = r _ {j}, \\quad j = 1, 2, \\dots , J, \\\\ x _ {i j} \\geqslant 0, \\quad i = 1, 2, \\dots , I, \\quad j = 1, 2, \\dots , J. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.505,
                0.738,
                0.585
            ],
            "angle": 0,
            "content": "这个问题还有更一般的版本，即最优运输问题。它是关心两个（离散、连续）测度的对应关系。具体地，若测度是离散的，我们想要确定的是离散点之间的对应关系；若测度是连续的，我们想要确定的是区域之间的对应关系。更多内容可以参考 [151]。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.607,
                0.343,
                0.623
            ],
            "angle": 0,
            "content": "2. 马尔可夫决策过程"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.637,
                0.738,
                0.716
            ],
            "angle": 0,
            "content": "在马尔可夫决策过程中，考虑终止时间 \\(T = \\infty\\) 的情形。因为折现因子 \\(0 < \\gamma < 1\\) ，所以如果单步奖励是有界的，则策略对应的奖励和是有界的。否则，如果奖励和无界，任意一个策略的奖励和都是无限的，失去了研究价值。在有界的假设下，Bellman 方程 (3.13.2) 可以转化为如下线性规划问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.236,
                0.722,
                0.669,
                0.789
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\max  _ {V \\in \\mathbb {R} ^ {| S |}} \\quad \\sum_ {i} V (i) \\\\ \\begin{array}{l} \\text {s . t .} V (i) \\geqslant \\sum_ {j} P _ {a} (i, j) (r (i, a) + \\gamma V (j)), \\forall i \\in \\mathcal {S}, \\forall a \\in \\mathcal {A}, \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.795,
                0.738,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(V(i)\\) 是向量 \\(V\\) 的第 \\(i\\) 个分量, 表示从状态 \\(i\\) 出发得到的累积奖励, \\(P_{a}(i,j)\\) 是转移概率, \\(r(i,a)\\) 是单步奖励以及 \\(\\gamma\\) 为折现因子. 通过求解上述优化问题,我们可以求出最优动作 \\(a(i)\\) 以及最优期望奖励"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "124"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.157,
                0.38,
                0.174
            ],
            "angle": 0,
            "content": "3. 基追踪问题"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.189,
                0.694,
                0.207
            ],
            "angle": 0,
            "content": "基追踪问题是压缩感知中的一个基本问题，可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.486,
                0.223,
                0.825,
                0.252
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| x \\| _ {1}, \\tag {4.1.8}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.497,
                0.254,
                0.596,
                0.267
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A x = b. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.285,
                0.712,
                0.302
            ],
            "angle": 0,
            "content": "对每个 \\(|x_{i}|\\) 引入一个新的变量 \\(z_{i}\\), 可以将问题 (4.1.8) 转化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.4,
                0.316,
                0.495,
                0.354
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {z \\in \\mathbb {R} ^ {n}} \\quad \\sum_ {i = 1} ^ {n} z _ {i},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.411,
                0.359,
                0.513,
                0.376
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A x = b, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.455,
                0.385,
                0.682,
                0.401
            ],
            "angle": 0,
            "content": "\\[\n- z _ {i} \\leqslant x _ {i} \\leqslant z _ {i}, \\quad i = 1, 2, \\dots , n,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.417,
                0.827,
                0.474
            ],
            "angle": 0,
            "content": "这是一个线性规划问题. 另外, 我们也可以引入 \\(x_{i}\\) 的正部和负部 \\(x_{i}^{+}, x_{i}^{-} \\geqslant 0\\),利用 \\(x_{i} = x_{i}^{+} - x_{i}^{-}, |x_{i}| = x_{i}^{+} + x_{i}^{-}\\), 问题 (4.1.8) 的另外一种等价的线性规划形式可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.442,
                0.477,
                0.619,
                0.513
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x ^ {+}, x ^ {-} \\in \\mathbb {R} ^ {n}} \\sum_ {i = 1} ^ {n} \\left(x _ {i} ^ {+} + x _ {i} ^ {-}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.475,
                0.519,
                0.642,
                0.536
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A x ^ {+} - A x ^ {-} = b, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.513,
                0.544,
                0.598,
                0.561
            ],
            "angle": 0,
            "content": "\\[\nx ^ {+}, x ^ {-} \\geqslant 0.\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.59,
                0.362,
                0.606
            ],
            "angle": 0,
            "content": "4. 数据拟合"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.622,
                0.802,
                0.639
            ],
            "angle": 0,
            "content": "在数据拟合中，除了常用的最小二乘模型外，还有最小 \\(\\ell_{1}\\) 范数模型"
        },
        {
            "type": "equation",
            "bbox": [
                0.474,
                0.657,
                0.826,
                0.681
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| A x - b \\| _ {1}, \\tag {4.1.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.693,
                0.412,
                0.71
            ],
            "angle": 0,
            "content": "和最小 \\(\\ell_{\\infty}\\) 范数模型"
        },
        {
            "type": "equation",
            "bbox": [
                0.472,
                0.717,
                0.826,
                0.741
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| A x - b \\| _ {\\infty}. \\tag {4.1.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.75,
                0.825,
                0.788
            ],
            "angle": 0,
            "content": "这两个问题都可以转化成线性规划的形式。对于问题 (4.1.9)，通过引入变量 \\( y = Ax - b \\)，可以得到如下等价问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.805,
                0.567,
                0.829
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, y \\in \\mathbb {R} ^ {n}} \\| y \\| _ {1},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.485,
                0.838,
                0.615,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & y = A x - b. \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "4.2 最小二乘问题"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "125"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.195
            ],
            "angle": 0,
            "content": "利用基追踪问题中类似的技巧，我们可以将上述绝对值优化问题转化成线性规划问题．对于问题(4.1.10)，令 \\(t = \\| Ax - b\\|_{\\infty}\\) ，则得到等价问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.357,
                0.206,
                0.448,
                0.228
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}, t \\in \\mathbb {R}} t,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.376,
                0.235,
                0.551,
                0.253
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\text {s . t .} \\quad \\| A x - b \\| _ {\\infty} \\leqslant t. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.262,
                0.462,
                0.278
            ],
            "angle": 0,
            "content": "利用 \\(\\ell_{\\infty}\\) 范数的定义，可以进一步写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.289,
                0.428,
                0.311
            ],
            "angle": 0,
            "content": "\\[\n\\min_{x\\in \\mathbb{R}^{n},t\\in \\mathbb{R}}\\quad t,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.319,
                0.569,
                0.334
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & - t \\mathbf {1} \\leqslant A x - b \\leqslant t \\mathbf {1}, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.345,
                0.352,
                0.361
            ],
            "angle": 0,
            "content": "这是一个线性规划问题"
        },
        {
            "type": "title",
            "bbox": [
                0.349,
                0.391,
                0.558,
                0.412
            ],
            "angle": 0,
            "content": "4.2 最小二乘问题"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.431,
                0.412,
                0.451
            ],
            "angle": 0,
            "content": "4.2.1 基本形式和应用背景"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.463,
                0.441,
                0.479
            ],
            "angle": 0,
            "content": "最小二乘问题的一般形式如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.485,
                0.737,
                0.522
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\sum_ {i = 1} ^ {m} r _ {i} ^ {2} (x), \\tag {4.2.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.529,
                0.739,
                0.587
            ],
            "angle": 0,
            "content": "其中 \\(r_i: \\mathbb{R}^n \\to \\mathbb{R}\\) 为实值函数。如果所有的 \\(r_i\\) 都是线性函数，我们称问题(4.2.1)为线性最小二乘问题，否则称其为非线性最小二乘问题。最小二乘问题是线性回归和非线性回归的基础。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.591,
                0.74,
                0.67
            ],
            "angle": 0,
            "content": "最小二乘问题也常用于线性（非线性）方程组问题当中（见第三章最小二乘建模）。当线性（非线性）观测带有噪声时，我们一般会基于该线性（非线性）系统建立最小二乘模型。特别地，如果噪声服从高斯分布，最小二乘问题的解对应于原问题的最大似然解（见第三章的回归分析）。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.694,
                0.314,
                0.712
            ],
            "angle": 0,
            "content": "4.2.2 应用举例"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.726,
                0.72,
                0.742
            ],
            "angle": 0,
            "content": "第三章中介绍的有些应用问题直接是最小二乘问题的形式，见表4.1."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.766,
                0.345,
                0.782
            ],
            "angle": 0,
            "content": "1. 线性最小二乘问题"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.796,
                0.691,
                0.812
            ],
            "angle": 0,
            "content": "线性最小二乘问题是回归分析中的一个基本模型，它可以表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.373,
                0.819,
                0.534,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\sum_ {i = 1} ^ {m} \\left(a _ {i} ^ {\\mathrm {T}} x - b _ {i}\\right) ^ {2},\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "126"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "table_caption",
            "bbox": [
                0.456,
                0.159,
                0.629,
                0.176
            ],
            "angle": 0,
            "content": "表 4.1 最小二乘问题"
        },
        {
            "type": "table",
            "bbox": [
                0.313,
                0.188,
                0.773,
                0.257
            ],
            "angle": 0,
            "content": "<table><tr><td>应用</td><td>ri(x)</td><td>对应问题</td></tr><tr><td>回归分析</td><td>aixi=bix</td><td>(3.2.4)</td></tr><tr><td>相位恢复</td><td>|&lt;aix&gt; |2-bi| |&lt;aix&gt; -bi</td><td>(3.6.2) (3.6.3)</td></tr></table>"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.279,
                0.825,
                0.318
            ],
            "angle": 0,
            "content": "即 \\(r_i(x) = a_i^{\\mathrm{T}}x - b_i, i = 1,2,\\dots ,m.\\) 记 \\(A = [a_{1},a_{2},\\dots ,a_{m}]^{\\mathrm{T}}\\) ，那么线性最小二乘问题可以等价地写成如下形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.327,
                0.651,
                0.358
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x) \\stackrel {\\text {d e f}} {=} \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.369,
                0.825,
                0.406
            ],
            "angle": 0,
            "content": "这是一个无约束二次目标函数的优化问题。因为二次函数 \\(f\\) 是凸的，故 \\(x \\in \\mathbb{R}^n\\) 为其全局极小解当且仅当 \\(x\\) 满足方程"
        },
        {
            "type": "equation",
            "bbox": [
                0.441,
                0.422,
                0.641,
                0.441
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x) = A ^ {\\mathrm {T}} (A x - b) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.458,
                0.557,
                0.475
            ],
            "angle": 0,
            "content": "事实上，因为 \\(f\\) 是二次的，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.328,
                0.488,
                0.755,
                0.55
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (y) = f (x) + \\nabla f (x) ^ {\\mathrm {T}} (y - x) + \\frac {1}{2} (y - x) ^ {\\mathrm {T}} \\nabla^ {2} f (x) (y - x) \\\\ = f (x) + \\nabla f (x) ^ {\\mathrm {T}} (y - x) + \\frac {1}{2} (y - x) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} A (y - x). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.562,
                0.623,
                0.58
            ],
            "angle": 0,
            "content": "因此，如果 \\(\\nabla f(x) = 0\\) ，根据 \\(A^{\\mathrm{T}}A\\) 的半正定性，"
        },
        {
            "type": "equation",
            "bbox": [
                0.449,
                0.596,
                0.634,
                0.614
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x), \\quad \\forall y \\in \\mathbb {R} ^ {n},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.63,
                0.466,
                0.647
            ],
            "angle": 0,
            "content": "即 \\(x\\) 为 \\(f(x)\\) 的全局极小解"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.651,
                0.825,
                0.715
            ],
            "angle": 0,
            "content": "反之，如果 \\(\\nabla f(x) \\neq 0\\) ，此时我们说明沿着负梯度方向目标函数将减小具体地，取 \\(y = x - t\\nabla f(x)\\) 且 \\(t = \\frac{1}{\\lambda_{\\max}(A^{\\mathrm{T}}A)}\\) ，其中 \\(\\lambda_{\\max}(A^{\\mathrm{T}}A)\\) 表示 \\(A^{\\mathrm{T}}A\\) 的最大特征值，那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.305,
                0.727,
                0.779,
                0.825
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (x - t \\nabla f (x)) \\leqslant f (x) - t \\| \\nabla f (x) \\| _ {2} ^ {2} + \\frac {1}{2} t ^ {2} \\lambda_ {\\max } \\left(A ^ {\\mathrm {T}} A\\right) \\| \\nabla f (x) \\| _ {2} ^ {2} \\\\ = f (x) + \\left(- t + \\frac {1}{2} t ^ {2} \\lambda_ {\\max } \\left(A ^ {T} A\\right)\\right) \\| \\nabla f (x) \\| _ {2} ^ {2} \\\\ = f (x) - \\frac {1}{2 \\lambda_ {\\max } \\left(A ^ {T} A\\right)} \\| \\nabla f (x) \\| _ {2} ^ {2} <   f (x). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.559,
                0.853
            ],
            "angle": 0,
            "content": "因而在全局极小解 \\(x\\) 处必有 \\(\\nabla f(x) = 0\\)"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "4.2 最小二乘问题"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "127"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.157,
                0.275,
                0.174
            ],
            "angle": 0,
            "content": "2. 数据插值"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.188,
                0.737,
                0.227
            ],
            "angle": 0,
            "content": "数据插值是数值分析的一个基本问题. 给定数据集 \\(\\{a_{i} \\in \\mathbb{R}^{p}, b_{i} \\in \\mathbb{R}^{q}, i = 1,2,\\dots,m\\}\\), 插值是求一个映射 \\(f\\), 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.35,
                0.244,
                0.556,
                0.262
            ],
            "angle": 0,
            "content": "\\[\nb _ {i} = f \\left(a _ {i}\\right), \\quad i = 1, 2, \\dots , m.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.279,
                0.737,
                0.339
            ],
            "angle": 0,
            "content": "在实际中，出于计算上的可行性，我们一般会限制在一个特定函数空间上来求 \\(f\\) 的一个逼近解．如果利用线性函数逼近，即 \\(f(a) = Xa + y\\) ，其中\\(X\\in \\mathbb{R}^{q\\times p},y\\in \\mathbb{R}^q\\) ，则为了求解 \\(X,y\\) ，可以建立如下最小二乘问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.35,
                0.35,
                0.559,
                0.389
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in \\mathbb {R} ^ {q \\times p}} \\quad \\sum_ {i = 1} ^ {m} \\| X a _ {i} + y - b _ {i} \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.399,
                0.737,
                0.438
            ],
            "angle": 0,
            "content": "一般地，假设 \\(\\{\\phi_i(a)\\}_{i=1}^n (n \\leqslant m)\\) 为插值空间的一组基，数据插值问题可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.3,
                0.438,
                0.607,
                0.473
            ],
            "angle": 0,
            "content": "\\[\nb _ {j} = f (a _ {j}) = \\sum_ {i = 1} ^ {n} x _ {i} \\phi_ {i} (a _ {j}), \\quad j = 1, 2, \\dots , m,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.482,
                0.737,
                0.52
            ],
            "angle": 0,
            "content": "其中 \\(x_{i}\\) 为待定系数．这是关于 \\(x\\) 的线性方程组．在实际中，我们考虑其替代的最小二乘模型"
        },
        {
            "type": "equation",
            "bbox": [
                0.343,
                0.52,
                0.562,
                0.559
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\sum_ {j = 1} ^ {m} \\| \\sum_ {i = 1} ^ {n} x _ {i} \\phi_ {i} (a _ {j}) - b _ {j} \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.567,
                0.737,
                0.605
            ],
            "angle": 0,
            "content": "对于基 \\(\\phi_i(a)\\) 的选取，一般要求其反映数据的物理性质或者内在性质以及计算比较方便."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.61,
                0.737,
                0.669
            ],
            "angle": 0,
            "content": "除了这种基函数的和的方式，深度学习 [122] 也通过一些简单函数的复合来逼近原未知函数。具体地，假设有一些简单的非线性向量函数 \\(\\phi_i(\\theta) : \\mathbb{R}^q \\to \\mathbb{R}^q\\)，并构造如下复合函数："
        },
        {
            "type": "equation",
            "bbox": [
                0.235,
                0.686,
                0.672,
                0.706
            ],
            "angle": 0,
            "content": "\\[\nf (\\theta) = \\phi_ {n} \\left(X _ {n} \\phi_ {n - 1} \\left(X _ {n - 1} \\dots \\phi_ {1} \\left(X _ {1} \\theta + y _ {1}\\right) \\dots + y _ {n - 1}\\right) + y _ {n}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.721,
                0.517,
                0.738
            ],
            "angle": 0,
            "content": "在实际中常用的简单非线性函数有ReLU，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.216,
                0.754,
                0.691,
                0.774
            ],
            "angle": 0,
            "content": "\\[\n\\phi_ {i} (\\theta) = \\left(\\mathrm {R e L U} (\\theta_ {1}), \\mathrm {R e L U} (\\theta_ {2}), \\dots , \\mathrm {R e L U} (\\theta_ {q})\\right) ^ {\\mathrm {T}}, \\quad i = 1, 2, \\dots , n,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.79,
                0.194,
                0.807
            ],
            "angle": 0,
            "content": "且"
        },
        {
            "type": "equation",
            "bbox": [
                0.361,
                0.805,
                0.547,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {R e L U} (t) = {\\left\\{ \\begin{array}{l l} {t,} & {t \\geqslant 0,} \\\\ {0,} & {{\\text {其 他}}.} \\end{array} \\right.}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "128"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.214
            ],
            "angle": 0,
            "content": "这样的做法往往会带来更多未知的非线性，因而可能在更大的函数空间中得到未知函数的一个更好的逼近。将点 \\(a_{i}\\) 处的取值代入，我们得到如下非线性方程组："
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.228,
                0.796,
                0.27
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (a _ {i}) - b _ {i} = \\phi_ {n} \\left(X _ {n} \\phi_ {n - 1} \\left(X _ {n - 1} \\dots \\phi_ {1} \\left(X _ {1} a _ {i} + y _ {1}\\right) \\dots + y _ {n - 1}\\right) + y _ {n}\\right) - b _ {i} \\\\ = 0, \\quad i = 1, 2, \\dots , n. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.284,
                0.825,
                0.322
            ],
            "angle": 0,
            "content": "这里，需要求解的是关于 \\(X_{1}\\in \\mathbb{R}^{q\\times p},X_{i}\\in \\mathbb{R}^{q\\times q},i = 2,3,\\dots ,n,y_{i}\\in \\mathbb{R}^{q},i =\\) \\(1,2,\\dots ,n\\) 的非线性方程组．我们一般考虑替代的最小二乘问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.451,
                0.333,
                0.635,
                0.369
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {\\{X _ {i}, y _ {i} \\}} \\quad \\sum_ {i = 1} ^ {m} \\| f (a _ {i}) - b _ {i} \\| ^ {2}.\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.388,
                0.384,
                0.405
            ],
            "angle": 0,
            "content": "3. 深度Q学习"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.418,
                0.825,
                0.497
            ],
            "angle": 0,
            "content": "在强化学习中，为了求解出最优策略及相应的期望奖励，往往需要考虑动作价值函数（action-value function）\\(Q: S \\times \\mathcal{A} \\to \\mathbb{R}\\)（注意，我们一般称 \\(V\\) 为价值函数，即 value function），其表示从状态 \\(s\\) 出发，采取动作 \\(a\\) 可以获得的最大期望奖励。根据最优性，其 Bellman 方程为"
        },
        {
            "type": "equation",
            "bbox": [
                0.375,
                0.512,
                0.825,
                0.54
            ],
            "angle": 0,
            "content": "\\[\nQ (s, a) = r (s, a) + \\gamma \\sum_ {s ^ {\\prime}} P _ {a} \\left(s, s ^ {\\prime}\\right) \\max  _ {a ^ {\\prime}} Q \\left(s ^ {\\prime}, a ^ {\\prime}\\right). \\tag {4.2.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.553,
                0.664,
                0.57
            ],
            "angle": 0,
            "content": "对于求解方程 (4.2.2)，一个常用的迭代算法的格式为："
        },
        {
            "type": "equation",
            "bbox": [
                0.361,
                0.584,
                0.825,
                0.613
            ],
            "angle": 0,
            "content": "\\[\nQ _ {k + 1} (s, a) = r (s, a) + \\gamma \\sum_ {s ^ {\\prime}} P _ {a} \\left(s, s ^ {\\prime}\\right) \\max  _ {a ^ {\\prime}} Q _ {k} \\left(s ^ {\\prime}, a ^ {\\prime}\\right). \\tag {4.2.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.625,
                0.741,
                0.642
            ],
            "angle": 0,
            "content": "同样地，每一轮更新都需要对所有状态动作对 \\((s,a)\\) 做一次迭代"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.646,
                0.826,
                0.766
            ],
            "angle": 0,
            "content": "然而在实际问题中，我们通常没有模型信息，也就是说，上式涉及的奖励和状态转移概率都是未知的。为此，我们必须与环境进行交互，获取经验来估计这些项的大小。在与环境交互获得的经验中，我们可以得到很多形如 \\((s_{t}, a_{t}, r_{t}, s_{t+1})\\) 的四元组，它记录了智能体在时刻 \\(t\\) 处于状态 \\(s_{t}\\) 时选择某个动作 \\(a_{t}\\)，转移至状态 \\(s_{t+1}\\)，同时获得奖励 \\(r_{t} = r(s_{t}, a_{t})\\)，算法可以根据这样的小段进行迭代更新："
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.782,
                0.763,
                0.806
            ],
            "angle": 0,
            "content": "\\[\nQ _ {k + 1} (s _ {t}, a _ {t}) = (1 - \\alpha) Q _ {k} (s _ {t}, a _ {t}) + \\alpha \\left(r _ {t} + \\gamma \\max  _ {a ^ {\\prime}} Q _ {k} \\left(s _ {t + 1}, a ^ {\\prime}\\right)\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.815,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(\\alpha \\in (0,1)\\). 观察上式右端第二项, \\(r_{t} + \\gamma \\max_{a^{\\prime}} Q_{k}(s_{t+1}, a^{\\prime})\\) 是对(4.2.3)式右端期望值的一个采样, 为无偏估计. 为了计算方便, 我们不会等到所有数"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "4.2 最小二乘问题"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "129"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.195
            ],
            "angle": 0,
            "content": "据生成后再计算平均值，而是利用凸组合的方式持续不断地将新的计算结果按一定权重加到原有数据上，这是强化学习中一种常用的均值计算技巧。"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.199,
                0.744,
                0.362
            ],
            "angle": 0,
            "content": "在实际应用场景中，状态空间通常非常大，甚至是连续的。当计算所有状态和动作对应的动作价值函数时，庞大的存储量和计算量会导致算法难以进行。为了解决这个问题，我们引入“归纳”的概念。简单来说，我们只学习状态集合中一部分状态的动作价值函数，然后用这些动作价值函数去归纳近似其他类似状态的动作价值函数。这样，只需要学习一部分状态上的动作价值函数，就可以得到整个空间中动作价值函数的估计。这里，归纳没有将每个动作状态对 \\((s, a)\\) 看做是独立的变量，而是考虑了它们之间的关系，并认为类似的状态必有相近的动作价值函数。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.364,
                0.744,
                0.568
            ],
            "angle": 0,
            "content": "实现归纳这一想法的基本途径是函数近似。我们用 \\(Q_{\\theta}(s,a)\\) 来近似动作价值函数，它带有参数 \\(\\theta \\in \\mathbb{R}^d\\)。当参数 \\(\\theta\\) 固定后，它就是一个关于状态 \\(s\\) 和动作 \\(a\\) 的二元函数。\\(Q_{\\theta}(s,a)\\) 有很多不同的形式，它可以是最简单的线性函数，也可以是复杂的神经网络（深度 Q 学习），其权重矩阵和偏差项作为这里的参数 \\(\\theta\\)。在带函数近似的 Q 学习方法中，我们不再对动作价值函数列表进行学习，而是学习参数 \\(\\theta\\)，使近似函数 \\(Q_{\\theta}(s,a)\\) 尽可能满足最优 Bellman 方程。参数 \\(\\theta\\) 的维数通常要比状态的个数小得多，改变参数中的一个维度，会对动作价值函数估计产生大范围的影响。由于引入了近似处理，\\(Q_{\\theta}(s,a)\\) 通常不会和真实动作价值函数完全相等。严格来讲，我们希望最小化平方损失，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.273,
                0.569,
                0.738,
                0.594
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {\\theta} L (\\theta) \\stackrel {\\text {d e f}} {=} \\mathbb {E} _ {(s, a) \\sim \\rho (s, a)} \\left[ \\left(y _ {\\theta} (s, a) - Q _ {\\theta} (s, a)\\right) ^ {2} \\right], \\tag {4.2.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.6,
                0.541,
                0.618
            ],
            "angle": 0,
            "content": "其中 \\(\\rho (s,a)\\) 是状态动作对 \\((s,a)\\) 出现的概率分布，"
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.631,
                0.622,
                0.661
            ],
            "angle": 0,
            "content": "\\[\ny _ {\\theta} (s, a) = r (s, a) + \\gamma \\sum_ {s ^ {\\prime}} P _ {a} (s, s ^ {\\prime}) \\max  _ {a ^ {\\prime}} Q _ {\\theta} \\left(s ^ {\\prime}, a ^ {\\prime}\\right)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.672,
                0.741,
                0.732
            ],
            "angle": 0,
            "content": "是近似函数希望逼近的目标。问题 (4.2.4) 实际上就是在极小化方程 (4.2.2)的残差。但在实际应用中，由于 (4.2.4) 比较复杂，深度 Q 学习采用迭代方式求解。在迭代的第 \\(i\\) 步近似求解如下优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.29,
                0.743,
                0.738,
                0.771
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {\\theta} L _ {i} (\\theta) \\stackrel {\\text {d e f}} {=} \\mathbb {E} _ {(s, a) \\sim \\rho (s, a)} \\left[ \\left(y _ {i} - Q _ {\\theta} (s, a)\\right) ^ {2} \\right], \\tag {4.2.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.781,
                0.211,
                0.797
            ],
            "angle": 0,
            "content": "其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.285,
                0.8,
                0.623,
                0.83
            ],
            "angle": 0,
            "content": "\\[\ny _ {i} = r (s, a) + \\gamma \\sum_ {s ^ {\\prime}} P _ {a} (s, s ^ {\\prime}) \\max  _ {a ^ {\\prime}} \\left\\{Q _ {\\theta_ {i - 1}} (s ^ {\\prime}, a ^ {\\prime}) \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "参数 \\(\\theta_{i-1}\\) 来自上一步迭代的估计。和问题 (4.2.4) 不同，\\(y_i\\) 与待优化的变量"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "130"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.195
            ],
            "angle": 0,
            "content": "\\(\\theta\\) 无关，因此可以认为问题(4.2.5)是随机版本的最小二乘问题．具体算法实现时还需进一步对它进行抽样处理."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.219,
                0.501,
                0.236
            ],
            "angle": 0,
            "content": "4. 带有微分方程约束优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.249,
                0.825,
                0.327
            ],
            "angle": 0,
            "content": "当约束中含微分方程时，我们称相应的优化问题为带微分方程约束的优化问题。它在最优控制、形状优化等各种领域中有着广泛应用。这里，我们以瓦斯油催化裂解为例。这个问题求解瓦斯油催化裂解生成气体和其他副产物的反应系数。反应过程可以由如下非线性常微分方程组表示："
        },
        {
            "type": "equation",
            "bbox": [
                0.465,
                0.337,
                0.825,
                0.389
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{l} \\dot {y} _ {1} = - \\left(\\theta_ {1} + \\theta_ {3}\\right) y _ {1} ^ {2}, \\\\ \\dot {y} _ {2} = \\theta_ {1} y _ {1} ^ {2} - \\theta_ {2} y _ {2}, \\end{array} \\right. \\tag {4.2.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.399,
                0.825,
                0.435
            ],
            "angle": 0,
            "content": "其中系数 \\(\\theta_{i} \\geqslant 0, i = 1,2,3\\)，且 \\(y_{1}, y_{2}\\) 的初值条件是已知的。我们考虑的问题是"
        },
        {
            "type": "equation",
            "bbox": [
                0.424,
                0.435,
                0.617,
                0.473
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {\\theta \\in \\mathbb {R} ^ {3}} \\quad \\sum_ {j = 1} ^ {n} \\| y (\\tau_ {j}; \\theta) - z _ {j} \\| ^ {2},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.433,
                0.481,
                0.659,
                0.498
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad y (\\tau ; \\theta) \\text {满 足 方 程 组} (4. 2. 6)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.506,
                0.687,
                0.523
            ],
            "angle": 0,
            "content": "这里 \\(z_{j}\\) 是在时刻 \\(\\tau_{j}\\) 的 \\(y\\) 的测量值，\\(n\\) 为测量的时刻数量。"
        },
        {
            "type": "title",
            "bbox": [
                0.437,
                0.553,
                0.646,
                0.575
            ],
            "angle": 0,
            "content": "4.3 复合优化问题"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.594,
                0.499,
                0.612
            ],
            "angle": 0,
            "content": "4.3.1 基本形式和应用背景"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.625,
                0.597,
                0.642
            ],
            "angle": 0,
            "content": "复合优化问题一般可以表示为如下形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.437,
                0.658,
                0.644,
                0.681
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\psi (x) = f (x) + h (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.691,
                0.825,
                0.811
            ],
            "angle": 0,
            "content": "其中 \\(f(x)\\) 是光滑函数（比如数据拟合项），\\(h(x)\\) 可能是非光滑的（比如 \\(\\ell_1\\) 范数正则项，约束集合的示性函数，或它们的线性组合）。从前文介绍的各种各样的应用问题不难发现，复合优化问题在实际中有着重要的应用，并且其中的函数 \\(h(x)\\) 一般都是凸的。由于应用问题的驱动，复合优化问题的算法近年来得到了大量的研究，比如次梯度法，近似点梯度法，Nesterov 加速算法和交替方向乘子法，等等，我们将在第六、八章中详细地介绍这些算法。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "在表4.2中，我们总结了实际中常用的复合优化问题的可能形式（可以由正则项，示性函数以及损失函数中的多个组成）."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "4.3 复合优化问题"
        },
        {
            "type": "page_number",
            "bbox": [
                0.709,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "131"
        },
        {
            "type": "table_caption",
            "bbox": [
                0.37,
                0.173,
                0.54,
                0.19
            ],
            "angle": 0,
            "content": "表 4.2 复合优化问题"
        },
        {
            "type": "table",
            "bbox": [
                0.186,
                0.204,
                0.722,
                0.835
            ],
            "angle": 0,
            "content": "<table><tr><td></td><td>函数形式</td><td>描述</td><td>对应问题</td></tr><tr><td rowspan=\"6\">正则项</td><td>||x||0</td><td>\\( \\ell_0 \\) 范数</td><td>(1.2.2), (3.1.6), (3.8.1)</td></tr><tr><td>\\( ||x||_1 \\)</td><td>\\( \\ell_1 \\) 范数</td><td>(1.2.3), (1.2.5), (3.1.7), (3.2.10), (3.2.11), (3.3.4), (3.4.5), (3.5.2), (3.9.1), (3.5.3), (3.5.4)</td></tr><tr><td>\\( ||x||_2^2 \\)</td><td>Tikhonov 正则项</td><td>(3.1.5), (3.2.6)</td></tr><tr><td>\\( ||X||_* \\)</td><td>核范数</td><td>(1.3.2), (1.3.3), (3.8.2)</td></tr><tr><td>\\( ||x||_{TV} \\)</td><td>全变差</td><td>(3.11.5)</td></tr><tr><td>\\( ||Wx||_1 \\)</td><td>变换下的 \\( \\ell_1 \\) 范数</td><td>(3.2.14), (3.2.15), (3.12.1)</td></tr><tr><td rowspan=\"7\">示性函数</td><td>\\( I_{||x||_1\\leqslant\\sigma} \\)</td><td>\\( \\ell_1 \\) 范数球</td><td>(3.2.8)</td></tr><tr><td>\\( I_{||x||_2\\leqslant\\sigma} \\)</td><td>\\( \\ell_2 \\) 范数球</td><td>(3.2.7)</td></tr><tr><td>\\( I_{||X||_F\\leqslant\\sigma} \\)</td><td>\\( F \\) 范数球</td><td>(3.9.1)</td></tr><tr><td>\\( I_{Ax=b} \\)</td><td>线性等式</td><td>(1.2.2), (1.2.3), (1.2.4), (1.3.1), (1.3.2), (3.8.1), (3.8.2), (4.1.1), (4.1.2)</td></tr><tr><td>\\( I_{Ax\\leqslant b} \\)</td><td>线性不等式</td><td>(3.4.2), (3.4.3), (4.1.1), (4.1.3)</td></tr><tr><td>\\( I_{||Ax-b||\\leqslant\\sigma} \\)</td><td>线性等式的扰动</td><td>(3.2.9), (3.5.4)</td></tr><tr><td>\\( I_{x\\geqslant 0} \\)</td><td>非负象限</td><td>(3.4.3), (3.10.5), (4.1.2)</td></tr><tr><td rowspan=\"6\">损失、奖励函数</td><td>\\( c^Tx \\)</td><td>线性函数</td><td>(4.1.1), (4.1.3), (4.1.2)</td></tr><tr><td>\\( ||Ax-b||_2^2 \\)</td><td>\\( \\ell_2 \\) 距离平方函数</td><td>(1.2.5), (1.3.3), (3.2.4), (3.2.6), (3.2.8), (3.2.7), (3.2.15), (3.9.1), (3.5.3), (3.12.2), (3.12.1), (3.12.3)</td></tr><tr><td>\\( ||Ax-b||_1 \\)</td><td>\\( \\ell_1 \\) 距离函数</td><td>(3.2.5), (3.2.11), (3.5.3)</td></tr><tr><td>\\( ||Ax-b||_2 \\)</td><td>\\( \\ell_2 \\) 距离函数</td><td>(3.2.10)</td></tr><tr><td>\\( \\sum_{i=1}^{m}\\ln(1+\\exp(-b_i\\cdot a_i^TX)) \\)</td><td>互熵损失</td><td>(3.3.4)</td></tr><tr><td>\\( \\sum_{i=1}^{m}\\max\\{1-b_i(a_i^TX+y),0\\} \\)</td><td>铰链损失</td><td>(3.4.4), (3.4.5)</td></tr></table>"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "132"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.402,
                0.174
            ],
            "angle": 0,
            "content": "4.3.2 应用举例"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.188,
                0.569,
                0.205
            ],
            "angle": 0,
            "content": "考虑带有 \\(\\ell_1\\) 范数正则项的优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.461,
                0.221,
                0.622,
                0.244
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} f (x) + \\mu \\| x \\| _ {1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.256,
                0.825,
                0.293
            ],
            "angle": 0,
            "content": "这里 \\(\\mu > 0\\) 为给定的参数。这个问题广泛存在于各种各样的应用中。我们根据第三章的内容，列出问题的具体形式："
        },
        {
            "type": "text",
            "bbox": [
                0.28,
                0.304,
                0.755,
                0.34
            ],
            "angle": 0,
            "content": "- \\(\\ell_{1}\\) 范数正则化最小二乘问题 (3.1.7): \\(f(x) = \\sum_{i=1}^{m} (b_{i} - \\phi_{i}(x))^{2}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.28,
                0.35,
                0.844,
                0.39
            ],
            "angle": 0,
            "content": "- \\(\\ell_1\\) 范数正则化回归分析问题 (3.2.10): \\(f(x) = \\| Ax - b\\| _2^2\\) ，以及问题(3.2.11): \\(f(x) = \\| Ax - b\\| _1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.28,
                0.4,
                0.825,
                0.437
            ],
            "angle": 0,
            "content": "- \\(\\ell_{1}\\) 范数正则化逻辑回归问题 (3.3.4): \\(f(x) = \\sum_{i=1}^{m} \\ln(1 + \\exp(-b_{i} \\cdot a_{i}^{\\mathrm{T}}x))\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.28,
                0.448,
                0.825,
                0.485
            ],
            "angle": 0,
            "content": "- \\(\\ell_{1}\\) 范数正则化支持向量机 (3.4.5): \\(f(x) = C\\sum_{i=1}^{m}\\max\\{1 - b_{i}(a_{i}^{\\mathrm{T}}x + y), 0\\}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.28,
                0.494,
                0.82,
                0.513
            ],
            "angle": 0,
            "content": "- \\(\\ell_{1}\\) 范数正则化精度矩阵估计 (3.5.2): \\(f(X) = -\\left(\\ln \\det(X) - \\operatorname{Tr}(XS)\\right)\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.28,
                0.527,
                0.578,
                0.546
            ],
            "angle": 0,
            "content": "- 矩阵分离问题 (3.8.2): \\(f(X) = \\| X\\|_*\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.555,
                0.741,
                0.586
            ],
            "angle": 0,
            "content": "- 字典学习问题 (3.9.1): \\( f(D, X) = \\frac{1}{2n} \\| DX - A \\|_F^2 + I_{\\|D\\|_F \\leqslant 1} \\)."
        },
        {
            "type": "list",
            "bbox": [
                0.28,
                0.304,
                0.844,
                0.586
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.594,
                0.825,
                0.631
            ],
            "angle": 0,
            "content": "根据复合优化问题的定义，我们可以将线性规划问题(4.1.2)也写成复合优化的形式，"
        },
        {
            "type": "equation",
            "bbox": [
                0.445,
                0.636,
                0.637,
                0.659
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} c ^ {\\mathrm {T}} x + I _ {A x = b} + I _ {x \\geqslant 0}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.666,
                0.825,
                0.725
            ],
            "angle": 0,
            "content": "在第三章介绍的各种应用问题，都可以写成复合优化问题的形式。具体地，在第三章中，我们介绍了图像重构问题的一般形式。这里我们以图像去噪和去模糊为例，给出线性算子 \\(A\\) 的具体形式。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.751,
                0.364,
                0.767
            ],
            "angle": 0,
            "content": "1. 图像去噪"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.782,
                0.825,
                0.819
            ],
            "angle": 0,
            "content": "图像去噪问题是指从一个带噪声的图像中恢复出不带噪声的原图. 记带噪声的图像为 \\(y\\)，噪声为 \\(\\varepsilon\\)，那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.502,
                0.839,
                0.581,
                0.854
            ],
            "angle": 0,
            "content": "\\[\ny = x + \\varepsilon ,\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "4.3 复合优化问题"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "133"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.194
            ],
            "angle": 0,
            "content": "其中 \\(x\\) 为要恢复的真实图像．利用全变差模型(3.11.5)，去噪问题可以表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.202,
                0.561,
                0.227
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n \\times n}} \\| x - y \\| _ {F} ^ {2} + \\lambda \\| x \\| _ {T V}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.236,
                0.744,
                0.298
            ],
            "angle": 0,
            "content": "这里，离散的线性算子为单位矩阵。我们也可以利用小波框架。小波变换可以很好地保护信号尖峰和突变信号，并且噪声对应的小波系数往往很小。因此，去噪问题的小波分解模型可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.308,
                0.586,
                0.34
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad \\| \\lambda \\odot (W x) \\| _ {1} + \\frac {1}{2} \\| x - y \\| _ {F} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.353,
                0.741,
                0.393
            ],
            "angle": 0,
            "content": "其中 \\(\\lambda = (\\lambda_{1},\\lambda_{2},\\dots ,\\lambda_{m})^{\\mathrm{T}}\\) 是给定的．类似地，我们还可以定义合成模型和平衡模型."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.424,
                0.276,
                0.44
            ],
            "angle": 0,
            "content": "2. 盲反卷积"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.456,
                0.741,
                0.537
            ],
            "angle": 0,
            "content": "盲反卷积（也称为去模糊）是图像处理中的一个基本问题，其目的是从一个模糊的图像恢复出原来清晰的图像。导致图像模糊的原因有很多种，比如相机抖动、聚焦不良，相机和拍摄物体所在的非静止环境以及成像设备的内在缺陷，等等。因此一般来说盲反卷积是不容易做到的。"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.54,
                0.741,
                0.64
            ],
            "angle": 0,
            "content": "为了让这个复杂的困难变得简单一些，我们做如下假设：模糊是线性的（不随图像变化）以及空间不变的（即与像素位置无关）。线性且空间不变的模糊可以表示成一个卷积。令 \\(x\\) 为原始的清晰图像，\\(a\\) 为未知的卷积核对应的矩阵，\\(y\\) 为观测到的模糊图像以及 \\(\\varepsilon\\) 为观测噪声。盲反卷积问题可以表示成"
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.651,
                0.507,
                0.667
            ],
            "angle": 0,
            "content": "\\[\ny = a * x + \\varepsilon ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.681,
                0.677,
                0.699
            ],
            "angle": 0,
            "content": "其中 * 为卷积算子. 假设噪声为高斯噪声, 则转化为求解优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.379,
                0.716,
                0.529,
                0.741
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {a, x} \\quad \\| y - a * x \\| _ {2} ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.755,
                0.741,
                0.793
            ],
            "angle": 0,
            "content": "再假设原始图像信号在小波变换下是稀疏的，我们进一步得到如下复合优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.319,
                0.8,
                0.589,
                0.825
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {a, x} \\quad \\| y - a * x \\| _ {2} ^ {2} + \\| \\lambda \\odot (W x) \\| _ {1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.836,
                0.624,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(W\\) 是小波框架， \\(\\lambda = (\\lambda_{1},\\lambda_{2},\\dots ,\\lambda_{m})^{\\mathrm{T}}\\) 用来控制稀疏度."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "134"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "title",
            "bbox": [
                0.437,
                0.155,
                0.646,
                0.177
            ],
            "angle": 0,
            "content": "4.4 随机优化问题"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.196,
                0.499,
                0.214
            ],
            "angle": 0,
            "content": "4.4.1 基本形式和应用背景"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.227,
                0.562,
                0.243
            ],
            "angle": 0,
            "content": "随机优化问题可以表示成以下形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.256,
                0.639,
                0.28
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathcal {X}} \\quad \\mathbb {E} _ {\\xi} [ F (x, \\xi) ] + h (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.288,
                0.827,
                0.41
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\) 表示决策变量 \\(x\\) 的可行域，\\(\\xi\\) 是一个随机变量（分布一般是未知的）。对于每个固定的 \\(\\xi\\)，\\(F(x, \\xi)\\) 表示样本 \\(\\xi\\) 上的损失或者奖励。正则项 \\(h(x)\\) 用来保证解的某种性质。由于变量 \\(\\xi\\) 分布的未知性，其数学期望 \\(\\mathbb{E}_{\\xi}[F(x, \\xi)]\\) 一般是不可计算的。为了得到目标函数值的一个比较好的估计，在实际问题中往往利用 \\(\\xi\\) 的经验分布来代替其真实分布。具体地，假设有 \\(N\\) 个样本 \\(\\xi_1, \\xi_2, \\dots, \\xi_N\\)，令 \\(f_i(x) = F(x, \\xi_i)\\)，我们得到优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.412,
                0.417,
                0.826,
                0.455
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathcal {X}} f (x) \\stackrel {\\text {d e f}} {=} \\frac {1}{N} \\sum_ {i = 1} ^ {N} f _ {i} (x) + h (x), \\tag {4.4.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.464,
                0.825,
                0.522
            ],
            "angle": 0,
            "content": "并称其为经验风险极小化问题或者采样平均极小化问题。这个问题通常是难以求解的，一方面是因为样本数 \\(N\\) 比较多（因此函数值、梯度计算代价比较高），另一方面是因为优化问题的可行域所在空间维数 \\(n\\) 比较大。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.526,
                0.826,
                0.584
            ],
            "angle": 0,
            "content": "这个问题在统计学、机器学习、计算机科学中有着重要的应用。对于该问题的求解，我们需要在传统的方法上引入随机性以减少算法中目标函数值和梯度等的计算代价，感兴趣的读者可以参考[86]的第八章。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.609,
                0.402,
                0.627
            ],
            "angle": 0,
            "content": "4.4.2 应用举例"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.64,
                0.825,
                0.679
            ],
            "angle": 0,
            "content": "第一、三章中的很多例子都可以写成随机优化的形式，见表4.3．我们这里还将介绍随机优化在随机主成分分析和分布式鲁棒优化问题中的应用."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.702,
                0.414,
                0.718
            ],
            "angle": 0,
            "content": "1. 随机主成分分析"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.731,
                0.825,
                0.769
            ],
            "angle": 0,
            "content": "在主成分分析(3.7.1)中，如果样本点 \\(\\xi\\) 服从某个零均值分布 \\(\\mathcal{D}\\)，那么找方差最大的 \\(d\\) 维子空间的优化问题可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.376,
                0.78,
                0.825,
                0.807
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {X \\in \\mathbb {R} ^ {p \\times d}} \\operatorname {T r} \\left(X ^ {\\mathrm {T}} \\mathbb {E} _ {\\xi \\sim \\mathcal {D}} [ \\xi \\xi^ {\\mathrm {T}} ] X\\right) \\quad \\text {s . t .} \\quad X ^ {\\mathrm {T}} X = I, \\tag {4.4.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.815,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(\\mathbb{E}_{\\xi \\sim \\mathcal{D}}[\\xi \\xi^{\\mathrm{T}}]\\) 为 \\(\\xi\\) 的协方差矩阵。在实际中，分布 \\(\\mathcal{D}\\) 是未知的，已知的只是关于分布 \\(\\mathcal{D}\\) 的采样。比如在在线主成分分析中，样本 \\(\\xi_{t}\\) 是随着时间流"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "4.4 随机优化问题"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "135"
        },
        {
            "type": "table_caption",
            "bbox": [
                0.37,
                0.159,
                0.538,
                0.176
            ],
            "angle": 0,
            "content": "表 4.3 随机优化问题"
        },
        {
            "type": "table",
            "bbox": [
                0.172,
                0.189,
                0.742,
                0.361
            ],
            "angle": 0,
            "content": "<table><tr><td>应用</td><td>ξ</td><td>F(x,ξ)</td><td>对应问题</td></tr><tr><td>稀疏优化</td><td>等概率取值于{1,2,···,m}</td><td>(aTix-bi)2</td><td>(1.2.5)</td></tr><tr><td>低秩矩阵恢复</td><td>等概率取值于Ω</td><td>(Xij-Mij)2</td><td>(1.3.2)</td></tr><tr><td>深度学习</td><td>等概率取值于{1,2,···,m}</td><td>(h(ai;x)-bi)2</td><td>(1.4.3)</td></tr><tr><td>逻辑回归</td><td>等概率取值于{1,2,···,m}</td><td>ln(1+exp(-bi·aTx))</td><td>(3.3.4)</td></tr><tr><td>支持向量机</td><td>等概率取值于{1,2,···,m}</td><td>max{1-bi(aTx+y),0}</td><td>(3.4.4), (3.4.5)</td></tr><tr><td>深度Q学习</td><td>状态动作分布</td><td>(yi-Qθ(s,a))2</td><td>(4.2.5)</td></tr></table>"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.382,
                0.739,
                0.462
            ],
            "angle": 0,
            "content": "逝依次获得的。这些已有的样本可以看作训练集。随机主成分分析关心的问题是在求得问题(4.4.2)的高逼近解过程中需要的样本数量以及所消耗的时间。受制于计算机内存的限制，我们还需要考虑在有限内存情况下的逼近解的计算与分析。读者可以参考[7,133]。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.485,
                0.327,
                0.501
            ],
            "angle": 0,
            "content": "2. 分布式鲁棒优化"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.514,
                0.739,
                0.573
            ],
            "angle": 0,
            "content": "深度学习是机器学习的一个分支，通过利用神经网络来对数据进行表征学习。深度学习的目的是从已有的未知分布的数据中学出一个好的预测器，其对应优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.576,
                0.524,
                0.599
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {h} \\mathbb {E} _ {z} [ F (h, z) ],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.603,
                0.74,
                0.744
            ],
            "angle": 0,
            "content": "其中预测器 \\(h\\) 是优化变量（见第 1.4 节），并对应于神经网络的参数。因为数据 \\(z\\) 的真实分布的未知性，我们只有有限的样本点 \\(z_{1}, z_{2}, \\dots, z_{n}\\)。在实际中的一种做法是将这些样本点对应的离散经验分布作为 \\(z\\) 的真实分布，对应的目标函数写成相应的有限和的形式，如第 1.4 节中所述。这种方式往往保证了在已有样本点上的高预测准确率。但是，当我们拿到一个新的样本点时，该预测器的准确率可能会下降很多，甚至给出不合理的预测结果。即预测器的泛化能力较差。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.749,
                0.739,
                0.786
            ],
            "angle": 0,
            "content": "为了提高预测器的泛化能力，另外一种常用的方法是考虑分布式鲁棒优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.367,
                0.79,
                0.54,
                0.813
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {h} \\max  _ {\\hat {z} \\in \\Gamma} \\mathbb {E} _ {\\hat {z}} [ F (h, \\hat {z}) ],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "这里集合 \\(\\Gamma\\) 中的随机变量的分布与真实数据的分布在一定意义下非常接近。具体地，在选取 \\(\\Gamma\\) 时，我们需要考虑其对应的实际意义、可解性和数值表"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "136"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.826,
                0.236
            ],
            "angle": 0,
            "content": "现．给定数据的经验分布，一种方式是通过分布之间的熵来定义分布间的距离，从而定义 \\(\\Gamma\\) 为与经验分布的距离小于给定数的分布的集合．目前常用的另外一种方式是利用分布间的 Wasserstein 距离，这种距离的好处是其可以改变原来经验分布的支撑集."
        },
        {
            "type": "title",
            "bbox": [
                0.462,
                0.278,
                0.621,
                0.3
            ],
            "angle": 0,
            "content": "4.5 半定规划"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.321,
                0.825,
                0.462
            ],
            "angle": 0,
            "content": "半定规划（semidefinite programming, SDP）是线性规划在矩阵空间中的一种推广。它的目标函数和等式约束均为关于矩阵的线性函数，而它与线性规划不同的地方是其自变量取值于半正定矩阵空间。作为一种特殊的矩阵优化问题（见第4.6节），半定规划在某些结构上和线性规划非常相似，很多研究线性规划的方法都可以作为研究半定规划的基础。由于半定规划地位的特殊性，我们将在本节中单独讨论半定规划的形式和应用。而一般的矩阵优化问题将在第4.6节中讨论。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.499,
                0.498,
                0.517
            ],
            "angle": 0,
            "content": "4.5.1 基本形式和应用背景"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.535,
                0.528,
                0.551
            ],
            "angle": 0,
            "content": "半定规划问题的一般形式如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.568,
                0.468,
                0.586
            ],
            "angle": 0,
            "content": "\\[\n\\min  c ^ {T} x,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.593,
                0.825,
                0.611
            ],
            "angle": 0,
            "content": "\\[\n\\text {s . t .} \\quad x _ {1} A _ {1} + x _ {2} A _ {2} + \\dots + x _ {n} A _ {n} + B \\preceq 0, \\tag {4.5.1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.619,
                0.496,
                0.634
            ],
            "angle": 0,
            "content": "\\[\nG x = h,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.652,
                0.825,
                0.732
            ],
            "angle": 0,
            "content": "其中 \\(c \\in \\mathbb{R}^n, A_i \\in S^m, i = 1,2,\\dots,m, B \\in S^m, G \\in \\mathbb{R}^{p \\times n}, h \\in \\mathbb{R}^p\\) 为已知的向量和矩阵，\\(x = (x_1,x_2,\\dots,x_n) \\in \\mathbb{R}^n\\) 是自变量。这里，如果矩阵 \\(A_i, B\\) 是对角的，那么问题(4.5.1)退化为线性规划问题。类似于线性规划问题，我们考虑半定规划的标准形式"
        },
        {
            "type": "equation",
            "bbox": [
                0.462,
                0.738,
                0.571,
                0.756
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\min  & \\langle C, X \\rangle , \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.472,
                0.763,
                0.613,
                0.781
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & \\langle A _ {1}, X \\rangle = b _ {1}, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.514,
                0.788,
                0.824,
                0.803
            ],
            "angle": 0,
            "content": "\\[\n\\dots \\tag {4.5.2}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.515,
                0.812,
                0.62,
                0.83
            ],
            "angle": 0,
            "content": "\\[\n\\langle A _ {m}, X \\rangle = b _ {m},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.513,
                0.838,
                0.564,
                0.853
            ],
            "angle": 0,
            "content": "\\[\nX \\succeq 0,\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.282,
                0.133
            ],
            "angle": 0,
            "content": "4.5 半定规划"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "137"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.28,
                0.173
            ],
            "angle": 0,
            "content": "和不等式形式"
        },
        {
            "type": "equation",
            "bbox": [
                0.296,
                0.184,
                0.737,
                0.212
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad c ^ {\\mathrm {T}} x, \\tag {4.5.3}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.307,
                0.211,
                0.61,
                0.226
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & x _ {1} A _ {1} + x _ {2} A _ {2} + \\dots + x _ {n} A _ {n} + B \\preceq 0. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.237,
                0.7,
                0.254
            ],
            "angle": 0,
            "content": "形如(4.5.1)式的优化问题都可以转化成(4.5.2)式或者(4.5.3)式的形式"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.258,
                0.744,
                0.4
            ],
            "angle": 0,
            "content": "1995年，文章[82]开创性地运用半定规划提出了一个求解NP难的最大割问题的0.8786-近似算法．半定规划被认为是自20世纪50年代著名的线性规划以后的另一个数学规划领域革命性的研究进展．半定规划的发展得益于线性规划的充分研究．尽管特殊的半定规划可看成线性规划，但作为一类特殊的矩阵优化问题，半定规划具有不同于经典线性与非线性优化问题的特点——其约束集合不是多面体．在实际应用方面，半定规划和线性规划一样，作为重要的凸优化量化建模工具被应用于工程、经济学等领域."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.424,
                0.315,
                0.442
            ],
            "angle": 0,
            "content": "4.5.2 应用举例"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.455,
                0.499,
                0.472
            ],
            "angle": 0,
            "content": "1. 二次约束二次规划问题的半定规划松弛"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.485,
                0.419,
                0.502
            ],
            "angle": 0,
            "content": "考虑二次约束二次规划问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.511,
                0.737,
                0.543
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} x ^ {\\mathrm {T}} A _ {0} x + 2 b _ {0} ^ {\\mathrm {T}} x + c _ {0}, \\tag {4.5.4}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.291,
                0.543,
                0.627,
                0.56
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad x ^ {T} A _ {i} x + 2 b _ {i} ^ {T} x + c _ {i} \\leqslant 0, \\quad i = 1, 2, \\dots , m,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.57,
                0.738,
                0.607
            ],
            "angle": 0,
            "content": "其中 \\(A_{i}\\) 为 \\(n \\times n\\) 对称矩阵。当部分 \\(A_{i}\\) 为对称不定矩阵时，问题 (4.5.4) 是 NP 难的非凸优化问题。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.612,
                0.738,
                0.649
            ],
            "angle": 0,
            "content": "现在我们写出问题(4.5.4)的半定规划松弛问题．对任意 \\(x\\in \\mathbb{R}^n\\) 以及\\(A\\in S^n\\) ，有恒等式"
        },
        {
            "type": "equation",
            "bbox": [
                0.291,
                0.661,
                0.617,
                0.681
            ],
            "angle": 0,
            "content": "\\[\nx ^ {\\mathrm {T}} A x = \\operatorname {T r} \\left(x ^ {\\mathrm {T}} A x\\right) = \\operatorname {T r} \\left(A x x ^ {\\mathrm {T}}\\right) = \\left\\langle A, x x ^ {\\mathrm {T}} \\right\\rangle ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.693,
                0.664,
                0.71
            ],
            "angle": 0,
            "content": "因此问题(4.5.4)中所有的二次项均可用下面的方式进行等价刻画："
        },
        {
            "type": "equation",
            "bbox": [
                0.292,
                0.722,
                0.615,
                0.742
            ],
            "angle": 0,
            "content": "\\[\nx ^ {\\mathrm {T}} A _ {i} x + 2 b _ {i} ^ {\\mathrm {T}} x + c _ {i} = \\left\\langle A _ {i}, x x ^ {\\mathrm {T}} \\right\\rangle + 2 b _ {i} ^ {\\mathrm {T}} x + c _ {i}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.755,
                0.349,
                0.771
            ],
            "angle": 0,
            "content": "所以，原始问题等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.276,
                0.781,
                0.479,
                0.806
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\langle A _ {0}, X \\rangle + 2 b _ {0} ^ {\\mathrm {T}} x + c _ {0}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.809,
                0.737,
                0.83
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & \\langle A _ {i}, X \\rangle + 2 b _ {i} ^ {\\mathrm {T}} x + c _ {i} \\leqslant 0, \\quad i = 1, 2, \\dots , m, \\end{array} \\tag {4.5.5}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.326,
                0.836,
                0.396,
                0.852
            ],
            "angle": 0,
            "content": "\\[\nX = x x ^ {\\mathrm {T}}.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "138"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.338,
                0.174
            ],
            "angle": 0,
            "content": "进一步地，"
        },
        {
            "type": "equation",
            "bbox": [
                0.367,
                0.179,
                0.718,
                0.248
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} x ^ {\\mathrm {T}} A _ {i} x + 2 b _ {i} ^ {\\mathrm {T}} x + c _ {i} = \\left\\langle \\left( \\begin{array}{c c} A _ {i} & b _ {i} \\\\ b _ {i} ^ {\\mathrm {T}} & c _ {i} \\end{array} \\right), \\left( \\begin{array}{c c} X & x \\\\ x ^ {\\mathrm {T}} & 1 \\end{array} \\right) \\right\\rangle , \\\\ \\stackrel {\\text {d e f}} {=} \\left\\langle \\overline {{A _ {i}}}, \\overline {{X}} \\right\\rangle , \\quad i = 0, 1, \\dots , m. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.255,
                0.827,
                0.333
            ],
            "angle": 0,
            "content": "接下来将等价问题 (4.5.5) 松弛为半定规划问题。在问题 (4.5.5) 中，唯一的非线性部分是约束 \\(X = xx^{\\mathrm{T}}\\) ，我们将其松弛成半正定约束 \\(X \\succeq xx^{\\mathrm{T}}\\) 。可以证明（见习题 4.8），\\(\\overline{X} \\succeq 0\\) 与 \\(X \\succeq xx^{\\mathrm{T}}\\) 是等价的。因此这个问题的半定规划松弛可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.411,
                0.342,
                0.675,
                0.432
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\quad \\langle \\overline {{A _ {0}}}, \\overline {{X}} \\rangle \\\\ \\begin{array}{l} \\text {s . t .} \\quad \\left\\langle \\bar {A} _ {i}, \\bar {X} \\right\\rangle \\leqslant 0, \\quad i = 1, 2, \\dots , m, \\end{array} \\\\ \\overline {{X}} \\succeq 0, \\\\ \\bar {X} _ {n + 1, n + 1} = 1. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.441,
                0.679,
                0.459
            ],
            "angle": 0,
            "content": "其中“松弛”来源于我们将 \\(X = xx^{\\mathrm{T}}\\) 替换成了 \\(X\\succeq xx^{\\mathrm{T}}\\)"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.482,
                0.5,
                0.499
            ],
            "angle": 0,
            "content": "2. 最大割问题的半定规划松弛"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.512,
                0.825,
                0.611
            ],
            "angle": 0,
            "content": "令 \\(G\\) 为一个无向图，其节点集合为 \\(V = \\{1,2,\\dots ,n\\}\\) 和边的集合为 \\(E\\) 令 \\(w_{ij} = w_{ji}\\) 为边 \\((i,j)\\in E\\) 上的权重，并假设 \\(w_{ij}\\geqslant 0,(i,j)\\in E\\) 最大割问题是找到节点集合 \\(V\\) 的一个子集 \\(S\\) 使得 \\(S\\) 与它的补集 \\(\\overline{S}\\) 之间相连边的权重之和最大化.我们可以将最大割问题写成如下整数规划的形式：令 \\(x_{j} = 1,j\\in S\\) 和 \\(x_{j} = -1,j\\in \\overline{S}\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.416,
                0.619,
                0.823,
                0.68
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\max  \\frac {1}{2} \\sum_ {i <   j} \\left(1 - x _ {i} x _ {j}\\right) w _ {i j} \\tag {4.5.6} \\\\ \\begin{array}{l l} \\text {s . t .} & x _ {j} \\in \\{- 1, 1 \\}, j = 1, 2, \\dots , n. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.687,
                0.825,
                0.745
            ],
            "angle": 0,
            "content": "在问题 (4.5.6) 中, 只有当 \\(x_{i}\\) 与 \\(x_{j}\\) 不同时, 目标函数中 \\(w_{ij}\\) 的系数非零. 最大割问题是一个离散优化问题, 很难在多项式时间内找到它的最优解. 接下来介绍如何将问题 (4.5.6) 松弛成一个半定规划问题."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.746,
                0.825,
                0.803
            ],
            "angle": 0,
            "content": "令 \\(W = (w_{ij})\\in \\mathcal{S}^n\\) ，并定义 \\(C = -\\frac{1}{4} (\\mathrm{Diag}(W\\mathbf{1}) - W)\\) 为图 \\(G\\) 的拉普拉斯矩阵的 \\(-\\frac{1}{4}\\) 倍，则问题(4.5.6)可以等价地写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.439,
                0.812,
                0.645,
                0.853
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  x ^ {T} C x, \\\\ \\begin{array}{l l} \\text {s . t .} & x _ {i} ^ {2} = 1, i = 1, 2, \\dots , n. \\end{array} \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.282,
                0.133
            ],
            "angle": 0,
            "content": "4.5 半定规划"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "139"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.737,
                0.215
            ],
            "angle": 0,
            "content": "由于目标函数是关于 \\(x\\) 的二次函数, 利用前面的技巧可将其等价替换为 \\(\\langle C, x x^{\\mathrm{T}} \\rangle\\). 接下来令 \\(X = x x^{\\mathrm{T}}\\), 注意到约束 \\(x_{i}^{2} = 1\\), 这意味着矩阵 \\(X\\) 对角线元素 \\(X_{ii} = 1\\). 因此利用矩阵形式我们将最大割问题转化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.23,
                0.455,
                0.248
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\min  & \\langle C, X \\rangle , \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.255,
                0.737,
                0.271
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad X _ {i i} = 1, i = 1, 2, \\dots , n, \\tag {4.5.7}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.279,
                0.55,
                0.296
            ],
            "angle": 0,
            "content": "\\[\nX \\succeq 0, \\operatorname {r a n k} (X) = 1.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.311,
                0.737,
                0.37
            ],
            "angle": 0,
            "content": "问题 (4.5.7) 和 (4.5.6) 是等价的, 这是因为 \\(X = x x^{\\mathrm{T}}\\) 可以用约束 \\(X \\succeq 0\\) 和 \\(\\operatorname{rank}(X) = 1\\) 等价刻画. 若在问题 (4.5.7) 中将秩一约束去掉, 我们就可以得到最大割问题的半定规划松弛形式"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.385,
                0.449,
                0.402
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\min  & \\langle C, X \\rangle \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.355,
                0.409,
                0.737,
                0.426
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad X _ {i i} = 1, i = 1, 2, \\dots , n, \\tag {4.5.8}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.434,
                0.448,
                0.451
            ],
            "angle": 0,
            "content": "\\[\nX \\succeq 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.466,
                0.737,
                0.503
            ],
            "angle": 0,
            "content": "问题 (4.5.8) 是原最大割问题的一个松弛，因此它们并不等价。文献 [82]指出求解问题 (4.5.8) 可以给出原最大割问题的一个 0.8786-近似解。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.53,
                0.343,
                0.546
            ],
            "angle": 0,
            "content": "3. 极小化最大特征值"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.561,
                0.737,
                0.599
            ],
            "angle": 0,
            "content": "设 \\(M(z)\\) 为一个以 \\(z\\) 为参数的对称矩阵，极小化最大特征值问题的最终目标是选取一个 \\(z\\) 使得 \\(M(z)\\) 的最大特征值最小，即我们要求解问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.616,
                0.737,
                0.639
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {z \\in \\mathbb {R} ^ {m}} \\lambda_ {\\max } (M (z)) \\tag {4.5.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.65,
                0.683,
                0.667
            ],
            "angle": 0,
            "content": "利用上方图的等价转换技巧容易将问题(4.5.9)转化为上方图的形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.367,
                0.684,
                0.737,
                0.707
            ],
            "angle": 0,
            "content": "\\[\n\\min  t, \\tag {4.5.10}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.703,
                0.539,
                0.72
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & t \\geqslant \\lambda_ {\\max } (M (z)). \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.734,
                0.736,
                0.771
            ],
            "angle": 0,
            "content": "注意, 问题 (4.5.10) 中 \\(t\\) 和 \\(z\\) 都是自变量。在实际应用中, \\(M(z)\\) 线性依赖于 \\(z\\) 的情形比较常见, 即"
        },
        {
            "type": "equation",
            "bbox": [
                0.372,
                0.771,
                0.537,
                0.808
            ],
            "angle": 0,
            "content": "\\[\nM (z) = A _ {0} + \\sum_ {i = 1} ^ {m} z _ {i} A _ {i},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(A_{0}, A_{1}, \\dots, A_{m}\\) 为对称矩阵。我们指出，当 \\(M(z)\\) 为 \\(z\\) 的上述线性映射时，问题 (4.5.10) 实际上等价于一个半定规划问题。实际上，读者可证明（见"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "140"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.825,
                0.193
            ],
            "angle": 0,
            "content": "习题 (4.8) \\(\\lambda_{\\max}(M(z)) \\leqslant \\eta\\) 当且仅当 \\(\\eta I - M(z) \\succeq 0\\). 因此我们得到半定规划问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.194,
                0.688,
                0.228
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad \\eta , \\quad \\text {s . t .} \\quad \\eta I - A _ {0} - \\sum_ {i = 1} ^ {m} z _ {i} A _ {i} \\succeq 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.233,
                0.707,
                0.249
            ],
            "angle": 0,
            "content": "其中变量为 \\(\\eta, z\\). 读者很容易该形式转化为标准形式 (4.5.2)."
        },
        {
            "type": "title",
            "bbox": [
                0.462,
                0.279,
                0.621,
                0.3
            ],
            "angle": 0,
            "content": "4.6 矩阵优化"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.32,
                0.498,
                0.338
            ],
            "angle": 0,
            "content": "4.6.1 基本形式和应用背景"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.351,
                0.51,
                0.367
            ],
            "angle": 0,
            "content": "矩阵优化问题具有如下形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.491,
                0.38,
                0.591,
                0.402
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in \\mathcal {X}} \\psi (X),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.408,
                0.825,
                0.485
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{X}\\) 为特定的矩阵空间，\\(\\psi(X): \\mathcal{X} \\to \\mathbb{R}\\) 为给定的函数，可能是非光滑的。对于矩阵优化问题，如果决策变量为一个 \\(n \\times n\\) 矩阵，那么我们可能需要确定 \\(n^2\\) 个元素。因此，决策变量的维数过大往往是矩阵优化问题难以快速求解的一个重要原因。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.491,
                0.825,
                0.59
            ],
            "angle": 0,
            "content": "矩阵优化是在近几十年发展起来的一类变量含有矩阵的优化问题。它广泛地出现在组合数学、材料科学、机器学习和统计学等各种各样的应用当中。和向量相比，矩阵有许多新的性质：例如秩、特征值等。所以矩阵优化问题的求解通常要困难一些。这里列出前面遇到的矩阵优化问题。由于矩阵变量函数的导数计算的复杂性，对于某些问题我们也给出其涉及的导数。"
        },
        {
            "type": "text",
            "bbox": [
                0.28,
                0.6,
                0.825,
                0.658
            ],
            "angle": 0,
            "content": "- 半定规划问题(4.5.2): 半定规划是一类特殊的矩阵优化问题, 它的目标函数和约束均为线性函数, 自变量 \\(X\\) 取值于半正定矩阵空间中. 在第4.5节中已经讨论了半定规划的具体形式和应用."
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.671,
                0.494,
                0.687
            ],
            "angle": 0,
            "content": "- 低秩矩阵恢复问题(1.3.2)："
        },
        {
            "type": "equation",
            "bbox": [
                0.418,
                0.693,
                0.705,
                0.731
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in \\mathbb {R} ^ {m \\times n}} \\quad \\mu \\| X \\| _ {*} + \\frac {1}{2} \\sum_ {(i, j) \\in \\Omega} (X _ {i j} - M _ {i j}) ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.738,
                0.579,
                0.755
            ],
            "angle": 0,
            "content": "考虑函数 \\(h(X) = \\| X\\|_{*}\\) ，其次微分为"
        },
        {
            "type": "equation",
            "bbox": [
                0.357,
                0.765,
                0.765,
                0.784
            ],
            "angle": 0,
            "content": "\\[\n\\partial h (X) = \\left\\{U V ^ {\\mathrm {T}} + W \\mid \\| W \\| _ {2} \\leqslant 1, U ^ {\\mathrm {T}} W = 0, W V = 0 \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.795,
                0.694,
                0.811
            ],
            "angle": 0,
            "content": "其中 \\(X = U\\Sigma V^{\\mathrm{T}}\\) 为 \\(X\\) 的约化奇异值分解. 对于函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.454,
                0.817,
                0.668,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nf (X) = \\frac {1}{2} \\sum_ {(i, j) \\in \\Omega} \\left(X _ {i j} - M _ {i j}\\right) ^ {2},\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "4.6 矩阵优化"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "141"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.157,
                0.365,
                0.174
            ],
            "angle": 0,
            "content": "令矩阵 \\(P \\in \\mathbb{R}^{m \\times n}\\) 为"
        },
        {
            "type": "equation",
            "bbox": [
                0.392,
                0.186,
                0.556,
                0.238
            ],
            "angle": 0,
            "content": "\\[\nP _ {i j} = \\left\\{ \\begin{array}{l l} 1, & (i, j) \\in \\Omega , \\\\ 0, & \\text {其 他}, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.25,
                0.587,
                0.281
            ],
            "angle": 0,
            "content": "那么， \\(f(X) = \\frac{1}{2}\\| P\\odot (X - M)\\| _F^2\\) 易知其梯度为"
        },
        {
            "type": "equation",
            "bbox": [
                0.382,
                0.292,
                0.566,
                0.31
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (X) = P \\odot (X - M).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.333,
                0.389,
                0.35
            ],
            "angle": 0,
            "content": "- 主成分分析问题(3.7.1):"
        },
        {
            "type": "equation",
            "bbox": [
                0.284,
                0.365,
                0.665,
                0.392
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in \\mathbb {R} ^ {p \\times d}} \\quad \\psi (X) = - \\operatorname {T r} (X ^ {\\mathrm {T}} A A ^ {\\mathrm {T}} X), \\quad \\text {s . t .} \\quad X ^ {\\mathrm {T}} X = I _ {d}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.423,
                0.387,
                0.439
            ],
            "angle": 0,
            "content": "通过简单计算，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.455,
                0.554,
                0.474
            ],
            "angle": 0,
            "content": "\\[\n\\nabla \\psi (X) = - 2 A A ^ {\\mathrm {T}} X.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.194,
                0.498,
                0.377,
                0.514
            ],
            "angle": 0,
            "content": "- 矩阵分离问题(3.8.2)："
        },
        {
            "type": "equation",
            "bbox": [
                0.342,
                0.529,
                0.605,
                0.576
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {X, S \\in \\mathbb {R} ^ {m \\times n}} \\psi (X, S) = \\| X \\| _ {*} + \\lambda \\| S \\| _ {1} \\\\ \\begin{array}{l l} \\text {s . t .} & X + S = M. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.593,
                0.738,
                0.61
            ],
            "angle": 0,
            "content": "在这里自变量 \\(X\\) 与 \\(S\\) 均为矩阵， \\(\\psi (X,S)\\) 关于 \\(X\\) 和 \\(S\\) 均为不可微函数"
        },
        {
            "type": "text",
            "bbox": [
                0.194,
                0.626,
                0.377,
                0.642
            ],
            "angle": 0,
            "content": "- 字典学习问题 (3.9.1):"
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.654,
                0.593,
                0.704
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {D, X} \\frac {1}{2 n} \\| D X - A \\| _ {F} ^ {2} + \\lambda \\| X \\| _ {1}, \\\\ \\begin{array}{l} \\text {s . t .} \\quad \\| D \\| _ {F} \\leqslant 1. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.719,
                0.497,
                0.75
            ],
            "angle": 0,
            "content": "令 \\(f(X, D) = \\frac{1}{2n} \\|DX - A\\|_F^2\\)，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.761,
                0.563,
                0.824
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\nabla_ {X} f = \\frac {1}{n} D ^ {\\mathrm {T}} (D X - A), \\\\ \\nabla_ {D} f = \\frac {1}{n} (D X - A) X ^ {\\mathrm {T}}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.836,
                0.7,
                0.853
            ],
            "angle": 0,
            "content": "在这里需要注意 \\(f(X, D)\\) 关于两个变量分别求梯度的形式的区别."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "142"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.402,
                0.174
            ],
            "angle": 0,
            "content": "4.6.2 应用举例"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.188,
                0.398,
                0.205
            ],
            "angle": 0,
            "content": "1. 非负矩阵分解"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.219,
                0.827,
                0.297
            ],
            "angle": 0,
            "content": "假设 \\(a\\) 为 \\(d\\) 维空间中的非负随机向量，其 \\(n\\) 个观测值为 \\(\\{a_i\\}_{i=1}^n\\)。记矩阵 \\(A = [a_1, a_2, \\dots, a_n] \\in \\mathbb{R}^{d \\times n}\\)，非负矩阵分解问题是指将 \\(A\\) 分解成非负 \\(d \\times p\\) 基矩阵 \\(X = [x_1, x_2, \\dots, x_p]\\) 和非负 \\(p \\times n\\) 系数矩阵 \\(Y = [y_1, y_2, \\dots, y_n]\\) 的乘积，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.507,
                0.305,
                0.577,
                0.319
            ],
            "angle": 0,
            "content": "\\[\nA = X Y.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.335,
                0.825,
                0.435
            ],
            "angle": 0,
            "content": "从上面的表达式可以看出，\\(y_{j}\\) 为观测点 \\(a_{j}\\) 在基矩阵 \\(X\\) 上的权重系数。也就是说，非负矩阵分解把数据分成基向量的线性组合。通常选取 \\(p \\ll d\\)，那么得到的基矩阵 \\(X\\) 的列张成了原数据空间的一个子空间。这本质上是将高维空间中的数据在一个低维空间中表示。当数据点的内蕴结构完全被基矩阵 \\(X\\) 包含时，我们就得到了一个很好的低维表示。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.439,
                0.826,
                0.497
            ],
            "angle": 0,
            "content": "一般情况下，由于观测含有噪声，原始数据矩阵 \\(A\\) 和分解 \\(XY\\) 不会完全吻合。在这种情况下我们应当寻找误差最小的解。利用矩阵的 \\(F\\) 范数可以定义相似性度量"
        },
        {
            "type": "equation",
            "bbox": [
                0.494,
                0.502,
                0.591,
                0.521
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| A - X Y \\right\\| _ {F} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.534,
                0.436,
                0.55
            ],
            "angle": 0,
            "content": "我们考虑如下优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.567,
                0.825,
                0.592
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in \\mathbb {R} ^ {d \\times p}, Y \\in \\mathbb {R} ^ {p \\times n}} \\| A - X Y \\| _ {F} ^ {2}, \\quad \\text {s . t .} \\quad X \\geqslant 0, Y \\geqslant 0, \\tag {4.6.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.606,
                0.587,
                0.623
            ],
            "angle": 0,
            "content": "其中 \\(\\geqslant 0\\) 表示矩阵的每个元素是非负的"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.627,
                0.826,
                0.748
            ],
            "angle": 0,
            "content": "从低维空间逼近的角度来看，非负矩阵分解模型和主成分分析模型类似。但在实际问题中，非负矩阵分解模型会得到比主成分分析模型更有实际意义的解。比如，给定很多幅人脸图片（都可以用元素值为 \\(0 \\sim 255\\) 的矩阵来表示其灰度图），我们想要提取脸部的特征。利用主成分分析得到的主成分可能包含负数像素值，这是不合理的。但是如果使用非负矩阵分解，则可以有效避免这类情形的发生。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.752,
                0.826,
                0.79
            ],
            "angle": 0,
            "content": "我们称问题(4.6.1)为基本的非负矩阵分解模型．根据具体应用的不同，有时还考虑带正则项的非负矩阵分解模型"
        },
        {
            "type": "equation",
            "bbox": [
                0.364,
                0.804,
                0.825,
                0.836
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in \\mathbb {R} ^ {d \\times p}, Y \\in \\mathbb {R} ^ {p \\times n}} \\| A - X Y \\| _ {F} ^ {2} + \\alpha_ {1} r _ {1} (X) + \\beta r _ {2} (Y), \\tag {4.6.2}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.435,
                0.838,
                0.591,
                0.853
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & X \\geqslant 0, \\quad Y \\geqslant 0, \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "4.6 矩阵优化"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "143"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.744,
                0.279
            ],
            "angle": 0,
            "content": "其中 \\(r_1(X)\\) 和 \\(r_2(Y)\\) 是正则项，\\(\\alpha, \\beta > 0\\) 是用来权衡拟合项和正则项的正则化参数。比如，如果 \\(Y\\) 的列是稀疏的，那么每一个观测值都可以用少数几个基向量来表示。相应地，我们可以惩罚 \\(Y\\) 的每一列的 \\(\\ell_1\\) 范数。为了保证基向量的线性无关性，往往还要求 \\(X\\) 的列之间是相互正交的。此外，如果数据矩阵 \\(A\\) 分布在一个低维的非线性流形上，则考虑流形或者图上的非负矩阵分解模型。关于更多非负矩阵分解模型的介绍，读者可以参考 [192]。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.308,
                0.379,
                0.327
            ],
            "angle": 0,
            "content": "2. 低秩相关系数矩阵估计"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.34,
                0.741,
                0.398
            ],
            "angle": 0,
            "content": "相关系数矩阵是对角元全为1的半正定矩阵，其第 \\((i,j)\\) 元素表示随机变量 \\(x_{i}\\) 和 \\(x_{j}\\) 之间的相关系数．相关系数矩阵估计问题来自于统计学、金融学等领域中."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.403,
                0.741,
                0.462
            ],
            "angle": 0,
            "content": "给定对称矩阵 \\(C \\in S^n\\) 和非负对称权重矩阵 \\(H \\in S^n\\)，低秩相关系数矩阵估计问题是从给定的矩阵 \\(C\\) 出发，求解一个秩小于等于 \\(p\\) 的相关系数矩阵 \\(X\\)，使得在结合了权重矩阵的某种度量下最小化："
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.473,
                0.737,
                0.554
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {X \\succeq 0} \\quad \\frac {1}{2} \\| H \\odot (X - C) \\| _ {F} ^ {2}, \\\\ s. t. \\quad X _ {i i} = 1, i = 1, 2, \\dots , n, \\tag {4.6.3} \\\\ \\operatorname {r a n k} (X) \\leqslant p. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.569,
                0.741,
                0.629
            ],
            "angle": 0,
            "content": "求解问题(4.6.3)的算法参见[75,170].将 \\(X\\) 上的秩约束 \\(\\mathrm{rank}(X)\\leqslant p\\) 表示为 \\(X = V^{\\mathrm{T}}V,\\) 其中 \\(V = [V_{1},V_{2},\\dots ,V_{n}]\\in \\mathbb{R}^{p\\times n}\\) ，问题(4.6.3)可以转化为多球约束的四次多项式优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.64,
                0.577,
                0.695
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {V \\in \\mathbb {R} ^ {p \\times n}} \\frac {1}{2} \\| H \\odot (V ^ {\\mathrm {T}} V - C) \\| _ {F} ^ {2}, \\\\ \\begin{array}{l l} \\text {s . t .} & \\| V _ {i} \\| _ {2} = 1, i = 1, 2, \\dots , n. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.722,
                0.31,
                0.739
            ],
            "angle": 0,
            "content": "3. 电子结构计算"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.753,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "分子、纳米级材料的性质在很大程度上是通过其原子中电子之间的相互作用来确定的。这些相互作用可以通过电子密度定量表征。令电子的个数是 \\( n_e \\)，位置是 \\( r_i \\in \\mathbb{R}^3, i = 1,2,\\dots ,n_e \\)，原子核的个数是 \\( n_u \\)，位置是 \\( \\hat{r}_j \\in \\mathbb{R}^3, j = 1,2,\\dots ,n_u \\)，令 \\( z_j \\) 是第 \\( j \\) 个原子核的电荷， \\( \\Delta_{r_i} \\) 为第 \\( i \\) 个电子对应的拉普拉斯算子， \\( \\mathcal{I} \\) 为恒等算子，则多电子哈密顿算子（Hamiltonian）可以表示"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.119,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "144"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.158,
                0.28,
                0.173
            ],
            "angle": 0,
            "content": "为"
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.179,
                0.761,
                0.221
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {H} = - \\frac {1}{2} \\sum_ {i = 1} ^ {n _ {e}} \\Delta_ {r _ {i}} - \\left(\\sum_ {j = 1} ^ {n _ {u}} \\sum_ {i = 1} ^ {n _ {e}} \\frac {z _ {j}}{\\| r _ {i} - \\hat {r} _ {j} \\|} - \\frac {1}{2} \\sum_ {1 \\leqslant i, j \\leqslant n _ {e}} \\frac {1}{\\| r _ {i} - r _ {j} \\|}\\right) \\mathcal {I}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.235,
                0.775,
                0.252
            ],
            "angle": 0,
            "content": "多原子系统的电子密度可以由多体薛定谔（Schrödinger）方程得到："
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.273,
                0.825,
                0.291
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {H} \\Psi (r _ {1}, r _ {2}, \\dots , r _ {n _ {e}}) = \\lambda \\Psi (r _ {1}, r _ {2}, \\dots , r _ {n _ {e}}), \\tag {4.6.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.311,
                0.825,
                0.348
            ],
            "angle": 0,
            "content": "其中 \\(\\Psi (r_1,r_2,\\dots ,r_{n_e})\\) 是多体波函数.对于 \\(\\Omega = \\Omega_{1}\\times \\Omega_{2}\\times \\dots \\times \\Omega_{n_{e}}\\) 和 \\(\\varOmega_{i}\\subseteq\\) \\(\\mathbb{R}^3,i\\in 1,2,\\dots ,n_e\\) ，它满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.481,
                0.364,
                0.826,
                0.394
            ],
            "angle": 0,
            "content": "\\[\n\\int_ {\\Omega} \\overline {{\\Psi}} ^ {\\mathrm {T}} \\Psi \\mathrm {d} \\Omega = 1. \\tag {4.6.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.409,
                0.825,
                0.488
            ],
            "angle": 0,
            "content": "可以看出问题 (4.6.4) 是一个特征值问题, 只能对于规模很小的系统才能直接求解. 假设 \\(r_{i}\\) 在 \\(m \\times m \\times m\\) 网格上离散, 则离散后 \\(\\mathcal{H}\\) 对应的矩阵的维数是 \\(n = m^{3 n_{e}}\\). 对于 \\(m = 32\\) 和 \\(n_{e} = 5\\) 的系统, \\(n\\) 大于 \\(3.5 \\times 10^{22}\\). 因此这是一个维数灾难问题, 无法直接处理."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.494,
                0.825,
                0.553
            ],
            "angle": 0,
            "content": "Kohn-Sham (KS) 方法利用单电子波函数来近似能量函数, 把整个系统简化成没有相互作用的电子在有效势场中运动的问题, 进而极大减小了问题的维数. 通过合适的离散化, KS 能量泛函可以表示成"
        },
        {
            "type": "equation",
            "bbox": [
                0.313,
                0.567,
                0.771,
                0.599
            ],
            "angle": 0,
            "content": "\\[\nE _ {\\mathrm {K S}} (X) \\stackrel {\\text {d e f}} {=} \\frac {1}{2} \\operatorname {T r} (\\overline {{X}} ^ {\\mathrm {T}} L X) + \\operatorname {T r} (\\overline {{X}} ^ {\\mathrm {T}} V _ {\\mathrm {i o n}} X) + \\frac {1}{2} \\rho^ {\\mathrm {T}} L ^ {\\dagger} \\rho + \\rho^ {\\mathrm {T}} \\varepsilon_ {\\mathrm {x c}} (\\rho),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.614,
                0.825,
                0.694
            ],
            "angle": 0,
            "content": "其中 \\(X \\in \\mathbb{C}^{n \\times n_e}\\) 是离散的波函数, \\(L\\) 是拉普拉斯算子的有限维表示, \\(L^{\\dagger}\\) 为其广义逆 (见附录 B.1.7), \\(\\rho = \\mathrm{diag}(X\\overline{X}^{\\mathrm{T}})\\) 为电荷密度, \\(V_{\\mathrm{ion}}\\) 是离子赝势, \\(\\varepsilon_{\\mathrm{xc}}\\) 表征交换相关能量. 由于归一化条件(4.6.5), 我们要求离散波函数正交, 即 \\(\\overline{X}^{\\mathrm{T}}X = I_{n_e}\\). 因此离散形式下的 KS 能量极小化问题可以表示成"
        },
        {
            "type": "equation",
            "bbox": [
                0.414,
                0.711,
                0.668,
                0.738
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in \\mathbb {C} ^ {n \\times n _ {e}}} E _ {\\mathrm {K S}} (X), \\quad \\text {s . t .} \\quad \\overline {{X}} ^ {\\mathrm {T}} X = I _ {n _ {e}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.753,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "这是一个正交约束优化问题。所有满足正交约束的矩阵称为Stiefel流形，所以该问题是流形约束优化的特例。KS极小化问题对应的最优性条件是一个非线性特征值问题，称为KS方程。基于KS方程，可以将问题化成一个非线性最小二乘问题。电子结构计算还有很多其他变形，如Hartree-Fock近似也是正交约束问题，而简化密度矩阵优化问题则可以是半定规划问题。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.282,
                0.133
            ],
            "angle": 0,
            "content": "4.7 整数规划"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "145"
        },
        {
            "type": "title",
            "bbox": [
                0.374,
                0.155,
                0.534,
                0.177
            ],
            "angle": 0,
            "content": "4.7 整数规划"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.195,
                0.411,
                0.214
            ],
            "angle": 0,
            "content": "4.7.1 基本形式和应用背景"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.227,
                0.737,
                0.263
            ],
            "angle": 0,
            "content": "不同于线性规划，整数规划要求优化变量必须取整数值，而不能是分数值。一般形式如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.265,
                0.43,
                0.281
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad c ^ {\\top} x\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.357,
                0.291,
                0.459,
                0.306
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A x \\leqslant b, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.315,
                0.556,
                0.332
            ],
            "angle": 0,
            "content": "\\[\nx _ {i} \\geqslant 0, i = 1, 2, \\dots , n,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.339,
                0.559,
                0.356
            ],
            "angle": 0,
            "content": "\\[\nx _ {i} \\in \\mathbb {Z}, i = 1, 2, \\dots , n,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.362,
                0.737,
                0.442
            ],
            "angle": 0,
            "content": "其中 \\(A \\in \\mathbb{R}^{m \\times n}, c \\in \\mathbb{R}^n, b \\in \\mathbb{R}^m\\) 是给定的矩阵和向量，\\(\\mathbb{Z}\\) 是整数集合。如果只要求部分 \\(x_i\\) 是整数，该问题被称为混合整数规划问题。整数规划问题广泛存在于资本预算（决策变量的取值于 \\(\\{0,1\\}\\)）、仓库位置选取、调度问题等不同的应用中。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.466,
                0.314,
                0.484
            ],
            "angle": 0,
            "content": "4.7.2 应用举例"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.497,
                0.275,
                0.513
            ],
            "angle": 0,
            "content": "1. 资本预算"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.527,
                0.739,
                0.627
            ],
            "angle": 0,
            "content": "在经典的资本预算问题中，我们需要对一些潜在的投资进行选择。选择合适的工厂位置或者对现有资本进行分配，放弃一些没有意义的项目，也就是决策变量 \\( x_{j} = 0 \\) 或者1，意味着第 \\( j \\) 个投资被拒绝或者接受。假设 \\( c_{j} \\) 为投资 \\( j \\) 项目的回报， \\( a_{ij} \\) 为将资源 \\( i \\) （例如现金或者人手）用在项目 \\( j \\) 上的数量。那么资本预算问题可以表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.633,
                0.441,
                0.671
            ],
            "angle": 0,
            "content": "\\[\n\\max  \\sum_ {j = 1} ^ {n} c _ {j} x _ {j}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.675,
                0.573,
                0.713
            ],
            "angle": 0,
            "content": "\\[\n\\text {s . t .} \\quad \\sum_ {j = 1} ^ {n} a _ {i j} x _ {j} \\leqslant b _ {i}, i = 1, \\dots , m\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.72,
                0.49,
                0.739
            ],
            "angle": 0,
            "content": "\\[\nx _ {j} = 0 \\text {或 者} 1,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.746,
                0.697,
                0.763
            ],
            "angle": 0,
            "content": "即在有限的资源 \\( b_{i} \\) 下找到一个投资项目使得回报在所有投资中最大化."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.786,
                0.31,
                0.803
            ],
            "angle": 0,
            "content": "2. 仓库位置选取"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.816,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "在分配系统的建模中，我们需要考虑仓库与消费者之间的运输成本以及仓库自身的运营成本。例如，一个管理者需要在 \\(m\\) 个可能的位置上修建"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.119,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "146"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.827,
                0.236
            ],
            "angle": 0,
            "content": "一些仓库去满足 \\( n \\) 个消费者的需求。需要做的决策是建造哪些仓库以及控制每个仓库到消费者的运输量。仓库位置选取问题是指，管理者根据消费者的需求以及仓库到消费者之间的运输成本，来确定所要修建的仓库的位置，以及如何以最低成本将物资从仓库运输到消费者。令"
        },
        {
            "type": "equation",
            "bbox": [
                0.418,
                0.243,
                0.663,
                0.294
            ],
            "angle": 0,
            "content": "\\[\ny _ {i} = \\left\\{ \\begin{array}{l l} 1, & \\text {如 果 仓 库} i \\text {被 修 建}, \\\\ 0, & \\text {如 果 仓 库} i \\text {未 被 修 建}, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.302,
                0.747,
                0.319
            ],
            "angle": 0,
            "content": "以及 \\(x_{ij}\\) 为从仓库 \\(i\\) 运输到消费者 \\(j\\) 的物资数量．相关的成本如下"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.332,
                0.675,
                0.349
            ],
            "angle": 0,
            "content": "\\[\nf _ {i} = \\text {仓 库} i \\text {的 固 定 运 营 成 本}, \\text {比 如 租 赁 费},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.364,
                0.356,
                0.718,
                0.373
            ],
            "angle": 0,
            "content": "\\[\nc _ {i j} = \\text {从 仓 库} i \\text {运 输 物 资 到 消 费 者} j \\text {的 运 输 成 本}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.383,
                0.827,
                0.476
            ],
            "angle": 0,
            "content": "该问题有三个条件：(1) 消费者的需求 \\(d_{j}\\) 必须被满足；(2) 只有已修建的仓库才能提供物资运输，如果仓库 \\(i\\) 未被修建 \\((y_{i} = 0)\\)，那么 \\(x_{ij} = 0\\)；(3) 每个仓库 \\(i\\) 的容量（所能提供的物资数量）是有限的，即 \\(\\sum_{j=1}^{n} x_{ij} \\leqslant u_{i}\\)，其中 \\(u_{i} > 0\\) 为仓库的容量。那么仓库位置选取问题可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.387,
                0.483,
                0.59,
                0.522
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, y} \\quad \\sum_ {i = 1} ^ {m} \\sum_ {j = 1} ^ {n} c _ {i j} x _ {i j} + \\sum_ {i = 1} ^ {m} f _ {i} y _ {i},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.526,
                0.629,
                0.562
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\text {s . t .} \\sum_ {i = 1} ^ {m} x _ {i j} = d _ {j}, j = 1, 2, \\dots , n, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.437,
                0.566,
                0.675,
                0.604
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {j = 1} ^ {n} x _ {i j} - y _ {i} u _ {i} \\leqslant 0, i = 1, 2, \\dots , m,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.436,
                0.611,
                0.696,
                0.629
            ],
            "angle": 0,
            "content": "\\[\nx _ {i j} \\geqslant 0, i = 1, \\dots , m; j = 1, 2, \\dots , n,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.436,
                0.636,
                0.636,
                0.653
            ],
            "angle": 0,
            "content": "\\[\ny _ {i} = 0 \\text {或 者} 1, i = 1, \\dots , m,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.663,
                0.827,
                0.763
            ],
            "angle": 0,
            "content": "其中，第一行约束表示运输的物资数量需要满足消费者的需求。第二行约束有两个作用：如果仓库 \\(i\\) 未被修建 \\((y_{i} = 0)\\)，那么所能运输的物资数量为0（对应条件(2)）；如果仓库 \\(i\\) 已被修建 \\((y_{i} = 1)\\)，则从仓库 \\(i\\) 运输的物资总量不能高于该仓库的容量上限（对应条件(3)）。在这个模型中运输量应该满足非负性，且仓库修建与否为整数变量。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.786,
                0.38,
                0.802
            ],
            "angle": 0,
            "content": "3. 旅行商问题"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "一个旅行商想要从家里开始，以最低代价走遍其他 \\((n - 1)\\) 个城市，最后返回家中。他必须走访每个城市且只走访一次。记从城市 \\(i\\) 到城市 \\(j\\) 的旅"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.282,
                0.132
            ],
            "angle": 0,
            "content": "4.7 整数规划"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "147"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.27,
                0.174
            ],
            "angle": 0,
            "content": "行成本为 \\(c_{ij}\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.307,
                0.176,
                0.597,
                0.228
            ],
            "angle": 0,
            "content": "\\[\nx _ {i j} = \\left\\{ \\begin{array}{l l} 1, & \\text {如 果 他 从 城 市} i \\text {到 城 市} j, \\\\ 0, & \\text {如 果 他 未 从 城 市} i \\text {到 城 市} j. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.238,
                0.736,
                0.273
            ],
            "angle": 0,
            "content": "并令 \\(u_{i}, i = 2,3,\\dots ,n\\) 为辅助变量，旅行商问题可以写成如下整数规划的形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.295,
                0.277,
                0.614,
                0.473
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\sum_ {i = 1} ^ {n} \\sum_ {j = 1, j \\neq i} ^ {n} c _ {i j} x _ {i j}, \\\\ \\begin{array}{l l} \\text {s . t .} & \\sum_ {i = 1, i \\neq j} ^ {n} x _ {i j} = 1, j = 1, 2, \\dots , n, \\end{array} \\\\ \\sum_ {j = 1, j \\neq i} ^ {n} x _ {i j} = 1, i = 1, 2, \\dots , n, \\\\ x _ {i j} \\in \\{0, 1 \\}, i, j = 1, 2, \\dots , n, \\\\ u _ {i} - u _ {j} + n x _ {i j} \\leqslant n - 1, 2 \\leqslant i \\neq j \\leqslant n, \\\\ u _ {i} \\in \\{1, 2, \\dots , n \\}, 2 \\leqslant i \\leqslant n, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.485,
                0.737,
                0.564
            ],
            "angle": 0,
            "content": "其中，第一个约束和第二个约束要求每个城市在路径中必须且只能出现一次，而且路径中不能出现自环（即从一个城市必须前往另一不同城市）。最后两行约束保证了此“路径”只有一个连通分支，而不是由多条不相交的路径合成的。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.569,
                0.737,
                0.648
            ],
            "angle": 0,
            "content": "这里，我们简要证明这样添加约束能够保证所走的路径是一个覆盖所有城市的闭环。假设满足前三行的约束的“路径”有两个以上连通分支，则必存在一个不经过城市1的连通分支，记其长度为 \\(k\\)。对约束中倒数第二行不等式按照该连通分支求和（注意到在此连通分支上有 \\(x_{ij} = 1\\)），则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.667,
                0.509,
                0.684
            ],
            "angle": 0,
            "content": "\\[\nn k \\leqslant (n - 1) k,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.703,
                0.737,
                0.783
            ],
            "angle": 0,
            "content": "矛盾. 另一方面, 我们也要保证每个覆盖所有城市的回路的可行性. 对任意一条可行的回路, 不妨将城市 1 作为起点和终点. 如果城市 \\(i\\) 是第 \\(t\\) 个到达的城市, 则令 \\(u_{i} = t, i = 2,3,\\dots ,n\\). 因为 \\(1 \\leqslant u_{i} \\leqslant n\\), 故有 \\(u_{i} - u_{j} \\leqslant n - 1\\). 因此, 当 \\(x_{ij} = 0\\) 时, 倒数第二个不等式是成立的. 对于 \\(x_{ij} = 1\\), 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.302,
                0.801,
                0.604,
                0.819
            ],
            "angle": 0,
            "content": "\\[\nu _ {i} - u _ {j} + n x _ {i j} = t - (t + 1) + n = n - 1,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.265,
                0.852
            ],
            "angle": 0,
            "content": "也满足约束."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "148"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "title",
            "bbox": [
                0.388,
                0.155,
                0.696,
                0.177
            ],
            "angle": 0,
            "content": "4.8 典型优化算法软件介绍"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.194,
                0.825,
                0.232
            ],
            "angle": 0,
            "content": "前面介绍了各种各样的优化问题。对于每一类优化问题，我们都有相应的求解算法以及一些流行的算法软件包。"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.246,
                0.825,
                0.346
            ],
            "angle": 0,
            "content": "- SDPT3: 这个开源软件包的基本代码是用 MATLAB 来写的, 但是关键的子程序是用 FORTRAN 和 C 语言通过 MEX 文件来完成的. 它可以求解锥规划问题, 其中锥可以是半定矩阵锥、二次锥和非负象限中的一个或者多个的乘积. 这个软件主要实现的算法是一种原始 - 对偶内点法. 更多内容可以参考 [185]."
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.36,
                0.826,
                0.501
            ],
            "angle": 0,
            "content": "- MOSEK: 这个商业软件包可以求解线性规划、二次锥规划、半定规划、二次规划等凸优化问题, 以及混合整数线性规划和混合整数二次规划等. 它的重点是求解大规模稀疏问题, 尤其在求解线性规划、二次锥规划和半定规划的内点法设计上做得非常有效. 除了内点法之外, MOSEK还实现了线性规划问题的单纯形算法, 特殊网络结构问题的网络单纯形算法以及针对混合整数规划问题的算法. 它提供 C, C#, Java, Python, MATLAB 和 R 等接口. 更多内容可以参考 [138]."
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.515,
                0.825,
                0.616
            ],
            "angle": 0,
            "content": "- CPLEX: 这个商业软件可以求解整数规划问题, 非常大规模的线性规划问题 (使用单纯形方法或者内点法), 凸和非凸二次规划问题, 二次锥规划问题. 它提供 \\(\\mathrm{C} + + , \\mathrm{C}\\# , \\mathrm{Java}, \\mathrm{Microsoft Excel}\\) 和 MATLAB 接口, 并且提供一个独立的交互式优化器可执行文件, 用于调试和其他目的. 更多内容可以参考 [49]."
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.629,
                0.825,
                0.73
            ],
            "angle": 0,
            "content": "- Gurobi: 这个商业软件可以求解线性规划（采用单纯形法和并行的内点法），二次规划（采用单纯形法和内点法），二次约束规划，混合整数线性规划，混合整数二次规划，混合整数二次约束规划。它提供 C, C++, Java, .NET, Python, MATLAB 和 R 等接口。更多内容可以参考 [93]."
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.743,
                0.825,
                0.822
            ],
            "angle": 0,
            "content": "- IPOPT: 这个开源软件可以求解大规模非线性规划问题, 主要实现了原始 - 对偶内点法, 并使用滤波 (filter) 方法代替线搜索. IPOPT 主要使用 C++ 语言编写, 并提供 C, C++, FORTRAN, Java, MATLAB 和 R 等接口. 更多内容可以参考 [188]."
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.836,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "- Knitro: 它是用来求解大规模非线性优化问题的商业软件。这个软件提"
        },
        {
            "type": "list",
            "bbox": [
                0.281,
                0.246,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "4.9 优化模型语言"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "149"
        },
        {
            "type": "text",
            "bbox": [
                0.209,
                0.157,
                0.744,
                0.34
            ],
            "angle": 0,
            "content": "供了四种不同的优化方法，两种内点型方法和两种积极集（active set）方法，可以用来求解一般非凸非线性规划问题，非线性方程组，线性规划，二次（线性）约束二次规划问题，线性（非线性）最小二乘问题，混合整数规划问题以及无导数优化问题，等等．Knitro支持的编程语言有C, FORTRAN, C++, C#, Java, MATLAB, R, Python等，以及模型语言AMPL，AIMMS，GAMS和MPL等．因其具有大量的用户友善的选项以及自动调试器，全局优化的并行多重启动策略，导数逼近和检查以及内部预分解器，在实际中被广泛采用．更多内容可以参考[103]."
        },
        {
            "type": "title",
            "bbox": [
                0.349,
                0.371,
                0.558,
                0.394
            ],
            "angle": 0,
            "content": "4.9 优化模型语言"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.41,
                0.744,
                0.552
            ],
            "angle": 0,
            "content": "模型语言的发展开始于19世纪70年代后期，其主要动因是计算机的出现。在优化模型语言中，优化模型可以写成和数学表达式很类似的方式，以此给用户带来更便捷的服务。模型的表达式形式是与求解器无关的，不同的求解器需要用优化模型语言将给定的模型和数据转为其求解的标准形式，然后再对其进行求解。这类工具有三个优点：一是将容易出错的转化步骤交给计算机完成，降低错误率；二是在模型和算法之间建立了一个清晰的界限；三是对于困难的问题，可以尝试不用的求解器，得到更好的结果。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.579,
                0.28,
                0.596
            ],
            "angle": 0,
            "content": "4.9.1 CVX"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.611,
                0.741,
                0.669
            ],
            "angle": 0,
            "content": "CVX 是一个以 MATLAB 为基础的优化模型语言，用来求解凸优化问题。它允许将优化问题的目标函数以及约束用 MATLAB 语法来写。比如考虑如下优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.386,
                0.671,
                0.522,
                0.689
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad \\| A x - b \\| _ {2},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.696,
                0.497,
                0.712
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & C x = d, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.436,
                0.72,
                0.512,
                0.737
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| x \\right\\| _ {\\infty} \\leqslant e,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.746,
                0.264,
                0.763
            ],
            "angle": 0,
            "content": "它可以写成"
        },
        {
            "type": "code",
            "bbox": [
                0.159,
                0.77,
                0.596,
                0.853
            ],
            "angle": 0,
            "content": "1 m = 20; n = 10; p = 4;  \n2 A = randn(m, n); b = randn(m, 1);  \n3 C = randn(p, n); d = randn(p, 1); e = rand;  \n4 cvx_begin"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "150"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "code",
            "bbox": [
                0.244,
                0.157,
                0.634,
                0.281
            ],
            "angle": 0,
            "content": "variable x(n)  \nminimize(norm(A * x - b, 2))  \nsubject to  \nC * x == d  \nnorm(x, Inf) <= e  \ncvx_end"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.298,
                0.825,
                0.46
            ],
            "angle": 0,
            "content": "代码中的前三行是关于 \\(A, b, C, d, e\\) 的构造。在调用 CVX 求解的时候，对应的代码需要以 cvx_begin 开始，并且以 cvx_end 结尾。在这两行语句之间，我们需要定义要求解的优化问题。在上面的例子中，variable x(n) 表示决策变量 \\(x\\) 为 \\(n\\) 维空间中的向量。目标函数 \\(\\|Ax - b\\|_2\\) 则用 norm(A * x - b, 2) 来表示，minimize 表示求解目标函数的极小值。最后以 subject to 开始描述问题的约束，C * x == d 和 norm(x, Inf) <= e 分别表示约束 \\(Cx = d\\) 和 \\(\\|x\\|_{\\infty} \\leqslant e\\)。执行上述代码，CVX 会选取默认的凸优化问题算法来返回上面问题的解。"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.464,
                0.825,
                0.584
            ],
            "angle": 0,
            "content": "CVX 采用了一种快速构造和识别凸性的准则，服从这个准则的凸问题都可以很快地被识别出来。之后 CVX 根据用户的选择调用已有软件包来求解变形后的凸优化问题，这些软件包括免费软件 SDPT3 和 SeDuMi 以及商业软件 Gurobi 和 MOSEK 等。除了一些典型问题外，CVX 还可以识别一些更复杂的凸优化问题，例如带 \\(\\ell_{1}\\) 范数的优化问题。更多内容可以参考 [88]。目前 CVX 还有 Julia 语言版本 [186] 和 Python 语言版本 CVXPY [60]。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.613,
                0.38,
                0.629
            ],
            "angle": 0,
            "content": "4.9.2 AMPL"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.644,
                0.825,
                0.764
            ],
            "angle": 0,
            "content": "AMPL (a mathematical programming language) 是用来描述高复杂度的大规模优化问题的模型语言. 几十个求解器支持 AMPL, 包含开源软件和商业软件, 例如 CBC, CPLEX, FortMP, Gurobi, MINOS, IPOPT, SNOPT, Knitro 和 LGO 等, 因此可以支持一大类问题的求解. 它的一个优点是其语法与数学表达式非常类似, 因此可对优化问题进行非常简洁并且可读的定义. 考虑优化问题:"
        },
        {
            "type": "equation",
            "bbox": [
                0.461,
                0.768,
                0.825,
                0.793
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {2}} f _ {1} ^ {2} (x) + f _ {2} ^ {2} (x), \\tag {4.9.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.801,
                0.824,
                0.84
            ],
            "angle": 0,
            "content": "其中 \\(f_{1}(x) = 10(x_{2} - x_{1}^{2}), f_{2}(x) = 1 - x_{1}\\). 描述该问题只需要如下代码, 然后其求解过程可以由支持 AMPL 和非线性无约束优化的求解器自动完成."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.256,
                0.133
            ],
            "angle": 0,
            "content": "4.10 总结"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.735,
                0.131
            ],
            "angle": 0,
            "content": "151"
        },
        {
            "type": "code",
            "bbox": [
                0.155,
                0.157,
                0.462,
                0.384
            ],
            "angle": 0,
            "content": "var x{1..2};  \nvar f1 = 10*(x[2] - x[1]^2);  \nvar f2 = 1 - x[1];  \nminimize norm:  \nf1^2 + f2^2;  \ndata;  \nvar x:=  \n1 -1.2  \n2 1;"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.398,
                0.737,
                0.456
            ],
            "angle": 0,
            "content": "这里，var x{1..2} 表示决策变量是 \\(\\mathbb{R}^2\\) 中的向量，第 2，3 行用来定义函数 \\(f_1\\) 和 \\(f_2\\)，第 5，6 行定义了优化问题 (4.9.1)，以及第 8—11 行表示选取的初始点为 \\((-1.2, 1)^{\\mathrm{T}}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.461,
                0.737,
                0.499
            ],
            "angle": 0,
            "content": "关于AMPL的更多内容，读者可以参考[71].除了AMPL，在实际中常用的模型语言还有YALMIP[127]等."
        },
        {
            "type": "title",
            "bbox": [
                0.392,
                0.529,
                0.515,
                0.551
            ],
            "angle": 0,
            "content": "4.10 总结"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.568,
                0.54,
                0.584
            ],
            "angle": 0,
            "content": "本章介绍了常见的最优化问题以及具体实例"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.588,
                0.737,
                0.646
            ],
            "angle": 0,
            "content": "线性规划作为动态规划问题的一种转化形式，其对于动态规划问题的理论与算法设计有着重要的参考意义。线性规划在管理、最优运输等领域中有着各式各样的应用，感兴趣的读者可以参考[178]。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.65,
                0.737,
                0.771
            ],
            "angle": 0,
            "content": "由于压缩感知的巨大成功，复合优化的研究得到了人们大量的关注。由于问题的解的稀疏性或者低秩性，通过有效地添加正则项，可以从理论上保证复合优化问题的解满足我们想要的性质。本章介绍的复合优化问题的正则项都是凸的，但是在实际中往往一些非凸正则项对应的模型能够更好地逼近原问题，相关内容可以参考[195]。随着大量复合优化问题的提出，复合优化问题的算法也得到了大量的研究，我们会在后续章节中详细介绍它们。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.737,
                0.853
            ],
            "angle": 0,
            "content": "矩阵优化是向量优化的推广，其中一个典型问题形式是半定规划。因为半定规划的良好的理论性质，并且得益于内点法的有效求解，人们倾向于建立半定规划模型或利用半定规划来松弛已有的非凸优化模型。但是当半定规划对应的半定矩阵维数很高时，现有的算法也很难求解。最新的一些研究则"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "152"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.236
            ],
            "angle": 0,
            "content": "是考虑用分解模型来逼近半定规划问题，从而有效地求解。对于聚类问题中的优化问题，其决策变量可以表达为置换矩阵，因为相应约束的复杂性，人们经常通过松弛来进行逼近求解。考虑到半定规划松弛后仍难以求解，人们提出了一些更有效的松弛方式，相关内容可以参考 [211]。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.241,
                0.826,
                0.34
            ],
            "angle": 0,
            "content": "随机优化的复兴得益于数据时代的到来。为了使得模型具有更好的泛化能力，建立的随机模型不仅要在已有的数据上表现良好，而且也要在未被采集到的数据上性能优异。那么关于数据的分布的估计，以及如何利用已有的数据衍生出更多的数据来增加模型的鲁棒性是人们在建立随机模型的时候考虑的重点，一个比较著名的工作可参考GAN [87]。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.346,
                0.845,
                0.446
            ],
            "angle": 0,
            "content": "在实际问题中还有一类重要的优化问题的变量落在某种微分流形上，称为流形优化，读者可以参考[2]．对于优化模型语言，常用的还有YALMIP[127].对于半定规划问题，最新的一些软件包有SDPNAL[214],SDPNAL+[204]和SSNSDP[121].对于流形优化，软件包有OptM[196]，Manopt[29]和ARNT[107]."
        },
        {
            "type": "title",
            "bbox": [
                0.507,
                0.485,
                0.58,
                0.507
            ],
            "angle": 0,
            "content": "习题4"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.527,
                0.723,
                0.544
            ],
            "angle": 0,
            "content": "4.1 将下面的问题转化为线性规划：给定 \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(b \\in \\mathbb{R}^n\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.563,
                0.604,
                0.586
            ],
            "angle": 0,
            "content": "(a) \\(\\min_{x\\in \\mathbb{R}^n}\\| Ax - b\\| _1,\\quad \\mathrm{s.t.}\\quad \\| x\\|_{\\infty}\\leqslant 1;\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.307,
                0.592,
                0.604,
                0.615
            ],
            "angle": 0,
            "content": "(b) \\(\\min_{x\\in \\mathbb{R}^n}\\| x\\| _1,\\quad \\mathrm{s.t.}\\quad \\| Ax - b\\|_\\infty \\leqslant 1;\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.308,
                0.621,
                0.532,
                0.644
            ],
            "angle": 0,
            "content": "(c) \\(\\min_{x\\in \\mathbb{R}^n}\\| Ax - b\\| _1 + \\| x\\|_\\infty ;\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.652,
                0.539,
                0.688
            ],
            "angle": 0,
            "content": "(d) \\(\\min_{x\\in \\mathbb{R}^n}\\sum_{i = 1}^{m}\\max \\{0,a_i^{\\mathrm{T}}x + b_i\\} .\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.306,
                0.563,
                0.604,
                0.688
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.703,
                0.638,
                0.72
            ],
            "angle": 0,
            "content": "4.2 求解下面的线性规划问题：给定向量 \\(c \\in \\mathbb{R}^n\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.738,
                0.551,
                0.763
            ],
            "angle": 0,
            "content": "(a) \\(\\min_{x\\in \\mathbb{R}^n}c^{\\mathrm{T}}x,\\quad \\mathrm{s.t.}\\quad 0\\leqslant x\\leqslant 1;\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.307,
                0.769,
                0.572,
                0.793
            ],
            "angle": 0,
            "content": "(b) \\(\\min_{x\\in \\mathbb{R}^n}c^{\\mathrm{T}}x,\\quad \\mathrm{s.t.}\\quad -1\\leqslant x\\leqslant 1;\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.308,
                0.8,
                0.589,
                0.825
            ],
            "angle": 0,
            "content": "(c) \\(\\min_{x\\in \\mathbb{R}^n}c^{\\mathrm{T}}x,\\quad \\mathrm{s.t.}\\quad -1\\leqslant 1^{\\mathrm{T}}x\\leqslant 1;\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.831,
                0.601,
                0.856
            ],
            "angle": 0,
            "content": "(d) \\(\\min_{x\\in \\mathbb{R}^n}c^{\\mathrm{T}}x,\\quad \\mathrm{s.t.}\\quad \\mathbf{1}^{\\mathrm{T}}x = 1,\\quad x\\geqslant 0;\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.306,
                0.738,
                0.601,
                0.856
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.223,
                0.132
            ],
            "angle": 0,
            "content": "习题4"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "153"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.157,
                0.737,
                0.193
            ],
            "angle": 0,
            "content": "4.3 在数据插值中，考虑一个简单的复合模型（取 \\(\\phi\\) 为恒等映射，两层复合模型）："
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.194,
                0.603,
                0.23
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X _ {1} \\in \\mathbb {R} ^ {q \\times p}, X _ {2} \\in \\mathbb {R} ^ {q \\times q}} \\quad \\sum_ {i = 1} ^ {m} \\| X _ {2} X _ {1} a _ {i} - b _ {i} \\| _ {2} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.235,
                0.481,
                0.252
            ],
            "angle": 0,
            "content": "其中 \\(a_{i}\\in \\mathbb{R}^{p},b_{i}\\in \\mathbb{R}^{q},i = 1,2,\\dots ,m.\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.266,
                0.518,
                0.282
            ],
            "angle": 0,
            "content": "(a) 试计算目标函数关于 \\(X_{1}, X_{2}\\) 的导数；"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.291,
                0.429,
                0.307
            ],
            "angle": 0,
            "content": "(b) 试给出该问题的最优解."
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.266,
                0.518,
                0.307
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.321,
                0.737,
                0.358
            ],
            "angle": 0,
            "content": "4.4 给定数据点 \\(a_{i} \\in \\mathbb{R}^{n}, b_{i} \\in \\mathbb{R}^{n}, i = 1,2,\\dots,m\\)，我们用二次函数拟合，即求 \\(X \\in S^{n}, y \\in \\mathbb{R}^{n}, z \\in \\mathbb{R}\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.299,
                0.372,
                0.65,
                0.391
            ],
            "angle": 0,
            "content": "\\[\nb _ {i} \\approx f (a _ {i}) = a _ {i} ^ {\\mathrm {T}} X a _ {i} + y ^ {\\mathrm {T}} a _ {i} + z, \\quad i = 1, 2, \\dots , m.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.405,
                0.737,
                0.422
            ],
            "angle": 0,
            "content": "这里假设数据点 \\(a_{i}\\in \\mathcal{B} = \\{a\\in \\mathbb{R}^{n}\\mid l\\leqslant a\\leqslant u\\}\\) .相应的最小二乘模型为"
        },
        {
            "type": "equation",
            "bbox": [
                0.412,
                0.433,
                0.535,
                0.469
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i = 1} ^ {m} \\left(f \\left(a _ {i}\\right) - b _ {i}\\right) ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.209,
                0.479,
                0.737,
                0.538
            ],
            "angle": 0,
            "content": "此外，对函数 \\(f\\) 有三个额外要求：(1) \\(f\\) 是凹函数；(2) \\(f\\) 在集合 \\(\\mathcal{B}\\) 上是非负的，即 \\(f(a) \\geqslant 0, \\forall a \\in \\mathcal{B}\\)；(3) \\(f\\) 在 \\(\\mathcal{B}\\) 上是单调非减的，即对任意的 \\(a, \\hat{a} \\in \\mathcal{B}\\) 且满足 \\(a \\leqslant \\hat{a}\\)，有 \\(f(a) \\leqslant f(\\hat{a})\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.546,
                0.632,
                0.563
            ],
            "angle": 0,
            "content": "请将上述问题表示成一个凸优化问题，并尽可能地简化"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.576,
                0.414,
                0.593
            ],
            "angle": 0,
            "content": "4.5 考虑下面的复合优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.376,
                0.607,
                0.572,
                0.631
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| x \\| _ {1} + \\| D x - a \\| _ {2} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.209,
                0.64,
                0.737,
                0.677
            ],
            "angle": 0,
            "content": "其中 \\(a \\in \\mathbb{R}^n, D = \\mathrm{Diag}(d_1, d_2, \\dots, d_n)\\) 均已知. 试给出最优解 \\(x^*\\) 的表达式."
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.691,
                0.414,
                0.708
            ],
            "angle": 0,
            "content": "4.6 考虑下面的复合优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.376,
                0.722,
                0.572,
                0.747
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| x \\| _ {0} + \\| D x - a \\| _ {2} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.209,
                0.755,
                0.737,
                0.792
            ],
            "angle": 0,
            "content": "其中 \\(a \\in \\mathbb{R}^n, D = \\mathrm{Diag}(d_1, d_2, \\dots, d_n)\\) 均已知. 试给出最优解 \\(x^*\\) 的表达式."
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.806,
                0.667,
                0.823
            ],
            "angle": 0,
            "content": "4.7 将不等式形式的半定规划问题 (4.5.1) 转化成标准形式 (4.5.2)."
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.836,
                0.324,
                0.853
            ],
            "angle": 0,
            "content": "4.8 证明如下结论"
        },
        {
            "type": "list",
            "bbox": [
                0.18,
                0.806,
                0.667,
                0.853
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "154"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.155,
                0.826,
                0.214
            ],
            "angle": 0,
            "content": "(a) 设 \\(x \\in \\mathbb{R}^n, X \\in \\mathcal{S}^n\\)，定义 \\(\\overline{X} = \\begin{bmatrix} X & x \\\\ x^{\\mathrm{T}} & 1 \\end{bmatrix}\\)，证明 \\(X \\succeq xx^{\\mathrm{T}}\\) 等价于 \\(\\overline{X} \\succeq 0\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.226,
                0.7,
                0.243
            ],
            "angle": 0,
            "content": "(b) 设 \\(z \\in \\mathbb{R}^m\\) ，矩阵值映射 \\(M(z): \\mathbb{R}^m \\to S^n\\) 定义为"
        },
        {
            "type": "list",
            "bbox": [
                0.305,
                0.155,
                0.826,
                0.243
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.457,
                0.256,
                0.705,
                0.293
            ],
            "angle": 0,
            "content": "\\[\nM (z) = A _ {0} + \\sum_ {i = 1} ^ {m} z _ {i} A _ {i}, \\quad A _ {i} \\in \\mathcal {S} ^ {n},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.334,
                0.305,
                0.655,
                0.323
            ],
            "angle": 0,
            "content": "证明： \\(\\eta \\geqslant \\lambda_{\\max}(M(z))\\) 等价于 \\(\\eta I\\succeq M(z)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.339,
                0.827,
                0.397
            ],
            "angle": 0,
            "content": "4.9 给定矩阵 \\(A_{i} \\in S^{m}, i = 0,1,\\dots,n\\)，定义线性映射 \\(A(x) = A_{0} + x_{1}A_{1} + \\dots + x_{n}A_{n}\\)，令 \\(\\lambda_{1}(x) \\geqslant \\lambda_{2}(x) \\geqslant \\dots \\lambda_{m}(x)\\) 为矩阵 \\(A(x)\\) 的特征值。将下面的优化问题转化为半定规划问题："
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.413,
                0.501,
                0.437
            ],
            "angle": 0,
            "content": "(a) \\(\\min_{x\\in \\mathbb{R}^n}\\lambda_1(x) - \\lambda_m(x).\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.307,
                0.443,
                0.464,
                0.479
            ],
            "angle": 0,
            "content": "(b) \\(\\min_{x\\in \\mathbb{R}^n}\\sum_{i = 1}^{m}|\\lambda_i(x)|.\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.306,
                0.413,
                0.501,
                0.479
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.491,
                0.604,
                0.508
            ],
            "angle": 0,
            "content": "4.10 将下面的优化问题转化为半定规划问题："
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.524,
                0.795,
                0.541
            ],
            "angle": 0,
            "content": "(a) 给定 \\((n + 1)\\) 个矩阵 \\(A_{i} \\in \\mathbb{R}^{p \\times q}, i = 0,1,\\dots,n\\)，考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.518,
                0.558,
                0.642,
                0.582
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| A (x) \\| _ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.333,
                0.593,
                0.825,
                0.631
            ],
            "angle": 0,
            "content": "其中 \\(A(x) = A_0 + x_1A_1 + \\dots +x_nA_n\\) 且 \\(\\| \\cdot \\| _2\\) 为矩阵的谱范数（即最大奇异值）；"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.641,
                0.826,
                0.678
            ],
            "angle": 0,
            "content": "(b) 给定 \\(c \\in \\mathbb{R}^n, A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, B \\in \\mathbb{R}^{p \\times n}\\) 以及 \\(d \\in \\mathbb{R}^p\\)，考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.488,
                0.681,
                0.672,
                0.752
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x \\in \\mathbb {R} ^ {n}} c ^ {\\mathrm {T}} x, \\\\ \\begin{array}{l} \\text {s . t .} \\quad \\| A x + b \\| _ {2} \\leqslant \\mathbf {1} ^ {\\mathrm {T}} x, \\end{array} \\\\ B x = d; \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.767,
                0.796,
                0.784
            ],
            "angle": 0,
            "content": "(c) 给定 \\(A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, F_i \\in S^m, i = 0,1,\\dots,n\\)，考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.8,
                0.77,
                0.826
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} (A x + b) ^ {\\mathrm {T}} F (x) ^ {- 1} (A x + b), \\quad \\text {s . t .} \\quad F (x) \\succ 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.334,
                0.836,
                0.654,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(F(x) = F_{0} + x_{1}F_{1} + x_{2}F_{2} + \\dots +x_{n}F_{n}\\)"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.223,
                0.133
            ],
            "angle": 0,
            "content": "习题4"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "155"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.156,
                0.69,
                0.192
            ],
            "angle": 0,
            "content": "4.11 对于对称矩阵 \\(C \\in S^n\\)，记其特征值分解为 \\(C = \\sum_{i=1}^{n} \\lambda_i u_i u_i^{\\mathrm{T}}\\)，假设"
        },
        {
            "type": "equation",
            "bbox": [
                0.332,
                0.202,
                0.615,
                0.22
            ],
            "angle": 0,
            "content": "\\[\n\\lambda_ {1} \\geqslant \\dots \\geqslant \\lambda_ {m} > 0 > \\lambda_ {m + 1} \\geqslant \\dots \\geqslant \\lambda_ {n},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.235,
                0.395,
                0.252
            ],
            "angle": 0,
            "content": "考虑如下半定规划问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.319,
                0.266,
                0.627,
                0.336
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {X \\in \\mathcal {S} ^ {n}} \\langle C, X \\rangle , \\\\ \\begin{array}{l l} \\text {s . t .} & u _ {i} ^ {\\mathrm {T}} X u _ {i} = 0, i = m + 1, m + 2, \\dots , n, \\end{array} \\\\ X \\succeq 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.351,
                0.444,
                0.368
            ],
            "angle": 0,
            "content": "试给出该问题最优解的表达式"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.381,
                0.737,
                0.418
            ],
            "angle": 0,
            "content": "4.12 如果在最大割问题(4.5.6)中，约束 \\(x_{j} \\in \\{-1, 1\\}\\) 改为 \\(x_{j} \\in \\{0, 1\\}\\)，即对应优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.352,
                0.417,
                0.596,
                0.48
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\max  \\frac {1}{2} \\sum_ {i <   j} w _ {i j} (1 - x _ {i} x _ {j}), \\\\ \\begin{array}{l l} \\text {s . t .} & x _ {j} \\in \\{0, 1 \\}, j = 1, 2, \\dots , n. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.486,
                0.427,
                0.503
            ],
            "angle": 0,
            "content": "试给出其一个半定规划松弛"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.517,
                0.389,
                0.533
            ],
            "angle": 0,
            "content": "4.13 对于非负矩阵分解问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.295,
                0.549,
                0.655,
                0.575
            ],
            "angle": 0,
            "content": "\\[\n\\min_{X\\in \\mathbb{R}^{d\\times p},Y\\in \\mathbb{R}^{p\\times n}}\\quad \\| A - XY\\|_{F}^{2},\\quad \\text{s.t.}\\quad X\\geqslant 0,Y\\geqslant 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.588,
                0.737,
                0.626
            ],
            "angle": 0,
            "content": "其中矩阵 \\(A \\in \\mathbb{R}^{d \\times n}\\) 是已知的. 证明: 在上面优化问题中添加约束 \\(\\boldsymbol{Y} \\boldsymbol{Y}^{\\mathrm{T}} = I\\), 其可以写成 K-均值聚类问题."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "156"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.826,
                0.134
            ],
            "angle": 0,
            "content": "第四章 典型优化问题"
        }
    ],
    [
        {
            "type": "title",
            "bbox": [
                0.291,
                0.252,
                0.619,
                0.283
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.334,
                0.744,
                0.435
            ],
            "angle": 0,
            "content": "正如第四章所介绍的，在实际中最优化问题的形式多种多样。给定一类具体的优化问题，我们首先需要分析其解的存在性。如果优化问题的解存在，再考虑如何设计算法求出其最优解。一般的非凸优化问题可能存在很多局部极小解，但其往往也能够满足实际问题的要求。对于这些局部（全局）极小解的求解，最优性理论是至关重要的。"
        },
        {
            "type": "title",
            "bbox": [
                0.301,
                0.464,
                0.607,
                0.486
            ],
            "angle": 0,
            "content": "5.1 最优化问题解的存在性"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.503,
                0.365,
                0.519
            ],
            "angle": 0,
            "content": "考虑优化问题(1.1.1)"
        },
        {
            "type": "equation",
            "bbox": [
                0.401,
                0.522,
                0.495,
                0.545
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.689,
                0.536,
                0.736,
                0.551
            ],
            "angle": 0,
            "content": "(5.1.1)"
        },
        {
            "type": "equation",
            "bbox": [
                0.412,
                0.552,
                0.506,
                0.567
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & x \\in \\mathcal {X}, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.576,
                0.741,
                0.676
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{X} \\subseteq \\mathbb{R}^n\\) 为可行域。对于问题 (5.1.1)，首先要考虑的是最优解的存在性，然后考虑如何求出其最优解。在数学分析课程中，我们学习过 Weierstrass 定理，即定义在紧集上的连续函数一定存在最大（最小）值点。而在许多实际问题中，定义域可能不是紧的，目标函数也不一定连续，因此需要将此定理推广来保证最优化问题解的存在性。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.689,
                0.754,
                0.726
            ],
            "angle": 0,
            "content": "定理5.1(Weierstrass定理）考虑一个适当且闭的函数 \\(f:\\mathcal{X}\\to (-\\infty , + \\infty ]\\) 假设下面三个条件中任意一个成立："
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.737,
                0.545,
                0.759
            ],
            "angle": 0,
            "content": "(1) \\(\\mathbf{dom}f\\stackrel {\\mathrm{def}}{=}\\left\\{x\\in \\mathcal{X}:f(x) <   + \\infty \\right\\}\\) 是有界的；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.771,
                0.444,
                0.787
            ],
            "angle": 0,
            "content": "(2) 存在一个常数 \\(\\bar{\\gamma}\\) 使得下水平集"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.737,
                0.545,
                0.787
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.371,
                0.799,
                0.577,
                0.822
            ],
            "angle": 0,
            "content": "\\[\nC _ {\\bar {\\gamma}} \\stackrel {\\mathrm {d e f}} {=} \\{x \\in \\mathcal {X}: f (x) \\leqslant \\bar {\\gamma} \\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.837,
                0.344,
                0.852
            ],
            "angle": 0,
            "content": "是非空且有界的；"
        },
        {
            "type": "page_number",
            "bbox": [
                0.44,
                0.869,
                0.471,
                0.882
            ],
            "angle": 0,
            "content": "157"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "158"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.156,
                0.8,
                0.174
            ],
            "angle": 0,
            "content": "(3) \\(f\\) 是强制的，即对于任一满足 \\(\\| x^{k}\\| \\to +\\infty\\) 的点列 \\(\\{x^k\\} \\subset \\mathcal{X}\\)，都有"
        },
        {
            "type": "equation",
            "bbox": [
                0.496,
                0.189,
                0.627,
                0.214
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} f (x ^ {k}) = + \\infty ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.229,
                0.825,
                0.266
            ],
            "angle": 0,
            "content": "那么，问题(5.1.1)的最小值点集 \\(\\{x\\in \\mathcal{X}\\mid f(x)\\leqslant f(y),\\forall y\\in \\mathcal{X}\\}\\) 是非空且紧的."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.279,
                0.826,
                0.384
            ],
            "angle": 0,
            "content": "证明．假设条件(2)成立．我们先证下确界 \\(t\\stackrel {\\mathrm{def}}{=}\\inf_{x\\in \\mathcal{X}}f(x) > - \\infty\\) 采用反证法．假设 \\(t = -\\infty\\) ，则存在点列 \\(\\{x^k\\}_{k = 1}^\\infty \\subset C_\\bar{\\gamma}\\) ，使得 \\(\\lim_{k\\to \\infty}f(x^k) = t = -\\infty .\\) 因为 \\(C_\\bar{\\gamma}\\) 的有界性，点列 \\(\\{x^{k}\\}\\) 一定存在聚点，记为 \\(x^{*}\\) ．根据上方图的闭性，我们知道 \\((x^{*},t)\\in \\mathbf{epi}f\\) ，即有 \\(f(x^{*})\\leqslant t = -\\infty\\) 这与函数的适当性矛盾，故\\(t > - \\infty\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.39,
                0.826,
                0.448
            ],
            "angle": 0,
            "content": "利用上面的论述, 我们知道 \\(f(x^{*}) \\leqslant t\\). 因为 \\(t\\) 是下确界, 故必有 \\(f(x^{*}) = t\\). 这就证明了下确界是可取得的. 再根据定理 2.2 以及 \\(C_{\\tilde{\\gamma}}\\) 的有界性, 易知最小值点集是紧的."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.452,
                0.825,
                0.51
            ],
            "angle": 0,
            "content": "假设条件 (1) 成立, 则 \\(\\operatorname{dom} f\\) 是有界的. 因为 \\(f\\) 是适当的, 即存在 \\(x_0 \\in \\mathcal{X}\\) 使得 \\(f(x_0) < +\\infty\\). 令 \\(\\bar{\\gamma} = f(x_0)\\), 则下水平集 \\(C_{\\bar{\\gamma}}\\) 是非空有界的, 那么利用条件 (2) 的结论, 可知问题 (5.1.1) 的最小值点集是非空且紧的."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.513,
                0.826,
                0.593
            ],
            "angle": 0,
            "content": "假设条件 (3) 成立。我们沿用上面定义的 \\(x_0, \\bar{\\gamma} \\stackrel{\\mathrm{def}}{=} f(x_0)\\) 以及下水平集 \\(C_{\\bar{\\gamma}}\\)。因为 \\(f\\) 是强制的，则 \\(C_{\\bar{\\gamma}}\\) 是非空有界的（假设无界，则存在点列 \\(\\{x^k\\} \\subset C_{\\bar{\\gamma}}\\) 满足 \\(\\lim_{k \\to \\infty} \\| x^k \\| = +\\infty\\)，由强制性有 \\(\\lim_{k \\to \\infty} f(x^k) = +\\infty\\)，这与 \\(f(x) \\leqslant \\bar{\\gamma}\\) 矛盾），即推出条件 (2)。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.608,
                0.825,
                0.687
            ],
            "angle": 0,
            "content": "定理5.1的三个条件在本质上都是保证 \\(f(x)\\) 的最小值不能在无穷远处取到，因此我们可以仅在一个有界的下水平集中考虑 \\(f(x)\\) 的最小值．同时要求 \\(f(x)\\) 为适当且闭的函数，并不需要 \\(f(x)\\) 的连续性．因此定理5.1比数学分析中的Weierstrass定理应用范围更广."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.692,
                0.825,
                0.77
            ],
            "angle": 0,
            "content": "当定义域不是有界闭集时, 我们通过例子来进一步解释上面的定理. 对于强制函数 \\(f(x) = x^{2}, x \\in \\mathcal{X} = \\mathbb{R}\\), 其全局最优解一定存在. 但对于适当且闭的函数 \\(f(x) = e^{-x}, x \\in \\mathcal{X} = \\mathbb{R}\\), 它不满足定理5.1三个条件中任意一个, 因此我们不能断言其全局极小值点存在. 事实上, 其全局极小值点不存在."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.774,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "定理5.1给出了最优解的存在性条件，但其对应的解可能不止一个。最优化问题解的唯一性在理论分析和算法比较中扮演着重要角色。比如，假设问题(5.1.1)的解是唯一存在的，记为 \\(x^{*}\\)，那么不同的算法最终都会收敛到 \\(x^{*}\\)。此时，我们通过比较不同算法的收敛速度来判断算法好坏是非常合理的。但"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.384,
                0.133
            ],
            "angle": 0,
            "content": "5.1 最优化问题解的存在性"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "159"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.234
            ],
            "angle": 0,
            "content": "如果是问题有多个最优值点，不同的算法收敛到的最优值点可能不同，那么这些算法收敛速度的比较就失去了参考价值。但是如果不同最优值点对应的目标函数值（即最优值）相同，我们可以比较不同算法对应的函数值收敛速度。"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.24,
                0.627,
                0.256
            ],
            "angle": 0,
            "content": "关于解的存在唯一性，我们这里考虑 \\(f\\) 是强拟凸的情况"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.27,
                0.737,
                0.307
            ],
            "angle": 0,
            "content": "定义5.1(强拟凸函数）给定凸集 \\(\\mathcal{X}\\) 和函数 \\(f:\\mathcal{X}\\to (-\\infty , + \\infty ]\\) .如果对任意的 \\(x\\neq y\\) 和 \\(\\lambda \\in (0,1)\\) ，都有"
        },
        {
            "type": "equation",
            "bbox": [
                0.308,
                0.322,
                0.597,
                0.34
            ],
            "angle": 0,
            "content": "\\[\nf (\\lambda x + (1 - \\lambda) y) <   \\max  \\{f (x), f (y) \\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.355,
                0.405,
                0.371
            ],
            "angle": 0,
            "content": "那么我们称函数 \\(f\\) 是强拟凸的."
        },
        {
            "type": "image",
            "bbox": [
                0.31,
                0.393,
                0.6,
                0.564
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.36,
                0.591,
                0.547,
                0.606
            ],
            "angle": 0,
            "content": "图5.1 一个强拟凸函数"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.632,
                0.737,
                0.71
            ],
            "angle": 0,
            "content": "强拟凸函数的几何意义是定义域内任何两点之间线段上的函数值不会大于两个端点处函数值的最大值，一般来说，强拟凸函数不一定是凸函数，但其任意一个下水平集都是凸集，并可以包含一部分性质较好的非凸函数。对于强拟凸函数，我们可以证出如下解的存在唯一性定理。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.724,
                0.736,
                0.781
            ],
            "angle": 0,
            "content": "定理5.2（唯一性定理）对于问题(5.1.1)，设 \\(\\mathcal{X}\\) 是 \\(\\mathbb{R}^n\\) 的一个非空、紧且凸的子集，如果 \\(f:\\mathcal{X}\\to (-\\infty , + \\infty ]\\) 是适当、闭且强拟凸函数，那么存在唯一的 \\(x^{*}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.337,
                0.785,
                0.569,
                0.804
            ],
            "angle": 0,
            "content": "\\[\nf (x ^ {*}) <   f (x), \\quad \\forall x \\in \\mathcal {X} \\backslash \\{x ^ {*} \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "证明．由Weierstrass定理知，问题(5.1.1)至少存在一个全局极小解 \\(x^{*}\\) .假设还有另外一个全局极小解 \\(y^{*}\\) ，那么 \\(f(x^{*}) = f(y^{*})\\) .根据强拟凸函数的定"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "160"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.157,
                0.416,
                0.174
            ],
            "angle": 0,
            "content": "义，对任意的 \\(\\lambda\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.349,
                0.188,
                0.732,
                0.207
            ],
            "angle": 0,
            "content": "\\[\nf \\left(\\lambda x ^ {*} + (1 - \\lambda) y ^ {*}\\right) <   \\max  \\left\\{f \\left(x ^ {*}\\right), f \\left(y ^ {*}\\right) \\right\\} = f \\left(x ^ {*}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.219,
                0.465,
                0.236
            ],
            "angle": 0,
            "content": "这与 \\(x^{*}\\) 的全局最优性矛盾"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.221,
                0.826,
                0.234
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.249,
                0.827,
                0.349
            ],
            "angle": 0,
            "content": "从强拟凸函数的定义可知，任意强凸函数均为强拟凸的，但凸函数并不一定是强拟凸的．利用上面的结论，对任何定义在有界凸集上的强凸函数(如 \\(f(x) = x^{2}\\) )，其最优解都是唯一存在的．但是对于一般的凸函数，其最优解可能不唯一，比如函数 \\(f(x) = \\max \\{x,0\\}\\) ，任意 \\(x\\leqslant 0\\) 都是 \\(f(x)\\) 的最优解."
        },
        {
            "type": "title",
            "bbox": [
                0.351,
                0.38,
                0.731,
                0.401
            ],
            "angle": 0,
            "content": "5.2 无约束可微问题的最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.419,
                0.614,
                0.435
            ],
            "angle": 0,
            "content": "无约束可微优化问题通常表示为如下形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.495,
                0.45,
                0.825,
                0.473
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x), \\tag {5.2.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.481,
                0.825,
                0.581
            ],
            "angle": 0,
            "content": "其中假设 \\(f\\) 是连续可微函数. 第一章已经引入了局部最优解和全局最优解的定义. 给定一个点 \\(\\bar{x}\\), 我们想要知道这个点是否是函数 \\(f\\) 的一个局部极小解或者全局极小解. 如果从定义出发, 需要对其邻域内的所有点进行判断, 这是不可行的. 因此, 需要一个更简单的方式来验证一个点是否为极小值点. 我们称其为最优性条件, 它主要包含一阶最优性条件和二阶最优性条件."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.606,
                0.46,
                0.625
            ],
            "angle": 0,
            "content": "5.2.1 一阶最优性条件"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.637,
                0.825,
                0.675
            ],
            "angle": 0,
            "content": "一阶最优性条件是利用梯度（一阶）信息来判断给定点的最优性。这里先考虑目标函数可微的情形，并给出下降方向的定义。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.687,
                0.825,
                0.722
            ],
            "angle": 0,
            "content": "定义5.2（下降方向）对于可微函数 \\(f\\) 和点 \\(x \\in \\mathbb{R}^n\\)，如果存在向量 \\(d\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.488,
                0.727,
                0.596,
                0.746
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x) ^ {\\mathrm {T}} d <   0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.755,
                0.564,
                0.772
            ],
            "angle": 0,
            "content": "那么称 \\(d\\) 为 \\(f\\) 在点 \\(x\\) 处的一个下降方向."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.784,
                0.825,
                0.822
            ],
            "angle": 0,
            "content": "由下降方向的定义，容易验证：如果 \\(f\\) 在点 \\(x\\) 处存在一个下降方向 \\(d\\) ，那么对于任意的 \\(T > 0\\) ，存在 \\(t \\in (0, T]\\) ，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.473,
                0.836,
                0.61,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nf (x + t d) <   f (x).\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.436,
                0.133
            ],
            "angle": 0,
            "content": "5.2 无约束可微问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "161"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.68,
                0.174
            ],
            "angle": 0,
            "content": "因此，在局部最优点处不能有下降方向。我们有如下一阶必要条件："
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.188,
                0.737,
                0.224
            ],
            "angle": 0,
            "content": "定理5.3（一阶必要条件）假设 \\(f\\) 在全空间 \\(\\mathbb{R}^n\\) 可微．如果 \\(x^{*}\\) 是一个局部极小点，那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.405,
                0.229,
                0.502,
                0.247
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x ^ {*}) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.261,
                0.592,
                0.277
            ],
            "angle": 0,
            "content": "证明. 任取 \\(v \\in \\mathbb{R}^n\\), 考虑 \\(f\\) 在点 \\(x = x^*\\) 处的泰勒展开"
        },
        {
            "type": "equation",
            "bbox": [
                0.302,
                0.293,
                0.605,
                0.312
            ],
            "angle": 0,
            "content": "\\[\nf (x ^ {*} + t v) = f (x ^ {*}) + t v ^ {\\mathrm {T}} \\nabla f (x ^ {*}) + o (t),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.328,
                0.228,
                0.344
            ],
            "angle": 0,
            "content": "整理得"
        },
        {
            "type": "equation",
            "bbox": [
                0.304,
                0.341,
                0.603,
                0.373
            ],
            "angle": 0,
            "content": "\\[\n\\frac {f (x ^ {*} + t v) - f (x ^ {*})}{t} = v ^ {\\mathrm {T}} \\nabla f (x ^ {*}) + o (1).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.379,
                0.663,
                0.396
            ],
            "angle": 0,
            "content": "根据 \\(x^{*}\\) 的最优性，在上式中分别对 \\(t\\) 取点0处的左、右极限可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.406,
                0.61,
                0.473
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\lim  _ {t \\rightarrow 0 ^ {+}} \\frac {f (x ^ {*} + t v) - f (x ^ {*})}{t} = v ^ {\\mathrm {T}} \\nabla f (x ^ {*}) \\geqslant 0, \\\\ \\lim  _ {t \\rightarrow 0 ^ {-}} \\frac {f (x ^ {*} + t v) - f (x ^ {*})}{t} = v ^ {\\mathrm {T}} \\nabla f (x ^ {*}) \\leqslant 0, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.485,
                0.637,
                0.503
            ],
            "angle": 0,
            "content": "即对任意的 \\(v\\) 有 \\(v^{\\mathrm{T}}\\nabla f(x^{*}) = 0\\) ，由 \\(v\\) 的任意性知 \\(\\nabla f(x^{*}) = 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.517,
                0.738,
                0.638
            ],
            "angle": 0,
            "content": "注意，上面的条件仅仅是必要的．对于 \\(f(x) = x^{2},x\\in \\mathbb{R}\\) ，我们知道满足 \\(f^{\\prime}(x) = 0\\) 的点为 \\(x^{*} = 0\\) ，并且其也是全局最优解．对于 \\(f(x) = x^{3},x\\in \\mathbb{R}\\) 满足 \\(f^{\\prime}(x) = 0\\) 的点为 \\(x^{*} = 0\\) ，但其不是一个局部最优解．实际上，我们称满足 \\(\\nabla f(x) = 0\\) 的点 \\(x\\) 为 \\(f\\) 的稳定点（有时也称为驻点或临界点）．可以看出，除了一阶必要条件，还需要对函数加一些额外的限制条件，才能保证最优解的充分性．我们会在后面的小节中继续讨论."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.664,
                0.371,
                0.681
            ],
            "angle": 0,
            "content": "5.2.2 二阶最优性条件"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.695,
                0.737,
                0.754
            ],
            "angle": 0,
            "content": "在没有额外假设时，如果一阶必要条件满足，我们仍然不能确定当前点是否是一个局部极小点。这里考虑使用二阶信息来进一步判断给定点的最优性。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.757,
                0.737,
                0.815
            ],
            "angle": 0,
            "content": "假设 \\(f\\) 在点 \\(x\\) 的一个开邻域内是二阶连续可微的。类似于一阶必要条件的推导，可以借助当前点处的二阶泰勒展开来逼近该函数在该点附近的取值情况，从而来判断最优性。具体地，在点 \\(x\\) 附近我们考虑泰勒展开"
        },
        {
            "type": "equation",
            "bbox": [
                0.245,
                0.826,
                0.66,
                0.857
            ],
            "angle": 0,
            "content": "\\[\nf (x + d) = f (x) + \\nabla f (x) ^ {\\mathrm {T}} d + \\frac {1}{2} d ^ {\\mathrm {T}} \\nabla^ {2} f (x) d + o (\\| d \\| ^ {2}).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "162"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.738,
                0.174
            ],
            "angle": 0,
            "content": "当一阶必要条件满足时，\\(\\nabla f(x) = 0\\)，那么上面的展开式简化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.186,
                0.825,
                0.217
            ],
            "angle": 0,
            "content": "\\[\nf (x + d) = f (x) + \\frac {1}{2} d ^ {\\mathrm {T}} \\nabla^ {2} f (x) d + o (\\| d \\| ^ {2}). \\tag {5.2.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.229,
                0.529,
                0.246
            ],
            "angle": 0,
            "content": "因此，我们有如下二阶最优性条件："
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.263,
                0.825,
                0.299
            ],
            "angle": 0,
            "content": "定理5.4 假设 \\(f\\) 在点 \\(x^{*}\\) 的一个开邻域内是二阶连续可微的，则以下最优性条件成立："
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.311,
                0.654,
                0.328
            ],
            "angle": 0,
            "content": "二阶必要条件如果 \\(x^{*}\\) 是 \\(f\\) 的一个局部极小点，那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.345,
                0.65,
                0.364
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x ^ {*}) = 0, \\quad \\nabla^ {2} f (x ^ {*}) \\succeq 0;\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.382,
                0.501,
                0.399
            ],
            "angle": 0,
            "content": "二阶充分条件如果在点 \\(x^{*}\\) 处有"
        },
        {
            "type": "equation",
            "bbox": [
                0.436,
                0.416,
                0.649,
                0.435
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x ^ {*}) = 0, \\quad \\nabla^ {2} f (x ^ {*}) \\succ 0\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.452,
                0.553,
                0.468
            ],
            "angle": 0,
            "content": "成立，那么 \\(x^{*}\\) 为 \\(f\\) 的一个局部极小点."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.484,
                0.825,
                0.543
            ],
            "angle": 0,
            "content": "证明．考虑 \\(f(x)\\) 在点 \\(x^{*}\\) 处的二阶泰勒展开(5.2.2)，这里因为一阶必要条件成立，所以 \\(\\nabla f(x^{*}) = 0\\) 反设 \\(\\nabla^2 f(x^*)\\succeq 0\\) 不成立，即 \\(\\nabla^2 f(x^*)\\) 有负的特征值．取 \\(d\\) 为其负特征值 \\(\\lambda_{-}\\) 对应的特征向量，通过对(5.2.2)式变形得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.361,
                0.554,
                0.722,
                0.591
            ],
            "angle": 0,
            "content": "\\[\n\\frac {f (x ^ {*} + d) - f (x ^ {*})}{\\| d \\| ^ {2}} = \\frac {1}{2} \\frac {d ^ {\\mathrm {T}}}{\\| d \\|} \\nabla^ {2} f (x ^ {*}) \\frac {d}{\\| d \\|} + o (1).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.603,
                0.531,
                0.638
            ],
            "angle": 0,
            "content": "这里注意 \\(\\frac{d}{\\|d\\|}\\) 是 \\(d\\) 的单位化，因此"
        },
        {
            "type": "equation",
            "bbox": [
                0.414,
                0.649,
                0.671,
                0.685
            ],
            "angle": 0,
            "content": "\\[\n\\frac {f (x ^ {*} + d) - f (x ^ {*})}{\\| d \\| ^ {2}} = \\frac {1}{2} \\lambda_ {-} + o (1).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.696,
                0.825,
                0.734
            ],
            "angle": 0,
            "content": "当 \\(\\| d\\|\\) 充分小时， \\(f(x^{*} + d) <   f(x^{*})\\) ，这和点 \\(x^{*}\\) 的最优性矛盾．因此二阶必要条件成立."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.738,
                0.825,
                0.777
            ],
            "angle": 0,
            "content": "当 \\(\\nabla^2 f(x) \\succ 0\\) 时，对任意的 \\(d \\neq 0\\) 有 \\(d^{\\mathrm{T}}\\nabla^{2}f(x^{*})d \\geqslant \\lambda_{\\min}\\| d\\|^{2} > 0\\) ，这里 \\(\\lambda_{\\min} > 0\\) 是 \\(\\nabla^2 f(x^*)\\) 的最小特征值．因此我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.409,
                0.788,
                0.675,
                0.824
            ],
            "angle": 0,
            "content": "\\[\n\\frac {f (x ^ {*} + d) - f (x ^ {*})}{\\| d \\| ^ {2}} \\geqslant \\frac {1}{2} \\lambda_ {\\min } + o (1).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.717,
                0.854
            ],
            "angle": 0,
            "content": "当 \\(\\| d\\|\\) 充分小时有 \\(f(x^{*} + d)\\geqslant f(x^{*})\\) ，即二阶充分条件成立."
        },
        {
            "type": "text",
            "bbox": [
                0.808,
                0.838,
                0.826,
                0.851
            ],
            "angle": 0,
            "content": "□"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.437,
                0.133
            ],
            "angle": 0,
            "content": "5.2 无约束可微问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "163"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.744,
                0.258
            ],
            "angle": 0,
            "content": "由定理5.4有如下结论：设点 \\(\\bar{x}\\) 满足一阶最优性条件（即 \\(\\nabla f(\\bar{x}) = 0\\)），且该点处的海瑟矩阵 \\(\\nabla^2 f(\\bar{x})\\) 不是半正定的，那么 \\(\\bar{x}\\) 不是一个局部极小点。进一步地，如果海瑟矩阵 \\(\\nabla^2 f(\\bar{x})\\) 既有正特征值又有负特征值，我们称稳定点 \\(\\bar{x}\\) 为一个鞍点。事实上，记 \\(d_1, d_2\\) 为其正负特征值对应的特征向量，那么对于任意充分小的 \\(t > 0\\)，我们都有 \\(f(\\bar{x} + td_1) > f(\\bar{x})\\) 且 \\(f(\\bar{x} + td_2) < f(\\bar{x})\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.261,
                0.744,
                0.32
            ],
            "angle": 0,
            "content": "注意，二阶最优性条件给出的仍然是关于局部最优性的判断。对于给定点的全局最优性判断，我们还需要借助实际问题的性质，比如目标函数是凸的、非线性最小二乘问题中目标函数值为0等。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.343,
                0.276,
                0.361
            ],
            "angle": 0,
            "content": "5.2.3 实例"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.374,
                0.741,
                0.412
            ],
            "angle": 0,
            "content": "我们以线性最小二乘问题为例来说明其最优性条件的具体形式．如第4.2节中所述，线性最小二乘问题可以表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.416,
                0.564,
                0.449
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x) \\stackrel {\\text {d e f}} {=} \\frac {1}{2} \\| b - A x \\| _ {2} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.453,
                0.741,
                0.491
            ],
            "angle": 0,
            "content": "其中 \\(A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^{m}\\) 分别是给定的矩阵和向量. 易知 \\(f(x)\\) 是可微且凸的, 因此, \\(x^{*}\\) 为一个全局最优解当且仅当"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.5,
                0.562,
                0.52
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x ^ {*}) = A ^ {\\mathrm {T}} (A x ^ {*} - b) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.529,
                0.738,
                0.567
            ],
            "angle": 0,
            "content": "因此，线性最小二乘问题本质上等于求解线性方程组，可以利用数值代数知识对其有效求解。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.571,
                0.741,
                0.608
            ],
            "angle": 0,
            "content": "在实际中，我们还经常遇到非线性最小二乘问题，如实数情形的相位恢复问题(3.6.4)，其一般形式如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.361,
                0.614,
                0.738,
                0.651
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x) \\stackrel {\\text {d e f}} {=} \\sum_ {i = 1} ^ {m} r _ {i} ^ {2} (x), \\tag {5.2.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.656,
                0.741,
                0.695
            ],
            "angle": 0,
            "content": "其中非线性函数 \\(r_i(x) = (a_i^{\\mathrm{T}}x)^2 -b_i^2,i = 1,2,\\dots ,m.\\) 这个问题是非凸的．在点 \\(x\\in \\mathbb{R}^n\\) 处，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.253,
                0.699,
                0.66,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\nabla f (x) = 2 \\sum_ {i = 1} ^ {m} r _ {i} (x) \\nabla r _ {i} (x) = 4 \\sum_ {i = 1} ^ {m} ((a _ {i} ^ {\\mathrm {T}} x) ^ {2} - b _ {i} ^ {2}) (a _ {i} ^ {\\mathrm {T}} x) a _ {i}, \\\\ \\nabla^ {2} f (x) = 2 \\sum_ {i = 1} ^ {m} \\nabla r _ {i} (x) \\nabla r _ {i} (x) ^ {\\mathrm {T}} + 2 \\sum_ {i = 1} ^ {m} r _ {i} (x) \\nabla^ {2} r _ {i} (x) \\\\ = 8 \\sum_ {i = 1} ^ {m} \\left(a _ {i} ^ {\\mathrm {T}} x\\right) ^ {2} a _ {i} a _ {i} ^ {\\mathrm {T}} + 4 \\sum_ {i = 1} ^ {m} \\left(\\left(a _ {i} ^ {\\mathrm {T}} x\\right) ^ {2} - b _ {i} ^ {2}\\right) a _ {i} a _ {i} ^ {\\mathrm {T}} \\\\ = \\sum_ {i = 1} ^ {m} \\left(1 2 \\left(a _ {i} ^ {\\mathrm {T}} x\\right) ^ {2} - 4 b _ {i} ^ {2}\\right) a _ {i} a _ {i} ^ {\\mathrm {T}}. \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.119,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "164"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.772,
                0.174
            ],
            "angle": 0,
            "content": "如果 \\(x^{*}\\) 为问题 (5.2.3) 的一个局部最优解，那么其满足一阶必要条件"
        },
        {
            "type": "equation",
            "bbox": [
                0.494,
                0.188,
                0.591,
                0.207
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x ^ {*}) = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.222,
                0.28,
                0.236
            ],
            "angle": 0,
            "content": "即"
        },
        {
            "type": "equation",
            "bbox": [
                0.43,
                0.237,
                0.652,
                0.273
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i = 1} ^ {m} \\left(\\left(a _ {i} ^ {\\mathrm {T}} x ^ {*}\\right) ^ {2} - b _ {i}\\right) \\left(a _ {i} ^ {\\mathrm {T}} x ^ {*}\\right) a _ {i} = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.279,
                0.4,
                0.295
            ],
            "angle": 0,
            "content": "以及二阶必要条件"
        },
        {
            "type": "equation",
            "bbox": [
                0.491,
                0.298,
                0.593,
                0.316
            ],
            "angle": 0,
            "content": "\\[\n\\nabla^ {2} f (x ^ {*}) \\succeq 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.327,
                0.28,
                0.342
            ],
            "angle": 0,
            "content": "即"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.342,
                0.65,
                0.378
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i = 1} ^ {m} \\left(1 2 \\left(a _ {i} ^ {\\mathrm {T}} x ^ {*}\\right) ^ {2} - 4 b _ {i} ^ {2}\\right) a _ {i} a _ {i} ^ {\\mathrm {T}} \\succeq 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.385,
                0.512,
                0.402
            ],
            "angle": 0,
            "content": "如果一个点 \\(x^{\\#}\\) 满足二阶充分条件"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.416,
                0.65,
                0.436
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x ^ {\\#}) = 0, \\quad \\nabla^ {2} f (x ^ {\\#}) \\succ 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.451,
                0.28,
                0.465
            ],
            "angle": 0,
            "content": "即"
        },
        {
            "type": "equation",
            "bbox": [
                0.315,
                0.465,
                0.768,
                0.502
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i = 1} ^ {m} \\left(\\left(a _ {i} ^ {\\mathrm {T}} x ^ {\\#}\\right) ^ {2} - b _ {i}\\right) \\left(a _ {i} ^ {\\mathrm {T}} x ^ {\\#}\\right) a _ {i} = 0, \\quad \\sum_ {i = 1} ^ {m} \\left(1 2 \\left(a _ {i} ^ {\\mathrm {T}} x ^ {\\#}\\right) ^ {2} - 4 b _ {i} ^ {2}\\right) a _ {i} a _ {i} ^ {\\mathrm {T}} \\succ 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.508,
                0.57,
                0.525
            ],
            "angle": 0,
            "content": "那么 \\(x^{\\#}\\) 为问题(5.2.3)的一个局部最优解"
        },
        {
            "type": "title",
            "bbox": [
                0.339,
                0.556,
                0.743,
                0.577
            ],
            "angle": 0,
            "content": "5.3 无约束不可微问题的最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.595,
                0.475,
                0.611
            ],
            "angle": 0,
            "content": "本节仍考虑问题(5.2.1)："
        },
        {
            "type": "equation",
            "bbox": [
                0.494,
                0.627,
                0.589,
                0.65
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.659,
                0.825,
                0.738
            ],
            "angle": 0,
            "content": "但其中 \\(f(x)\\) 为不可微函数. 很多实际问题的目标函数不是光滑的, 例如 \\(f(x) = \\|x\\|_{1}\\). 对于此类问题, 由于目标函数可能不存在梯度和海瑟矩阵, 因此第5.2节中的一阶和二阶条件不适用. 此时我们必须使用其他最优性条件来判断不可微问题的最优点."
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.763,
                0.538,
                0.782
            ],
            "angle": 0,
            "content": "5.3.1 凸优化问题一阶充要条件"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.795,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "对于目标函数是凸函数的情形，我们已经引入了次梯度的概念并给出了其计算法则。一个自然的问题是：可以利用次梯度代替梯度来构造最优性条件吗？答案是肯定的，实际上有如下定理："
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.3 无约束不可微问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "165"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.737,
                0.193
            ],
            "angle": 0,
            "content": "定理5.5假设 \\(f\\) 是适当且凸的函数，则 \\(x^{*}\\) 为问题(5.2.1)的一个全局极小点当且仅当"
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.198,
                0.498,
                0.216
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial f (x ^ {*}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.229,
                0.557,
                0.245
            ],
            "angle": 0,
            "content": "证明．先证必要性．因为 \\(x^{*}\\) 为全局极小点，所以"
        },
        {
            "type": "equation",
            "bbox": [
                0.276,
                0.259,
                0.629,
                0.279
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x ^ {*}) = f (x ^ {*}) + 0 ^ {\\mathrm {T}} (y - x ^ {*}), \\quad \\forall y \\in \\mathbb {R} ^ {n}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.293,
                0.31,
                0.31
            ],
            "angle": 0,
            "content": "因此， \\(0\\in \\partial f(x^{*})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.314,
                0.62,
                0.33
            ],
            "angle": 0,
            "content": "再证充分性．如果 \\(0\\in \\partial f(x^{*})\\) ，那么根据次梯度的定义"
        },
        {
            "type": "equation",
            "bbox": [
                0.276,
                0.344,
                0.63,
                0.364
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f \\left(x ^ {*}\\right) + 0 ^ {\\mathrm {T}} \\left(y - x ^ {*}\\right) = f \\left(x ^ {*}\\right), \\quad \\forall y \\in \\mathbb {R} ^ {n}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.378,
                0.377,
                0.395
            ],
            "angle": 0,
            "content": "因而 \\(x^{*}\\) 为一个全局极小点"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.408,
                0.739,
                0.509
            ],
            "angle": 0,
            "content": "定理5.5说明条件 \\(0 \\in \\partial f(x^{*})\\) 是 \\(x^{*}\\) 为全局最优解的充要条件。这个结论比定理5.3要强，其原因是凸问题有非常好的性质，它的稳定点中不存在鞍点。因此，可以通过计算凸函数的次梯度集合来求解其对应的全局极小点。相较于非凸函数，凸函数的最优性分析简单，计算以及验证起来比较方便，因此在实际建模中受到广泛的关注。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.534,
                0.49,
                0.552
            ],
            "angle": 0,
            "content": "5.3.2 复合优化问题的一阶必要条件"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.565,
                0.739,
                0.684
            ],
            "angle": 0,
            "content": "在实际问题中，目标函数不一定是凸函数，但它可以写成一个光滑函数与一个非光滑凸函数的和，例如第4.3节介绍的复合优化问题就具有这样的形式。其中目标函数的光滑项可能是凸的，比如LASSO问题、图像去噪问题和盲反卷积问题；也可能是非凸的，例如字典学习问题和神经网络的损失函数。因此研究此类问题的最优性条件十分必要。这里，我们考虑一般复合优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.686,
                0.737,
                0.711
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\psi (x) \\stackrel {\\text {d e f}} {=} f (x) + h (x), \\tag {5.3.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.717,
                0.737,
                0.755
            ],
            "angle": 0,
            "content": "其中 \\(f\\) 为光滑函数（可能非凸），\\(h\\) 为凸函数（可能非光滑）。对于其任何局部最优解，我们给出如下一阶必要条件："
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.768,
                0.737,
                0.804
            ],
            "angle": 0,
            "content": "定理5.6(复合优化问题一阶必要条件）令 \\(x^{*}\\) 为问题(5.3.1)的一个局部极小点，那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.38,
                0.808,
                0.529,
                0.827
            ],
            "angle": 0,
            "content": "\\[\n- \\nabla f (x ^ {*}) \\in \\partial h (x ^ {*}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.538,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(\\partial h(x^{*})\\) 为凸函数 \\(h\\) 在点 \\(x^{*}\\) 处的次梯度集合."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.119,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "166"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.118,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.825,
                0.193
            ],
            "angle": 0,
            "content": "证明．因为 \\(x^{*}\\) 为一个局部极小点，所以对于任意单位向量 \\(d\\in \\mathbb{R}^n\\) 和足够小的 \\(t > 0\\) ，"
        },
        {
            "type": "equation",
            "bbox": [
                0.385,
                0.198,
                0.696,
                0.217
            ],
            "angle": 0,
            "content": "\\[\nf (x ^ {*} + t d) + h (x ^ {*} + t d) \\geqslant f (x ^ {*}) + h (x ^ {*}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.225,
                0.826,
                0.263
            ],
            "angle": 0,
            "content": "给定任一方向 \\(d \\in \\mathbb{R}^n\\)，其中 \\(\\| d \\| = 1\\)。因为对光滑函数和凸函数都可以考虑方向导数（凸函数的方向导数参考定义2.22），根据方向导数的定义，"
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.271,
                0.677,
                0.363
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\psi^ {\\prime} (x ^ {*}; d) = \\lim  _ {t \\rightarrow 0 +} \\frac {\\psi (x ^ {*} + t d) - \\psi (x ^ {*})}{t} \\\\ = \\nabla f (x ^ {*}) ^ {\\mathrm {T}} d + \\partial h (x ^ {*}; d) \\\\ = \\nabla f (x ^ {*}) ^ {\\mathrm {T}} d + \\sup  _ {\\theta \\in \\partial h (x ^ {*})} \\theta^ {\\mathrm {T}} d, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.372,
                0.825,
                0.452
            ],
            "angle": 0,
            "content": "其中 \\(\\partial h(x^{*};d)\\) 表示凸函数 \\(h(x)\\) 在点 \\(x^{*}\\) 处的方向导数，最后一个等式利用了凸函数方向导数和次梯度的关系（定理2.20）。现在用反证法证明我们所需要的结论。反设 \\(-\\nabla f(x^{*}) \\notin \\partial h(x^{*})\\)，根据定理2.17可知 \\(\\partial h(x^{*})\\) 是有界闭凸集，又根据定理2.6（严格分离定理），存在 \\(d \\in \\mathbb{R}^n\\) 以及常数 \\(b\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.464,
                0.684,
                0.484
            ],
            "angle": 0,
            "content": "\\[\n\\theta^ {\\mathrm {T}} d <   b <   - \\nabla f (x ^ {*}) ^ {\\mathrm {T}} d, \\quad \\forall \\theta \\in \\partial h (x ^ {*}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.497,
                0.564,
                0.514
            ],
            "angle": 0,
            "content": "根据 \\(\\partial h(x^{*})\\) 是有界闭集可知对此方向 \\(d\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.527,
                0.688,
                0.557
            ],
            "angle": 0,
            "content": "\\[\n\\psi^{\\prime}(x^{*};d) = \\nabla f(x^{*})^{\\mathrm{T}}d + \\sup_{\\theta \\in \\partial h(x^{*})}\\theta^{\\mathrm{T}}d <   0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.567,
                0.487,
                0.583
            ],
            "angle": 0,
            "content": "这说明对充分小的非负实数 \\(t\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.465,
                0.598,
                0.618,
                0.616
            ],
            "angle": 0,
            "content": "\\[\n\\psi (x ^ {*} + t d) <   \\psi (x ^ {*}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.629,
                0.66,
                0.647
            ],
            "angle": 0,
            "content": "这与 \\(x^{*}\\) 的局部极小性矛盾．因此 \\(-\\nabla f(x^{*})\\in \\partial h(x^{*})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.659,
                0.825,
                0.739
            ],
            "angle": 0,
            "content": "定理5.6在之后我们推导复合优化问题算法性质的时候非常重要，它给出了当目标函数一部分是非光滑凸函数时的一阶必要条件。在这里注意，由于目标函数可能是整体非凸的，因此一般没有一阶充分条件。在第八章中我们介绍邻近算子时会用到这个定理。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.763,
                0.585,
                0.782
            ],
            "angle": 0,
            "content": "*5.3.3 非光滑非凸问题的最优性条件"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "当函数 \\(f\\) 不可微且非凸时，其梯度和通常意义的次梯度都可能不存在。为了能得到和可微情形类似的结果，我们必须对次梯度和次微分概念进行某种推广。实际上，对适当下半连续函数依然可以定义次微分。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.3 无约束不可微问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "167"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.157,
                0.691,
                0.175
            ],
            "angle": 0,
            "content": "定义5.3(次微分）设 \\(f:\\mathbb{R}^n\\to (-\\infty , + \\infty ]\\) 是适当下半连续函数"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.187,
                0.737,
                0.224
            ],
            "angle": 0,
            "content": "(1) 对给定的 \\(x \\in \\mathbf{dom} f\\)，满足如下条件的所有向量 \\(u \\in \\mathbb{R}^n\\) 的集合定义为 \\(f\\) 在点 \\(x\\) 处的 Fréchet 次微分："
        },
        {
            "type": "equation",
            "bbox": [
                0.335,
                0.234,
                0.614,
                0.27
            ],
            "angle": 0,
            "content": "\\[\n\\liminf_{y\\to x,y\\neq x}\\frac{f(y) - f(x) - \\langle u,y - x\\rangle}{\\|y - x\\|} \\geqslant 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.279,
                0.631,
                0.299
            ],
            "angle": 0,
            "content": "记为 \\(\\hat{\\partial} f(x)\\)。当 \\(x \\notin \\mathbf{dom}f\\) 时，将 \\(\\hat{\\partial} f(x)\\) 定义为空集 \\(\\varnothing\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.311,
                0.651,
                0.329
            ],
            "angle": 0,
            "content": "(2) \\(f\\) 在点 \\(x \\in \\mathbb{R}^n\\) 处的极限次微分（或简称为次微分）定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.24,
                0.342,
                0.707,
                0.362
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (x) = \\left\\{u \\in \\mathbb {R} ^ {n}: \\exists x ^ {k} \\rightarrow x, f \\left(x ^ {k}\\right)\\rightarrow f (x), u ^ {k} \\in \\hat {\\partial} f \\left(x ^ {k}\\right)\\rightarrow u \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.209,
                0.376,
                0.734,
                0.393
            ],
            "angle": 0,
            "content": "即极限次微分是通过对 \\(x\\) 附近的点处的Frechet次微分取极限得到的."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.405,
                0.475,
                0.423
            ],
            "angle": 0,
            "content": "我们对非凸函数次微分做如下说明："
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.434,
                0.737,
                0.473
            ],
            "angle": 0,
            "content": "注5.1 (1) 容易证明 \\(\\hat{\\partial} f(x) \\subseteq \\partial f(x)\\), 前者是闭凸集, 后者是闭集. 并非在所有的 \\(x \\in \\mathbf{dom} f\\) 处都存在 Fréchet 次微分."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.486,
                0.728,
                0.503
            ],
            "angle": 0,
            "content": "(2) 和凸函数次微分（定义2.21）比较可知，凸函数的次梯度要求不等式"
        },
        {
            "type": "equation",
            "bbox": [
                0.331,
                0.518,
                0.617,
                0.537
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f (x) + \\langle g, y - x \\rangle , \\quad g \\in \\partial f (x)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.551,
                0.735,
                0.568
            ],
            "angle": 0,
            "content": "在定义域内全局成立，而对非凸函数只需要其在极限意义下成立即可."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.581,
                0.661,
                0.598
            ],
            "angle": 0,
            "content": "(3) 当 \\( f \\) 是可微函数时，Frechet 次微分和次微分都退化成梯度."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.61,
                0.738,
                0.689
            ],
            "angle": 0,
            "content": "引入非凸函数次微分的目的是为了推导出非凸非光滑函数局部极小点的一阶必要条件。定理5.5已经说明了对凸函数 \\(f\\)，\\(x\\) 是 \\(f(x)\\) 的全局极小点当且仅当 \\(0 \\in \\partial f(x)\\)。而对适当下半连续函数 \\(f\\)，我们依然有类似的一阶必要条件。"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.702,
                0.737,
                0.74
            ],
            "angle": 0,
            "content": "定理5.7（一阶必要条件）设 \\(f\\) 是适当下半连续函数．若 \\(x^{*}\\) 是 \\(f(x)\\) 的一个局部极小点，则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.409,
                0.743,
                0.498,
                0.762
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial f (x ^ {*}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.773,
                0.738,
                0.813
            ],
            "angle": 0,
            "content": "证明．实际上我们可以证明 \\(0\\in \\hat{\\partial} f(x^{*})\\) ，直接对0验证它是否是点 \\(x^{*}\\) 处的一个Frechet次微分即可．对任意 \\(x\\in \\mathbf{dom}f\\) ，考虑"
        },
        {
            "type": "equation",
            "bbox": [
                0.351,
                0.821,
                0.559,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\frac {f (x) - f (x ^ {*}) - \\langle 0 , x - x ^ {*} \\rangle}{\\| x - x ^ {*} \\|}.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "168"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "image",
            "bbox": [
                0.372,
                0.162,
                0.716,
                0.326
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.448,
                0.338,
                0.635,
                0.355
            ],
            "angle": 0,
            "content": "图5.2 非凸函数次微分"
        },
        {
            "type": "image_footnote",
            "bbox": [
                0.331,
                0.358,
                0.752,
                0.376
            ],
            "angle": 0,
            "content": "\\((f(x)\\) 在点 \\(x_{3}\\) 处不存在Frechet次微分，但存在次微分）"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.404,
                0.76,
                0.422
            ],
            "angle": 0,
            "content": "注意到在 \\(x^{*}\\) 的任意邻域内有 \\(f(x)\\geqslant f(x^{*})\\) ，对 \\(x\\to x^*\\) 取下极限得"
        },
        {
            "type": "equation",
            "bbox": [
                0.44,
                0.434,
                0.644,
                0.47
            ],
            "angle": 0,
            "content": "\\[\n\\liminf_{x\\to x^{*},x\\neq x^{*}}\\frac{f(x) - f(x^{*})}{\\|x - x^{*}\\|} \\geqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.483,
                0.402,
                0.503
            ],
            "angle": 0,
            "content": "这说明 \\(0 \\in \\hat{\\partial} f(x^{*})\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.486,
                0.826,
                0.499
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.534,
                0.362,
                0.552
            ],
            "angle": 0,
            "content": "5.3.4 实例"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.568,
                0.825,
                0.605
            ],
            "angle": 0,
            "content": "我们以 \\(\\ell_{1}\\) 范数优化问题为例，给出其最优解的最优性条件。第一章和第四章介绍了各种各样的 \\(\\ell_{1}\\) 范数优化问题，其一般形式可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.621,
                0.825,
                0.647
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\psi (x) \\stackrel {\\text {d e f}} {=} f (x) + \\mu \\| x \\| _ {1}, \\tag {5.3.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.66,
                0.825,
                0.698
            ],
            "angle": 0,
            "content": "其中 \\(f(x):\\mathbb{R}^n\\to \\mathbb{R}\\) 为光滑函数，正则系数 \\(\\mu >0\\) 用来调节解的稀疏度．尽管 \\(\\| x\\| _1\\) 不是可微的，但我们可以计算其次微分"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.712,
                0.648,
                0.787
            ],
            "angle": 0,
            "content": "\\[\n\\partial_ {i} \\| x \\| _ {1} = \\left\\{ \\begin{array}{l l} \\{1 \\}, & x _ {i} > 0, \\\\ [ - 1, 1 ], & x _ {i} = 0, \\\\ \\{- 1 \\}, & x _ {i} <   0. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.801,
                0.721,
                0.818
            ],
            "angle": 0,
            "content": "因此，如果 \\(x^{*}\\) 是问题(5.3.2)的一个局部最优解，那么其满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.461,
                0.836,
                0.622,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n- \\nabla f (x ^ {*}) \\in \\mu \\partial \\| x ^ {*} \\| _ {1},\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.282,
                0.133
            ],
            "angle": 0,
            "content": "5.4 对偶理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "169"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.157,
                0.194,
                0.173
            ],
            "angle": 0,
            "content": "即"
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.173,
                0.586,
                0.248
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {i} f (x ^ {*}) = \\left\\{ \\begin{array}{l l} - \\mu , & x _ {i} ^ {*} > 0, \\\\ a \\in [ - \\mu , \\mu ], & x _ {i} ^ {*} = 0, \\\\ \\mu , & x _ {i} ^ {*} <   0. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.257,
                0.736,
                0.302
            ],
            "angle": 0,
            "content": "进一步地，如果 \\( f(x) \\) 是凸的（比如在LASSO问题中 \\( f(x) = \\frac{1}{2} \\| Ax - b\\|^2 \\)），那么满足上式的 \\( x^* \\) 就是问题(5.3.2)的全局最优解。"
        },
        {
            "type": "title",
            "bbox": [
                0.374,
                0.333,
                0.533,
                0.354
            ],
            "angle": 0,
            "content": "5.4 对偶理论"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.371,
                0.606,
                0.388
            ],
            "angle": 0,
            "content": "这一节以及本章之后的章节考虑一般的约束优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.367,
                0.402,
                0.462,
                0.426
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.429,
                0.737,
                0.449
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{r l} \\text {s . t .} & c _ {i} (x) \\leqslant 0, i \\in \\mathcal {I}, \\end{array} \\tag {5.4.1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.417,
                0.456,
                0.54,
                0.475
            ],
            "angle": 0,
            "content": "\\[\nc _ {i} (x) = 0, i \\in \\mathcal {E},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.487,
                0.737,
                0.525
            ],
            "angle": 0,
            "content": "其中 \\(c_{i}\\) 为定义在 \\(\\mathbb{R}^n\\) 或其子集上的实值函数，\\(\\mathcal{I}\\) 和 \\(\\mathcal{E}\\) 分别表示不等式约束和等式约束对应的下标集合且各下标互不相同。这个问题的可行域定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.266,
                0.541,
                0.64,
                0.561
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {X} = \\{x \\in \\mathbb {R} ^ {n} | c _ {i} (x) \\leqslant 0, i \\in \\mathcal {I}   \\text {且}   c _ {i} (x) = 0, i \\in \\mathcal {E} \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.575,
                0.738,
                0.676
            ],
            "angle": 0,
            "content": "我们可以通过将 \\(\\mathcal{X}\\) 的示性函数加到目标函数中得到无约束优化问题。但是转化后问题的目标函数是不连续的、不可微的以及不是有限的，这导致我们难以分析其理论性质以及设计有效的算法。对于约束优化问题，可行性问题是应该最先考虑的。因此，对其约束集合的几何性质以及代数性质的分析尤为重要。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.701,
                0.452,
                0.72
            ],
            "angle": 0,
            "content": "5.4.1 拉格朗日函数与对偶问题"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.733,
                0.74,
                0.854
            ],
            "angle": 0,
            "content": "研究问题 (5.4.1) 的重要工具之一是拉格朗日函数, 它的基本思想是给该问题中的每一个约束指定一个拉格朗日乘子, 以乘子为加权系数将约束增加到目标函数中. 令 \\(\\lambda_{i}\\) 为对应于第 \\(i\\) 个不等式约束的拉格朗日乘子, \\(\\nu_{i}\\) 为对应于第 \\(i\\) 个等式约束的拉格朗日乘子. 为了构造合适的对偶问题, 基本原则是对拉格朗日乘子添加合适的约束条件, 使得 \\(f(x)\\) 在问题 (5.4.1) 的任意可行点 \\(x\\) 处大于或等于相应拉格朗日函数值. 根据这个原则, 我们要求 \\(\\lambda \\geqslant 0\\)."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "170"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.156,
                0.825,
                0.193
            ],
            "angle": 0,
            "content": "记 \\(m = |\\mathcal{I}|, p = |\\mathcal{E}|\\)，则拉格朗日函数的具体形式 \\(L: \\mathbb{R}^n \\times \\mathbb{R}_+^m \\times \\mathbb{R}^p \\to \\mathbb{R}\\) 定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.383,
                0.196,
                0.826,
                0.226
            ],
            "angle": 0,
            "content": "\\[\nL (x, \\lambda , \\nu) = f (x) + \\sum_ {i \\in \\mathcal {I}} \\lambda_ {i} c _ {i} (x) + \\sum_ {i \\in \\mathcal {E}} \\nu_ {i} c _ {i} (x). \\tag {5.4.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.231,
                0.825,
                0.268
            ],
            "angle": 0,
            "content": "注意，函数 (5.4.2) 中的加号也可以修改为减号，同时调整相应乘子的约束条件使得上述下界原则满足即可。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.272,
                0.826,
                0.31
            ],
            "angle": 0,
            "content": "对拉格朗日函数 \\(L(x,\\lambda ,\\nu)\\) 中的 \\(\\mathcal{X}\\) 取下确界可定义拉格朗日对偶函数，这一函数将在对偶理论中起到很关键的作用."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.321,
                0.825,
                0.359
            ],
            "angle": 0,
            "content": "定义5.4 拉格朗日对偶函数 \\(g: \\mathbb{R}_+^m \\times \\mathbb{R}^p \\to [-\\infty, +\\infty)\\) 是拉格朗日函数 \\(L(x, \\lambda, \\nu)\\) 对于 \\(\\lambda \\in \\mathbb{R}_+^m, \\nu \\in \\mathbb{R}^p\\) 关于 \\(x\\) 取的下确界："
        },
        {
            "type": "equation",
            "bbox": [
                0.448,
                0.372,
                0.825,
                0.395
            ],
            "angle": 0,
            "content": "\\[\ng (\\lambda , \\nu) = \\inf  _ {x \\in \\mathbb {R} ^ {n}} L (x, \\lambda , \\nu). \\tag {5.4.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.402,
                0.825,
                0.501
            ],
            "angle": 0,
            "content": "固定 \\((\\lambda, \\nu)\\), 如果拉格朗日函数关于 \\(x\\) 无界, 那么对偶函数在 \\((\\lambda, \\nu)\\) 处的取值为 \\(-\\infty\\). 因为拉格朗日对偶函数是逐点定义的一族关于 \\((\\lambda, \\nu)\\) 的仿射函数的下确界, 根据定理 2.13的(5)可知其为凹函数 (无论原始问题是否为凸问题). 这个性质是十分重要的, 它能帮助我们推导出许多拥有良好性质的算法."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.506,
                0.825,
                0.544
            ],
            "angle": 0,
            "content": "对每一对满足 \\(\\lambda \\geqslant 0\\) 的乘子对 \\((\\lambda, \\nu)\\)，拉格朗日对偶函数 \\(g(\\lambda, \\nu)\\) 给原优化问题(5.4.1)的最优值 \\(p^*\\) 提供了下界，且该下界依赖于参数 \\(\\lambda\\) 和 \\(\\nu\\) 的选取。"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.555,
                0.825,
                0.592
            ],
            "angle": 0,
            "content": "引理5.1(弱对偶原理)对于任意的 \\(\\lambda \\geqslant 0\\) 和 \\(\\nu\\), 拉格朗日对偶函数给出了优化问题(5.4.1)最优值的一个下界, 即"
        },
        {
            "type": "equation",
            "bbox": [
                0.46,
                0.606,
                0.825,
                0.623
            ],
            "angle": 0,
            "content": "\\[\ng (\\lambda , \\nu) \\leqslant p ^ {*}, \\quad \\lambda \\geqslant 0. \\tag {5.4.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.636,
                0.825,
                0.673
            ],
            "angle": 0,
            "content": "证明．假设 \\(\\tilde{x}\\) 是问题(5.4.1)的一个可行解，即 \\(c_{i}(\\tilde{x})\\leqslant 0,i\\in \\mathcal{I}\\) 和 \\(c_{i}(\\tilde{x}) =\\) \\(0,i\\in \\mathcal{E}\\) 对于任意的 \\(i\\) 均成立．由于 \\(\\lambda \\geqslant 0\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.44,
                0.685,
                0.825,
                0.714
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i \\in \\mathcal {I}} \\lambda_ {i} c _ {i} (\\tilde {x}) + \\sum_ {i \\in \\mathcal {E}} v _ {i} c _ {i} (\\tilde {x}) \\leqslant 0. \\tag {5.4.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.723,
                0.641,
                0.74
            ],
            "angle": 0,
            "content": "将上式代入拉格朗日函数的定义中，我们可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.353,
                0.751,
                0.825,
                0.781
            ],
            "angle": 0,
            "content": "\\[\nL (\\tilde {x}, \\lambda , \\nu) = f (\\tilde {x}) + \\sum_ {i \\in \\mathcal {I}} \\lambda_ {i} c _ {i} (\\tilde {x}) + \\sum_ {i \\in \\mathcal {E}} \\nu_ {i} c _ {i} (\\tilde {x}) \\leqslant f (\\tilde {x}), \\tag {5.4.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.79,
                0.299,
                0.805
            ],
            "angle": 0,
            "content": "并且"
        },
        {
            "type": "equation",
            "bbox": [
                0.383,
                0.809,
                0.825,
                0.832
            ],
            "angle": 0,
            "content": "\\[\ng (\\lambda , \\nu) = \\inf  _ {x} L (x, \\lambda , \\nu) \\leqslant L (\\tilde {x}, \\lambda , \\nu) \\leqslant f (\\tilde {x}). \\tag {5.4.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "所以对于任意的可行解 \\(\\tilde{x}, g(\\lambda, \\nu) \\leqslant f(\\tilde{x})\\) 都成立，从而 \\(g(\\lambda, \\nu) \\leqslant p^{*}\\) 成立."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.282,
                0.133
            ],
            "angle": 0,
            "content": "5.4 对偶理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.735,
                0.131
            ],
            "angle": 0,
            "content": "171"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.737,
                0.195
            ],
            "angle": 0,
            "content": "那么一个自然的问题是，从拉格朗日对偶函数获得的下界中，哪个是最优的呢？为了求解该最优的下界，便有如下拉格朗日对偶问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.209,
                0.737,
                0.234
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {\\lambda \\geqslant 0, \\nu} g (\\lambda , \\nu) = \\max  _ {\\lambda \\geqslant 0, \\nu} \\inf  _ {x \\in \\mathbb {R} ^ {n}} L (x, \\lambda , \\nu). \\tag {5.4.8}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.244,
                0.739,
                0.345
            ],
            "angle": 0,
            "content": "向量 \\(\\lambda\\) 和 \\(\\nu\\) 也称为问题(5.4.1)的对偶变量或者拉格朗日乘子向量．由于其目标函数的凹性和约束集合的凸性，拉格朗日对偶问题是一个凸优化问题（见注1.1).当 \\(g(\\lambda ,\\nu) = -\\infty\\) 时，对偶函数提供的 \\(p^*\\) 的下界变得没有实际意义只有当 \\(g(\\lambda ,\\nu) > - \\infty\\) 时，对偶函数生成的关于原始问题最优解 \\(p^*\\) 的下界才是非平凡的．因此我们规定拉格朗日对偶函数的定义域"
        },
        {
            "type": "equation",
            "bbox": [
                0.302,
                0.359,
                0.605,
                0.379
            ],
            "angle": 0,
            "content": "\\[\n\\mathbf {d o m} g = \\left\\{\\left(\\lambda , \\nu\\right) \\mid \\lambda \\geqslant 0, g (\\lambda , \\nu) > - \\infty \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.391,
                0.741,
                0.472
            ],
            "angle": 0,
            "content": "当 \\((\\lambda, \\nu) \\in \\mathbf{dom} g\\) 时，称其为对偶可行解。记对偶问题的最优值为 \\(q^{*}\\)，称 \\(p^{*} - q^{*} (\\geqslant 0)\\) 为对偶间隙。如果对偶间隙为 \\(0(p^{*} = q^{*})\\)，称强对偶原理成立。假设 \\((\\lambda^{*}, \\nu^{*})\\) 是使得对偶问题取得最优值的解，称其为对偶最优解或者最优拉格朗日乘子。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.474,
                0.739,
                0.573
            ],
            "angle": 0,
            "content": "推导拉格朗日对偶问题最重要的是能把拉格朗日对偶函数的具体形式方便地写出来。需要指出的是，拉格朗日对偶问题的写法并不唯一。如果问题(5.4.1)中有些约束，比如对应于下标集 \\(\\mathcal{I}_1 = \\{i_1, i_2, \\dots, i_q\\}\\) 的不等式约束，比较简单，则可以不把这些约束松弛到拉格朗日函数里。此时拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.18,
                0.587,
                0.738,
                0.62
            ],
            "angle": 0,
            "content": "\\[\nL (x, s, \\nu) = f (x) + \\sum_ {i \\in \\mathcal {I} \\backslash \\mathcal {I} _ {1}} s _ {i} c _ {i} (x) + \\sum_ {i \\in \\mathcal {E}} \\nu_ {i} c _ {i} (x), s \\geqslant 0, c _ {i} (x) \\leqslant 0, i \\in \\mathcal {I} _ {1}, \\tag {5.4.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.631,
                0.332,
                0.647
            ],
            "angle": 0,
            "content": "相应地，对偶问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.294,
                0.656,
                0.737,
                0.693
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {s \\geqslant 0, \\nu} \\left\\{\\inf  _ {x \\in \\mathbb {R} ^ {n}} L (x, s, \\nu), \\text {s . t .} c _ {i} (x) \\leqslant 0, i \\in \\mathcal {I} _ {1} \\right\\}. \\tag {5.4.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.701,
                0.592,
                0.718
            ],
            "angle": 0,
            "content": "需要指出的是，不同写法的拉格朗日对偶问题是等价的。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.743,
                0.53,
                0.762
            ],
            "angle": 0,
            "content": "5.4.2 带广义不等式约束优化问题的对偶"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "问题 (5.4.1) 中的不等式约束 \\(c_{i}(x), i \\in \\mathcal{I}\\) 都是实值函数的形式。在许多实际应用中，我们还会遇到大量带广义不等式约束的优化问题，例如自变量 \\(x\\) 可能取值于半正定矩阵空间中。对于这类约束我们不易将其化为 \\(c_{i}(x) \\leqslant 0\\) 的形式，此时又该如何构造拉格朗日对偶函数呢？"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "172"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.157,
                0.449,
                0.174
            ],
            "angle": 0,
            "content": "1. 适当锥和广义不等式"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.187,
                0.594,
                0.204
            ],
            "angle": 0,
            "content": "定义广义不等式需要利用适当锥的概念"
        },
        {
            "type": "text",
            "bbox": [
                0.291,
                0.218,
                0.805,
                0.235
            ],
            "angle": 0,
            "content": "定义5.5(适当锥）称满足如下条件的锥 \\(K\\) 为适当锥（propercone)："
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.248,
                0.378,
                0.264
            ],
            "angle": 0,
            "content": "(1) \\(K\\) 是凸锥；"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.279,
                0.379,
                0.295
            ],
            "angle": 0,
            "content": "(2) \\(K\\) 是闭集；"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.31,
                0.57,
                0.326
            ],
            "angle": 0,
            "content": "(3) \\(K\\) 是实心的 (solid)，即 \\(\\mathbf{int}K\\neq \\emptyset\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.34,
                0.825,
                0.377
            ],
            "angle": 0,
            "content": "(4) \\(K\\) 是尖的 (pointed), 即对任意非零向量 \\(x\\), 若 \\(x \\in K\\), 则 \\(-x \\notin K\\), 也即 \\(K\\) 中无法容纳直线."
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.248,
                0.825,
                0.377
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.291,
                0.392,
                0.789,
                0.409
            ],
            "angle": 0,
            "content": "适当锥 \\(K\\) 可以诱导出广义不等式, 它定义了全空间上的偏序关系:"
        },
        {
            "type": "equation",
            "bbox": [
                0.451,
                0.426,
                0.632,
                0.442
            ],
            "angle": 0,
            "content": "\\[\nx \\preceq_ {K} y \\iff y - x \\in K.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.458,
                0.529,
                0.475
            ],
            "angle": 0,
            "content": "类似地，可以定义严格广义不等式："
        },
        {
            "type": "equation",
            "bbox": [
                0.438,
                0.492,
                0.644,
                0.509
            ],
            "angle": 0,
            "content": "\\[\nx \\prec_ {K} y \\iff y - x \\in \\operatorname {i n t} K.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.524,
                0.827,
                0.583
            ],
            "angle": 0,
            "content": "本书常用的是 \\(K = \\mathbb{R}_{+}^{n}\\) （非负锥）和 \\(K = S_{+}^{n}\\) （半正定锥）．当 \\(K = \\mathbb{R}_{+}^{n}\\) 时，\\(x \\preceq_{K} y\\) 是我们之前经常使用的记号 \\(x \\leqslant y\\) ，即 \\(x\\) 每个分量小于等于 \\(y\\) 的对应分量；当 \\(K = S_{+}^{n}\\) 时，\\(X \\preceq_{K} Y\\) 的含义为 \\(Y - X \\succeq 0\\) ，即 \\(Y - X\\) 是半正定矩阵."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.586,
                0.825,
                0.623
            ],
            "angle": 0,
            "content": "广义不等式满足许多我们熟悉的性质，例如自反性、反对称性、传递性，这里不详细展开."
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.648,
                0.347,
                0.664
            ],
            "angle": 0,
            "content": "2. 对偶锥"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.678,
                0.825,
                0.756
            ],
            "angle": 0,
            "content": "在构造拉格朗日对偶函数时，针对不等式约束 \\(c_{i}(x)\\leqslant 0\\) 我们引入拉格朗日乘子 \\(\\lambda_{i}\\geqslant 0\\) ，之后将 \\(\\lambda_{i}c_{i}(x)\\) （ \\(\\leqslant 0\\) ）作为拉格朗日函数中的一项．那么对于广义不等式，应该如何对拉格朗日乘子提出限制呢？此时需要借助对偶锥的概念."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.771,
                0.688,
                0.787
            ],
            "angle": 0,
            "content": "定义5.6（对偶锥）令 \\(K\\) 为全空间 \\(\\Omega\\) 的子集，称集合"
        },
        {
            "type": "equation",
            "bbox": [
                0.405,
                0.803,
                0.677,
                0.822
            ],
            "angle": 0,
            "content": "\\[\nK ^ {*} = \\{y \\in \\Omega \\mid \\langle x, y \\rangle \\geqslant 0, \\quad \\forall x \\in K \\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.836,
                0.591,
                0.853
            ],
            "angle": 0,
            "content": "为其对偶锥，其中 \\(\\langle \\cdot ,\\cdot \\rangle\\) 是 \\(\\Omega\\) 上的一个内积"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.283,
                0.133
            ],
            "angle": 0,
            "content": "5.4 对偶理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "173"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.157,
                0.744,
                0.238
            ],
            "angle": 0,
            "content": "正如其定义所说，对偶锥是一个锥（哪怕原始集合 \\(K\\) 不是锥）。我们在图5.3中给出了 \\(\\mathbb{R}^2\\) 平面上的一个例子。图中深色区域表示锥 \\(K\\)，根据对偶锥的定义，\\(K^*\\) 中的向量和 \\(K\\) 中所有向量夹角均为锐角或直角。因此，对偶锥 \\(K^*\\) 为图5.3的浅色区域。注意，在这个例子中 \\(K\\) 也为 \\(K^*\\) 一部分。"
        },
        {
            "type": "image",
            "bbox": [
                0.283,
                0.248,
                0.627,
                0.455
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.3,
                0.465,
                0.607,
                0.483
            ],
            "angle": 0,
            "content": "图5.3 \\(\\mathbb{R}^2\\) 平面上的锥 \\(K\\) 及其对偶锥 \\(K^{*}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.505,
                0.741,
                0.545
            ],
            "angle": 0,
            "content": "如果 \\(K = \\mathbb{R}_{+}^{n},\\Omega = \\mathbb{R}^{n}\\) 并且定义 \\(\\langle x,y\\rangle = x^{\\mathrm{T}}y\\) ，那么易知 \\(K^{*} = \\mathbb{R}_{+}^{n}\\) 假设\\(K = \\mathcal{S}_{+}^{n},\\Omega = \\mathcal{S}^{n}\\) 并且定义 \\(\\langle X,Y\\rangle = \\operatorname {Tr}(XY^{\\mathrm{T}})\\) ，可以证明（见习题5.3）"
        },
        {
            "type": "equation",
            "bbox": [
                0.308,
                0.56,
                0.602,
                0.58
            ],
            "angle": 0,
            "content": "\\[\n\\langle X, Y \\rangle \\geqslant 0, \\forall X \\in \\mathcal {S} _ {+} ^ {n} \\iff Y \\in \\mathcal {S} _ {+} ^ {n},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.594,
                0.739,
                0.632
            ],
            "angle": 0,
            "content": "即半正定锥的对偶锥仍为半正定锥。此外，称满足 \\(K = K^{*}\\) 的锥 \\(K\\) 为自对偶锥，因此非负锥和半正定锥都是自对偶锥。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.636,
                0.739,
                0.674
            ],
            "angle": 0,
            "content": "直观来说，对偶锥 \\(K^{*}\\) 中向量和原锥 \\(K\\) 中向量的内积恒非负，这一性质可以被用来构造拉格朗日对偶函数."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.7,
                0.55,
                0.718
            ],
            "angle": 0,
            "content": "3. 广义不等式约束优化问题拉格朗日函数的构造"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.73,
                0.741,
                0.768
            ],
            "angle": 0,
            "content": "如果将不等式约束函数换成向量函数，并且推广定义相应的广义不等式约束，我们可以得到如下形式的优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.361,
                0.783,
                0.455,
                0.806
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.809,
                0.737,
                0.83
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad c _ {i} (x) \\preceq_ {K _ {i}} 0, i \\in \\mathcal {I}, \\tag {5.4.11}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.41,
                0.836,
                0.534,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nc _ {i} (x) = 0, i \\in \\mathcal {E},\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "174"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.156,
                0.825,
                0.234
            ],
            "angle": 0,
            "content": "其中 \\(f: \\mathbb{R}^n \\to \\mathbb{R}, c_i: \\mathbb{R}^n \\to \\mathbb{R}, i \\in \\mathcal{E}\\) 为实值函数, \\(c_i: \\mathbb{R}^n \\to \\mathbb{R}^{k_i}, k_i \\in \\mathbb{N}_+, i \\in \\mathcal{I}\\) 为向量值函数, \\(K_i\\) 为某种适当锥且 \\(\\preceq_{K_i}\\) 表示由锥 \\(K_i\\) 定义的广义不等式. 因此, 问题 (5.4.1) 是在问题 (5.4.11) 中取 \\(k_i = 1\\), \\(K_i = \\mathbb{R}_+\\), \\(\\forall i \\in \\mathcal{I}\\) 时的特殊情形."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.24,
                0.825,
                0.277
            ],
            "angle": 0,
            "content": "根据 \\(K_{i}, i \\in \\mathcal{I}\\) 的对偶锥 \\(K_{i}^{*}\\)，我们对广义不等式约束分别引入乘子 \\(\\lambda_{i} \\in K_{i}^{*}, i \\in \\mathcal{I}\\)，对等式约束引入乘子 \\(\\nu_{i} \\in \\mathbb{R}, i \\in \\mathcal{E}\\)，构造如下拉格朗日函数："
        },
        {
            "type": "equation",
            "bbox": [
                0.307,
                0.29,
                0.776,
                0.32
            ],
            "angle": 0,
            "content": "\\[\nL (x, \\lambda , \\nu) = f (x) + \\sum_ {i \\in \\mathcal {I}} \\left\\langle c _ {i} (x), \\lambda_ {i} \\right\\rangle + \\sum_ {i \\in \\mathcal {E}} \\nu_ {i} c _ {i} (x), \\quad \\lambda_ {i} \\in K _ {i} ^ {*}, \\nu_ {i} \\in \\mathbb {R}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.33,
                0.825,
                0.368
            ],
            "angle": 0,
            "content": "容易验证 \\(L(x,\\lambda ,\\nu)\\leqslant f(x),\\forall x\\in \\mathcal{X},\\lambda_i\\in K_i^*,\\nu_i\\in \\mathbb{R}\\) ，类似于(5.4.3)式，式我们定义拉格朗日对偶函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.448,
                0.382,
                0.632,
                0.406
            ],
            "angle": 0,
            "content": "\\[\ng (\\lambda , \\nu) = \\inf  _ {x \\in \\mathbb {R} ^ {n}} L (x, \\lambda , \\nu).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.415,
                0.4,
                0.431
            ],
            "angle": 0,
            "content": "因此，对偶问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.469,
                0.434,
                0.613,
                0.459
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {\\lambda_ {i} \\in K _ {i} ^ {*},   \\nu_ {i} \\in \\mathbb {R}} g (\\lambda , \\nu).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.465,
                0.825,
                0.606
            ],
            "angle": 0,
            "content": "对偶问题在最优化理论中扮演着重要角色。每个优化问题都对应一个对偶问题。相比原始问题，对偶问题总是凸的，其最优值给出了原始问题最优值（极小化问题）的一个下界。如果原始问题满足一定的条件，我们可以从理论上证明原始问题和对偶问题的最优值是相等的。当原始问题的约束个数比决策变量维数更小时，对偶问题的决策变量维数会比原始问题的小，从而可能在相对较小的决策空间中求解。因此，对于对偶问题的研究非常必要。接下来我们会给出一些例子来说明如何求出给定问题的对偶问题。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.631,
                0.362,
                0.649
            ],
            "angle": 0,
            "content": "5.4.3 实例"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.662,
                0.825,
                0.7
            ],
            "angle": 0,
            "content": "这一小节用四个例子说明拉格朗日对偶问题应当如何计算，并简要从对偶理论的角度分析这些问题具有的性质."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.723,
                0.448,
                0.74
            ],
            "angle": 0,
            "content": "1. 线性规划问题的对偶"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.754,
                0.476,
                0.77
            ],
            "angle": 0,
            "content": "考虑如下线性规划问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.487,
                0.781,
                0.568,
                0.805
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} c ^ {\\mathrm {T}} x,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.496,
                0.81,
                0.824,
                0.827
            ],
            "angle": 0,
            "content": "\\[\n\\text {s . t .} \\quad A x = b, \\tag {5.4.12}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.536,
                0.838,
                0.582,
                0.852
            ],
            "angle": 0,
            "content": "\\[\nx \\geqslant 0.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.282,
                0.133
            ],
            "angle": 0,
            "content": "5.4 对偶理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "175"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.195
            ],
            "angle": 0,
            "content": "对于等式约束 \\(Ax = b\\) ，我们引入拉格朗日乘子 \\(\\nu\\) ；对于非负约束 \\(x\\geqslant 0\\) ，我们引入拉格朗日乘子 \\(s\\geqslant 0\\) .根据上述准则，可构造如下拉格朗日函数："
        },
        {
            "type": "equation",
            "bbox": [
                0.217,
                0.21,
                0.691,
                0.23
            ],
            "angle": 0,
            "content": "\\[\nL (x, s, \\nu) = c ^ {\\mathrm {T}} x + \\nu^ {\\mathrm {T}} (A x - b) - s ^ {\\mathrm {T}} x = - b ^ {\\mathrm {T}} \\nu + (A ^ {\\mathrm {T}} \\nu - s + c) ^ {\\mathrm {T}} x,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.246,
                0.347,
                0.263
            ],
            "angle": 0,
            "content": "其拉格朗日对偶函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.25,
                0.274,
                0.655,
                0.327
            ],
            "angle": 0,
            "content": "\\[\ng (s, \\nu) = \\inf  _ {x} L (x, s, \\nu) = \\left\\{ \\begin{array}{l l} {- b ^ {\\mathrm {T}} \\nu ,} & {\\quad A ^ {\\mathrm {T}} \\nu - s + c = 0,} \\\\ {- \\infty ,} & {\\quad \\text {其 他}.} \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.339,
                0.737,
                0.378
            ],
            "angle": 0,
            "content": "注意到只需考虑 \\(A^{\\mathrm{T}} \\nu - s + c = 0\\) 情形，其余情况对应于不可行情形，因此线性规划问题(5.4.12)的对偶问题是"
        },
        {
            "type": "equation",
            "bbox": [
                0.361,
                0.39,
                0.543,
                0.463
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\max  _ {s, \\nu} \\quad - b ^ {\\mathrm {T}} \\nu , \\\\ \\begin{array}{l l} \\text {s . t .} & A ^ {\\mathrm {T}} \\nu - s + c = 0, \\end{array} \\\\ s \\geqslant 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.478,
                0.583,
                0.495
            ],
            "angle": 0,
            "content": "经过变量代换 \\(y = -\\nu\\) 后，上述问题等价于常见的形式"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.509,
                0.737,
                0.582
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\max  _ {s, y} \\quad b ^ {\\mathrm {T}} y, \\\\ \\begin{array}{l l} \\text {s . t .} & A ^ {\\mathrm {T}} y + s = c, \\end{array} \\tag {5.4.13} \\\\ s \\geqslant 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.598,
                0.522,
                0.615
            ],
            "angle": 0,
            "content": "如果问题(5.4.12)的拉格朗日函数直接写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.225,
                0.63,
                0.682,
                0.65
            ],
            "angle": 0,
            "content": "\\[\nL (x, s, y) = c ^ {\\mathrm {T}} x - y ^ {\\mathrm {T}} (A x - b) - s ^ {\\mathrm {T}} x = b ^ {\\mathrm {T}} y + \\left(c - A ^ {\\mathrm {T}} y - s\\right) ^ {\\mathrm {T}} x,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.666,
                0.737,
                0.703
            ],
            "angle": 0,
            "content": "其中 \\(y\\) 为等式约束 \\(Ax = b\\) 的乘子以及 \\(s \\geqslant 0\\) 为非负约束 \\(x \\geqslant 0\\) 的乘子，则对偶问题(5.4.13)可以直接导出."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.708,
                0.737,
                0.746
            ],
            "angle": 0,
            "content": "线性规划问题(5.4.12)的对偶问题也可以通过保留约束 \\(x \\geqslant 0\\) 写出。对于等式约束 \\(Ax = b\\)，引入乘子 \\(y\\)，则相应的拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.268,
                0.761,
                0.639,
                0.781
            ],
            "angle": 0,
            "content": "\\[\nL (x, y) = c ^ {\\mathrm {T}} x - y ^ {\\mathrm {T}} (A x - b) = b ^ {\\mathrm {T}} y + (c - A ^ {\\mathrm {T}} y) ^ {\\mathrm {T}} x.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.797,
                0.473,
                0.813
            ],
            "angle": 0,
            "content": "而对偶问题需要将 \\(x \\geqslant 0\\) 添加到约束里："
        },
        {
            "type": "equation",
            "bbox": [
                0.283,
                0.826,
                0.624,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {y} \\left\\{\\inf  _ {x} b ^ {\\mathrm {T}} y + (c - A ^ {\\mathrm {T}} y) ^ {\\mathrm {T}} x, \\quad \\text {s . t .} \\quad x \\geqslant 0 \\right\\}.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "176"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.357,
                0.173
            ],
            "angle": 0,
            "content": "简化后得出："
        },
        {
            "type": "equation",
            "bbox": [
                0.48,
                0.176,
                0.825,
                0.208
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {y} b ^ {\\mathrm {T}} y, \\tag {5.4.14}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.497,
                0.209,
                0.603,
                0.226
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A ^ {\\mathrm {T}} y \\leqslant c. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.236,
                0.701,
                0.252
            ],
            "angle": 0,
            "content": "事实上，由对偶问题(5.4.13)也可以消掉变量 \\(s\\) 得到(5.4.14)."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.257,
                0.825,
                0.336
            ],
            "angle": 0,
            "content": "下面我们推导问题(5.4.14)的对偶问题。先通过极小化目标函数的相反数将其等价地转化为如下极小化问题（另一种方式是直接构造极大化问题的拉格朗日函数，通过引入乘子并确定其符号使得构造的拉格朗日函数为 \\(b^{\\mathrm{T}}y\\) 的一个上界，之后再求拉格朗日函数关于 \\(x\\) 的上确界得对偶函数）："
        },
        {
            "type": "equation",
            "bbox": [
                0.482,
                0.351,
                0.589,
                0.377
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {y} \\quad - b ^ {\\mathrm {T}} y,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.493,
                0.381,
                0.601,
                0.402
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A ^ {\\mathrm {T}} y \\leqslant c. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.417,
                0.825,
                0.454
            ],
            "angle": 0,
            "content": "对于不等式约束 \\(A^{\\mathrm{T}}y \\leqslant c\\)，我们引入拉格朗日乘子 \\(x \\geqslant 0\\)，则相应的拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.342,
                0.47,
                0.741,
                0.49
            ],
            "angle": 0,
            "content": "\\[\nL (y, x) = - b ^ {\\mathrm {T}} y + x ^ {\\mathrm {T}} \\left(A ^ {\\mathrm {T}} y - c\\right) = - c ^ {\\mathrm {T}} x + \\left(A x - b\\right) ^ {\\mathrm {T}} y.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.507,
                0.402,
                0.523
            ],
            "angle": 0,
            "content": "因此得到对偶函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.392,
                0.536,
                0.688,
                0.587
            ],
            "angle": 0,
            "content": "\\[\ng (x) = \\inf  _ {y} L (y, x) = \\left\\{ \\begin{array}{l l} {- c ^ {\\mathrm {T}} x,} & {A x = b,} \\\\ {- \\infty ,} & {\\text {其 他},} \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.6,
                0.401,
                0.616
            ],
            "angle": 0,
            "content": "相应的对偶问题是"
        },
        {
            "type": "equation",
            "bbox": [
                0.485,
                0.619,
                0.593,
                0.643
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {x} \\quad - c ^ {\\mathrm {T}} x,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.497,
                0.646,
                0.825,
                0.665
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\text {s . t .} A x = b, \\end{array} \\tag {5.4.15}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.536,
                0.674,
                0.585,
                0.689
            ],
            "angle": 0,
            "content": "\\[\nx \\geqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.701,
                0.825,
                0.738
            ],
            "angle": 0,
            "content": "观察到问题 (5.4.15) 与问题 (5.4.12) 完全等价，这表明线性规划问题与其对偶问题互为对偶。"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.767,
                0.45,
                0.784
            ],
            "angle": 0,
            "content": "2. \\(\\ell_{1}\\) 正则化问题的对偶"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.798,
                0.442,
                0.814
            ],
            "angle": 0,
            "content": "对于 \\(\\ell_1\\) 正则化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.825,
                0.825,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\frac {1}{2} \\| A x - b \\| ^ {2} + \\mu \\| x \\| _ {1}, \\tag {5.4.16}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.282,
                0.133
            ],
            "angle": 0,
            "content": "5.4 对偶理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "177"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.195
            ],
            "angle": 0,
            "content": "其中 \\(A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^{m}\\) 分别为给定的矩阵和向量, \\(\\mu\\) 为正则化参数来控制稀疏度. 通过引入 \\(Ax - b = r\\), 可以将问题 (5.4.16) 转化为如下等价的形式:"
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.206,
                0.737,
                0.236
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\frac {1}{2} \\| r \\| ^ {2} + \\mu \\| x \\| _ {1}, \\quad \\text {s . t .} \\quad A x - b = r, \\tag {5.4.17}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.247,
                0.314,
                0.264
            ],
            "angle": 0,
            "content": "其拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.259,
                0.273,
                0.647,
                0.338
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} L (x, r, \\lambda) = \\frac {1}{2} \\| r \\| ^ {2} + \\mu \\| x \\| _ {1} - \\langle \\lambda , A x - b - r \\rangle \\\\ = \\frac {1}{2} \\| r \\| ^ {2} + \\lambda^ {\\mathrm {T}} r + \\mu \\| x \\| _ {1} - (A ^ {\\mathrm {T}} \\lambda) ^ {\\mathrm {T}} x + b ^ {\\mathrm {T}} \\lambda . \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.348,
                0.65,
                0.365
            ],
            "angle": 0,
            "content": "利用二次函数最小值的性质及 \\(\\| \\cdot \\| _1\\) 的对偶范数的定义，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.247,
                0.377,
                0.657,
                0.43
            ],
            "angle": 0,
            "content": "\\[\ng (\\lambda) = \\inf  _ {x, r} L (x, r, \\lambda) = \\left\\{ \\begin{array}{l l} b ^ {\\mathrm {T}} \\lambda - \\frac {1}{2} \\| \\lambda \\| ^ {2}, & \\| A ^ {\\mathrm {T}} \\lambda \\| _ {\\infty} \\leqslant \\mu , \\\\ - \\infty , & \\text {其 他}. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.443,
                0.295,
                0.459
            ],
            "angle": 0,
            "content": "那么对偶问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.296,
                0.47,
                0.611,
                0.501
            ],
            "angle": 0,
            "content": "\\[\n\\max  \\quad b ^ {\\mathrm {T}} \\lambda - \\frac {1}{2} \\| \\lambda \\| ^ {2}, \\quad \\text {s . t .} \\quad \\| A ^ {\\mathrm {T}} \\lambda \\| _ {\\infty} \\leqslant \\mu .\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.521,
                0.395,
                0.538
            ],
            "angle": 0,
            "content": "3. 半定规划问题的对偶问题"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.551,
                0.434,
                0.568
            ],
            "angle": 0,
            "content": "考虑标准形式的半定规划问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.582,
                0.431,
                0.605
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in S ^ {n}} \\langle C, X \\rangle ,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.607,
                0.737,
                0.63
            ],
            "angle": 0,
            "content": "\\[\n\\text {s . t .} \\quad \\langle A _ {i}, X \\rangle = b _ {i}, i = 1, 2, \\dots , m, \\tag {5.4.18}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.636,
                0.428,
                0.654
            ],
            "angle": 0,
            "content": "\\[\nX \\succeq 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.667,
                0.737,
                0.706
            ],
            "angle": 0,
            "content": "其中 \\(A_{i}\\in \\mathcal{S}^{n},i = 1,2,\\dots ,m,C\\in \\mathcal{S}^{n},b\\in \\mathbb{R}^{m}\\) ，对于等式约束和半正定锥约束分别引入乘子 \\(y\\in \\mathbb{R}^m\\) 和 \\(S\\in S_{+}^{n}\\) ，拉格朗日函数可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.236,
                0.718,
                0.672,
                0.755
            ],
            "angle": 0,
            "content": "\\[\nL (X, y, S) = \\langle C, X \\rangle - \\sum_ {i = 1} ^ {m} y _ {i} (\\langle A _ {i}, X \\rangle - b _ {i}) - \\langle S, X \\rangle , \\quad S \\succeq 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.766,
                0.279,
                0.783
            ],
            "angle": 0,
            "content": "则对偶函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.245,
                0.794,
                0.66,
                0.855
            ],
            "angle": 0,
            "content": "\\[\ng (y, S) = \\inf  _ {X} L (X, y, S) = \\left\\{ \\begin{array}{l l} b ^ {\\mathrm {T}} y, & \\sum_ {i = 1} ^ {m} y _ {i} A _ {i} - C + S = 0, \\\\ - \\infty , & \\text {其 他}. \\end{array} \\right.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "178"
        },
        {
            "type": "header",
            "bbox": [
                0.667,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.157,
                0.4,
                0.173
            ],
            "angle": 0,
            "content": "因此，对偶问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.437,
                0.174,
                0.543,
                0.199
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {y \\in \\mathbb {R} ^ {m}} - b ^ {\\mathrm {T}} y,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.448,
                0.204,
                0.824,
                0.24
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad \\sum_ {i = 1} ^ {m} y _ {i} A _ {i} - C + S = 0, \\tag {5.4.19}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.488,
                0.248,
                0.536,
                0.263
            ],
            "angle": 0,
            "content": "\\[\nS \\succeq 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.271,
                0.453,
                0.287
            ],
            "angle": 0,
            "content": "它也可以写成不等式形式"
        },
        {
            "type": "equation",
            "bbox": [
                0.466,
                0.298,
                0.571,
                0.323
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {y \\in \\mathbb {R} ^ {m}} - b ^ {\\mathrm {T}} y,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.477,
                0.323,
                0.824,
                0.364
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad \\sum_ {i = 1} ^ {m} y _ {i} A _ {i} \\preceq C. \\tag {5.4.20}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.374,
                0.825,
                0.41
            ],
            "angle": 0,
            "content": "对于对偶问题(5.4.20)，我们还可以求其对偶问题．对不等式约束引入乘子 \\(X\\in S^n\\) 并且 \\(X\\succeq 0\\) ，拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.418,
                0.692,
                0.5
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} L (y, X) = - b ^ {\\mathrm {T}} y + \\langle X, \\left(\\sum_ {i = 1} ^ {m} y _ {i} A _ {i}\\right) - C \\rangle , \\\\ = \\sum_ {i = 1} ^ {m} y _ {i} (- b _ {i} + \\langle A _ {i}, X \\rangle) - \\langle C, X \\rangle . \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.51,
                0.625,
                0.526
            ],
            "angle": 0,
            "content": "因为上式对 \\(y\\) 是仿射的，故对偶函数可以描述为"
        },
        {
            "type": "equation",
            "bbox": [
                0.306,
                0.536,
                0.776,
                0.588
            ],
            "angle": 0,
            "content": "\\[\ng (X) = \\inf  _ {y} L (y, X) = \\left\\{ \\begin{array}{l l} {- \\langle C, X \\rangle ,} & {\\langle A _ {i}, X \\rangle = b _ {i}, i = 1, 2, \\dots , m,} \\\\ {- \\infty ,} & {\\text {其 他}.} \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.597,
                0.454,
                0.613
            ],
            "angle": 0,
            "content": "因此，对偶问题可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.413,
                0.625,
                0.518,
                0.648
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in S ^ {n}} \\langle C, X \\rangle ,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.425,
                0.652,
                0.825,
                0.672
            ],
            "angle": 0,
            "content": "\\[\n\\text {s . t .} \\quad \\langle A _ {i}, X \\rangle = b _ {i}, i = 1, 2, \\dots , m, \\tag {5.4.21}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.464,
                0.68,
                0.516,
                0.696
            ],
            "angle": 0,
            "content": "\\[\nX \\succeq 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.708,
                0.721,
                0.724
            ],
            "angle": 0,
            "content": "这就是问题 (5.4.18)，即半定规划问题与其对偶问题互为对偶。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.748,
                0.38,
                0.764
            ],
            "angle": 0,
            "content": "4. 最大割问题"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.778,
                0.471,
                0.794
            ],
            "angle": 0,
            "content": "考虑最大割问题(4.5.6):"
        },
        {
            "type": "equation",
            "bbox": [
                0.435,
                0.805,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\max  _ {x \\in \\mathbb {R} ^ {n}} x ^ {\\mathrm {T}} C x, \\tag {5.4.22} \\\\ \\begin{array}{l l} \\text {s . t .} & x _ {i} ^ {2} = 1, i = 1, 2, \\dots , n, \\end{array} \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.5 一般约束优化问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "179"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.195
            ],
            "angle": 0,
            "content": "其中 \\(C \\in \\mathbb{R}^{n \\times n}\\) 为图的拉普拉斯矩阵. 这里, \\(x_{i}^{2} = 1\\) 表明 \\(x_{i} = 1\\) 或者 \\(x_{i} = -1\\). 引入拉格朗日乘子 \\(y \\in \\mathbb{R}^{n}\\), 拉格朗日函数可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.226,
                0.201,
                0.685,
                0.238
            ],
            "angle": 0,
            "content": "\\[\nL (x, y) = - x ^ {\\mathrm {T}} C x + \\sum_ {i = 1} ^ {n} y _ {i} \\left(x _ {i} ^ {2} - 1\\right) = x ^ {\\mathrm {T}} (\\operatorname {D i a g} (y) - C) x - \\mathbf {1} ^ {\\mathrm {T}} y,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.243,
                0.281,
                0.26
            ],
            "angle": 0,
            "content": "则对偶函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.27,
                0.265,
                0.637,
                0.317
            ],
            "angle": 0,
            "content": "\\[\ng (y) = \\inf  _ {x} L (x, y) = \\left\\{ \\begin{array}{l l} {- {\\bf 1} ^ {\\mathrm {T}} y,} & {\\mathrm {D i a g} (y) - C \\succeq 0,} \\\\ {- \\infty ,} & {\\text {其 他}.} \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.322,
                0.314,
                0.339
            ],
            "angle": 0,
            "content": "因此，对偶问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.362,
                0.338,
                0.737,
                0.371
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {y \\in \\mathbb {R} ^ {n}} \\mathbf {1} ^ {\\mathrm {T}} y \\tag {5.4.23}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.373,
                0.372,
                0.543,
                0.39
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & \\operatorname {D i a g} (y) - C \\succeq 0. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.394,
                0.354,
                0.41
            ],
            "angle": 0,
            "content": "这是一个半定规划问题"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.415,
                0.737,
                0.452
            ],
            "angle": 0,
            "content": "对于对偶问题(5.4.23)，我们还可以求其对偶问题。对于半正定约束，引入拉格朗日乘子 \\(X\\succeq 0\\) ，拉格朗日函数可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.23,
                0.457,
                0.679,
                0.494
            ],
            "angle": 0,
            "content": "\\[\nL (y, X) = \\mathbf {1} ^ {\\mathrm {T}} y - \\left\\langle \\operatorname {D i a g} (y) - C, X \\right\\rangle = \\sum_ {i = 1} ^ {n} (1 - X _ {i i}) y _ {i} + \\left\\langle C, X \\right\\rangle .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.499,
                0.281,
                0.516
            ],
            "angle": 0,
            "content": "则对偶函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.246,
                0.522,
                0.66,
                0.573
            ],
            "angle": 0,
            "content": "\\[\ng (X) = \\inf  _ {y} L (y, X) = \\left\\{ \\begin{array}{l l} {\\langle C, X \\rangle ,} & {X _ {i i} = 1,   i = 1, 2, \\dots , n,} \\\\ {- \\infty ,} & {\\text {其 他}.} \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.578,
                0.421,
                0.595
            ],
            "angle": 0,
            "content": "因此，问题(5.4.23)的对偶问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.603,
                0.452,
                0.621
            ],
            "angle": 0,
            "content": "\\[\n\\max  \\quad \\langle C, X \\rangle ,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.357,
                0.628,
                0.563,
                0.644
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & X _ {i i} = 1,   i = 1, 2, \\dots , n, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.396,
                0.653,
                0.449,
                0.668
            ],
            "angle": 0,
            "content": "\\[\nX \\succeq 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.677,
                0.738,
                0.715
            ],
            "angle": 0,
            "content": "容易看出，此问题不是最大割问题，而是一个半定规划问题。这个分析也给出了最大割问题半定松弛的一种理解方式，参见第四章中问题(4.5.8)。"
        },
        {
            "type": "title",
            "bbox": [
                0.251,
                0.744,
                0.657,
                0.766
            ],
            "angle": 0,
            "content": "5.5 一般约束优化问题的最优性理论"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.785,
                0.373,
                0.803
            ],
            "angle": 0,
            "content": "5.5.1 一阶最优性条件"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.816,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "类似于无约束优化问题，约束优化问题(5.4.1)的最优性条件要从下降方向开始讨论．因为决策变量限制在可行域当中，所以只需要关注“可行”"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.291,
                0.13
            ],
            "angle": 0,
            "content": "180"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.525,
                0.174
            ],
            "angle": 0,
            "content": "的方向. 先引入可行域的几何性质"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.198,
                0.414,
                0.215
            ],
            "angle": 0,
            "content": "1. 切锥和约束品性"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.228,
                0.825,
                0.306
            ],
            "angle": 0,
            "content": "在给出最优性条件之前，我们先介绍一些必要的概念。与无约束优化问题类似，首先需要定义问题(5.4.1)的下降方向。这里因为约束的存在，我们只考虑可行方向，即可行序列对应的极限方向。特别地，称这样的方向为切向量。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.32,
                0.825,
                0.364
            ],
            "angle": 0,
            "content": "定义5.7(切锥) 给定可行域 \\(\\mathcal{X}\\) 及其内一点 \\(x\\), 若存在可行序列 \\(\\{z_k\\}_{k=1}^{\\infty} \\subset \\mathcal{X}\\) 逼近 \\(x\\) (即 \\(\\lim_{k \\to \\infty} z_k = x\\)) 以及正标量序列 \\(\\{t_k\\}_{k=1}^{\\infty}, t_k \\to 0\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.48,
                0.376,
                0.603,
                0.406
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\frac {z _ {k} - x}{t _ {k}} = d,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.416,
                0.824,
                0.454
            ],
            "angle": 0,
            "content": "则称向量 \\(d\\) 为 \\(\\mathcal{X}\\) 在点 \\(x\\) 处的一个切向量．所有点 \\(x\\) 处的切向量构成的集合称为切锥，用 \\(T_{\\mathcal{X}}(x)\\) 表示."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.467,
                0.825,
                0.567
            ],
            "angle": 0,
            "content": "我们以 \\(\\mathbb{R}^2\\) 上的约束集合为例来直观地给出切锥的几何结构。图5.4(a)中深色区域 \\(\\mathcal{X}\\) 表示两个不等式约束，其在点 \\(x\\) 处的切锥 \\(T_{\\mathcal{X}}(x)\\) 为图中浅色区域。注意，\\(\\mathcal{X}\\) 也为 \\(T_{\\mathcal{X}}(x)\\) 的一部分。图5.4(b)中则是考虑等式约束，这里可行域 \\(\\mathcal{X}\\) 是图5.4(a)中可行域的边界。根据切锥的定义，此时 \\(T_{\\mathcal{X}}(x)\\) 对应点 \\(x\\) 处与 \\(\\mathcal{X}\\) 相切的两条射线。"
        },
        {
            "type": "image",
            "bbox": [
                0.268,
                0.583,
                0.517,
                0.757
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.346,
                0.765,
                0.43,
                0.777
            ],
            "angle": 0,
            "content": "(a) 不等式约束"
        },
        {
            "type": "image",
            "bbox": [
                0.578,
                0.583,
                0.8,
                0.756
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.661,
                0.765,
                0.734,
                0.777
            ],
            "angle": 0,
            "content": "(b) 等式约束"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.436,
                0.797,
                0.648,
                0.813
            ],
            "angle": 0,
            "content": "图5.4 \\(\\mathbb{R}^2\\) 上的约束和切锥"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.836,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "有了切锥的定义之后，可以从几何上刻画问题(5.4.1)的最优性条件。与"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.5 一般约束优化问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "181"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.195
            ],
            "angle": 0,
            "content": "无约束优化类似，我们要求切锥（可行方向集合）不包含使得目标函数值下降的方向。具体地，有下面的一阶必要条件，称为几何最优性条件。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.211,
                0.736,
                0.248
            ],
            "angle": 0,
            "content": "定理 5.8 (几何最优性条件[175]) 假设可行点 \\(x^{*}\\) 是问题 (5.4.1) 的一个局部极小点。如果 \\(f(x)\\) 和 \\(c_{i}(x), i \\in \\mathcal{I} \\cup \\mathcal{E}\\) 在点 \\(x^{*}\\) 处是可微的，那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.34,
                0.265,
                0.566,
                0.284
            ],
            "angle": 0,
            "content": "\\[\nd ^ {\\mathrm {T}} \\nabla f (x ^ {*}) \\geqslant 0, \\quad \\forall d \\in T _ {\\mathcal {X}} (x ^ {*})\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.301,
                0.226,
                0.317
            ],
            "angle": 0,
            "content": "等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.322,
                0.584,
                0.341
            ],
            "angle": 0,
            "content": "\\[\nT _ {\\mathcal {X}} \\left(x ^ {*}\\right) \\cap \\left\\{d \\mid \\nabla f \\left(x ^ {*}\\right) ^ {\\mathrm {T}} d <   0 \\right\\} = \\varnothing .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.356,
                0.738,
                0.435
            ],
            "angle": 0,
            "content": "证明．采用反证法．假设在点 \\(x^{*}\\) 处有 \\(T_{\\mathcal{X}}(x^{*})\\cap \\{d\\mid \\nabla f(x^{*})^{\\mathrm{T}}d < 0\\} \\neq \\emptyset\\) 令 \\(d\\in T_{\\mathcal{X}}(x^{*})\\cap \\{d\\mid \\nabla f(x^{*})^{\\mathrm{T}}d < 0\\}\\) 根据切向量的定义，存在 \\(\\{t_k\\}_{k = 1}^{\\infty}\\) 和\\(\\{d_k\\}_{k = 1}^{\\infty}\\) 使得 \\(x^{*} + t_{k}d_{k}\\in \\mathcal{X}\\) ，其中 \\(t_k\\to 0\\) 且 \\(d_{k}\\rightarrow d\\) 由于 \\(\\nabla f(x^{*})^{\\mathrm{T}}d < 0\\) ，对于充分大的 \\(k\\) ，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.217,
                0.451,
                0.691,
                0.54
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f \\left(x ^ {*} + t _ {k} d _ {k}\\right) = f \\left(x ^ {*}\\right) + t _ {k} \\nabla f \\left(x ^ {*}\\right) ^ {\\mathrm {T}} d _ {k} + o \\left(t _ {k}\\right) \\\\ = f \\left(x ^ {*}\\right) + t _ {k} \\nabla f \\left(x ^ {*}\\right) ^ {\\mathrm {T}} d + t _ {k} \\nabla f \\left(x ^ {*}\\right) ^ {\\mathrm {T}} \\left(d _ {k} - d\\right) + o \\left(t _ {k}\\right) \\\\ = f \\left(x ^ {*}\\right) + t _ {k} \\nabla f \\left(x ^ {*}\\right) ^ {\\mathrm {T}} d + o \\left(t _ {k}\\right) \\\\ <   f (x ^ {*}). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.556,
                0.377,
                0.573
            ],
            "angle": 0,
            "content": "这与 \\(x^{*}\\) 的局部极小性矛盾"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.592,
                0.738,
                0.651
            ],
            "angle": 0,
            "content": "因为切锥是根据可行域的几何性质来定义的，其计算往往是不容易的。因此，我们需要寻找代数方法来计算可行方向，进而更容易地判断最优性条件。我们给出另一个容易计算的可行方向集合的定义，即线性化可行方向锥。"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.666,
                0.738,
                0.725
            ],
            "angle": 0,
            "content": "定义5.8（线性化可行方向锥）对于可行点 \\(x \\in \\mathcal{X}\\)，该点处的积极集(active set) \\(\\mathcal{A}(x)\\) 定义为两部分下标的集合，一部分是等式约束对应的下标，另外一部分是不等式约束中等号成立的约束对应的下标，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.331,
                0.742,
                0.576,
                0.761
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {A} (x) = \\mathcal {E} \\cup \\{i \\in \\mathcal {I}: c _ {i} (x) = 0 \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.777,
                0.519,
                0.794
            ],
            "angle": 0,
            "content": "进一步地，点 \\(x\\) 处的线性化可行方向锥定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.29,
                0.806,
                0.617,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {F} (x) = \\left\\{d \\middle | \\begin{array}{l} d ^ {\\mathrm {T}} \\nabla c _ {i} (x) = 0,   \\forall   i \\in \\mathcal {E}, \\\\ d ^ {\\mathrm {T}} \\nabla c _ {i} (x) \\leqslant 0,   \\forall   i \\in \\mathcal {A} (x) \\cap \\mathcal {I} \\end{array} \\right.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "182"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "image",
            "bbox": [
                0.368,
                0.154,
                0.718,
                0.382
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.367,
                0.394,
                0.719,
                0.412
            ],
            "angle": 0,
            "content": "图5.5 \\(\\mathbb{R}^2\\) 上的约束集合和线性化可行方向锥"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.443,
                0.825,
                0.503
            ],
            "angle": 0,
            "content": "图5.5直观地展示了 \\(\\mathbb{R}^2\\) 中的不等式约束集合和线性化可行方向锥\\(\\mathcal{F}(x)\\) .在点 \\(x\\) 处，已知两个不等式约束的等号均成立．而 \\(\\mathcal{F}(x)\\) 中的向量应保证和 \\(\\nabla c_i(x),i = 1,2\\) 的夹角为钝角或直角."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.508,
                0.825,
                0.607
            ],
            "angle": 0,
            "content": "直观来说，线性化可行方向锥中的向量应该保证和等式约束中函数的梯度垂直，这样才能尽量保证 \\(c_{i}(x), i \\in \\mathcal{E}\\) 的值不变；而对积极集 \\(\\mathcal{A}(x) \\cap \\mathcal{I}\\) 中的指标 \\(i\\)，沿着该向量 \\(c_{i}(x)\\) 的值不能增加，因此线性化可行方向对 \\(c_{i}(x), i \\in \\mathcal{A}(x) \\cap \\mathcal{I}\\) 可以是一个下降方向；而对非积极集中的约束，无需提出任何对线性化可行方向的要求。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.614,
                0.75,
                0.631
            ],
            "angle": 0,
            "content": "线性化可行方向锥一般比切锥要大，实际上我们有如下结果："
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.652,
                0.78,
                0.669
            ],
            "angle": 0,
            "content": "命题5.1 设 \\(c_{i}(x), i \\in \\mathcal{E} \\cup \\mathcal{I}\\) 一阶连续可微，则对任意可行点 \\(x\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.483,
                0.689,
                0.6,
                0.709
            ],
            "angle": 0,
            "content": "\\[\nT _ {\\mathcal {X}} (x) \\subseteq \\mathcal {F} (x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.731,
                0.813,
                0.748
            ],
            "angle": 0,
            "content": "证明．不失一般性，假设积极集 \\(\\mathcal{A}(x) = \\mathcal{E}\\cup \\mathcal{I}\\) 设 \\(d\\in T_{\\mathcal{X}}(x)\\) ，由定义，"
        },
        {
            "type": "equation",
            "bbox": [
                0.433,
                0.763,
                0.652,
                0.794
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} t _ {k} = 0, \\quad \\lim  _ {k \\rightarrow \\infty} \\frac {z _ {k} - x}{t _ {k}} = d,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.808,
                0.351,
                0.825
            ],
            "angle": 0,
            "content": "上式等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.474,
                0.837,
                0.61,
                0.853
            ],
            "angle": 0,
            "content": "\\[\nz _ {k} = x + t _ {k} d + e _ {k},\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.5 一般约束优化问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "183"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.595,
                0.174
            ],
            "angle": 0,
            "content": "其中残量 \\(e_k\\) 满足 \\(\\| e_k\\| = o(t_k)\\) ，对 \\(i\\in \\mathcal{E}\\) ，根据泰勒展开，"
        },
        {
            "type": "equation",
            "bbox": [
                0.303,
                0.193,
                0.605,
                0.294
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} 0 = \\frac {1}{t _ {k}} c _ {i} (z _ {k}) \\\\ = \\frac {1}{t _ {k}} \\left(c _ {i} (x) + \\nabla c _ {i} (x) ^ {\\mathrm {T}} \\left(t _ {k} d + e _ {k}\\right) + o \\left(t _ {k}\\right)\\right) \\\\ = \\nabla c _ {i} (x) ^ {\\mathrm {T}} d + \\frac {\\nabla c _ {i} (x) ^ {\\mathrm {T}} e _ {k}}{t _ {k}} + o (1). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.311,
                0.462,
                0.345
            ],
            "angle": 0,
            "content": "注意到 \\(\\frac{\\|e_k\\|}{t_k}\\to 0\\) ，令 \\(k\\to \\infty\\) 即可得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.361,
                0.537,
                0.38
            ],
            "angle": 0,
            "content": "\\[\n\\nabla c _ {i} (x) ^ {\\mathrm {T}} d = 0, \\quad i \\in \\mathcal {E}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.403,
                0.411,
                0.42
            ],
            "angle": 0,
            "content": "同理，对 \\(i\\in \\mathcal{I}\\) ，根据泰勒展开，"
        },
        {
            "type": "equation",
            "bbox": [
                0.303,
                0.438,
                0.605,
                0.538
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} 0 \\geqslant \\frac {1}{t _ {k}} c _ {i} (z _ {k}) \\\\ = \\frac {1}{t _ {k}} \\left(c _ {i} (x) + \\nabla c _ {i} (x) ^ {\\mathrm {T}} \\left(t _ {k} d + e _ {k}\\right) + o \\left(t _ {k}\\right)\\right) \\\\ = \\nabla c _ {i} (x) ^ {\\mathrm {T}} d + \\frac {\\nabla c _ {i} (x) ^ {\\mathrm {T}} e _ {k}}{t _ {k}} + o (1). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.556,
                0.446,
                0.573
            ],
            "angle": 0,
            "content": "注意到 \\(c_{i}(x) = 0, i \\in \\mathcal{I}\\), 因此我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.594,
                0.537,
                0.613
            ],
            "angle": 0,
            "content": "\\[\n\\nabla c _ {i} (x) ^ {\\mathrm {T}} d \\leqslant 0, \\quad i \\in \\mathcal {I}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.635,
                0.496,
                0.652
            ],
            "angle": 0,
            "content": "结合以上两点，最终可得到 \\(T_{\\mathcal{X}}(x)\\subseteq \\mathcal{F}(x)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.691,
                0.724,
                0.708
            ],
            "angle": 0,
            "content": "以上命题的结论反过来是不成立的，我们给出具体的例子。考虑问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.359,
                0.73,
                0.735,
                0.758
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R}} f (x) = x, \\tag {5.5.1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.369,
                0.76,
                0.549,
                0.774
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & c (x) = - x + 3 \\leqslant 0. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.794,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "根据切锥的定义，可以算出点 \\(x^{*} = 3\\) 处的切锥为 \\(T_{\\chi}(x^{*}) = \\{d|d\\geqslant 0\\}\\) ，如图5.6所示.对于线性化可行方向锥，由于 \\(c^{\\prime}(x^{*}) = -1\\) ，故 \\(\\mathcal{F}(x^{*}) = \\{d:d\\geqslant 0\\}\\) 此时，我们有 \\(T_{\\chi}(x^{*}) = \\mathcal{F}(x^{*})\\)"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "184"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "image",
            "bbox": [
                0.457,
                0.171,
                0.624,
                0.308
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.439,
                0.338,
                0.645,
                0.355
            ],
            "angle": 0,
            "content": "图5.6 问题(5.5.1)的切锥"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.379,
                0.507,
                0.395
            ],
            "angle": 0,
            "content": "将问题(5.5.1)的约束变形为"
        },
        {
            "type": "equation",
            "bbox": [
                0.456,
                0.409,
                0.625,
                0.427
            ],
            "angle": 0,
            "content": "\\[\nc (x) = (- x + 3) ^ {3} \\leqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.439,
                0.827,
                0.581
            ],
            "angle": 0,
            "content": "因为可行域没有改变，所以在点 \\(x^{*} = 3\\) 处，切锥 \\(T_{\\mathcal{X}}(x^{*}) = \\{d:d\\geqslant 0\\}\\) 不变．对于线性化可行方向锥，由于 \\(c^{\\prime}(x^{*}) = -3(x^{*} - 3)^{2} = 0\\) ，所以 \\(\\mathcal{F}(x^{*}) = \\{d\\mid d\\in \\mathbb{R}\\}\\) 此时， \\(\\mathcal{F}(x^{*})\\supset T_{\\mathcal{X}}(x^{*})\\) （严格包含）．这个例子告诉我们线性化可行方向锥 \\(\\mathcal{F}(x)\\) 不但受到问题可行域 \\(\\mathcal{X}\\) 的影响，还会受到 \\(\\mathcal{X}\\) 的代数表示方式的影响．在不改变 \\(\\mathcal{X}\\) 的条件下改变定义 \\(\\mathcal{X}\\) 的等式（不等式）的数学形式会影响 \\(\\mathcal{F}(x)\\) 包含的元素．而切锥 \\(T_{\\mathcal{X}}(x)\\) 的定义直接依赖于可行域 \\(\\mathcal{X}\\) 因此它不受到 \\(\\mathcal{X}\\) 代数表示方式的影响."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.585,
                0.827,
                0.706
            ],
            "angle": 0,
            "content": "线性化可行方向锥容易计算和使用，但会受到问题形式的影响；切锥比较直接地体现了可行域 \\(\\mathcal{X}\\) 的性质，但比较难计算．为了刻画线性化可行方向锥 \\(\\mathcal{F}(x)\\) 与切锥 \\(T_{\\mathcal{X}}(x)\\) 之间的关系，我们引入约束品性这个概念．简单来说，大部分的约束品性都是为了保证在最优点 \\(x^{*}\\) 处， \\(\\mathcal{F}(x^{*}) = T_{\\mathcal{X}}(x^{*})\\) ，这一性质使得我们能够使用 \\(\\mathcal{F}(x)\\) 代替 \\(T_{\\mathcal{X}}(x)\\) ，进而更方便地研究约束最优化问题的最优性条件．这里给出一些常用的约束品性的定义."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.717,
                0.825,
                0.775
            ],
            "angle": 0,
            "content": "定义5.9(线性无关约束品性）给定可行点 \\(x\\) 及相应的积极集 \\(\\mathcal{A}(x)\\) ．如果积极集对应的约束函数的梯度，即 \\(\\nabla c_{i}(x), i\\in \\mathcal{A}(x)\\) ，是线性无关的，则称线性无关约束品性（LICQ）在点 \\(x\\) 处成立."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.787,
                0.696,
                0.804
            ],
            "angle": 0,
            "content": "当LICQ成立时，切锥和线性化可行方向锥是相同的"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "引理5.2 给定任意可行点 \\(x \\in \\mathcal{X}\\)，如果在该点处LICQ成立，则有 \\(T_{\\mathcal{X}}(x) = \\mathcal{F}(x)\\)"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.5 一般约束优化问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "185"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.638,
                0.174
            ],
            "angle": 0,
            "content": "证明．不失一般性，我们假设积极集 \\(\\mathcal{A}(x) = \\mathcal{E}\\cup \\mathcal{I}\\) ，记矩阵"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.187,
                0.538,
                0.207
            ],
            "angle": 0,
            "content": "\\[\nA (x) = \\left[ \\nabla c _ {i} (x) \\right] _ {i \\in \\mathcal {I} \\cup \\mathcal {E}} ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.22,
                0.737,
                0.258
            ],
            "angle": 0,
            "content": "假设集合 \\(\\mathcal{A}(x)\\) 的元素个数为 \\(m\\) ，那么矩阵 \\(A(x)\\in \\mathbb{R}^{m\\times n}\\) 并且 \\(\\operatorname {rank}(A) = m.\\) 令矩阵 \\(Z\\in \\mathbb{R}^{n\\times (n - m)}\\) 为 \\(A(x)\\) 的零空间的基矩阵，则 \\(Z\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.331,
                0.272,
                0.575,
                0.289
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {r a n k} (Z) = n - m, \\quad A (x) Z = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.304,
                0.737,
                0.341
            ],
            "angle": 0,
            "content": "令 \\(d \\in \\mathcal{F}(x)\\) 为任意线性化可行方向，给定任意一列满足 \\(\\lim_{k \\to \\infty} t_k = 0\\) 的正标量 \\(\\{t_k\\}_{k=1}^{\\infty}\\)，定义映射 \\(R: \\mathbb{R}^n \\times \\mathbb{R} \\to \\mathbb{R}^n\\)："
        },
        {
            "type": "equation",
            "bbox": [
                0.348,
                0.351,
                0.56,
                0.395
            ],
            "angle": 0,
            "content": "\\[\nR (z, t) = \\left[ \\begin{array}{c} c (z) - t A (x) d \\\\ Z ^ {\\mathrm {T}} (z - x - t d) \\end{array} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.403,
                0.737,
                0.442
            ],
            "angle": 0,
            "content": "其中 \\(c(z)\\) 为向量值函数，其第 \\(i\\) 个分量为 \\(c_{i}(z)\\)。考虑 \\(R\\) 的零点，即满足 \\(R(z,t) = 0\\) 的 \\((z,t)\\)。在点 \\((x,0)\\) 处，我们有 \\(R(x,0) = 0\\) 并且"
        },
        {
            "type": "equation",
            "bbox": [
                0.376,
                0.45,
                0.531,
                0.495
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\partial R (x , 0)}{\\partial z} = \\left[ \\begin{array}{c} A (x) \\\\ Z ^ {\\mathrm {T}} \\end{array} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.505,
                0.739,
                0.572
            ],
            "angle": 0,
            "content": "根据 \\(Z\\) 的构造，雅可比矩阵 \\(\\frac{\\partial R(x,0)}{\\partial z}\\) 是非奇异的．因此，由隐函数定理，对任意充分小的 \\(t_k\\) ，都存在唯一的 \\(z_{k}\\) ，使得 \\(R(z_{k},t_{k}) = 0\\) 由于 \\(R(z_{k},t_{k}) = 0\\) 故 \\(c_{i}(z_{k}) = t_{k}\\nabla c_{i}(x)^{\\mathrm{T}}d,i\\in \\mathcal{I}\\cup \\mathcal{E}\\) 根据线性化可行方向 \\(^d\\) 的定义，"
        },
        {
            "type": "equation",
            "bbox": [
                0.318,
                0.586,
                0.589,
                0.603
            ],
            "angle": 0,
            "content": "\\[\nc _ {i} \\left(z _ {k}\\right) \\geqslant 0, i \\in \\mathcal {I}, c _ {i} \\left(z _ {k}\\right) = 0, i \\in \\mathcal {E},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.617,
                0.289,
                0.633
            ],
            "angle": 0,
            "content": "即 \\(z_{k}\\) 为可行点"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.638,
                0.434,
                0.654
            ],
            "angle": 0,
            "content": "进一步地，由泰勒展开可得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.281,
                0.664,
                0.624,
                0.8
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} 0 = R (z _ {k}, t _ {k}) = \\left[ \\begin{array}{c} c (z _ {k}) - t _ {k} A (x) d \\\\ Z ^ {\\mathrm {T}} (z _ {k} - x - t _ {k} d) \\end{array} \\right] \\\\ = \\left[ \\begin{array}{c} A (x) (z _ {k} - x) + e _ {k} - t _ {k} A (x) d \\\\ Z ^ {\\mathrm {T}} (z _ {k} - x - t _ {k} d) \\end{array} \\right] \\\\ = \\left[ \\begin{array}{c} A (x) \\\\ Z ^ {\\mathrm {T}} \\end{array} \\right] (z _ {k} - x - t _ {k} d) + \\left[ \\begin{array}{c} e _ {k} \\\\ 0 \\end{array} \\right]. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.81,
                0.737,
                0.857
            ],
            "angle": 0,
            "content": "其中残量 \\(e_k\\) 满足 \\(\\| e_k\\| = o(t_k)\\)．对于上式，两边同时作用 \\(\\left[ \\begin{array}{c}A(x)\\\\ Z^{\\mathrm{T}} \\end{array} \\right]^{-1}\\) 并除以"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.119,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "186"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.158,
                0.327,
                0.173
            ],
            "angle": 0,
            "content": "\\(t_k\\) ，则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.427,
                0.171,
                0.656,
                0.223
            ],
            "angle": 0,
            "content": "\\[\n\\frac {z _ {k} - x}{t _ {k}} = d + \\left[ \\begin{array}{c} A (x) \\\\ Z ^ {\\mathrm {T}} \\end{array} \\right] ^ {- 1} \\left[ \\begin{array}{c} \\frac {e _ {k}}{t _ {k}} \\\\ 0 \\end{array} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.225,
                0.425,
                0.242
            ],
            "angle": 0,
            "content": "根据 \\(\\| e_k\\| = o(t_k)\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.481,
                0.24,
                0.602,
                0.271
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\frac {z _ {k} - x}{t _ {k}} = d,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.273,
                0.774,
                0.29
            ],
            "angle": 0,
            "content": "即 \\(d\\in T_{\\mathcal{X}}(x)\\) .故 \\(\\mathcal{F}(x)\\subseteq T_{\\mathcal{X}}(x)\\) .又 \\(T_{\\mathcal{X}}(x)\\subseteq \\mathcal{F}(x)\\) ，则两集合相同."
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.275,
                0.824,
                0.287
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.303,
                0.825,
                0.34
            ],
            "angle": 0,
            "content": "关于LICQ的一个常用推广是Mangasarian-Fromovitz约束品性，简称为MFCQ."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.351,
                0.825,
                0.388
            ],
            "angle": 0,
            "content": "定义5.10(MFCQ) 给定可行点 \\(x\\) 及相应的积极集 \\(\\mathcal{A}(x)\\)。如果存在一个向量 \\(w \\in \\mathbb{R}^n\\)，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.421,
                0.397,
                0.662,
                0.44
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\nabla c _ {i} (x) ^ {\\mathrm {T}} w <   0, \\quad \\forall i \\in \\mathcal {A} (x) \\cap \\mathcal {I}, \\\\ \\nabla c _ {i} (x) ^ {\\mathrm {T}} w = 0, \\quad \\forall i \\in \\mathcal {E}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.448,
                0.825,
                0.485
            ],
            "angle": 0,
            "content": "并且等式约束对应的梯度集 \\(\\{\\nabla c_{i}(x), i \\in \\mathcal{E}\\}\\) 是线性无关的，则称MFCQ在点 \\(x\\) 处成立."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.497,
                0.83,
                0.555
            ],
            "angle": 0,
            "content": "可以验证MFCQ是LICQ的一个弱化版本，即由LICQ可以推出MFCQ，但是反过来不成立．在MFCQ成立的情况下，我们也可以证明 \\(T_{\\mathcal{X}}(x) = \\mathcal{F}(x)\\)，参见[218]引理8.2.12."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.559,
                0.761,
                0.575
            ],
            "angle": 0,
            "content": "另外一个用来保证 \\(T_{\\mathcal{X}}(x) = \\mathcal{F}(x)\\) 的约束品性是线性约束品性"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.586,
                0.825,
                0.623
            ],
            "angle": 0,
            "content": "定义5.11（线性约束品性）如果所有的约束函数 \\(c_{i}(x), i \\in \\mathcal{I} \\cup \\mathcal{E}\\) 都是线性的，则称线性约束品性成立。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.634,
                0.826,
                0.713
            ],
            "angle": 0,
            "content": "当线性约束品性成立时，也有 \\(T_{\\mathcal{X}}(x) = \\mathcal{F}(x)\\)。因此对只含线性约束的优化问题，例如线性规划、二次规划，很自然地有 \\(T_{\\mathcal{X}}(x) = \\mathcal{F}(x), \\forall x\\)。我们无需再关注约束函数的梯度是否线性无关。一般来说，线性约束品性和LICQ之间没有互相包含的关系。"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.736,
                0.571,
                0.753
            ],
            "angle": 0,
            "content": "2. Karush-Kuhn-Tucker (KKT) 条件"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.766,
                0.826,
                0.824
            ],
            "angle": 0,
            "content": "基于几何最优性条件，即定理5.8，我们想要得到一个计算上更易验证的形式．切锥和线性化可行方向锥的联系给我们提供了一种方式．具体地，在定理5.8中，如果在局部最优解 \\(x^{*}\\) 处有"
        },
        {
            "type": "equation",
            "bbox": [
                0.478,
                0.836,
                0.604,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nT _ {\\mathcal {X}} \\left(x ^ {*}\\right) = \\mathcal {F} \\left(x ^ {*}\\right)\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.5 一般约束优化问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "187"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.637,
                0.174
            ],
            "angle": 0,
            "content": "成立 (如果 LICQ 在点 \\(x^{*}\\) 处成立, 上式自然满足), 那么集合"
        },
        {
            "type": "equation",
            "bbox": [
                0.316,
                0.184,
                0.737,
                0.253
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{l} d \\left| \\begin{array}{l} d ^ {\\mathrm {T}} \\nabla f \\left(x ^ {*}\\right) <   0, \\\\ d ^ {\\mathrm {T}} \\nabla c _ {i} \\left(x ^ {*}\\right) = 0, i \\in \\mathcal {E}, \\\\ d ^ {\\mathrm {T}} \\nabla c _ {i} \\left(x ^ {*}\\right) \\leqslant 0, i \\in \\mathcal {A} \\left(x ^ {*}\\right) \\cap \\mathcal {I} \\end{array} \\right. \\end{array} \\right\\} \\tag {5.5.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.264,
                0.737,
                0.302
            ],
            "angle": 0,
            "content": "是空集．(5.5.2)式的验证仍然是非常麻烦的，我们需要将其转化为一个更直接的方式．这里介绍一个重要的引理，称为Farkas引理."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.314,
                0.737,
                0.352
            ],
            "angle": 0,
            "content": "引理5.3 (Farkas引理) 设 \\(p\\) 和 \\(q\\) 为两个非负整数，给定向量组 \\(\\{a_{i} \\in \\mathbb{R}^{n}, i = 1,2,\\dots,p\\}\\)，\\(\\{b_{i} \\in \\mathbb{R}^{n}, i = 1,2,\\dots,q\\}\\) 和 \\(c \\in \\mathbb{R}^{n}\\)。满足以下条件："
        },
        {
            "type": "equation",
            "bbox": [
                0.35,
                0.366,
                0.737,
                0.384
            ],
            "angle": 0,
            "content": "\\[\nd ^ {\\mathrm {T}} a _ {i} = 0, \\quad i = 1, 2, \\dots , p, \\tag {5.5.3}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.351,
                0.39,
                0.737,
                0.409
            ],
            "angle": 0,
            "content": "\\[\nd ^ {\\mathrm {T}} b _ {i} \\geqslant 0, \\quad i = 1, 2, \\dots , q, \\tag {5.5.4}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.415,
                0.737,
                0.433
            ],
            "angle": 0,
            "content": "\\[\nd ^ {\\mathrm {T}} c <   0 \\tag {5.5.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.447,
                0.719,
                0.464
            ],
            "angle": 0,
            "content": "的 \\(d\\) 不存在当且仅当存在 \\(\\lambda_{i}, i = 1,2,\\dots ,p\\) 和 \\(\\mu_i\\geqslant 0,i = 1,2,\\dots ,q\\) ，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.475,
                0.737,
                0.512
            ],
            "angle": 0,
            "content": "\\[\nc = \\sum_ {i = 1} ^ {p} \\lambda_ {i} a _ {i} + \\sum_ {i = 1} ^ {q} \\mu_ {i} b _ {i}. \\tag {5.5.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.522,
                0.737,
                0.559
            ],
            "angle": 0,
            "content": "证明．若存在 \\(\\lambda_{i}\\) 和 \\(\\mu_{i} \\geqslant 0\\) 使得 (5.5.6) 式成立，则对任意满足式 (5.5.3) 式和 (5.5.4) 式的 \\(d\\)，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.334,
                0.569,
                0.572,
                0.607
            ],
            "angle": 0,
            "content": "\\[\nd ^ {\\mathrm {T}} c = \\sum_ {i = 1} ^ {p} \\lambda_ {i} d ^ {\\mathrm {T}} a _ {i} + \\sum_ {i = 1} ^ {q} \\mu_ {i} d ^ {\\mathrm {T}} b _ {i} \\geqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.617,
                0.499,
                0.633
            ],
            "angle": 0,
            "content": "因此不等式系统(5.5.3)-(5.5.5)的解不存在"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.637,
                0.737,
                0.675
            ],
            "angle": 0,
            "content": "若不等式系统(5.5.3)-(5.5.5)的解不存在．我们利用反证法．假设不存在 \\(\\lambda_{i}\\) 和 \\(\\mu_{i}\\geqslant 0\\) 使得(5.5.6)式成立．定义集合"
        },
        {
            "type": "equation",
            "bbox": [
                0.278,
                0.684,
                0.63,
                0.728
            ],
            "angle": 0,
            "content": "\\[\nS = \\left\\{z \\mid z = \\sum_ {i = 1} ^ {p} \\lambda_ {i} a _ {i} + \\sum_ {i = 1} ^ {q} \\mu_ {i} b _ {i}, \\lambda_ {i} \\in \\mathbb {R}, \\mu_ {i} \\geqslant 0 \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.736,
                0.737,
                0.772
            ],
            "angle": 0,
            "content": "易知 \\(S\\) 是一个闭凸锥. 因为 \\(c \\notin S\\), 则由凸集的严格分离超平面定理可知: 存在 \\(d \\in \\mathbb{R}^n\\), 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.361,
                0.776,
                0.547,
                0.794
            ],
            "angle": 0,
            "content": "\\[\nd ^ {\\mathrm {T}} c <   \\alpha <   d ^ {\\mathrm {T}} z, \\quad \\forall z \\in S,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.805,
                0.424,
                0.821
            ],
            "angle": 0,
            "content": "其中 \\(\\alpha\\) 为常数．因为 \\(0\\in S\\) ，则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.404,
                0.835,
                0.503,
                0.852
            ],
            "angle": 0,
            "content": "\\[\n\\alpha <   d ^ {\\mathrm {T}} 0 = 0,\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "188"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.156,
                0.825,
                0.194
            ],
            "angle": 0,
            "content": "这说明 \\(d^{\\mathrm{T}}c < 0\\) ，另一方面，对于任意的 \\(b_{i}, i = 1,2,\\dots ,q\\) ，有 \\(tb_{i}\\in S,t\\geqslant 0.\\) 又由"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.2,
                0.616,
                0.218
            ],
            "angle": 0,
            "content": "\\[\nt d ^ {T} b _ {i} > \\alpha , \\quad \\forall t > 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.233,
                0.382,
                0.249
            ],
            "angle": 0,
            "content": "令 \\(t\\to +\\infty\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.506,
                0.255,
                0.577,
                0.273
            ],
            "angle": 0,
            "content": "\\[\nd ^ {T} b _ {i} \\geqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.288,
                0.737,
                0.305
            ],
            "angle": 0,
            "content": "类似地，对于任意的 \\(a_i, i = 1,2,\\dots ,p\\) ，有 \\(t a_{i}\\in S,\\forall t\\in \\mathbb{R}\\) 又由"
        },
        {
            "type": "equation",
            "bbox": [
                0.466,
                0.322,
                0.617,
                0.34
            ],
            "angle": 0,
            "content": "\\[\nt d ^ {\\mathrm {T}} a _ {i} > \\alpha , \\quad \\forall t \\in \\mathbb {R},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.359,
                0.538,
                0.376
            ],
            "angle": 0,
            "content": "分别令 \\(t \\to +\\infty\\) 和 \\(t \\to -\\infty\\)，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.506,
                0.393,
                0.577,
                0.41
            ],
            "angle": 0,
            "content": "\\[\nd ^ {\\mathrm {T}} a _ {i} = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.43,
                0.639,
                0.447
            ],
            "angle": 0,
            "content": "故 \\(d\\) 为不等式系统(5.5.3)-(5.5.5)的一个解. 矛盾"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.431,
                0.825,
                0.444
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.469,
                0.825,
                0.526
            ],
            "angle": 0,
            "content": "利用Farkas引理，在(5.5.3)-(5.5.5)式中取 \\(a_{i} = \\nabla c_{i}(x^{*}),i\\in \\mathcal{E}\\) ， \\(b_{i} =\\) \\(\\nabla c_i(x^*),i\\in \\mathcal{A}(x^*)\\cap \\mathcal{I}\\) 以及 \\(c = -\\nabla f(x^{*})\\) ，集合(5.5.2)是空集等价于下式成立："
        },
        {
            "type": "equation",
            "bbox": [
                0.362,
                0.532,
                0.825,
                0.565
            ],
            "angle": 0,
            "content": "\\[\n- \\nabla f \\left(x ^ {*}\\right) = \\sum_ {i \\in \\mathcal {E}} \\lambda_ {i} ^ {*} \\nabla c _ {i} \\left(x ^ {*}\\right) + \\sum_ {i \\in \\mathcal {A} \\left(x ^ {*}\\right) \\cap \\mathcal {I}} \\lambda_ {i} ^ {*} \\nabla c _ {i} \\left(x ^ {*}\\right), \\tag {5.5.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.573,
                0.825,
                0.611
            ],
            "angle": 0,
            "content": "其中 \\(\\lambda_i^* \\in \\mathbb{R}, i \\in \\mathcal{E}, \\lambda_i^* \\geqslant 0, i \\in \\mathcal{A}(x^*) \\cap \\mathcal{I}\\). 如果补充定义 \\(\\lambda_i^* = 0, i \\in \\mathcal{I} \\backslash \\mathcal{A}(x^*)\\) 那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.616,
                0.652,
                0.647
            ],
            "angle": 0,
            "content": "\\[\n- \\nabla f (x ^ {*}) = \\sum_ {i \\in \\mathcal {I} \\cup \\mathcal {E}} \\lambda_ {i} ^ {*} \\nabla c _ {i} (x ^ {*}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.656,
                0.825,
                0.693
            ],
            "angle": 0,
            "content": "这恰好对应于拉格朗日函数关于 \\(x\\) 的一阶最优性条件．另外，对于任意的\\(i\\in \\mathcal{I}\\) ，我们注意到"
        },
        {
            "type": "equation",
            "bbox": [
                0.49,
                0.7,
                0.592,
                0.718
            ],
            "angle": 0,
            "content": "\\[\n\\lambda_ {i} ^ {*} c _ {i} \\left(x ^ {*}\\right) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.732,
                0.825,
                0.811
            ],
            "angle": 0,
            "content": "上式称为互补松弛条件。这个条件表明对不等式约束，以下两种情况至少出现一种：乘子 \\(\\lambda_{i}^{*} = 0\\) ，或 \\(c_{i}(x^{*}) = 0\\) （即 \\(i\\in \\mathcal{A}(x^{*})\\cap \\mathcal{I}\\) 。当以上两种情况恰好只有一种满足时，我们也称此时严格互补松弛条件成立。一般来说，具有严格互补松弛条件的最优值点有比较好的性质，算法能够很快收敛。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "综上所述，我们有如下一阶必要条件，也称作KKT条件，并称满足条件(5.5.8)的变量对 \\((x^{*},\\lambda^{*})\\) 为KKT对."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.5 一般约束优化问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "189"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.739,
                0.193
            ],
            "angle": 0,
            "content": "定理5.9 (KKT条件[145]) 假设 \\(x^{*}\\) 是问题(5.4.1)的一个局部最优点. 如果"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.199,
                0.517,
                0.217
            ],
            "angle": 0,
            "content": "\\[\nT _ {\\mathcal {X}} \\left(x ^ {*}\\right) = \\mathcal {F} \\left(x ^ {*}\\right)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.228,
                0.571,
                0.245
            ],
            "angle": 0,
            "content": "成立，那么存在拉格朗日乘子 \\(\\lambda_{i}^{*}\\) 使得如下条件成立："
        },
        {
            "type": "equation",
            "bbox": [
                0.225,
                0.258,
                0.672,
                0.289
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{r l} {{\\mathrm {稳 定 性 条 件}}} & {{\\nabla_ {x} L (x ^ {*}, \\lambda^ {*}) = \\nabla f (x ^ {*}) + \\sum_ {i \\in \\mathcal {I} \\cup \\mathcal {E}} \\lambda_ {i} ^ {*} \\nabla c _ {i} (x ^ {*}) = 0,}} \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.192,
                0.295,
                0.471,
                0.312
            ],
            "angle": 0,
            "content": "原始可行性条件 \\(c_{i}(x^{*}) = 0, \\forall i \\in \\mathcal{E},\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.192,
                0.314,
                0.738,
                0.337
            ],
            "angle": 0,
            "content": "原始可行性条件 \\(c_{i}(x^{*})\\leqslant 0,\\forall i\\in \\mathcal{I},\\) (5.5.8)"
        },
        {
            "type": "text",
            "bbox": [
                0.192,
                0.344,
                0.445,
                0.361
            ],
            "angle": 0,
            "content": "对偶可行性条件 \\(\\lambda_i^* \\geqslant 0, \\forall i \\in \\mathcal{I},\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.369,
                0.487,
                0.386
            ],
            "angle": 0,
            "content": "互补松弛条件 \\(\\lambda_i^* c_i(x^*) = 0, \\forall i \\in \\mathcal{I}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.4,
                0.741,
                0.478
            ],
            "angle": 0,
            "content": "证明．因为 \\(T_{\\mathcal{X}}(x^{*}) = \\mathcal{F}(x^{*})\\) ，根据定理5.8，(5.5.2)式对应的集合为空集因此，由Farkas引理可知，存在乘子 \\(\\lambda_i^*\\in \\mathbb{R},i\\in \\mathcal{E},\\lambda_i^*\\geqslant 0,i\\in \\mathcal{A}(x^*)\\cap \\mathcal{I}\\) ，使得(5.5.7)式成立．令 \\(\\lambda_i^* = 0,i\\in \\mathcal{I}\\backslash \\mathcal{A}(x^*)\\) ，结合 \\(x^{*}\\) 的可行性，我们有(5.5.8)式成立. □"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.495,
                0.739,
                0.575
            ],
            "angle": 0,
            "content": "我们称满足(5.5.8)式的点 \\(x^{*}\\) 为KKT点．注意，上面的定理只给出了切锥与线性化可行方向锥相同时的最优性条件．也就是说，如果在局部最优点\\(x^{*}\\) 处 \\(T_{\\mathcal{X}}(x^{*})\\neq \\mathcal{F}(x^{*})\\) ，那么 \\(x^{*}\\) 不一定是KKT点．同样地，因为KKT条件只是必要的，所以KKT点不一定是局部最优点."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.602,
                0.372,
                0.621
            ],
            "angle": 0,
            "content": "5.5.2 二阶最优性条件"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.633,
                0.739,
                0.755
            ],
            "angle": 0,
            "content": "对于问题 (5.4.1), 如果存在一个点 \\(x^{*}\\) 满足 KKT 条件, 我们知道沿着任意线性化可行方向目标函数的一阶近似不会下降. 此时一阶条件无法判断 \\(x^{*}\\) 是否是最优值点, 需要利用二阶信息来进一步判断在其可行邻域内的目标函数值. 具体地, 假设 \\(T_{\\mathcal{X}}(x^{*}) = \\mathcal{F}(x^{*})\\), 要判断满足 \\(d^{\\mathrm{T}} \\nabla f(x^{*}) = 0\\) 的线性化可行方向 \\(d\\) 是否为 \\(f(x^{*})\\) 的下降方向. 我们以拉格朗日函数在这些方向上的曲率信息为桥梁来判断点 \\(x^{*}\\) 处的最优性. 下面给出临界锥的定义."
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.768,
                0.719,
                0.786
            ],
            "angle": 0,
            "content": "定义5.12 (临界锥) 设 \\((x^{*},\\lambda^{*})\\) 满足KKT条件(5.5.8)，定义临界锥为"
        },
        {
            "type": "equation",
            "bbox": [
                0.198,
                0.801,
                0.71,
                0.82
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {C} (x ^ {*}, \\lambda^ {*}) = \\left\\{d \\in \\mathcal {F} (x ^ {*}) \\mid \\nabla c _ {i} (x ^ {*}) ^ {\\mathrm {T}} d = 0, \\forall i \\in \\mathcal {A} (x ^ {*}) \\cap \\mathcal {I} \\text {且} \\lambda_ {i} ^ {*} > 0 \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.502,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{F}(x^{*})\\) 为点 \\(x^{*}\\) 处的线性化可行方向锥"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.291,
                0.13
            ],
            "angle": 0,
            "content": "190"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.235
            ],
            "angle": 0,
            "content": "临界锥是线性化可行方向锥 \\(\\mathcal{F}(x^{*})\\) 的子集，沿着临界锥中的方向进行优化，所有等式约束和 \\(\\lambda_{i}^{*} > 0\\) 对应的不等式约束（此时这些不等式约束中的等号均成立）都会尽量保持不变．注意，对一般的 \\(d\\in \\mathcal{F}(x^{*})\\) 我们仅仅能保证 \\(d^{\\mathrm{T}}\\nabla c_{i}(x^{*})\\leqslant 0.\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.24,
                0.528,
                0.256
            ],
            "angle": 0,
            "content": "利用上述定义，可得如下结论："
        },
        {
            "type": "equation",
            "bbox": [
                0.362,
                0.273,
                0.718,
                0.293
            ],
            "angle": 0,
            "content": "\\[\nd \\in \\mathcal {C} \\left(x ^ {*}, \\lambda^ {*}\\right) \\Rightarrow \\lambda_ {i} ^ {*} \\nabla c _ {i} \\left(x ^ {*}\\right) ^ {\\mathrm {T}} d = 0, \\quad \\forall i \\in \\mathcal {E} \\cup \\mathcal {I}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.309,
                0.356,
                0.326
            ],
            "angle": 0,
            "content": "更进一步地，"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.341,
                0.735,
                0.372
            ],
            "angle": 0,
            "content": "\\[\nd \\in \\mathcal {C} (x ^ {*}, \\lambda^ {*}) \\Rightarrow d ^ {\\mathrm {T}} \\nabla f (x ^ {*}) = \\sum_ {i \\in \\mathcal {E} \\cup \\mathcal {I}} \\lambda_ {i} ^ {*} d ^ {\\mathrm {T}} \\nabla c _ {i} (x ^ {*}) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.385,
                0.825,
                0.443
            ],
            "angle": 0,
            "content": "也就是说，临界锥定义了依据一阶导数不能判断是否为下降或上升方向的线性化可行方向，必须使用高阶导数信息加以判断。这里给出如下的二阶最优性条件，其具体证明可以在 [145] 中的定理 12.5 以及定理 12.6 中找到："
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.459,
                0.825,
                0.517
            ],
            "angle": 0,
            "content": "定理5.10（二阶必要条件）假设 \\(x^{*}\\) 是问题(5.4.1)的一个局部最优解，并且 \\(T_{\\chi}(x^{*}) = \\mathcal{F}(x^{*})\\) 成立．令 \\(\\lambda^{*}\\) 为相应的拉格朗日乘子，即 \\((x^{*},\\lambda^{*})\\) 满足KKT条件，那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.396,
                0.533,
                0.685,
                0.553
            ],
            "angle": 0,
            "content": "\\[\nd ^ {\\mathrm {T}} \\nabla_ {x x} ^ {2} L \\left(x ^ {*}, \\lambda^ {*}\\right) d \\geqslant 0, \\quad \\forall d \\in \\mathcal {C} \\left(x ^ {*}, \\lambda^ {*}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.57,
                0.824,
                0.607
            ],
            "angle": 0,
            "content": "定理5.11（二阶充分条件）假设在可行点 \\(x^{*}\\) 处，存在一个拉格朗日乘子 \\(\\lambda^{*}\\)，使得 \\((x^{*},\\lambda^{*})\\) 满足KKT条件．如果"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.624,
                0.712,
                0.643
            ],
            "angle": 0,
            "content": "\\[\nd ^ {\\mathrm {T}} \\nabla_ {x x} ^ {2} L (x ^ {*}, \\lambda^ {*}) d > 0, \\quad \\forall d \\in \\mathcal {C} (x ^ {*}, \\lambda^ {*}), d \\neq 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.66,
                0.606,
                0.675
            ],
            "angle": 0,
            "content": "那么 \\(x^{*}\\) 为问题(5.4.1)的一个严格局部极小解"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.692,
                0.825,
                0.771
            ],
            "angle": 0,
            "content": "我们比对无约束优化问题的二阶最优性条件（定理5.4）不难发现，约束优化问题的二阶最优性条件也要求某种“正定性”，但只需要考虑临界锥 \\(\\mathcal{C}(x^{*},\\lambda^{*})\\) 中的向量而无需考虑全空间的向量．因此有些教材中又将其称为“投影半正定性”"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.776,
                0.825,
                0.813
            ],
            "angle": 0,
            "content": "为了更深刻地理解约束优化的最优性理论，我们考虑一个具体的例子。给定如下约束优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.824,
                0.688,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\min  & x _ {1} ^ {2} + x _ {2} ^ {2}, \\quad \\text {s . t .} \\quad \\frac {x _ {1} ^ {2}}{4} + x _ {2} ^ {2} - 1 = 0, \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.6 带约束凸优化问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "191"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.314,
                0.174
            ],
            "angle": 0,
            "content": "其拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.319,
                0.179,
                0.589,
                0.212
            ],
            "angle": 0,
            "content": "\\[\nL (x, \\lambda) = x _ {1} ^ {2} + x _ {2} ^ {2} + \\lambda (\\frac {x _ {1} ^ {2}}{4} + x _ {2} ^ {2} - 1).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.217,
                0.655,
                0.234
            ],
            "angle": 0,
            "content": "该问题可行域在任意一点 \\(x = (x_{1}, x_{2})^{\\mathrm{T}}\\) 处的线性化可行方向锥为"
        },
        {
            "type": "equation",
            "bbox": [
                0.316,
                0.241,
                0.591,
                0.269
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {F} (x) = \\left\\{\\left(d _ {1}, d _ {2}\\right) \\mid \\frac {x _ {1}}{4} d _ {1} + x _ {2} d _ {2} = 0 \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.274,
                0.737,
                0.313
            ],
            "angle": 0,
            "content": "因为只有一个等式约束且其对应函数的梯度非零，故有LICQ成立，且在KKT对 \\((x,\\lambda)\\) 处有 \\(\\mathcal{C}(x,\\lambda) = \\mathcal{F}(x)\\) 可以计算出其4个KKT对"
        },
        {
            "type": "equation",
            "bbox": [
                0.215,
                0.322,
                0.692,
                0.342
            ],
            "angle": 0,
            "content": "\\[\n(x ^ {\\mathrm {T}}, \\lambda) = (2, 0, - 4), \\quad (- 2, 0, - 4), \\quad (0, 1, - 1) \\quad \\text {和} \\quad (0, - 1, - 1).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.35,
                0.737,
                0.388
            ],
            "angle": 0,
            "content": "我们考虑第一个KKT对 \\(y = (2,0, - 4)^{\\mathrm{T}}\\) 和第三个KKT对 \\(z = (0,1, - 1)^{\\mathrm{T}}\\) 计算可得，"
        },
        {
            "type": "equation",
            "bbox": [
                0.266,
                0.388,
                0.641,
                0.431
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {x x} ^ {2} L (y) = \\left[ \\begin{array}{c c} 0 & 0 \\\\ 0 & - 6 \\end{array} \\right], \\quad \\mathcal {C} (y) = \\{(d _ {1}, d _ {2}) \\mid d _ {1} = 0 \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.434,
                0.301,
                0.451
            ],
            "angle": 0,
            "content": "取 \\(d = (0,1)\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.366,
                0.453,
                0.542,
                0.472
            ],
            "angle": 0,
            "content": "\\[\nd ^ {\\mathrm {T}} \\nabla_ {x x} ^ {2} L (y) d = - 6 <   0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.48,
                0.637,
                0.497
            ],
            "angle": 0,
            "content": "因此 \\(y\\) 不是局部最优点．类似地，对于KKT对 \\(z = (0,1, - 1)\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.27,
                0.502,
                0.637,
                0.555
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {x x} ^ {2} L (z) = \\left[ \\begin{array}{c c} \\frac {3}{2} & 0 \\\\ 0 & 0 \\end{array} \\right], \\quad \\mathcal {C} (z) = \\{(d _ {1}, d _ {2}) \\mid d _ {2} = 0 \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.559,
                0.421,
                0.576
            ],
            "angle": 0,
            "content": "对于任意的 \\(d = (d_1,0)\\) 且 \\(d_{1}\\neq 0\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.364,
                0.582,
                0.543,
                0.613
            ],
            "angle": 0,
            "content": "\\[\nd ^ {\\mathrm {T}} \\nabla_ {x x} ^ {2} L (z) d = \\frac {3}{2} d _ {1} ^ {2} > 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.618,
                0.416,
                0.635
            ],
            "angle": 0,
            "content": "因此， \\(z\\) 为一个严格局部最优点"
        },
        {
            "type": "title",
            "bbox": [
                0.251,
                0.665,
                0.657,
                0.687
            ],
            "angle": 0,
            "content": "5.6 带约束凸优化问题的最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.703,
                0.737,
                0.783
            ],
            "angle": 0,
            "content": "在实际问题中, 优化问题 (5.4.1) 的目标函数和约束函数往往是凸的 (可能不可微), 比如稀疏优化问题 (1.2.3), 低秩矩阵恢复问题 (1.3.2), 矩阵分离问题 (3.8.2) 以及回归分析中的问题 (3.2.8), 等等。因此, 凸优化问题的最优性条件的研究具有重要意义。这里考虑如下形式的凸优化问题:"
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.791,
                0.422,
                0.814
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathcal {D}} f (x),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.34,
                0.814,
                0.737,
                0.833
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & c _ {i} (x) \\leqslant 0, \\quad i = 1, 2, \\dots , m, \\end{array} \\tag {5.6.1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.379,
                0.837,
                0.442,
                0.853
            ],
            "angle": 0,
            "content": "\\[\nA x = b,\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "192"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.826,
                0.214
            ],
            "angle": 0,
            "content": "其中 \\(f(x)\\) 为适当的凸函数，\\(c_{i}(x), i = 1,2,\\dots,m\\) 是凸函数且 \\(\\mathbf{dom} c_{i} = \\mathbb{R}^{n}\\)，以及 \\(A \\in \\mathbb{R}^{p \\times n}, b \\in \\mathbb{R}^{p}\\) 是已知的。我们用集合 \\(\\mathcal{D}\\) 表示自变量 \\(x\\) 的自然定义域，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.42,
                0.221,
                0.663,
                0.239
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {D} = \\operatorname {d o m} f = \\{x \\mid f (x) <   + \\infty \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.251,
                0.825,
                0.289
            ],
            "angle": 0,
            "content": "自变量 \\(x\\) 除了受到自然定义域的限制以外，还需要受到约束的限制。我们定义可行域"
        },
        {
            "type": "equation",
            "bbox": [
                0.357,
                0.306,
                0.726,
                0.325
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {X} = \\left\\{x \\in \\mathcal {D}: c _ {i} (x) \\leqslant 0, i = 1, 2, \\dots , m; A x = b \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.341,
                0.825,
                0.42
            ],
            "angle": 0,
            "content": "在这里注意，由于凸优化问题的可行域是凸集，因此等式约束只可能是线性约束。凸优化问题(5.6.1)有很多好的性质。一个自然的问题是：我们能否像研究无约束问题那样找到该问题最优解的一阶充要条件？如果这样的条件存在，它在什么样的约束品性下成立？本节将比较具体地回答这一问题。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.45,
                0.577,
                0.468
            ],
            "angle": 0,
            "content": "5.6.1 Slater约束品性与强对偶原理"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.482,
                0.825,
                0.562
            ],
            "angle": 0,
            "content": "在通常情况下，优化问题的对偶间隙都是大于 0 的，即强对偶原理不满足。但是，对于很多凸优化问题，在特定约束品性满足的情况下可以证明强对偶原理。简单直观的一种约束品性是存在满足所有约束条件的严格可行解。首先，我们给出集合 \\(\\mathcal{D}\\) 的相对内点集 relint \\(\\mathcal{D}\\) 的定义。"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.577,
                0.824,
                0.614
            ],
            "angle": 0,
            "content": "定义5.13（相对内点集）给定集合 \\(\\mathcal{D}\\)，记其仿射包为 affine \\(\\mathcal{D}\\)（见定义2.14）。集合 \\(\\mathcal{D}\\) 的相对内点集定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.632,
                0.76,
                0.65
            ],
            "angle": 0,
            "content": "\\[\n\\mathbf {r e l i n t} \\mathcal {D} = \\{x \\in \\mathcal {D} \\mid \\exists r > 0, \\text {使 得} B (x, r) \\cap \\mathbf {a f f i n e} \\mathcal {D} \\subseteq \\mathcal {D} \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.667,
                0.826,
                0.726
            ],
            "angle": 0,
            "content": "相对内点是内点的推广，我们知道若 \\(x\\) 是集合 \\(\\mathcal{D} \\in \\mathbb{R}^n\\) 的内点，则存在一个以 \\(x\\) 为球心的 \\(n\\) 维球含于集合 \\(\\mathcal{D}\\)。若 \\(\\mathcal{D}\\) 本身的“维数”较低，则 \\(\\mathcal{D}\\) 不可能有内点；但如果在它的仿射包 affine \\(\\mathcal{D}\\) 中考虑，则 \\(\\mathcal{D}\\) 可能有相对内点。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.731,
                0.663,
                0.747
            ],
            "angle": 0,
            "content": "借助相对内点的定义，我们给出 Slater 约束品性"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.763,
                0.825,
                0.799
            ],
            "angle": 0,
            "content": "定义5.14(Slater约束品性）若对凸优化问题(5.6.1)，存在 \\(x\\in \\mathsf{relint}\\mathcal{D}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.806,
                0.682,
                0.824
            ],
            "angle": 0,
            "content": "\\[\nc _ {i} (x) <   0, \\quad i = 1, 2, \\dots , m, \\quad A x = b,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.796,
                0.853
            ],
            "angle": 0,
            "content": "则称对此问题 Slater 约束品性满足。有时也称该约束品性为 Slater 条件。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.6 带约束凸优化问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "193"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.215
            ],
            "angle": 0,
            "content": "Slater 约束品性实际上是要求自然定义域 \\(\\mathcal{D}\\) 的相对内点中存在使得不等式约束严格成立的点。对于很多凸优化问题，自然定义域 \\(\\mathcal{D}\\) 的仿射包 affine \\(\\mathcal{D} = \\mathbb{R}^n\\)，在这种情况下 Slater 条件中的相对内点就是内点。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.252,
                0.739,
                0.31
            ],
            "angle": 0,
            "content": "注5.2 当一些不等式约束是仿射函数时，Slater条件可以适当放宽．不妨假设前 \\(k\\) 个不等式约束是仿射函数，此时Slater约束品性可变为：存在\\(x\\in \\mathsf{relint}\\mathcal{D}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.183,
                0.341,
                0.724,
                0.358
            ],
            "angle": 0,
            "content": "\\[\nc _ {i} (x) \\leqslant 0, \\quad i = 1, 2, \\dots , k; \\quad c _ {i} (x) <   0, \\quad i = k + 1, k + 2 \\dots , m; \\quad A x = b,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.39,
                0.541,
                0.406
            ],
            "angle": 0,
            "content": "即对线性不等式约束无需要求其存在严格可行点."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.443,
                0.74,
                0.502
            ],
            "angle": 0,
            "content": "若凸优化问题 (5.6.1) 满足 Slater 条件, 一个很重要的结论就是强对偶原理成立。此外当 \\(d^{*} > -\\infty\\) 时, 对偶问题的最优解可以取到, 即存在对偶可行解 \\((\\lambda^{*}, \\nu^{*})\\), 满足 \\(g(\\lambda^{*}, \\nu^{*}) = d^{*} = p^{*}\\). 实际上我们有下面的定理:"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.538,
                0.738,
                0.575
            ],
            "angle": 0,
            "content": "定理5.12(强对偶原理[31]第5.3.2小节）如果凸优化问题(5.6.1)满足Slater条件，则强对偶原理成立."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.612,
                0.738,
                0.671
            ],
            "angle": 0,
            "content": "证明. 为了使证明简单化, 我们这里假设集合 \\(\\mathcal{D}\\) 内部非空 (即 \\(\\operatorname{relint} \\mathcal{D} = \\operatorname{int} \\mathcal{D}\\)), \\(A\\) 行满秩 (否则可以去掉多余的线性等式约束) 以及原始问题最优函数值 \\(p^{*}\\) 有限. 定义集合"
        },
        {
            "type": "equation",
            "bbox": [
                0.269,
                0.7,
                0.642,
                0.719
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {A} = \\left\\{(u, v, t) \\mid \\exists x \\in \\mathcal {D}, c _ {i} (x) \\leqslant u _ {i}, i = 1, 2, \\dots , m, \\right.\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.32,
                0.724,
                0.738,
                0.743
            ],
            "angle": 0,
            "content": "\\[\nA x - b = v, f (x) \\leqslant t \\}. \\tag {5.6.2}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.272,
                0.749,
                0.569,
                0.768
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {B} = \\left\\{\\left(0, 0, s\\right) \\in \\mathbb {R} ^ {m} \\times \\mathbb {R} ^ {p} \\times \\mathbb {R} \\mid s <   p ^ {*} \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.795,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "可以证明集合 \\(\\mathbb{A}\\) 和 \\(\\mathbb{B}\\) 是不相交的（如图5.7). 事实上, 假设 \\((u,v,t) \\in \\mathbb{A} \\cap \\mathbb{B}\\). 根据 \\((u,v,t) \\in \\mathbb{B}\\), 有 \\(u = 0, v = 0\\) 和 \\(t < p^{*}\\). 由 \\((u,v,t) \\in \\mathbb{A}\\), 可知 \\(f(x) \\leqslant t < p^{*}\\), 这与 \\(p^{*}\\) 是原始问题最优值矛盾."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "194"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "image",
            "bbox": [
                0.399,
                0.155,
                0.689,
                0.364
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.365,
                0.374,
                0.716,
                0.39
            ],
            "angle": 0,
            "content": "图5.7 集合A和B在 \\(u - t\\) 方向投影的示意图"
        },
        {
            "type": "image_footnote",
            "bbox": [
                0.316,
                0.394,
                0.767,
                0.412
            ],
            "angle": 0,
            "content": "(A 一般为有内点的凸集, B 是一条射线且不含点 \\((0,0,p^{*})\\) )"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.437,
                0.825,
                0.473
            ],
            "angle": 0,
            "content": "因为 \\(\\mathbb{A}\\) 和 \\(\\mathbb{B}\\) 均为凸集，由超平面分离定理，存在 \\((\\lambda, \\nu, \\mu) \\neq 0\\) 和 \\(\\alpha\\)，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.4,
                0.474,
                0.682,
                0.493
            ],
            "angle": 0,
            "content": "\\[\n\\lambda^ {\\mathrm {T}} u + \\nu^ {\\mathrm {T}} v + \\mu t \\geqslant \\alpha , \\quad \\forall (u, v, t) \\in \\mathbb {A},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.4,
                0.497,
                0.68,
                0.518
            ],
            "angle": 0,
            "content": "\\[\n\\lambda^ {\\mathrm {T}} u + \\nu^ {\\mathrm {T}} v + \\mu t \\leqslant \\alpha , \\quad \\forall (u, v, t) \\in \\mathbb {B}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.526,
                0.825,
                0.603
            ],
            "angle": 0,
            "content": "我们断言 \\(\\lambda \\geqslant 0\\) 和 \\(\\mu \\geqslant 0\\) （否则可以取 \\(u_{i}\\) 和 \\(t\\) 为任意大的正实数以及 \\(\\nu = 0\\) 这会导致 \\(\\lambda^{\\mathrm{T}}u + \\mu t\\) 在集合 \\(\\mathbb{A}\\) 上无下界）. 同时，由于 \\(\\mu t \\leqslant \\alpha\\) 对于所有 \\(t < p^{*}\\) 成立，可得 \\(\\mu p^{*} \\leqslant \\alpha\\). 对任意 \\(x \\in \\mathcal{D}\\)，取 \\((u, v, t) = (c_{i}(x), Ax - b, f(x)) \\in \\mathbb{A}\\)，可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.375,
                0.604,
                0.826,
                0.641
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i = 1} ^ {m} \\lambda_ {i} c _ {i} (x) + v ^ {\\mathrm {T}} (A x - b) + \\mu f (x) \\geqslant \\alpha \\geqslant \\mu p ^ {*}. \\tag {5.6.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.646,
                0.378,
                0.663
            ],
            "angle": 0,
            "content": "假设 \\(\\mu > 0\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.473,
                0.662,
                0.61,
                0.698
            ],
            "angle": 0,
            "content": "\\[\nL \\left(x, \\frac {\\lambda}{\\mu}, \\frac {\\nu}{\\mu}\\right) \\geqslant p ^ {*}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.703,
                0.825,
                0.786
            ],
            "angle": 0,
            "content": "进一步地，我们有 \\(g\\left(\\frac{\\lambda}{\\mu}, \\frac{\\nu}{\\mu}\\right) \\geqslant p^{*}\\)，根据弱对偶性 \\(g\\left(\\frac{\\lambda}{\\mu}, \\frac{\\nu}{\\mu}\\right) \\leqslant p^{*}\\) 自然成立。因此，必有 \\(g\\left(\\frac{\\lambda}{\\mu}, \\frac{\\nu}{\\mu}\\right) = p^{*}\\) 成立。说明在此情况下强对偶性满足，且对偶最优解可以达到。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.79,
                0.712,
                0.807
            ],
            "angle": 0,
            "content": "考虑 \\(\\mu = 0\\) 的情况，可以从上面得到对于所有的 \\(x\\in \\mathcal{D}\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.432,
                0.819,
                0.826,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i = 1} ^ {m} \\lambda_ {i} c _ {i} (x) + v ^ {\\mathrm {T}} (A x - b) \\geqslant 0. \\tag {5.6.4}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.6 带约束凸优化问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "195"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.42,
                0.174
            ],
            "angle": 0,
            "content": "取满足 Slater 条件的点 \\(x_{S}\\)，即有"
        },
        {
            "type": "equation",
            "bbox": [
                0.392,
                0.184,
                0.513,
                0.22
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i = 1} ^ {m} \\lambda_ {i} c _ {i} (x _ {S}) \\geqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.231,
                0.614,
                0.248
            ],
            "angle": 0,
            "content": "又 \\(c_{i}(x_{S}) < 0\\) 和 \\(\\lambda_{i} \\geqslant 0\\) ，我们得到 \\(\\lambda = 0\\) ，即(5.6.4)式化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.35,
                0.262,
                0.737,
                0.281
            ],
            "angle": 0,
            "content": "\\[\n\\nu^ {\\mathrm {T}} (A x - b) \\geqslant 0, \\quad \\forall x \\in \\mathcal {D}. \\tag {5.6.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.295,
                0.739,
                0.354
            ],
            "angle": 0,
            "content": "同时, 根据 \\((\\lambda, \\nu, \\mu) \\neq 0\\) 可知 \\(\\nu \\neq 0\\), 结合 \\(A\\) 行满秩可以得到 \\(A^{\\mathrm{T}} \\nu \\neq 0\\). 由于 \\(x_{S}\\) 是可行解, 我们有 \\(\\nu^{\\mathrm{T}}(Ax_{S} - b) = 0\\). 因为 \\(x_{S} \\in \\mathbf{int} \\mathcal{D}\\), 则存在点 \\(\\tilde{x} = x_{S} + e \\in \\mathcal{D}\\), 满足 \\(\\nu^{\\mathrm{T}}(A \\tilde{x} - b) < 0\\). 这与 (5.6.5) 式矛盾."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.358,
                0.502,
                0.375
            ],
            "angle": 0,
            "content": "综上所述，Slater条件能保证强对偶性"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.388,
                0.739,
                0.426
            ],
            "angle": 0,
            "content": "在上面定理的证明中，Slater条件用来保证 \\(\\mu \\neq 0\\) ，这里，我们假设了relint \\(\\mathcal{D} = \\mathrm{int}\\mathcal{D}\\) .对于直接使用相对内点的证明，读者可以参考[164]定理31.1."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.452,
                0.353,
                0.47
            ],
            "angle": 0,
            "content": "5.6.2 一阶充要条件"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.483,
                0.74,
                0.562
            ],
            "angle": 0,
            "content": "对于一般的约束优化问题，当问题满足特定约束品性时，我们知道KKT条件是局部最优解处的必要条件。而对于凸优化问题，当Slater条件满足时，KKT条件则变为局部最优解的充要条件（根据凸性，局部最优解也是全局最优解）。实际上我们有如下定理。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.575,
                0.737,
                0.613
            ],
            "angle": 0,
            "content": "定理5.13（凸问题KKT条件）对于凸优化问题(5.6.1)，如果Slater条件成立，那么 \\(x^{*},\\lambda^{*}\\) 分别是原始、对偶全局最优解当且仅当"
        },
        {
            "type": "equation",
            "bbox": [
                0.26,
                0.623,
                0.638,
                0.655
            ],
            "angle": 0,
            "content": "\\[\n\\text {稳 定 性 条 件} \\quad 0 \\in \\partial f (x ^ {*}) + \\sum_ {i \\in \\mathcal {I}} \\lambda_ {i} ^ {*} \\partial c _ {i} (x ^ {*}) + \\sum_ {i \\in \\mathcal {E}} \\lambda_ {i} ^ {*} a _ {i},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.227,
                0.661,
                0.492,
                0.677
            ],
            "angle": 0,
            "content": "\\[\n\\text {原 始 可 行 性 条 件} A x ^ {*} = b, \\forall i \\in \\mathcal {E},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.227,
                0.68,
                0.737,
                0.703
            ],
            "angle": 0,
            "content": "\\[\n\\text {原 始 可 行 性 条 件} \\quad c _ {i} (x ^ {*}) \\leqslant 0, \\forall i \\in \\mathcal {I}, \\tag {5.6.6}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.226,
                0.709,
                0.48,
                0.727
            ],
            "angle": 0,
            "content": "\\[\n\\text {对 偶 可 行 性 条 件} \\quad \\lambda_ {i} ^ {*} \\geqslant 0, \\forall i \\in \\mathcal {I},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.244,
                0.734,
                0.521,
                0.751
            ],
            "angle": 0,
            "content": "\\[\n\\text {互 补 松 弛 条 件} \\quad \\lambda_ {i} ^ {*} c _ {i} \\left(x ^ {*}\\right) = 0, \\forall i \\in \\mathcal {I}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.764,
                0.383,
                0.781
            ],
            "angle": 0,
            "content": "其中 \\(a_{i}\\) 是矩阵 \\(A^{\\mathrm{T}}\\) 的第 \\(i\\) 列."
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.795,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "在这里条件 (5.6.6) 和条件 (5.5.8) 略有不同。在凸优化问题中没有假设 \\(f(x)\\) 和 \\(c_{i}(x)\\) 是可微函数，因此我们在这里使用的是次梯度。当 \\(f(x)\\) 和 \\(c_{i}(x)\\) 都是凸可微函数时，条件 (5.6.6) 就是条件 (5.5.8)。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "196"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.825,
                0.195
            ],
            "angle": 0,
            "content": "定理5.13的充分性比较容易说明．实际上，设存在 \\((\\bar{x},\\bar{\\lambda})\\) 满足KKT条件(5.6.6)，我们考虑凸优化问题的拉格朗日函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.208,
                0.713,
                0.239
            ],
            "angle": 0,
            "content": "\\[\nL (x, \\lambda) = f (x) + \\sum_ {i \\in \\mathcal {I}} \\lambda_ {i} c _ {i} (x) + \\sum_ {i \\in \\mathcal {E}} \\lambda_ {i} \\left(a _ {i} ^ {\\mathrm {T}} x - b _ {i}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.249,
                0.825,
                0.309
            ],
            "angle": 0,
            "content": "当固定 \\(\\lambda = \\bar{\\lambda}\\) 时，注意到 \\(\\bar{\\lambda}_i\\geqslant 0,i\\in \\mathcal{I}\\) 以及 \\(\\bar{\\lambda}_i(a_i^{\\mathrm{T}}x),i\\in \\mathcal{E}\\) 是线性函数可知\\(L(x,\\bar{\\lambda})\\) 是关于 \\(x\\) 的凸函数．由凸函数全局最优点的一阶充要性可知，此时\\(\\bar{x}\\) 就是 \\(L(x,\\bar{\\lambda})\\) 的全局极小点．根据拉格朗日对偶函数的定义，"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.322,
                0.651,
                0.347
            ],
            "angle": 0,
            "content": "\\[\nL (\\bar {x}, \\bar {\\lambda}) = \\inf  _ {x \\in \\mathcal {D}} L (x, \\bar {\\lambda}) = g (\\bar {\\lambda}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.357,
                0.818,
                0.375
            ],
            "angle": 0,
            "content": "根据原始可行性条件 \\(A\\bar{x} = b\\) 以及互补松弛条件 \\(\\bar{\\lambda}_i c_i(\\bar{x}) = 0, i \\in \\mathcal{I}\\) 可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.425,
                0.389,
                0.657,
                0.408
            ],
            "angle": 0,
            "content": "\\[\nL (\\bar {x}, \\bar {\\lambda}) = f (\\bar {x}) + 0 + 0 = f (\\bar {x}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.422,
                0.39,
                0.438
            ],
            "angle": 0,
            "content": "根据弱对偶原理，"
        },
        {
            "type": "equation",
            "bbox": [
                0.416,
                0.453,
                0.825,
                0.473
            ],
            "angle": 0,
            "content": "\\[\nL (\\bar {x}, \\bar {\\lambda}) = f (\\bar {x}) \\geqslant p ^ {*} \\geqslant d ^ {*} \\geqslant g (\\bar {\\lambda}). \\tag {5.6.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.486,
                0.825,
                0.524
            ],
            "angle": 0,
            "content": "由于 \\(L(\\bar{x},\\bar{\\lambda}) = g(\\bar{\\lambda})\\) ，(5.6.7)式中的不等号皆为等号，因此我们有 \\(p^* = d^*\\) 且 \\(\\bar{x},\\bar{\\lambda}\\) 分别是原始问题和对偶问题的最优解"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.528,
                0.827,
                0.649
            ],
            "angle": 0,
            "content": "定理5.13的充分性说明，若能直接求解出凸优化问题(5.6.1)的KKT对，则其就对应问题的最优解．注意，在充分性部分的证明中，我们没有使用Slater条件，这是因为在证明的一开始假设了KKT点是存在的．Slater条件的意义在于当问题(5.6.1)最优解存在时，其相应KKT条件也会得到满足．换句话说，当Slater条件不满足时，即使原始问题存在全局极小值点，也可能不存在 \\((x^{*},\\lambda^{*})\\) 满足KKT条件(5.6.6)."
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.652,
                0.825,
                0.689
            ],
            "angle": 0,
            "content": "定理5.13的必要性证明比较复杂，我们在下一个小节给出。读者可根据自己实际情况自行阅读。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.715,
                0.586,
                0.734
            ],
            "angle": 0,
            "content": "*5.6.3 一阶充要条件：必要性的证明"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.747,
                0.367,
                0.763
            ],
            "angle": 0,
            "content": "考虑问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.495,
                0.767,
                0.825,
                0.79
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathcal {X}} f (x), \\tag {5.6.8}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{X}\\) 是闭凸集并且 \\(f(x)\\) 是凸函数。事实上，问题(5.6.8)也给出了凸优化问题的一般形式。为了证明KKT条件的必要性，我们首先引入极锥和法锥的概念。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.6 带约束凸优化问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "197"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.157,
                0.579,
                0.174
            ],
            "angle": 0,
            "content": "定义5.15（极锥）设 \\(K\\) 为 \\(\\mathbb{R}^n\\) 的子集，我们称集合"
        },
        {
            "type": "equation",
            "bbox": [
                0.335,
                0.189,
                0.572,
                0.209
            ],
            "angle": 0,
            "content": "\\[\nK ^ {\\circ} = \\{y \\mid \\langle x, y \\rangle \\leqslant 0, \\quad \\forall x \\in K \\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.223,
                0.556,
                0.24
            ],
            "angle": 0,
            "content": "为 \\(K\\) 的极锥，其中 \\(\\langle \\cdot ,\\cdot \\rangle\\) 是 \\(K\\) 所在空间的一个内积"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.253,
                0.739,
                0.312
            ],
            "angle": 0,
            "content": "对比对偶锥的定义（定义5.6）可知极锥实际上是对偶锥中元素取相反元素得到的，一个直观的例子如图5.8所示。极锥有非常好的性质，当 \\(K\\) 为凸锥时，根据定义容易验证下面的结论成立："
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.325,
                0.471,
                0.342
            ],
            "angle": 0,
            "content": "命题5.2 设 \\(K\\) 为 \\(\\mathbb{R}^n\\) 上的凸锥，则"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.355,
                0.316,
                0.371
            ],
            "angle": 0,
            "content": "(1) \\(K^{\\circ}\\) 是闭凸锥；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.385,
                0.461,
                0.402
            ],
            "angle": 0,
            "content": "(2) \\(K^{\\circ} = (\\bar{K})^{\\circ}\\), 其中 \\(\\bar{K}\\) 为 \\(K\\) 的闭包;"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.417,
                0.443,
                0.433
            ],
            "angle": 0,
            "content": "(3) 若 \\(K\\) 还为闭凸锥, 则 \\(K^{\\circ \\circ} = K\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.355,
                0.461,
                0.433
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image",
            "bbox": [
                0.283,
                0.454,
                0.627,
                0.687
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.316,
                0.698,
                0.591,
                0.715
            ],
            "angle": 0,
            "content": "图5.8 \\(\\mathbb{R}^2\\) 上的锥 \\(K\\) 以及其极锥 \\(K^{\\circ}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.741,
                0.591,
                0.757
            ],
            "angle": 0,
            "content": "法锥是另一个重要的概念，它和极锥有密切的关系。"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.771,
                0.689,
                0.788
            ],
            "angle": 0,
            "content": "定义5.16(法锥）设 \\(\\mathcal{X}\\) 为凸集， \\(x\\in \\mathcal{X}\\) 为其上一点．我们称集合"
        },
        {
            "type": "equation",
            "bbox": [
                0.305,
                0.799,
                0.601,
                0.822
            ],
            "angle": 0,
            "content": "\\[\nN _ {\\mathcal {X}} (x) \\stackrel {{\\mathrm {d e f}}} {{=}} \\left\\{z \\mid \\langle z, y - x \\rangle \\leqslant 0, \\forall y \\in \\mathcal {X} \\right\\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.837,
                0.344,
                0.853
            ],
            "angle": 0,
            "content": "为 \\(\\mathcal{X}\\) 在点 \\(x\\) 处的法锥"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "198"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.826,
                0.234
            ],
            "angle": 0,
            "content": "法锥是几何学中法线的推广，一个直观的例子见图5.9，图中我们选取集合 \\(\\mathcal{X}\\) 的四个不同点来计算其法锥，当点 \\(x\\) 在 \\(\\mathcal{X}\\) 光滑边界上时，\\(N_{\\mathcal{X}}(x)\\) 退化成外法线。此外，从法锥的定义容易看出，法锥实际上是将 \\(x\\) 平移到原点后的极锥，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.457,
                0.239,
                0.626,
                0.258
            ],
            "angle": 0,
            "content": "\\[\nN _ {\\mathcal {X}} (x) = \\left(\\mathcal {X} - \\{x \\}\\right) ^ {\\circ}.\n\\]"
        },
        {
            "type": "image",
            "bbox": [
                0.37,
                0.291,
                0.719,
                0.496
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.41,
                0.507,
                0.673,
                0.525
            ],
            "angle": 0,
            "content": "图5.9 集合 \\(\\mathcal{X}\\) 在不同点处的法锥"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.55,
                0.825,
                0.587
            ],
            "angle": 0,
            "content": "法锥的重要性之一是其和示性函数的次微分是等价的. 实际上我们有如下的结果:"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.601,
                0.825,
                0.639
            ],
            "angle": 0,
            "content": "命题5.3设 \\(\\mathcal{X}\\) 是闭凸集， \\(x\\in \\mathcal{X}\\) ，则 \\(N_{\\mathcal{X}}(x) = \\partial I_{\\mathcal{X}}(x)\\) ，其中 \\(I_{\\mathcal{X}}(x)\\) 是集合 \\(\\mathcal{X}\\) 的示性函数."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.653,
                0.627,
                0.671
            ],
            "angle": 0,
            "content": "证明. 若 \\(z \\in \\partial I_{\\mathcal{X}}(x)\\), 则对任意 \\(y \\in \\mathbf{dom} I_{\\mathcal{X}}\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.421,
                0.686,
                0.662,
                0.706
            ],
            "angle": 0,
            "content": "\\[\n0 = I _ {\\mathcal {X}} (y) - I _ {\\mathcal {X}} (x) \\geqslant z ^ {\\mathrm {T}} (y - x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.72,
                0.585,
                0.737
            ],
            "angle": 0,
            "content": "这说明 \\(\\partial I_{\\mathcal{X}}(x)\\subseteq N_{\\mathcal{X}}(x)\\) ，反之结论亦成立"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.751,
                0.7,
                0.768
            ],
            "angle": 0,
            "content": "借助法锥的概念我们可给出问题(5.6.8)的最优性条件"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.782,
                0.825,
                0.82
            ],
            "angle": 0,
            "content": "定理5.14 设 Slater 条件满足．对于 \\(x^{*} \\in \\mathcal{X}\\)，其为问题(5.6.8)的全局极小解当且仅当存在 \\(s \\in \\partial f(x^{*})\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.489,
                0.836,
                0.596,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n- s \\in N _ {\\mathcal {X}} \\left(x ^ {*}\\right).\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.6 带约束凸优化问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "199"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.193
            ],
            "angle": 0,
            "content": "证明．通过利用凸集 \\(\\mathcal{X}\\) 的示性函数 \\(I_{\\mathcal{X}}(x)\\) ，我们可以将问题(5.6.8)转化为如下无约束优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.198,
                0.531,
                0.22
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x) + I _ {\\mathcal {X}} (x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.226,
                0.519,
                0.241
            ],
            "angle": 0,
            "content": "由定理5.5可知，\\(x^{*}\\) 为其全局极小解当且仅当"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.255,
                0.525,
                0.273
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial (f + I _ {\\mathcal {X}}) (x ^ {*}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.286,
                0.697,
                0.303
            ],
            "angle": 0,
            "content": "由于 Slater 条件成立，根据 Moreau-Rockafellar 定理（定理 2.22）有"
        },
        {
            "type": "equation",
            "bbox": [
                0.32,
                0.316,
                0.585,
                0.334
            ],
            "angle": 0,
            "content": "\\[\n\\partial (f + I _ {\\mathcal {X}}) \\left(x ^ {*}\\right) = \\partial f \\left(x ^ {*}\\right) + \\partial I _ {\\mathcal {X}} \\left(x ^ {*}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.348,
                0.515,
                0.365
            ],
            "angle": 0,
            "content": "因此，一定存在一个次梯度 \\(s \\in \\partial f(x^{*})\\) ，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.4,
                0.378,
                0.508,
                0.395
            ],
            "angle": 0,
            "content": "\\[\n- s \\in \\partial I _ {\\mathcal {X}} \\left(x ^ {*}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.408,
                0.492,
                0.426
            ],
            "angle": 0,
            "content": "又 \\(\\partial I_{\\mathcal{X}}(x^{*}) = N_{\\mathcal{X}}(x^{*})\\) ，故有 \\(-s\\in N_{\\mathcal{X}}(x^{*})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.429,
                0.714,
                0.447
            ],
            "angle": 0,
            "content": "反之，假设存在 \\(s \\in \\partial f(x^{*})\\) 满足 \\(-s \\in N_{\\mathcal{X}}(x^{*})\\)。根据次梯度的定义，"
        },
        {
            "type": "equation",
            "bbox": [
                0.311,
                0.458,
                0.597,
                0.478
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f \\left(x ^ {*}\\right) + s ^ {\\mathrm {T}} \\left(y - x ^ {*}\\right), \\quad \\forall y \\in \\mathcal {X}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.49,
                0.381,
                0.507
            ],
            "angle": 0,
            "content": "因为 \\(-s\\in N_{\\mathcal{X}}(x^{*})\\) ，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.519,
                0.512,
                0.539
            ],
            "angle": 0,
            "content": "\\[\ns ^ {T} (y - x ^ {*}) \\geqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.552,
                0.217,
                0.568
            ],
            "angle": 0,
            "content": "因此，"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.572,
                0.547,
                0.59
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\geqslant f \\left(x ^ {*}\\right), \\quad \\forall y \\in \\mathcal {X},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.599,
                0.326,
                0.615
            ],
            "angle": 0,
            "content": "即 \\(x^{*}\\) 为全局极小解"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.629,
                0.737,
                0.667
            ],
            "angle": 0,
            "content": "定理5.14将最优性条件转化为了 \\(\\partial f(x)\\) 和法锥 \\(N_{\\chi}(x)\\) 的关系．对问题(5.6.8)，其可行域实际上可写成若干集合的交，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.295,
                0.674,
                0.612,
                0.719
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {X} = \\left(\\bigcap_ {i \\in \\mathcal {I}} \\{x \\mid c _ {i} (x) \\leqslant 0 \\}\\right) \\cap \\{x \\mid A x = b \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.725,
                0.737,
                0.763
            ],
            "angle": 0,
            "content": "通常来说，对每个约束 \\(c_{i}(x)\\leqslant 0\\) 研究其法锥是比较容易的，我们给出如下关于多个集合交集的法锥的引理："
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.774,
                0.738,
                0.831
            ],
            "angle": 0,
            "content": "引理5.4（交集的法锥为逐个集合对应法锥的和）给定 \\(\\mathbb{R}^n\\) 中的一列闭凸集 \\(\\mathcal{X}_1,\\mathcal{X}_2,\\dots ,\\mathcal{X}_m\\) ，记其交集为 \\(\\mathcal{X} = \\mathcal{X}_1\\cap \\mathcal{X}_2\\cap \\dots \\cap \\mathcal{X}_m\\) .对其内一点 \\(x\\in \\mathcal{X}\\) 如果"
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.836,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {i n t} \\mathcal {X} _ {1} \\cap \\operatorname {i n t} \\mathcal {X} _ {2} \\cap \\dots \\cap \\operatorname {i n t} \\mathcal {X} _ {m} \\neq \\emptyset , \\tag {5.6.9}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "200"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.158,
                0.28,
                0.172
            ],
            "angle": 0,
            "content": "则"
        },
        {
            "type": "equation",
            "bbox": [
                0.38,
                0.177,
                0.703,
                0.195
            ],
            "angle": 0,
            "content": "\\[\nN _ {\\mathcal {X}} (x) = N _ {\\mathcal {X} _ {1}} (x) + N _ {\\mathcal {X} _ {2}} (x) + \\dots + N _ {\\mathcal {X} _ {m}} (x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.204,
                0.679,
                0.221
            ],
            "angle": 0,
            "content": "若某 \\(\\mathcal{X}_i\\) 是多面体，则在(5.6.9)式中int \\(\\mathcal{X}_i\\) 可减弱为 \\(\\mathcal{X}_i\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.233,
                0.825,
                0.292
            ],
            "angle": 0,
            "content": "引理5.4实际上是Moreau-Rockafellar定理的推论，利用法锥和示性函数次微分的等价性可自然得到此结果．详细讨论可参考[164]定理23.8．下面的引理给出了每个集合 \\(\\{x\\mid c_i(x)\\leqslant 0\\}\\) 法锥的具体形式."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.303,
                0.825,
                0.34
            ],
            "angle": 0,
            "content": "引理5.5 设 \\(c(x)\\) 是适当的闭凸函数，\\(\\partial c(x_0)\\) 存在、非空且 \\(c(x_0) = 0\\) 定义集合"
        },
        {
            "type": "equation",
            "bbox": [
                0.466,
                0.344,
                0.618,
                0.363
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {X} = \\{x \\mid c (x) \\leqslant 0 \\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.372,
                0.824,
                0.407
            ],
            "angle": 0,
            "content": "且存在 \\(x_{s}\\) 使得 \\(c(x_{s}) < 0\\) ，则有 \\(N_{\\mathcal{X}}(x_0) = \\mathrm{cone}\\partial c(x_0)\\) ，其中coneS是集合\\(S\\) 的锥包，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.429,
                0.41,
                0.654,
                0.43
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {c o n e} S \\stackrel {{\\text {d e f}}} {{=}} \\left\\{t x \\mid x \\in S, t \\geqslant 0 \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.441,
                0.69,
                0.459
            ],
            "angle": 0,
            "content": "证明. 为了记号方便，定义 \\(K_{0} = \\operatorname{cone}(\\mathcal{X} - \\{x_{0}\\})\\) 以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.46,
                0.472,
                0.825,
                0.49
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {G} \\left(x _ {0}\\right) = \\mathbf {c o n e} \\partial c \\left(x _ {0}\\right). \\tag {5.6.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.504,
                0.505,
                0.52
            ],
            "angle": 0,
            "content": "根据法锥的定义，我们实际上有"
        },
        {
            "type": "equation",
            "bbox": [
                0.478,
                0.534,
                0.605,
                0.552
            ],
            "angle": 0,
            "content": "\\[\n\\left(K _ {0}\\right) ^ {\\circ} = N _ {\\mathcal {X}} \\left(x _ {0}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.565,
                0.825,
                0.603
            ],
            "angle": 0,
            "content": "设 \\(d \\in K_0\\)，由 \\(\\mathcal{X}\\) 的凸性知，当 \\(\\tau\\) 充分小时有 \\(x_0 + \\tau d \\in \\mathcal{X}\\)。现在考虑点 \\(x_0\\) 处的方向导数 \\(\\partial c(x_0; d)\\)，根据定理 2.21，"
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.615,
                0.64,
                0.645
            ],
            "angle": 0,
            "content": "\\[\n0 \\geqslant \\partial c (x _ {0}; d) = \\sup  _ {s \\in \\partial c (x _ {0})} d ^ {\\mathrm {T}} s.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.655,
                0.315,
                0.671
            ],
            "angle": 0,
            "content": "这说明"
        },
        {
            "type": "equation",
            "bbox": [
                0.457,
                0.674,
                0.625,
                0.693
            ],
            "angle": 0,
            "content": "\\[\nd ^ {T} s \\leqslant 0, \\quad \\forall s \\in \\mathcal {G} (x _ {0}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.701,
                0.482,
                0.719
            ],
            "angle": 0,
            "content": "即我们证明了 \\(K_0 \\subseteq \\left(\\mathcal{G}(x_0)\\right)^\\circ\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.722,
                0.825,
                0.76
            ],
            "angle": 0,
            "content": "反过来，若 \\(d \\in \\left(\\mathcal{G}(x_0)\\right)^\\circ\\)，则 \\(\\partial c(x_0;d) \\leqslant 0\\)。下面我们分两种情况进行讨论。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.765,
                0.825,
                0.801
            ],
            "angle": 0,
            "content": "(1) \\(\\partial c(x_0;d) < 0\\). 由方向导数的定义知, 当 \\(\\tau\\) 充分小时有 \\(c(x_0 + \\tau d) < 0\\), 即 \\(x_0 + \\tau d \\in \\mathcal{X}\\). 这说明 \\(d \\in K_0\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.291,
                0.805,
                0.614,
                0.822
            ],
            "angle": 0,
            "content": "(2) \\(\\partial c(x_0;d) = 0\\) 此时构造一系列方向 \\(d^k\\):"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.835,
                0.652,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nd ^ {k} = (1 - \\alpha_ {k}) d + \\alpha_ {k} (x _ {s} - x _ {0}),\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.454,
                0.133
            ],
            "angle": 0,
            "content": "5.6 带约束凸优化问题的最优性理论"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "201"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.737,
                0.194
            ],
            "angle": 0,
            "content": "其中 \\(x_{s}\\) 为满足 Slater 条件的点，\\(\\{\\alpha_{k}\\}\\) 为单调下降趋于 0 的正序列，则 \\(d^{k}\\) 处的方向导数有如下估计："
        },
        {
            "type": "equation",
            "bbox": [
                0.255,
                0.205,
                0.65,
                0.32
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\partial c \\left(x _ {0}; d ^ {k}\\right) = \\sup  _ {s \\in \\partial c \\left(x _ {0}\\right)} s ^ {\\mathrm {T}} d ^ {k} \\\\ \\leqslant \\left(1 - \\alpha_ {k}\\right) \\sup  _ {s \\in \\partial c \\left(x _ {0}\\right)} s ^ {\\mathrm {T}} d + \\alpha_ {k} \\sup  _ {s \\in \\partial c \\left(x _ {0}\\right)} s ^ {\\mathrm {T}} \\left(x _ {s} - x _ {0}\\right) \\\\ \\leqslant \\left(1 - \\alpha_ {k}\\right) \\partial c \\left(x _ {0}; d\\right) + \\alpha_ {k} c \\left(x _ {s}\\right) \\\\ <   0, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.333,
                0.737,
                0.371
            ],
            "angle": 0,
            "content": "其中第二个不等式用到了次梯度的定义以及定理2.20的结果．根据(1)的论断，可知 \\(d^{k}\\in K_{0},\\forall k\\) ，取极限可知 \\(d\\in \\overline{K_0}\\) ，即 \\(d\\) 为 \\(K_{0}\\) 闭包中的元素."
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.374,
                0.647,
                0.392
            ],
            "angle": 0,
            "content": "结合上面的两种情况可知 \\(\\left(\\mathcal{G}(x_0)\\right)^\\circ \\subseteq \\overline{K_0}\\). 进一步, 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.375,
                0.405,
                0.737,
                0.425
            ],
            "angle": 0,
            "content": "\\[\nK _ {0} \\subseteq (\\mathcal {G} (x _ {0})) ^ {\\circ} \\subseteq \\overline {{K _ {0}}}. \\tag {5.6.11}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.439,
                0.737,
                0.477
            ],
            "angle": 0,
            "content": "注意到极锥是闭集，对(5.6.11)式中的三个集合取闭包可知 \\(\\left(\\mathcal{G}(x_0)\\right)^{\\circ} = \\overline{K_0}\\). 再次取二者的极锥，我们最终得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.309,
                0.49,
                0.599,
                0.512
            ],
            "angle": 0,
            "content": "\\[\nN _ {\\mathcal {X}} \\left(x _ {0}\\right) = \\left(\\overline {{K _ {0}}}\\right) ^ {\\circ} = \\left(\\mathcal {G} \\left(x _ {0}\\right)\\right) ^ {\\circ \\circ} = \\mathcal {G} \\left(x _ {0}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.525,
                0.737,
                0.582
            ],
            "angle": 0,
            "content": "利用上面的若干结论，我们可以进一步刻画出凸优化问题(5.6.1)的法锥．在这里注意，由于 \\(\\mathbf{dom}c_{i} = \\mathbb{R}^{n}\\) ，根据定理2.14，这意味着 \\(c_{i}(x)\\) 在全空间是连续的凸函数"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.596,
                0.737,
                0.632
            ],
            "angle": 0,
            "content": "定理5.15 假设 Slater 条件满足．若 \\(x\\) 为问题(5.6.1)的可行域 \\(\\mathcal{X}\\) 中的点，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.328,
                0.635,
                0.737,
                0.667
            ],
            "angle": 0,
            "content": "\\[\nN _ {\\mathcal {X}} (x) = \\sum_ {i \\in \\mathcal {A} (x) \\cap \\mathcal {I}} \\mathcal {G} _ {i} (x) + \\mathcal {R} \\left(A ^ {\\mathrm {T}}\\right), \\tag {5.6.12}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.674,
                0.737,
                0.713
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{G}_i(x)\\) 针对每个 \\(c_{i}(x)\\) 根据 (5.6.10) 式定义, \\(\\mathcal{R}(A^{\\mathrm{T}})\\) 表示矩阵 \\(A^{\\mathrm{T}}\\) 的像空间, \\(\\mathcal{A}(x)\\) 为点 \\(x\\) 处的积极集 (见定义 5.8)."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.726,
                0.636,
                0.742
            ],
            "angle": 0,
            "content": "证明. 该定理的证明实际上是前面结论的结合. 定义可行域"
        },
        {
            "type": "equation",
            "bbox": [
                0.232,
                0.751,
                0.675,
                0.795
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {X} = \\left(\\bigcap_ {i \\in \\mathcal {I}} \\left\\{x \\mid c _ {i} (x) \\leqslant 0 \\right\\}\\right) \\cap \\left\\{x \\mid A x = b \\right\\} \\stackrel {\\text {d e f}} {=} \\left(\\bigcap_ {i \\in \\mathcal {I}} \\mathcal {X} _ {i}\\right) \\cap L.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.804,
                0.465,
                0.82
            ],
            "angle": 0,
            "content": "由 Slater 条件成立，使用引理 5.4 可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.258,
                0.836,
                0.649,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nN _ {\\mathcal {X}} (x) = N _ {\\mathcal {X} _ {1}} (x) + N _ {\\mathcal {X} _ {2}} (x) + \\dots + N _ {\\mathcal {X} _ {m}} (x) + N _ {L} (x).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "202"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.157,
                0.576,
                0.174
            ],
            "angle": 0,
            "content": "下面分别计算上式中各项法锥的具体形式"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.179,
                0.826,
                0.237
            ],
            "angle": 0,
            "content": "在点 \\(x\\) 处，若 \\(c_{i}(x) < 0\\) ，根据连续性可知 \\(c_{i}\\) 在点 \\(x\\) 的一个开邻域内均为负．由法锥的定义可知此时 \\(N_{\\mathcal{X}_i}(x) = \\{0\\}\\) ，这说明针对不起作用的约束，在最优性条件中可不作考虑."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.242,
                0.669,
                0.259
            ],
            "angle": 0,
            "content": "若 \\(c_{i}(x) = 0\\) ，根据引理5.5，可知 \\(N_{\\mathcal{X}_i}(x) = \\mathcal{G}_i(x)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.263,
                0.825,
                0.3
            ],
            "angle": 0,
            "content": "最后只需要计算出 \\( L = \\{x \\mid Ax = b\\} \\) 的法锥即可．注意到对任意 \\( y \\in L \\)，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.487,
                0.307,
                0.596,
                0.325
            ],
            "angle": 0,
            "content": "\\[\nA (y - x) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.338,
                0.822,
                0.356
            ],
            "angle": 0,
            "content": "这说明 \\(\\mathcal{R}(A^{\\mathrm{T}}) \\subseteq N_{L}(x)\\). 另一方面, 根据 \\(\\mathbb{R}^{n}\\) 上的分解, 对任意 \\(w \\in \\mathbb{R}^{n}\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.411,
                0.373,
                0.672,
                0.391
            ],
            "angle": 0,
            "content": "\\[\nw = z + A ^ {\\mathrm {T}} s, \\quad z \\in \\mathcal {N} (A),   s \\in \\mathbb {R} ^ {p},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.409,
                0.692,
                0.426
            ],
            "angle": 0,
            "content": "可知若 \\(w\\in N_L(x)\\) 则必有 \\(z = 0\\) ，这说明 \\(N_{L}(x)\\subseteq \\mathcal{R}(A^{\\mathrm{T}})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.431,
                0.605,
                0.448
            ],
            "angle": 0,
            "content": "结合上述的推导我们可知(5.6.12)式成立"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.432,
                0.825,
                0.445
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.469,
                0.825,
                0.507
            ],
            "angle": 0,
            "content": "将(5.6.12)式代入定理5.14，我们就可以证明定理5.13描述的KKT条件的必要性."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.53,
                0.801,
                0.546
            ],
            "angle": 0,
            "content": "证明（KKT条件的必要性）．结合定理5.14和定理5.15我们可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.562,
                0.685,
                0.595
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial f (x ^ {*}) + \\sum_ {i \\in \\mathcal {A} (x ^ {*}) \\cap \\mathcal {I}} \\mathcal {G} _ {i} (x ^ {*}) + \\mathcal {R} (A ^ {\\mathrm {T}}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.61,
                0.713,
                0.627
            ],
            "angle": 0,
            "content": "根据 \\(\\mathcal{G}_i(x^*)\\) 和 \\(\\mathcal{R}(A^{\\mathrm{T}})\\) 的定义，存在 \\(\\lambda_i^* \\geqslant 0\\) 和 \\(\\nu_i^* \\in \\mathbb{R}\\)，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.643,
                0.696,
                0.675
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial f (x ^ {*}) + \\sum_ {i \\in \\mathcal {A} (x ^ {*}) \\cap \\mathcal {I}} \\lambda_ {i} ^ {*} \\partial c _ {i} (x ^ {*}) + \\sum_ {i \\in \\mathcal {E}} v _ {i} ^ {*} a _ {i}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.689,
                0.66,
                0.707
            ],
            "angle": 0,
            "content": "若补充定义 \\(\\lambda_i^* = 0, i \\in \\mathcal{I} \\backslash \\mathcal{A}(x^*)\\)，则我们最终可得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.722,
                0.675,
                0.753
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial f (x ^ {*}) + \\sum_ {i \\in \\mathcal {I}} \\lambda_ {i} ^ {*} \\partial c _ {i} (x ^ {*}) + \\sum_ {i \\in \\mathcal {E}} v _ {i} a _ {i}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.766,
                0.78,
                0.783
            ],
            "angle": 0,
            "content": "由可行性知 \\(Ax^{*} = b\\) 以及 \\(c_{i}(x^{*})\\leqslant 0\\) ，由 \\(\\lambda_i^*\\) 的定义可知互补松弛条件"
        },
        {
            "type": "equation",
            "bbox": [
                0.462,
                0.801,
                0.621,
                0.818
            ],
            "angle": 0,
            "content": "\\[\n\\lambda_ {i} ^ {*} c _ {i} \\left(x ^ {*}\\right) = 0, \\quad i \\in \\mathcal {I}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.497,
                0.853
            ],
            "angle": 0,
            "content": "成立．KKT条件的必要性得证"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.838,
                0.825,
                0.85
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.437,
                0.133
            ],
            "angle": 0,
            "content": "5.7 约束优化最优性理论应用实例"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "203"
        },
        {
            "type": "title",
            "bbox": [
                0.264,
                0.155,
                0.646,
                0.177
            ],
            "angle": 0,
            "content": "5.7 约束优化最优性理论应用实例"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.195,
                0.737,
                0.234
            ],
            "angle": 0,
            "content": "这一部分，我们以实例的方式更进一步地解释光滑凸优化问题、非光滑凸优化问题以及光滑非凸优化问题的最优性条件。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.263,
                0.411,
                0.282
            ],
            "angle": 0,
            "content": "5.7.1 仿射空间的投影问题"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.296,
                0.315,
                0.312
            ],
            "angle": 0,
            "content": "考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.386,
                0.312,
                0.522,
                0.366
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x \\in \\mathbb {R} ^ {n}} \\frac {1}{2} \\| x - y \\| _ {2} ^ {2}, \\\\ \\begin{array}{l l} \\text {s . t .} & A x = b, \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.377,
                0.737,
                0.457
            ],
            "angle": 0,
            "content": "其中 \\(A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m\\) 以及 \\(y \\in \\mathbb{R}^n\\) 为给定的矩阵和向量。这里不妨设矩阵 \\(A\\) 是行满秩的（否则，可以按照 \\(A\\) 的行之间的相关性消除约束冗余，从而使得消去后的 \\(\\tilde{A}\\) 是行满秩的）。这个问题可以看成仿射平面 \\(\\{x \\in \\mathbb{R}^n \\mid Ax = b\\}\\) 的投影问题。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.461,
                0.717,
                0.478
            ],
            "angle": 0,
            "content": "对于等式约束，我们引入拉格朗日乘子 \\(\\lambda \\in \\mathbb{R}^m\\) ，构造拉格朗日函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.319,
                0.489,
                0.589,
                0.52
            ],
            "angle": 0,
            "content": "\\[\nL (x, \\lambda) = \\frac {1}{2} \\| x - y \\| ^ {2} + \\lambda^ {\\mathrm {T}} (A x - b).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.532,
                0.737,
                0.569
            ],
            "angle": 0,
            "content": "因为只有仿射约束，故 Slater 条件满足。\\(x^{*}\\) 为一个全局最优解，当且仅当存在 \\(\\lambda^{*} \\in \\mathbb{R}^{m}\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.376,
                0.569,
                0.534,
                0.62
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{c} x ^ {*} - y + A ^ {\\mathrm {T}} \\lambda = 0, \\\\ A x ^ {*} = b. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.628,
                0.603,
                0.645
            ],
            "angle": 0,
            "content": "由上述KKT条件第一式，等号左右两边同时左乘 \\(A\\) 可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.363,
                0.661,
                0.545,
                0.68
            ],
            "angle": 0,
            "content": "\\[\nA x ^ {*} - A y + A A ^ {T} \\lambda = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.697,
                0.599,
                0.714
            ],
            "angle": 0,
            "content": "注意到 \\(Ax^{*} = b\\) 以及 \\(AA^{\\mathrm{T}}\\) 是可逆矩阵，因此可解出乘子"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.731,
                0.542,
                0.75
            ],
            "angle": 0,
            "content": "\\[\n\\lambda = \\left(A A ^ {\\mathrm {T}}\\right) ^ {- 1} (A y - b),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.767,
                0.413,
                0.783
            ],
            "angle": 0,
            "content": "将 \\(\\lambda\\) 代回KKT条件第一式可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.337,
                0.8,
                0.57,
                0.82
            ],
            "angle": 0,
            "content": "\\[\nx ^ {*} = y - A ^ {\\mathrm {T}} \\left(A A ^ {\\mathrm {T}}\\right) ^ {- 1} (A y - b).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.835,
                0.677,
                0.854
            ],
            "angle": 0,
            "content": "因此，点 \\(y\\) 到集合 \\(\\{x \\mid Ax = b\\}\\) 的投影为 \\(y - A^{\\mathrm{T}}(AA^{\\mathrm{T}})^{-1}(Ay - b)\\)."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "204"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.44,
                0.174
            ],
            "angle": 0,
            "content": "5.7.2 线性规划问题"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.188,
                0.436,
                0.204
            ],
            "angle": 0,
            "content": "考虑线性规划问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.478,
                0.206,
                0.577,
                0.23
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} c ^ {\\mathrm {T}} x,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.487,
                0.234,
                0.824,
                0.252
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad A x = b, \\tag {5.7.1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.543,
                0.263,
                0.591,
                0.277
            ],
            "angle": 0,
            "content": "\\[\nx \\geqslant 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.289,
                0.687,
                0.305
            ],
            "angle": 0,
            "content": "其中 \\(A\\in \\mathbb{R}^{m\\times n},b\\in \\mathbb{R}^m,c\\in \\mathbb{R}^n\\) 分别为给定的矩阵和向量"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.31,
                0.588,
                0.326
            ],
            "angle": 0,
            "content": "线性规划问题(5.7.1)的拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.342,
                0.709,
                0.385
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} L (x, s, \\nu) = c ^ {\\mathrm {T}} x + \\nu^ {\\mathrm {T}} (A x - b) - s ^ {\\mathrm {T}} x \\\\ = - b ^ {\\mathrm {T}} \\nu + \\left(A ^ {\\mathrm {T}} \\nu - s + c\\right) ^ {\\mathrm {T}} x, \\quad s \\geqslant 0, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.402,
                0.825,
                0.44
            ],
            "angle": 0,
            "content": "其中 \\(s \\in \\mathbb{R}^n, \\nu \\in \\mathbb{R}^m\\)。由于线性规划是凸问题且满足 Slater 条件，因此对于任意一个全局最优解 \\(x^*\\)，我们有如下 KKT 条件："
        },
        {
            "type": "equation",
            "bbox": [
                0.465,
                0.451,
                0.826,
                0.575
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{c} c + A ^ {\\mathrm {T}} v ^ {*} - s ^ {*} = 0, \\\\ A x ^ {*} = b, \\\\ x ^ {*} \\geqslant 0, \\\\ s ^ {*} \\geqslant 0, \\\\ s ^ {*} \\odot x ^ {*} = 0, \\end{array} \\right. \\tag {5.7.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.585,
                0.825,
                0.643
            ],
            "angle": 0,
            "content": "其中 \\(\\odot\\) 表示向量的 Hadamard 积, 即 \\((x \\odot y)_i = x_i y_i\\). 上述 KKT 条件也是充分的, 即满足上式的 \\(x^*\\) 也为问题(5.7.1)的全局最优解, 并且 \\(s^*, \\nu^*\\) 也为其对偶问题的全局最优解."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.648,
                0.826,
                0.706
            ],
            "angle": 0,
            "content": "我们可以进一步说明线性规划问题的解和其对偶问题的解之间的关系.设原始问题和对偶问题最优解处函数值分别为 \\(p^*\\) 和 \\(d^{*}\\) ，则根据 \\(p^*\\) 取值情况有如下三种可能："
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.721,
                0.825,
                0.78
            ],
            "angle": 0,
            "content": "(1) 如果 \\(-\\infty < p^{*} < +\\infty\\) (有界), 那么原始问题可行而且存在最优解. 由 Slater 条件知强对偶原理成立, 因此有 \\(d^{*} = p^{*}\\), 即对偶问题也是可行的且存在最优解."
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.795,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "(2) 如果 \\(p^{*} = -\\infty\\) ，那么原始问题可行，但目标函数值无下界．由弱对偶原理知 \\(d^{*} \\leqslant p^{*} = -\\infty\\) ，即 \\(d^{*} = -\\infty\\) ．因为对偶问题是对目标函数极大化，所以此时对偶问题不可行."
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.721,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.437,
                0.133
            ],
            "angle": 0,
            "content": "5.7 约束优化最优性理论应用实例"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "205"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.157,
                0.744,
                0.278
            ],
            "angle": 0,
            "content": "(3) 如果 \\( p^* = +\\infty \\), 那么原始问题无可行解. 注意到 Slater 条件对原始问题不成立, 此时对偶问题既可能函数值无界 (对应 \\( d^* = +\\infty \\)), 也可能无可行解 (对应 \\( d^* = -\\infty \\)). 我们指出, 此时不可能出现 \\(-\\infty < d^* < +\\infty\\) 的情形, 这是因为如果对偶问题可行且存在最优解, 那么可对对偶问题应用强对偶原理, 进而导出原始问题也存在最优解, 这与 \\( p^* = +\\infty \\) 矛盾."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.303,
                0.741,
                0.341
            ],
            "angle": 0,
            "content": "最后，我们将原始问题和对偶问题解的关系总结在表5.1中．可以看到，针对线性规划问题及其对偶问题，解的情况只有四种可能的组合."
        },
        {
            "type": "table_caption",
            "bbox": [
                0.266,
                0.363,
                0.642,
                0.38
            ],
            "angle": 0,
            "content": "表 5.1 线性规划原始问题和对偶问题的对应关系"
        },
        {
            "type": "table",
            "bbox": [
                0.316,
                0.392,
                0.591,
                0.501
            ],
            "angle": 0,
            "content": "<table><tr><td rowspan=\"2\">原始问题</td><td colspan=\"3\">对偶问题</td></tr><tr><td>有界</td><td>无界</td><td>不可行</td></tr><tr><td>有界</td><td>✓</td><td>×</td><td>×</td></tr><tr><td>无界</td><td>×</td><td>×</td><td>✓</td></tr><tr><td>不可行</td><td>×</td><td>✓</td><td>✓</td></tr></table>"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.573,
                0.295,
                0.591
            ],
            "angle": 0,
            "content": "5.7.3 基追踪"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.612,
                0.678,
                0.629
            ],
            "angle": 0,
            "content": "如第1.2节中介绍，压缩感知中的一个常用模型是基追踪问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.65,
                0.737,
                0.68
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| x \\| _ {1}, \\tag {5.7.3}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.409,
                0.681,
                0.509,
                0.695
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A x = b. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.718,
                0.741,
                0.756
            ],
            "angle": 0,
            "content": "利用分解 \\(x_{i} = x_{i}^{+} - x_{i}^{-}\\) ，其中 \\(x_{i}^{+} = \\max \\{x_{i},0\\} ,x_{i}^{-} = \\max \\{-x_{i},0\\}\\) 分别表示\\(x_{i}\\) 的正部和负部，问题(5.7.3)的一种等价形式可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.775,
                0.509,
                0.806
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\sum_ {i} x _ {i} ^ {+} + x _ {i} ^ {-},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.375,
                0.812,
                0.543,
                0.829
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A x ^ {+} - A x ^ {-} = b, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.414,
                0.837,
                0.5,
                0.853
            ],
            "angle": 0,
            "content": "\\[\nx ^ {+}, x ^ {-} \\geqslant 0.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "206"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.155,
                0.826,
                0.182
            ],
            "angle": 0,
            "content": "进一步地，令 \\(y = \\left[ \\begin{array}{c}x_i^+\\\\ x_i^- \\end{array} \\right]\\in \\mathbb{R}^{2n}\\) ，我们将问题(5.7.3)转为化如下线性规划问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.461,
                0.194,
                0.55,
                0.221
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {y \\in \\mathbb {R} ^ {2 n}} \\quad \\mathbf {1} ^ {\\mathrm {T}} y,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.474,
                0.228,
                0.621,
                0.246
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & [ A, - A ] y = b, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.513,
                0.254,
                0.562,
                0.27
            ],
            "angle": 0,
            "content": "\\[\ny \\geqslant 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.282,
                0.474,
                0.3
            ],
            "angle": 0,
            "content": "其中 \\(\\mathbf{1} = (1,1,\\dots ,1)^{\\mathrm{T}}\\in \\mathbb{R}^{2n}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.304,
                0.825,
                0.34
            ],
            "angle": 0,
            "content": "根据上面一般线性规划问题的最优性条件，求解基追踪问题(5.7.3)等价于求解"
        },
        {
            "type": "equation",
            "bbox": [
                0.442,
                0.34,
                0.826,
                0.462
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{c} \\mathbf {1} + [ A, - A ] ^ {\\mathrm {T}} v ^ {*} - s ^ {*} = 0, \\\\ \\quad [ A, - A ] y ^ {*} = b, \\\\ \\quad y ^ {*} \\geqslant 0, \\\\ \\quad s ^ {*} \\geqslant 0, \\\\ \\quad s ^ {*} \\odot y ^ {*} = 0, \\end{array} \\right. \\tag {5.7.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.467,
                0.434,
                0.484
            ],
            "angle": 0,
            "content": "其中 \\(s^* \\in \\mathbb{R}^{2n}, \\nu^* \\in \\mathbb{R}^m\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.488,
                0.825,
                0.546
            ],
            "angle": 0,
            "content": "另外，我们还可以直接利用非光滑凸优化问题的最优性理论来推导问题(5.7.3)的最优性条件。对于等式约束，我们引入拉格朗日乘子 \\(\\nu \\in \\mathbb{R}^m\\) ，拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.55,
                0.652,
                0.568
            ],
            "angle": 0,
            "content": "\\[\nL (x, \\nu) = \\| x \\| _ {1} + \\nu^ {\\mathrm {T}} (A x - b).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.578,
                0.594,
                0.595
            ],
            "angle": 0,
            "content": "\\(x^{*}\\) 为全局最优解当且仅当存在 \\(\\nu^{*} \\in \\mathbb{R}^{m}\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.451,
                0.605,
                0.826,
                0.656
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{c} 0 \\in \\partial \\| x ^ {*} \\| _ {1} + A ^ {\\mathrm {T}} v ^ {*}, \\\\ A x ^ {*} = b. \\end{array} \\right. \\tag {5.7.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.665,
                0.827,
                0.704
            ],
            "angle": 0,
            "content": "最优性条件(5.7.4)和(5.7.5)本质上是等价的．事实上，对于条件(5.7.4)，令 \\(x_{i}^{*} = y_{i}^{*} - y_{n + i}^{*},i = 1,2,\\dots ,n\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.718,
                0.652,
                0.736
            ],
            "angle": 0,
            "content": "\\[\n[ A, - A ] y ^ {*} = b \\Longrightarrow A x ^ {*} = b.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.75,
                0.585,
                0.768
            ],
            "angle": 0,
            "content": "对于等式 \\(\\mathbf{1} + [A, - A]^{\\mathrm{T}}\\nu^{*} - s^{*} = 0\\) ，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.782,
                0.715,
                0.801
            ],
            "angle": 0,
            "content": "\\[\n\\left(A ^ {\\mathrm {T}} \\nu^ {*}\\right) _ {i} = - 1 + s _ {i} ^ {*} = 1 - s _ {n + i} ^ {*}, \\quad i = 1, 2, \\dots , n.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.826,
                0.855
            ],
            "angle": 0,
            "content": "因此， \\(s_i^* + s_{n+i}^* = 2, i = 1,2,\\dots,n.\\) 根据互补条件， \\(y_i^*, y_{n+i}^*\\) 至少有一个为0. 故若 \\(x_i^* = 0\\) ，则有 \\(y_i^* = y_{n+i}^* = 0.\\) 此时根据 \\(s_i^*, s_{n+i}^* \\geqslant 0\\) ，则有 \\((A^{\\mathrm{T}}\\nu^*)_i \\in [-1,1]\\)."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.437,
                0.133
            ],
            "angle": 0,
            "content": "5.7 约束优化最优性理论应用实例"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "207"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.156,
                0.744,
                0.219
            ],
            "angle": 0,
            "content": "若 \\(x_{i}^{*} < 0\\) ，则有 \\(y_{i}^{*} = 0, y_{n + i}^{*} > 0\\) ，此时有 \\((A^{\\mathrm{T}}\\nu^{*})_{i} = 1.\\) 类似地，对于 \\(x_{i}^{*} > 0\\) 我们有 \\((A^{\\mathrm{T}}\\nu^{*})_{i} = -1\\) ．以上过程均可逆推，这就证明了条件(5.7.4)和条件(5.7.5)是等价的."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.244,
                0.629,
                0.263
            ],
            "angle": 0,
            "content": "5.7.4 最大割问题的半定规划松弛及其非凸分解模型"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.275,
                0.68,
                0.292
            ],
            "angle": 0,
            "content": "第三章介绍了最大割问题的半定规划松弛问题，其有如下形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.307,
                0.449,
                0.325
            ],
            "angle": 0,
            "content": "\\[\n\\max  \\quad \\langle C, X \\rangle\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.358,
                0.331,
                0.737,
                0.349
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad X _ {i i} = 1, i = 1, 2, \\dots , n \\tag {5.7.6}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.356,
                0.451,
                0.372
            ],
            "angle": 0,
            "content": "\\[\nX \\succeq 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.387,
                0.741,
                0.466
            ],
            "angle": 0,
            "content": "易知该问题为一个凸优化问题，并且 Slater 约束品性成立（\\(X = I\\) 为一个相对内点）。对于等式约束，我们引入拉格朗日乘子 \\(\\mu_i \\in \\mathbb{R}, i = 1,2,\\dots,n\\)；对于半正定约束，根据对偶锥，我们引入拉格朗日乘子 \\(\\Lambda \\in S_+^n\\)，拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.208,
                0.478,
                0.7,
                0.515
            ],
            "angle": 0,
            "content": "\\[\nL (X, \\mu , \\Lambda) = \\langle C, X \\rangle + \\sum_ {i = 1} ^ {n} \\mu_ {i} (X _ {i i} - 1) - \\operatorname {T r} (X \\Lambda), \\quad \\Lambda \\in \\mathcal {S} _ {+} ^ {n}, \\mu \\in \\mathbb {R} ^ {n}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.526,
                0.738,
                0.562
            ],
            "angle": 0,
            "content": "根据约束优化的最优性条件，可行点 \\(X^{*}\\) 为全局极小解当且仅当存在 \\(\\Lambda^{*},\\mu^{*}\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.31,
                0.563,
                0.602,
                0.686
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{l} C + \\operatorname {D i a g} (\\mu^ {*}) - \\Lambda^ {*} = 0, \\\\ \\qquad \\qquad X _ {i i} ^ {*} = 1,   i = 1, \\dots , n, \\\\ \\qquad \\qquad X ^ {*} \\succeq 0, \\\\ \\qquad \\qquad \\Lambda^ {*} \\succeq 0, \\\\ \\qquad \\operatorname {T r} (X ^ {*} \\Lambda^ {*}) = 0. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.691,
                0.738,
                0.729
            ],
            "angle": 0,
            "content": "由于 \\(X^{*}\\) 与 \\(\\Lambda^{*}\\) 的半正定性，上述条件中的 \\(\\operatorname{Tr}(X^{*}\\Lambda^{*}) = 0\\) 可以等价地用 \\(X^{*}\\Lambda^{*} = 0\\) 来代替."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.733,
                0.741,
                0.792
            ],
            "angle": 0,
            "content": "如果问题 (5.7.6) 中的决策矩阵 \\(X\\) 的维数很大，数值求解的代价往往会难以接受。在实际中，我们经常考虑其非凸分解模型。具体地，令 \\(X = YY^{\\mathrm{T}}\\)，\\(Y \\in \\mathbb{R}^{n \\times p}\\)，那么问题 (5.7.6) 转化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.363,
                0.804,
                0.738,
                0.837
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {Y \\in \\mathbb {R} ^ {n \\times p}} \\operatorname {T r} \\left(C Y Y ^ {\\mathrm {T}}\\right) \\tag {5.7.7}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.385,
                0.837,
                0.543,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & \\operatorname {d i a g} (Y Y ^ {\\mathrm {T}}) = \\mathbf {1}. \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "208"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "注意，此时 \\(YY^{\\mathrm{T}} \\succeq 0\\) 自然满足。这里 \\(p\\) 的选取与问题 (5.7.6) 的全局最优解 \\(X^{*}\\) 的秩 \\(p^{*}\\) 有关。如果 \\(p \\geqslant p^{*}\\)，可以证明由问题 (5.7.7) 的局部最优解 \\(Y^{*}\\) 可以构造出 (5.7.6) 的一个全局最优解 \\(Y^{*}(Y^{*})^{\\mathrm{T}}\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.218,
                0.825,
                0.257
            ],
            "angle": 0,
            "content": "对于非凸优化问题(5.7.7)，在可行点 \\(Y = [y_{1},y_{2},\\dots ,y_{n}]^{\\mathrm{T}}\\) 处，其约束可以表示为 \\(c_{i}(Y) = \\| y_{i}\\|^{2} - 1 = 0,i = 1,2,\\dots ,n.\\) 在 \\(Y\\) 处，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.422,
                0.267,
                0.66,
                0.286
            ],
            "angle": 0,
            "content": "\\[\n\\nabla c _ {i} (Y) = 2 \\left[ 0, \\dots , 0, y _ {i}, 0, \\dots 0 \\right] ^ {\\mathrm {T}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.297,
                0.825,
                0.334
            ],
            "angle": 0,
            "content": "因为 \\(y_{i}\\neq 0\\) ，故 \\(\\{c_i(Y)\\}_{i = 1}^n\\) 是线性无关的，即LICQ成立．对于等式约束，我们引入拉格朗日乘子 \\(\\lambda_{i}\\in \\mathbb{R},i\\in 1,2,\\dots ,n\\) ，构造拉格朗日函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.414,
                0.341,
                0.668,
                0.377
            ],
            "angle": 0,
            "content": "\\[\nL (Y, \\lambda) = \\operatorname {T r} \\left(C Y Y ^ {\\mathrm {T}}\\right) + \\sum_ {i = 1} ^ {n} \\lambda_ {i} c _ {i} (Y).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.384,
                0.657,
                0.4
            ],
            "angle": 0,
            "content": "根据约束优化问题的最优性理论，有如下KKT条件："
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.406,
                0.682,
                0.456
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{c} 2 C Y - 2 [ \\lambda_ {1} y _ {1}, \\lambda_ {2} y _ {2}, \\dots , \\lambda_ {n} y _ {n} ] ^ {\\mathrm {T}} = 0, \\\\ \\operatorname {d i a g} (Y Y ^ {\\mathrm {T}}) = \\mathbf {1}. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.463,
                0.655,
                0.48
            ],
            "angle": 0,
            "content": "令 \\(\\Lambda = \\operatorname{Diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)\\)，上述第一式可以转换为"
        },
        {
            "type": "equation",
            "bbox": [
                0.486,
                0.492,
                0.597,
                0.509
            ],
            "angle": 0,
            "content": "\\[\n(C - \\Lambda) Y = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.521,
                0.641,
                0.537
            ],
            "angle": 0,
            "content": "因为该问题只有等式约束，故临界锥就是切锥，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.389,
                0.547,
                0.692,
                0.567
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {C} (Y, \\lambda) = \\left\\{D \\in \\mathbb {R} ^ {n \\times p} \\mid \\operatorname {d i a g} \\left(Y ^ {\\mathrm {T}} D\\right) = 0 \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.577,
                0.538,
                0.594
            ],
            "angle": 0,
            "content": "拉格朗日函数的海瑟矩阵算子形式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.605,
                0.655,
                0.624
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {Y Y} ^ {2} L (X, \\lambda) [ D ] = 2 (C - \\Lambda) D.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.634,
                0.701,
                0.651
            ],
            "angle": 0,
            "content": "假设 \\(Y\\) 为一个局部最优解，则存在 \\(\\lambda_{i}, i = 1,2,\\dots ,n\\) ，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.386,
                0.659,
                0.698,
                0.727
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{r l} \\langle (C - \\Lambda) D, D \\rangle \\geqslant 0, & \\forall \\mathrm {d i a g} (Y ^ {\\mathrm {T}} D) = 0, \\\\ \\mathrm {d i a g} (Y Y ^ {\\mathrm {T}}) = \\mathbf {1}, \\\\ (C - \\Lambda) Y = 0. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.736,
                0.616,
                0.752
            ],
            "angle": 0,
            "content": "假设在一点 \\(Y\\) 处，存在 \\(\\lambda_{i}, i = 1,2,\\dots ,n\\) ，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.386,
                0.76,
                0.697,
                0.828
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{r l} \\langle (C - \\Lambda) D, D \\rangle > 0, & \\forall \\mathrm {d i a g} (Y ^ {\\mathrm {T}} D) = 0, \\\\ \\mathrm {d i a g} (Y Y ^ {\\mathrm {T}}) = \\mathbf {1}, \\\\ (C - \\Lambda) Y = 0, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.836,
                0.6,
                0.853
            ],
            "angle": 0,
            "content": "那么 \\(Y\\) 为问题(5.7.7)的一个严格局部最优解"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "5.8 总结"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "209"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.739,
                0.219
            ],
            "angle": 0,
            "content": "注5.3 利用关系式 \\((C - \\Lambda)Y = 0\\) 和约束 \\(\\mathrm{diag}(YY^{\\mathrm{T}}) = 1\\) 可以显式求得 \\(\\lambda = \\mathrm{diag}(CYY^{\\mathrm{T}})\\). 换句话说，在这个例子中根据约束的特殊结构，我们能显式给出乘子 \\(\\lambda\\) 的表达式。这个性质在一般约束优化问题中是没有的。"
        },
        {
            "type": "title",
            "bbox": [
                0.399,
                0.247,
                0.51,
                0.268
            ],
            "angle": 0,
            "content": "5.8 总结"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.286,
                0.741,
                0.344
            ],
            "angle": 0,
            "content": "本章介绍了约束和无约束优化问题的最优性理论，并对于凸优化问题的理论给出了进一步的分析。关于一般优化问题的理论，更多内容以及本章省略的证明和细节可以在[145, 175]中找到。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.348,
                0.744,
                0.448
            ],
            "angle": 0,
            "content": "对于非光滑非凸问题的最优性条件，我们一般借助其 Fréchet 次微分来研究。当函数光滑的时候，该次微分就是正常的梯度。对非光滑凸函数，Fréchet 次微分就是其次梯度的集合。对于非光滑非凸的情形，我们则用 Fréchet 次微分来判断最优性条件（考虑无约束的情形，任一局部最优解 \\( x^{*} \\) 处的 Fréchet 次微分必定包含零向量）。相关内容读者可以参考 [135]。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.452,
                0.741,
                0.51
            ],
            "angle": 0,
            "content": "优化问题的对偶问题与其目标函数的共轭函数的极值问题在某种程度上是等价的。对于更多形式的广义不等式约束优化问题及其拉格朗日函数对偶推导，以及更详细的凸优化问题的理论，读者可以参考[31, 164]。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.513,
                0.741,
                0.572
            ],
            "angle": 0,
            "content": "本章凸优化问题的最优性理论部分的编写参考了[31, 166], 一般光滑问题的最优性理论的编写参考了[145], 对偶理论的编写参考了 Lieven Vandenberghe 教授的课件, 更多细节和解释可以在其中找到."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.576,
                0.74,
                0.676
            ],
            "angle": 0,
            "content": "最后，我们在表5.2和表5.3中总结本章涉及的无约束优化问题和约束优化问题的最优性条件和所需的约束品性（仅约束优化问题），其中“---”表示无需考虑（凸问题）或本书未进行讨论（非凸问题）的条件。在实际应用中读者需要根据问题种类使用合适的最优性条件对问题进行理论分析和算法构建。"
        },
        {
            "type": "table_caption",
            "bbox": [
                0.301,
                0.69,
                0.607,
                0.708
            ],
            "angle": 0,
            "content": "表 5.2 无约束优化问题及其最优性条件"
        },
        {
            "type": "table",
            "bbox": [
                0.197,
                0.72,
                0.714,
                0.851
            ],
            "angle": 0,
            "content": "<table><tr><td>问题</td><td>一阶条件</td><td>二阶条件</td></tr><tr><td>可微问题</td><td>∇f(x*)=0(必要)</td><td>∇2f(x*)≥0(必要) ∇2f(x*)&gt;0(充分)</td></tr><tr><td>凸问题</td><td>0∈∂f(x*) (充要)</td><td>-</td></tr><tr><td>复合优化问题</td><td>-∇f(x*)∈∂h(x*) (必要)</td><td>-</td></tr><tr><td>非凸非光滑</td><td>0∈∂f(x*) (必要)</td><td>-</td></tr></table>"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "210"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "table_caption",
            "bbox": [
                0.345,
                0.159,
                0.738,
                0.176
            ],
            "angle": 0,
            "content": "表 5.3 约束优化问题的最优性条件和相应约束品性"
        },
        {
            "type": "table",
            "bbox": [
                0.294,
                0.188,
                0.792,
                0.277
            ],
            "angle": 0,
            "content": "<table><tr><td>问题</td><td>一阶条件</td><td>二阶条件</td><td>约束品性</td></tr><tr><td>一般问题</td><td>KKT 条件（必要）</td><td>定理 5.10（必要）\n定理 5.11（充分）¹</td><td>LICQ²</td></tr><tr><td>凸问题</td><td>KKT 条件（充要）</td><td>—</td><td>Slater</td></tr></table>"
        },
        {
            "type": "title",
            "bbox": [
                0.507,
                0.323,
                0.58,
                0.344
            ],
            "angle": 0,
            "content": "习题5"
        },
        {
            "type": "title",
            "bbox": [
                0.269,
                0.362,
                0.409,
                0.378
            ],
            "angle": 0,
            "content": "5.1 考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.484,
                0.381,
                0.64,
                0.406
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} x ^ {\\mathrm {T}} A x + 2 b ^ {\\mathrm {T}} x,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.297,
                0.411,
                0.825,
                0.448
            ],
            "angle": 0,
            "content": "其中 \\(A \\in \\mathcal{S}^n, b \\in \\mathbb{R}^n\\)。为了保证该问题最优解存在，\\(A, b\\) 需要满足什么性质？"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.462,
                0.825,
                0.5
            ],
            "angle": 0,
            "content": "5.2 试举例说明对无约束光滑优化问题，二阶必要条件不是充分的，二阶充分条件也不是必要的（见定理5.4）."
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.513,
                0.484,
                0.53
            ],
            "angle": 0,
            "content": "5.3 证明下列锥是自对偶锥："
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.544,
                0.628,
                0.561
            ],
            "angle": 0,
            "content": "(a) 半正定锥 \\(\\{X \\mid X \\succeq 0\\}\\) （全空间为 \\(S^n\\)）；"
        },
        {
            "type": "text",
            "bbox": [
                0.304,
                0.569,
                0.722,
                0.587
            ],
            "angle": 0,
            "content": "(b) 二次锥 \\(\\{(x,t)\\in \\mathbb{R}^{n + 1}\\mid t\\geqslant \\| x\\| _2\\}\\) （全空间为 \\(\\mathbb{R}^{n + 1})\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.304,
                0.544,
                0.722,
                0.587
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.269,
                0.601,
                0.409,
                0.617
            ],
            "angle": 0,
            "content": "5.4 考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.418,
                0.632,
                0.705,
                0.657
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad x ^ {\\mathrm {T}} A x + 2 b ^ {\\mathrm {T}} x, \\quad \\text {s . t .} \\quad \\| x \\| _ {2} \\leqslant \\Delta ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.297,
                0.667,
                0.695,
                0.684
            ],
            "angle": 0,
            "content": "其中 \\(A\\in \\mathcal{S}_{++}^n\\) ， \\(b\\in \\mathbb{R}^n\\) ， \\(\\Delta >0\\) .求出该问题的最优解"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.697,
                0.825,
                0.737
            ],
            "angle": 0,
            "content": "5.5 考虑函数 \\( f(x) = 2x_{1}^{2} + x_{2}^{2} - 2x_{1}x_{2} + 2x_{1}^{3} + x_{1}^{4} \\)，求出其所有一阶稳定点，并判断它们是否为局部最优点（极小或极大）、鞍点或全局最优点？"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.75,
                0.517,
                0.766
            ],
            "angle": 0,
            "content": "5.6 给出下列优化问题的显式解："
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.779,
                0.726,
                0.803
            ],
            "angle": 0,
            "content": "(a) \\(\\min_{x\\in \\mathbb{R}^n}c^{\\mathrm{T}}x,\\quad \\mathrm{s.t.}\\quad Ax = b\\) ，其中 \\(A\\in \\mathbb{R}^{m\\times n},b\\in \\mathbb{R}^{m};\\)"
        },
        {
            "type": "page_footnote",
            "bbox": [
                0.283,
                0.824,
                0.603,
                0.837
            ],
            "angle": 0,
            "content": "一般约束优化问题的二阶充分条件不需要LICQ作为前提"
        },
        {
            "type": "page_footnote",
            "bbox": [
                0.283,
                0.839,
                0.525,
                0.852
            ],
            "angle": 0,
            "content": "2或其他可推出 \\(T_{\\chi}(x^{*}) = \\mathcal{F}(x^{*})\\) 的约束品性"
        },
        {
            "type": "list",
            "bbox": [
                0.283,
                0.824,
                0.603,
                0.852
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.223,
                0.132
            ],
            "angle": 0,
            "content": "习题5"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "211"
        },
        {
            "type": "text",
            "bbox": [
                0.218,
                0.156,
                0.457,
                0.179
            ],
            "angle": 0,
            "content": "(b) \\(\\min_{x\\in \\mathbb{R}^n}\\| x\\| _2,\\quad \\mathrm{s.t.}\\quad Ax = b;\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.22,
                0.182,
                0.499,
                0.206
            ],
            "angle": 0,
            "content": "(c) \\(\\min_{x\\in \\mathbb{R}^n}c^{\\mathrm{T}}x,\\quad \\mathrm{s.t.}\\quad \\mathbf{1}^{\\mathrm{T}}x = 1,x\\geqslant 0;\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.209,
                0.666,
                0.241
            ],
            "angle": 0,
            "content": "(d) \\(\\min_{X\\in \\mathbb{R}^{m\\times n}}\\| X\\| _* + \\frac{1}{2}\\| X - Y\\| _F^2\\) ，其中 \\(Y\\in \\mathbb{R}^{m\\times n}\\) 是已知的."
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.156,
                0.666,
                0.241
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.247,
                0.444,
                0.264
            ],
            "angle": 0,
            "content": "5.7 计算下列优化问题的对偶问题"
        },
        {
            "type": "text",
            "bbox": [
                0.218,
                0.276,
                0.456,
                0.298
            ],
            "angle": 0,
            "content": "(a) \\(\\min_{x\\in \\mathbb{R}^n}\\| x\\| _1,\\quad \\mathrm{s.t.}\\quad Ax = b;\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.218,
                0.3,
                0.384,
                0.323
            ],
            "angle": 0,
            "content": "(b) \\(\\min_{x\\in \\mathbb{R}^n}\\| Ax - b\\| _1;\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.22,
                0.324,
                0.388,
                0.347
            ],
            "angle": 0,
            "content": "(c) \\(\\min_{x\\in \\mathbb{R}^n}\\| Ax - b\\|_{\\infty};\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.349,
                0.678,
                0.374
            ],
            "angle": 0,
            "content": "(d) \\(\\min_{x\\in \\mathbb{R}^n}x^{\\mathrm{T}}Ax + 2b^{\\mathrm{T}}x,\\quad \\mathrm{s.t.}\\quad \\| x\\| _2^2\\leqslant 1,\\) 其中 \\(A\\) 为正定矩阵"
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.276,
                0.678,
                0.374
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.38,
                0.414,
                0.396
            ],
            "angle": 0,
            "content": "5.8 如下论断正确吗？为什么？"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.401,
                0.371,
                0.417
            ],
            "angle": 0,
            "content": "对等式约束优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.389,
                0.428,
                0.558,
                0.465
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  f (x), \\\\ \\begin{array}{l l} \\text {s . t .} & c _ {i} (x) = 0, i \\in \\mathcal {E}. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.474,
                0.446,
                0.491
            ],
            "angle": 0,
            "content": "考虑与之等价的约束优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.387,
                0.501,
                0.737,
                0.541
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  f (x), \\\\ \\begin{array}{l} \\text {s . t .} c _ {i} ^ {2} (x) = 0, i \\in \\mathcal {E}. \\end{array} \\tag {5.8.1} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.549,
                0.641,
                0.567
            ],
            "angle": 0,
            "content": "设 \\(x^{\\sharp}\\) 是上述问题的一个KKT点，根据(5.5.8)式，\\(x^{\\sharp}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.341,
                0.575,
                0.605,
                0.629
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} 0 = \\nabla f (x ^ {\\sharp}) + 2 \\sum_ {i \\in \\mathcal {E}} \\lambda_ {i} ^ {\\sharp} c _ {i} (x ^ {\\sharp}) \\nabla c _ {i} (x ^ {\\sharp}), \\\\ 0 = c _ {i} (x ^ {\\sharp}), \\quad i \\in \\mathcal {E}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.64,
                0.737,
                0.679
            ],
            "angle": 0,
            "content": "其中 \\(\\lambda_{l}^{\\sharp}\\) 是相应的拉格朗日乘子. 整理上式得 \\(\\nabla f(x^{\\sharp}) = 0\\). 这说明对等式约束优化问题, 我们依然能给出类似无约束优化问题的最优性条件."
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.69,
                0.737,
                0.709
            ],
            "angle": 0,
            "content": "5.9 证明: 若在点 \\(x\\) 处线性约束品性 (见定义 5.11) 满足, 则有 \\(T_{\\chi}(x) = \\mathcal{F}(x)\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.72,
                0.32,
                0.736
            ],
            "angle": 0,
            "content": "5.10 考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.739,
                0.589,
                0.81
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x \\in \\mathbb {R} ^ {2}} x _ {1}, \\\\ \\begin{array}{l} \\text {s . t .} \\quad 1 6 - (x _ {1} - 4) ^ {2} - x _ {2} ^ {2} \\geqslant 0, \\end{array} \\\\ x _ {1} ^ {2} + (x _ {2} - 2) ^ {2} - 4 = 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.816,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "求出该优化问题的 KKT 点，并判断它们是否是局部极小点、鞍点以及全局极小点？"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "212"
        },
        {
            "type": "header",
            "bbox": [
                0.666,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.157,
                0.512,
                0.174
            ],
            "angle": 0,
            "content": "5.11 考虑对称矩阵的特征值问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.447,
                0.186,
                0.677,
                0.211
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad x ^ {\\mathrm {T}} A x, \\quad \\text {s . t .} \\quad \\| x \\| _ {2} = 1,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.218,
                0.796,
                0.235
            ],
            "angle": 0,
            "content": "其中 \\(A \\in S^n\\)．试分析其所有的局部极小点、鞍点以及全局极小点."
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.248,
                0.827,
                0.285
            ],
            "angle": 0,
            "content": "5.12 类似于线性规划问题，试分析半定规划 (5.4.20) 与其对偶问题 (5.4.21) 的最优值的关系（强对偶性什么时候成立，什么时候失效）."
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.298,
                0.827,
                0.356
            ],
            "angle": 0,
            "content": "5.13 在介绍半定规划问题的最优性条件时，我们提到互补松弛条件可以是 \\(\\langle X, S \\rangle = 0\\) 或 \\(XS = 0\\) ，证明这两个条件是等价的，即对 \\(X \\succeq 0\\) 与 \\(S \\succeq 0\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.481,
                0.361,
                0.644,
                0.377
            ],
            "angle": 0,
            "content": "\\[\n\\langle X, S \\rangle = 0 \\Leftrightarrow X S = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.387,
                0.825,
                0.424
            ],
            "angle": 0,
            "content": "提示：证明 \\(X\\) 和 \\(S\\) 可以同时正交对角化且对应的特征值满足互补松弛条件."
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.438,
                0.529,
                0.455
            ],
            "angle": 0,
            "content": "5.14 考虑等式约束的最小二乘问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.467,
                0.69,
                0.491
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| A x - b \\| _ {2} ^ {2}, \\quad \\text {s . t .} \\quad G x = h,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.5,
                0.75,
                0.518
            ],
            "angle": 0,
            "content": "其中 \\(A\\in \\mathbb{R}^{m\\times n}\\) 且 \\(\\operatorname {rank}(A) = n\\) ， \\(G\\in \\mathbb{R}^{p\\times n}\\) 且 \\(\\operatorname {rank}(G) = p\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.53,
                0.52,
                0.547
            ],
            "angle": 0,
            "content": "(a) 写出该问题的对偶问题；"
        },
        {
            "type": "text",
            "bbox": [
                0.304,
                0.555,
                0.704,
                0.571
            ],
            "angle": 0,
            "content": "(b) 给出原始问题和对偶问题的最优解的显式表达式."
        },
        {
            "type": "list",
            "bbox": [
                0.304,
                0.53,
                0.704,
                0.571
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.584,
                0.409,
                0.6
            ],
            "angle": 0,
            "content": "5.15 考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.42,
                0.613,
                0.704,
                0.638
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} x ^ {\\mathrm {T}} A x + 2 b ^ {\\mathrm {T}} x, \\quad \\text {s . t .} \\quad \\| x \\| _ {2} \\leqslant 1,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.645,
                0.825,
                0.682
            ],
            "angle": 0,
            "content": "其中 \\(A \\in \\mathcal{S}^n, b \\in \\mathbb{R}^n\\). 写出该问题的对偶问题，以及对偶问题的对偶问题."
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.696,
                0.461,
                0.712
            ],
            "angle": 0,
            "content": "5.16 考虑支持向量机问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.422,
                0.72,
                0.702,
                0.805
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x \\in \\mathbb {R} ^ {n}, \\xi} \\frac {1}{2} \\| x \\| _ {2} ^ {2} + \\mu \\sum_ {i = 1} ^ {m} \\xi_ {i}, \\\\ \\begin{array}{l l} \\text {s . t .} & b _ {i} a _ {i} ^ {\\mathrm {T}} x \\geqslant 1 - \\xi_ {i}, i = 1, 2, \\dots , m, \\end{array} \\\\ \\xi_ {i} \\geqslant 0, i = 1, 2, \\dots , m, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.816,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(\\mu > 0\\) 为常数且 \\(b_{i} \\in \\mathbb{R}, a_{i} \\in \\mathbb{R}^{n}, i = 1,2,\\dots,m\\) 是已知的。写出该问题的对偶问题。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.223,
                0.133
            ],
            "angle": 0,
            "content": "习题5"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "213"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.157,
                0.321,
                0.174
            ],
            "angle": 0,
            "content": "5.17 考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.368,
                0.171,
                0.581,
                0.208
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R}, y > 0} e ^ {- x}, \\quad \\text {s . t .} \\quad \\frac {x ^ {2}}{y} \\leqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.215,
                0.736,
                0.252
            ],
            "angle": 0,
            "content": "(a) 证明这是一个凸优化问题，求出最小值并判断 Slater 条件是否成立；"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.261,
                0.736,
                0.278
            ],
            "angle": 0,
            "content": "(b) 写出该问题的对偶问题，并求出对偶问题的最优解以及对偶间隙."
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.215,
                0.736,
                0.278
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.292,
                0.321,
                0.308
            ],
            "angle": 0,
            "content": "5.18 考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.267,
                0.324,
                0.68,
                0.35
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {Z \\in \\mathbb {R} ^ {n \\times q}, V \\in \\mathbb {R} ^ {q \\times p}} \\| X - Z V \\| _ {F} ^ {2}, \\quad \\text {s . t .} \\quad V ^ {\\mathrm {T}} V = I, \\quad Z ^ {\\mathrm {T}} \\mathbf {1} = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.361,
                0.519,
                0.379
            ],
            "angle": 0,
            "content": "其中 \\(X \\in \\mathbb{R}^{n \\times p}\\). 请给出该优化问题的解"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "214"
        },
        {
            "type": "header",
            "bbox": [
                0.667,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第五章 最优性理论"
        }
    ],
    [
        {
            "type": "title",
            "bbox": [
                0.255,
                0.252,
                0.654,
                0.285
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.335,
                0.441,
                0.352
            ],
            "angle": 0,
            "content": "本章考虑如下无约束优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.367,
                0.737,
                0.39
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x), \\tag {6.0.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.399,
                0.739,
                0.519
            ],
            "angle": 0,
            "content": "其中 \\(f(x)\\) 是 \\(\\mathbb{R}^n\\to \\mathbb{R}\\) 的函数．无约束优化问题是众多优化问题中最基本的问题，它对自变量 \\(x\\) 的取值范围不加限制，所以无需考虑 \\(x\\) 的可行性．对于光滑函数，我们可以较容易地利用梯度和海瑟矩阵的信息来设计算法；对于非光滑函数，我们可以利用次梯度来构造迭代格式．很多无约束优化问题的算法思想可以推广到其他优化问题上，因此掌握如何求解无约束优化问题的方法是设计其他优化算法的基础."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.523,
                0.74,
                0.685
            ],
            "angle": 0,
            "content": "无约束优化问题的优化算法主要分为两大类：线搜索类型的优化算法和信赖域类型的优化算法。它们都是对函数 \\( f(x) \\) 在局部进行近似，但处理近似问题的方式不同。线搜索类算法根据搜索方向的不同可以分为梯度类算法、次梯度算法、牛顿算法、拟牛顿算法等。一旦确定了搜索的方向，下一步即沿着该方向寻找下一个迭代点。而信赖域算法主要针对 \\( f(x) \\) 二阶可微的情形，它是在一个给定的区域内使用二阶模型近似原问题，通过不断直接求解该二阶模型从而找到最优值点。我们在本章中将初步介绍这些算法的思想和具体执行过程，并简要分析它们的性质。"
        },
        {
            "type": "title",
            "bbox": [
                0.361,
                0.715,
                0.547,
                0.736
            ],
            "angle": 0,
            "content": "6.1 线搜索方法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.754,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "对于优化问题(6.0.1)，我们将求解 \\( f(x) \\) 的最小值点的过程比喻成下山的过程．假设一个人处于某点 \\( x \\) 处， \\( f(x) \\) 表示此地的高度，为了寻找最低点，在点 \\( x \\) 处需要确定如下两件事情：第一，下一步该向哪一方向行走；第二，沿着该方向行走多远后停下以便选取下一个下山方向．以上这两个因素确定后，便可以一直重复，直至到达 \\( f(x) \\) 的最小值点."
        },
        {
            "type": "page_number",
            "bbox": [
                0.44,
                0.87,
                0.469,
                0.882
            ],
            "angle": 0,
            "content": "215"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "216"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.825,
                0.195
            ],
            "angle": 0,
            "content": "线搜索类算法的数学表述为：给定当前迭代点 \\(x^{k}\\)，首先通过某种算法选取向量 \\(d^{k}\\)，之后确定正数 \\(\\alpha_{k}\\)，则下一步的迭代点可写作"
        },
        {
            "type": "equation",
            "bbox": [
                0.477,
                0.21,
                0.825,
                0.229
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} + \\alpha_ {k} d ^ {k}. \\tag {6.1.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.245,
                0.825,
                0.324
            ],
            "angle": 0,
            "content": "我们称 \\(d^{k}\\) 为迭代点 \\(x^{k}\\) 处的搜索方向, \\(\\alpha_{k}\\) 为相应的步长. 这里要求 \\(d^{k}\\) 是一个下降方向, 即 \\((d^{k})^{\\mathrm{T}} \\nabla f(x^{k}) < 0\\). 这个下降性质保证了沿着此方向搜索函数 \\(f\\) 的值会减小. 线搜索类算法的关键是如何选取一个好的方向 \\(d^{k} \\in \\mathbb{R}^{n}\\) 以及合适的步长 \\(\\alpha_{k}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.329,
                0.825,
                0.367
            ],
            "angle": 0,
            "content": "在本节中，我们将回答如何选取 \\(\\alpha_{k}\\) 这一问题．这是因为选取 \\(d^{k}\\) 的方法千差万别，但选取 \\(\\alpha_{k}\\) 的方法在不同算法中非常相似．首先构造辅助函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.464,
                0.382,
                0.617,
                0.402
            ],
            "angle": 0,
            "content": "\\[\n\\phi (\\alpha) = f \\left(x ^ {k} + \\alpha d ^ {k}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.417,
                0.825,
                0.476
            ],
            "angle": 0,
            "content": "其中 \\(d^{k}\\) 是给定的下降方向， \\(\\alpha >0\\) 是该辅助函数的自变量．函数 \\(\\phi (\\alpha)\\) 的几何含义非常直观：它是目标函数 \\(f(x)\\) 在射线 \\(\\{x^k +\\alpha d^k:\\alpha >0\\}\\) 上的限制.注意到 \\(\\phi (\\alpha)\\) 是一个一元函数，而我们研究一元函数相对比较方便"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.48,
                0.825,
                0.539
            ],
            "angle": 0,
            "content": "线搜索的目标是选取合适的 \\(\\alpha_{k}\\) 使得 \\(\\phi (\\alpha_{k})\\) 尽可能减小．但这一工作并不容易： \\(\\alpha_{k}\\) 应该使得 \\(f\\) 充分下降，与此同时不应在寻找 \\(\\alpha_{k}\\) 上花费过多的计算量．我们需要权衡这两个方面．一个自然的想法是寻找 \\(\\alpha_{k}\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.47,
                0.556,
                0.612,
                0.582
            ],
            "angle": 0,
            "content": "\\[\n\\alpha_{k} = \\operatorname *{arg  min}_{\\alpha >0}\\phi (\\alpha),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.594,
                0.826,
                0.715
            ],
            "angle": 0,
            "content": "即 \\(\\alpha_{k}\\) 为最佳步长。这种线搜索算法被称为精确线搜索算法。需要指出的是，使用精确线搜索算法时我们可以在多数情况下得到优化问题的解，但选取 \\(\\alpha_{k}\\) 通常需要很大计算量，在实际应用中较少使用。另一个想法不要求 \\(\\alpha_{k}\\) 是 \\(\\phi(\\alpha)\\) 的最小值点，而是仅仅要求 \\(\\phi(\\alpha_{k})\\) 满足某些不等式性质。这种线搜索方法被称为非精确线搜索算法。由于非精确线搜索算法结构简单，在实际应用中较为常见，接下来我们介绍该算法的结构。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.742,
                0.421,
                0.76
            ],
            "angle": 0,
            "content": "6.1.1 线搜索准则"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.774,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "在非精确线搜索算法中，选取 \\(\\alpha_{k}\\) 需要满足一定的要求，这些要求被称为线搜索准则。这里指出，线搜索准则的合适与否直接决定了算法的收敛性，若选取不合适的线搜索准则将会导致算法无法收敛。为此我们给出一个例子。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.1 线搜索方法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "217"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.157,
                0.457,
                0.174
            ],
            "angle": 0,
            "content": "例6.1 考虑一维无约束优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.387,
                0.188,
                0.52,
                0.214
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} f (x) = x ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.225,
                0.741,
                0.285
            ],
            "angle": 0,
            "content": "迭代初始点 \\(x^0 = 1\\) ，由于问题是一维的，下降方向只有 \\(\\{-1, + 1\\}\\) 两种.我们选取 \\(d^{k} = -\\mathrm{sign}(x^{k})\\) ，且只要求选取的步长满足迭代点处函数值单调下降，即 \\(f(x^{k} + \\alpha_{k}d^{k}) <   f(x^{k})\\) ，考虑选取如下两种步长："
        },
        {
            "type": "equation",
            "bbox": [
                0.341,
                0.294,
                0.567,
                0.327
            ],
            "angle": 0,
            "content": "\\[\n\\alpha_ {k, 1} = \\frac {1}{3 ^ {k + 1}}, \\quad \\alpha_ {k, 2} = 1 + \\frac {2}{3 ^ {k + 1}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.336,
                0.349,
                0.353
            ],
            "angle": 0,
            "content": "通过简单计算可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.29,
                0.363,
                0.617,
                0.4
            ],
            "angle": 0,
            "content": "\\[\nx _ {1} ^ {k} = \\frac {1}{2} \\left(1 + \\frac {1}{3 ^ {k}}\\right), \\quad x _ {2} ^ {k} = \\frac {(- 1) ^ {k}}{2} \\left(1 + \\frac {1}{3 ^ {k}}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.41,
                0.738,
                0.45
            ],
            "angle": 0,
            "content": "显然，序列 \\(\\{f(x_1^k)\\}\\) 和序列 \\(\\{f(x_2^k)\\}\\) 均单调下降，但序列 \\(\\{x_1^k\\}\\) 收敛的点不是极小值点，序列 \\(\\{x_2^k\\}\\) 则在原点左右振荡，不存在极限."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.462,
                0.741,
                0.522
            ],
            "angle": 0,
            "content": "出现上述情况的原因是在迭代过程中函数值 \\( f(x^{k}) \\) 的下降量不够充分，以至于算法无法收敛到极小值点。为了避免这种情况发生，必须引入一些更合理的线搜索准则来确保迭代的收敛性。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.545,
                0.303,
                0.563
            ],
            "angle": 0,
            "content": "1. Armijo准则"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.576,
                0.739,
                0.614
            ],
            "angle": 0,
            "content": "我们首先引入Armijo准则，它是一个常用的线搜索准则。引入Armijo准则的目的是保证每一步迭代充分下降。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.626,
                0.629,
                0.645
            ],
            "angle": 0,
            "content": "定义6.1（Armijo准则）设 \\(d^k\\) 是点 \\(x^k\\) 处的下降方向，若"
        },
        {
            "type": "equation",
            "bbox": [
                0.312,
                0.659,
                0.737,
                0.68
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k} + \\alpha d ^ {k}\\right) \\leqslant f \\left(x ^ {k}\\right) + c _ {1} \\alpha \\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} d ^ {k}, \\tag {6.1.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.694,
                0.62,
                0.712
            ],
            "angle": 0,
            "content": "则称步长 \\(\\alpha\\) 满足Armijo准则，其中 \\(c_{1}\\in (0,1)\\) 是一个常数."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.725,
                0.738,
                0.762
            ],
            "angle": 0,
            "content": "Armijo 准则(6.1.2)有非常直观的几何含义，它指的是点 \\((\\alpha, \\phi(\\alpha))\\) 必须在直线"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.765,
                0.56,
                0.786
            ],
            "angle": 0,
            "content": "\\[\nl (\\alpha) = \\phi (0) + c _ {1} \\alpha \\nabla f (x ^ {k}) ^ {\\mathrm {T}} d ^ {k}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.795,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "的下方。如图 6.1 所示，区间 \\([0, \\alpha_1]\\) 中的点均满足 Armijo 准则。我们注意到 \\(d^k\\) 为下降方向，这说明 \\(l(\\alpha)\\) 的斜率为负，选取符合条件(6.1.2)的 \\(\\alpha\\) 确实会使得函数值下降。在实际应用中，参数 \\(c_1\\) 通常选为一个很小的正数，例如"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "218"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "image",
            "bbox": [
                0.318,
                0.171,
                0.768,
                0.31
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.463,
                0.334,
                0.621,
                0.35
            ],
            "angle": 0,
            "content": "图6.1 Armijo准则"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.377,
                0.827,
                0.457
            ],
            "angle": 0,
            "content": "\\(c_{1} = 10^{-3}\\), 这使得 Armijo 准则非常容易得到满足. 但是仅仅使用 Armijo 准则并不能保证迭代的收敛性, 这是因为 \\(\\alpha = 0\\) 显然满足条件(6.1.2), 而这意味着迭代序列中的点固定不变, 研究这样的步长是没有意义的. 为此, Armijo 准则需要配合其他准则共同使用."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.46,
                0.828,
                0.52
            ],
            "angle": 0,
            "content": "在优化算法的实现中，寻找一个满足Armijo准则的步长是比较容易的，一个最常用的算法是回退法．给定初值 \\(\\hat{\\alpha}\\) ，回退法通过不断以指数方式缩小试探步长，找到第一个满足Armijo准则(6.1.2)的点．具体来说，回退法选取"
        },
        {
            "type": "equation",
            "bbox": [
                0.502,
                0.534,
                0.583,
                0.553
            ],
            "angle": 0,
            "content": "\\[\n\\alpha_ {k} = \\gamma^ {j _ {0}} \\hat {\\alpha},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.568,
                0.299,
                0.584
            ],
            "angle": 0,
            "content": "其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.296,
                0.599,
                0.787,
                0.62
            ],
            "angle": 0,
            "content": "\\[\nj _ {0} = \\min  \\{j = 0, 1, \\dots \\mid f (x ^ {k} + \\gamma^ {j} \\hat {\\alpha} d ^ {k}) \\leqslant f (x ^ {k}) + c _ {1} \\gamma^ {j} \\hat {\\alpha} \\nabla f (x ^ {k}) ^ {\\mathrm {T}} d ^ {k} \\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.635,
                0.78,
                0.652
            ],
            "angle": 0,
            "content": "参数 \\(\\gamma \\in (0,1)\\) 为一个给定的实数. 回退法的基本过程如算法6.1所示"
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.668,
                0.432,
                0.685
            ],
            "angle": 0,
            "content": "算法6.1线搜索回退法"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.689,
                0.671,
                0.707
            ],
            "angle": 0,
            "content": "1. 选择初始步长 \\(\\hat{\\alpha}\\)，参数 \\(\\gamma, c \\in (0,1)\\)。初始化 \\(\\alpha \\leftarrow \\hat{\\alpha}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.709,
                0.631,
                0.727
            ],
            "angle": 0,
            "content": "2. while \\( f(x^{k} + \\alpha d^{k}) > f(x^{k}) + c\\alpha \\nabla f(x^{k})^{\\mathrm{T}}d^{k} \\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.732,
                0.393,
                0.748
            ],
            "angle": 0,
            "content": "3. 令 \\(\\alpha \\leftarrow \\gamma \\alpha\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.752,
                0.373,
                0.767
            ],
            "angle": 0,
            "content": "4. end while"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.773,
                0.383,
                0.789
            ],
            "angle": 0,
            "content": "5. 输出 \\(\\alpha_{k} = \\alpha\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.689,
                0.671,
                0.789
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "该算法被称为回退法是因为 \\(\\alpha\\) 的试验值是由大至小的，它可以确保输出的 \\(\\alpha_{k}\\) 能尽量地大．此外算法6.1不会无限进行下去，因为 \\(d^{k}\\) 是一个下降方"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.118,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.1 线搜索方法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "219"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.195
            ],
            "angle": 0,
            "content": "向，当 \\(\\alpha\\) 充分小时，Armijo准则总是成立的．在实际应用中我们通常也会给 \\(\\alpha\\) 设置一个下界，防止步长过小."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.221,
                0.323,
                0.237
            ],
            "angle": 0,
            "content": "2. Goldstein准则"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.251,
                0.737,
                0.33
            ],
            "angle": 0,
            "content": "为了克服 Armijo 准则的缺陷，我们需要引入其他准则来保证每一步的 \\(\\alpha^k\\) 不会太小。既然 Armijo 准则只要求点 \\((\\alpha, \\phi(\\alpha))\\) 必须处在某直线下方，我们也可使用相同的形式使得该点必须处在另一条直线的上方。这就是 Armijo-Goldstein 准则，简称 Goldstein 准则。"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.345,
                0.651,
                0.362
            ],
            "angle": 0,
            "content": "定义6.2(Goldstein准则)设 \\(d^k\\) 是点 \\(x^{k}\\) 处的下降方向，若"
        },
        {
            "type": "equation",
            "bbox": [
                0.292,
                0.377,
                0.737,
                0.396
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k} + \\alpha d ^ {k}\\right) \\leqslant f \\left(x ^ {k}\\right) + c \\alpha \\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} d ^ {k}, \\tag {6.1.3a}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.294,
                0.401,
                0.737,
                0.421
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k} + \\alpha d ^ {k}\\right) \\geqslant f \\left(x ^ {k}\\right) + (1 - c) \\alpha \\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} d ^ {k}, \\tag {6.1.3b}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.435,
                0.56,
                0.471
            ],
            "angle": 0,
            "content": "则称步长 \\(\\alpha\\) 满足Goldstein准则，其中 \\(c\\in \\left(0,\\frac{1}{2}\\right)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.478,
                0.737,
                0.517
            ],
            "angle": 0,
            "content": "同样，Goldstein准则(6.1.3)也有非常直观的几何含义，它指的是点 \\((\\alpha, \\phi(\\alpha))\\) 必须在两条直线"
        },
        {
            "type": "equation",
            "bbox": [
                0.322,
                0.529,
                0.543,
                0.549
            ],
            "angle": 0,
            "content": "\\[\nl _ {1} (\\alpha) = \\phi (0) + c \\alpha \\nabla f (x ^ {k}) ^ {\\mathrm {T}} d ^ {k},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.326,
                0.552,
                0.583,
                0.574
            ],
            "angle": 0,
            "content": "\\[\nl _ {2} (\\alpha) = \\phi (0) + (1 - c) \\alpha \\nabla f (x ^ {k}) ^ {\\mathrm {T}} d ^ {k}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.587,
                0.737,
                0.625
            ],
            "angle": 0,
            "content": "之间．如图6.2所示，区间 \\([\\alpha_{1},\\alpha_{2}]\\) 中的点均满足Goldstein准则．同时我们也注意到Goldstein准则确实去掉了过小的 \\(\\alpha\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.22,
                0.652,
                0.691,
                0.79
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.364,
                0.814,
                0.545,
                0.831
            ],
            "angle": 0,
            "content": "图6.2 Goldstein准则"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "220"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.157,
                0.38,
                0.173
            ],
            "angle": 0,
            "content": "3. Wolfe 准则"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.187,
                0.825,
                0.245
            ],
            "angle": 0,
            "content": "Goldstein 准则能够使得函数值充分下降, 但是它可能避开了最优的函数值. 如图6.2所示, 一维函数 \\(\\phi(\\alpha)\\) 的最小值点并不在满足 Goldstein 准则的区间 \\([\\alpha_{1}, \\alpha_{2}]\\) 中. 为此我们引入 Armijo-Wolfe 准则, 简称 Wolfe 准则."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.256,
                0.706,
                0.273
            ],
            "angle": 0,
            "content": "定义6.3 (Wolfe准则) 设 \\(d^k\\) 是点 \\(x^k\\) 处的下降方向，若"
        },
        {
            "type": "equation",
            "bbox": [
                0.4,
                0.284,
                0.826,
                0.304
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k} + \\alpha d ^ {k}\\right) \\leqslant f \\left(x ^ {k}\\right) + c _ {1} \\alpha \\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} d ^ {k}, \\tag {6.1.4a}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.308,
                0.826,
                0.328
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f \\left(x ^ {k} + \\alpha d ^ {k}\\right) ^ {\\mathrm {T}} d ^ {k} \\geqslant c _ {2} \\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} d ^ {k}, \\tag {6.1.4b}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.34,
                0.805,
                0.357
            ],
            "angle": 0,
            "content": "则称步长 \\(\\alpha\\) 满足Wolfe准则，其中 \\(c_{1}, c_{2} \\in (0,1)\\) 为给定的常数且 \\(c_{1} < c_{2}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.367,
                0.827,
                0.528
            ],
            "angle": 0,
            "content": "在准则(6.1.4)中，第一个不等式(6.1.4a)即是Armijo准则，而第二个不等式(6.1.4b)则是Wolfe准则的本质要求。注意到 \\(\\nabla f(x^{k} + \\alpha d^{k})^{\\mathrm{T}}d^{k}\\) 恰好就是\\(\\phi (\\alpha)\\) 的导数，Wolfe准则实际要求 \\(\\phi (\\alpha)\\) 在点 \\(\\alpha\\) 处切线的斜率不能小于 \\(\\phi^{\\prime}(0)\\) 的 \\(c_{2}\\) 倍。如图6.3所示，在区间 \\([\\alpha_1,\\alpha_2]\\) 中的点均满足Wolfe准则。注意到在\\(\\phi (\\alpha)\\) 的极小值点 \\(\\alpha^{*}\\) 处有 \\(\\phi^{\\prime}(\\alpha^{*}) = \\nabla f(x^{k} + \\alpha^{*}d^{k})^{\\mathrm{T}}d^{k} = 0\\) ，因此 \\(\\alpha^{*}\\) 永远满足条件(6.1.4b)。而选择较小的 \\(c_{1}\\) 可使得 \\(\\alpha^{*}\\) 同时满足条件(6.1.4a)，即Wolfe准则在绝大多数情况下会包含线搜索子问题的精确解。在实际应用中，参数 \\(c_{2}\\) 通常取为0.9。"
        },
        {
            "type": "image",
            "bbox": [
                0.289,
                0.556,
                0.796,
                0.694
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.468,
                0.719,
                0.616,
                0.735
            ],
            "angle": 0,
            "content": "图6.3 Wolfe准则"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.765,
                0.432,
                0.781
            ],
            "angle": 0,
            "content": "4. 非单调线搜索准则"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "以上介绍的三种准则都有一个共同点：使用这些准则产生的迭代点列都是单调的。在实际应用中，非单调算法有时会有更好的效果。这就需要我们应用非单调线搜索准则，这里介绍其中两种。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.1 线搜索方法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "221"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.737,
                0.195
            ],
            "angle": 0,
            "content": "定义6.4 (Grippo[91]) 设 \\(d^k\\) 是点 \\(x^k\\) 处的下降方向，\\(M > 0\\) 为给定的正整数。以下不等式可作为一种线搜索准则："
        },
        {
            "type": "equation",
            "bbox": [
                0.264,
                0.209,
                0.737,
                0.236
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k} + \\alpha d ^ {k}\\right) \\leqslant \\max  _ {0 \\leqslant j \\leqslant \\min  \\{k, M \\}} f \\left(x ^ {k - j}\\right) + c _ {1} \\alpha \\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} d ^ {k}, \\tag {6.1.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.248,
                0.399,
                0.264
            ],
            "angle": 0,
            "content": "其中 \\(c_{1}\\in (0,1)\\) 为给定的常数"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.278,
                0.738,
                0.377
            ],
            "angle": 0,
            "content": "准则(6.1.5)和Armijo准则非常相似，区别在于Armijo准则要求下一次迭代的函数值 \\( f(x^{k + 1}) \\) 相对于本次迭代的函数值 \\( f(x^{k}) \\) 有充分下降，而准则(6.1.5)只需要下一步函数值相比前面至多 \\( M \\) 步以内迭代的函数值有下降就可以了。显然这一准则的要求比Armijo准则更宽，它也不要求 \\( f(x^{k}) \\) 的单调性。"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.381,
                0.523,
                0.397
            ],
            "angle": 0,
            "content": "另一种非单调线搜索准则的定义更加宽泛"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.411,
                0.737,
                0.449
            ],
            "angle": 0,
            "content": "定义6.5 (Zhang, Hager [209]) 设 \\(d^k\\) 是点 \\(x^k\\) 处的下降方向，以下不等式可作为一种线搜索准则："
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.463,
                0.737,
                0.483
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k} + \\alpha d ^ {k}\\right) \\leqslant C ^ {k} + c _ {1} \\alpha \\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} d ^ {k}, \\tag {6.1.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.495,
                0.737,
                0.543
            ],
            "angle": 0,
            "content": "其中 \\(C^k\\) 满足递推式 \\(C^0 = f(x^0), C^{k + 1} = \\frac{1}{Q^{k + 1}} (\\eta Q^k C^k +f(x^{k + 1}))\\) ，序列 \\(\\{Q^k\\}\\) 满足 \\(Q^0 = 1, Q^{k + 1} = \\eta Q^k +1\\) ，参数 \\(\\eta ,c_{1}\\in (0,1)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.556,
                0.738,
                0.636
            ],
            "angle": 0,
            "content": "我们可以用以下的方式理解这个准则：变量 \\(C^k\\) 实际上是本次搜索准则的参照函数值，即充分下降性质的起始标准；而下一步的标准 \\(C^{k + 1}\\) 则是函数值 \\(f(x^{k + 1})\\) 和 \\(C^k\\) 的凸组合，并非仅仅依赖于 \\(f(x^{k + 1})\\) ，而凸组合的两个系数由参数 \\(\\eta\\) 决定．可以看到当 \\(\\eta = 0\\) 时，此准则就是Armijo准则."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.66,
                0.334,
                0.679
            ],
            "angle": 0,
            "content": "6.1.2 线搜索算法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.692,
                0.74,
                0.854
            ],
            "angle": 0,
            "content": "本小节介绍在实际中使用的线搜索算法。之前的讨论已经初步介绍了回退法（算法6.1），并指出该算法可以用于寻找Armijo准则(6.1.2)的步长。实际上只要修改一下算法的终止条件，回退法就可以被用在其他线搜索准则之上，例如之前我们提到的两种非单调线搜索准则(6.1.5)和(6.1.6)。回退法的实现简单、原理直观，所以它是最常用的线搜索算法之一。然而，回退法的缺点也很明显：第一，它无法保证找到满足Wolfe准则的步长，即条件(6.1.4b)不一定成立，但对一些优化算法而言，找到满足Wolfe准则的步长是十分必要的；第二，回退法以指数的方式缩小步长，因此对初值 \\(\\hat{\\alpha}\\) 和参"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "222"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "数 \\(\\gamma\\) 的选取比较敏感, 当 \\(\\gamma\\) 过大时每一步试探步长改变量很小, 此时回退法效率比较低, 当 \\(\\gamma\\) 过小时回退法过于激进, 导致最终找到的步长太小, 错过了选取大步长的机会. 下面简单介绍其他类型的线搜索算法."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.219,
                0.825,
                0.319
            ],
            "angle": 0,
            "content": "为了提高回退法的效率，我们有基于多项式插值的线搜索算法。假设初始步长 \\(\\hat{\\alpha}_0\\) 已给定，如果经过验证，\\(\\hat{\\alpha}_0\\) 不满足 Armijo 准则，下一步就需要减小试探步长。和回退法不同，我们不直接将 \\(\\hat{\\alpha}_0\\) 缩小常数倍，而是基于 \\(\\phi(0), \\phi'(0), \\phi(\\hat{\\alpha}_0)\\) 这三个信息构造一个二次插值函数 \\(p_2(\\alpha)\\)，即寻找二次函数 \\(p_2(\\alpha)\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.359,
                0.33,
                0.722,
                0.349
            ],
            "angle": 0,
            "content": "\\[\np _ {2} (0) = \\phi (0), \\quad p _ {2} ^ {\\prime} (0) = \\phi^ {\\prime} (0), \\quad p _ {2} (\\hat {\\alpha} _ {0}) = \\phi (\\hat {\\alpha} _ {0}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.358,
                0.825,
                0.436
            ],
            "angle": 0,
            "content": "由于二次函数只有三个参数，以上三个条件可以唯一决定 \\(p_2(\\alpha)\\)，而且不难验证 \\(p_2(\\alpha)\\) 的最小值点恰好位于 \\((0, \\hat{\\alpha}_0)\\) 内。此时取 \\(p_2(\\alpha)\\) 的最小值点 \\(\\hat{\\alpha}_1\\) 作为下一个试探点，利用同样的方式不断递归下去直至找到满足 Armijo 准则的点。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.441,
                0.825,
                0.52
            ],
            "angle": 0,
            "content": "基于插值的线搜索算法可以有效减少试探次数，但仍然不能保证找到的步长满足Wolfe准则．为此，Fletcher提出了一个用于寻找满足Wolfe准则的算法[69]．这个算法比较复杂，有较多细节，这里不展开叙述，读者可以参考[69,145]."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.545,
                0.42,
                0.563
            ],
            "angle": 0,
            "content": "6.1.3 收敛性分析"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.576,
                0.825,
                0.634
            ],
            "angle": 0,
            "content": "这一小节给出使用不同线搜索准则导出的算法的收敛性。此收敛性建立在一般的线搜索类算法的框架上，因此得到的结论也比较弱。不过它可以帮助我们理解线搜索类算法收敛的本质要求。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.644,
                0.825,
                0.702
            ],
            "angle": 0,
            "content": "定理6.1(Zoutendijk) 考虑一般的迭代格式(6.1.1)，其中 \\(d^k\\) 是搜索方向，\\(\\alpha_k\\) 是步长，且在迭代过程中Wolfe准则(6.1.4)满足．假设目标函数 \\(f\\) 下有界、连续可微且梯度 \\(L\\) -利普希茨连续，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.713,
                0.712,
                0.731
            ],
            "angle": 0,
            "content": "\\[\n\\| \\nabla f (x) - \\nabla f (y) \\| \\leqslant L \\| x - y \\|, \\quad \\forall x, y \\in \\mathbb {R} ^ {n},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.742,
                0.297,
                0.756
            ],
            "angle": 0,
            "content": "那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.436,
                0.756,
                0.825,
                0.793
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {k = 0} ^ {\\infty} \\cos^ {2} \\theta_ {k} \\| \\nabla f (x ^ {k}) \\| ^ {2} <   + \\infty , \\tag {6.1.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.797,
                0.724,
                0.814
            ],
            "angle": 0,
            "content": "其中 \\(\\cos \\theta_{k}\\) 为负梯度 \\(-\\nabla f(x^{k})\\) 和下降方向 \\(d^k\\) 夹角的余弦，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.449,
                0.82,
                0.634,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\cos \\theta_ {k} = \\frac {- \\nabla f (x ^ {k}) ^ {\\mathrm {T}} d ^ {k}}{\\| \\nabla f (x ^ {k}) \\| \\| d ^ {k} \\|.}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.1 线搜索方法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "223"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.157,
                0.478,
                0.174
            ],
            "angle": 0,
            "content": "不等式(6.1.7)也被称为Zoutendijk条件"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.187,
                0.355,
                0.204
            ],
            "angle": 0,
            "content": "证明. 由条件(6.1.4b),"
        },
        {
            "type": "equation",
            "bbox": [
                0.275,
                0.213,
                0.635,
                0.243
            ],
            "angle": 0,
            "content": "\\[\n\\left(\\nabla f (x ^ {k + 1}) - \\nabla f (x ^ {k})\\right) ^ {\\mathrm {T}} d ^ {k} \\geqslant \\left(c _ {2} - 1\\right) \\nabla f (x ^ {k}) ^ {\\mathrm {T}} d ^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.254,
                0.495,
                0.271
            ],
            "angle": 0,
            "content": "由柯西不等式和梯度 \\(L\\)-利普希茨连续性质，"
        },
        {
            "type": "equation",
            "bbox": [
                0.199,
                0.279,
                0.71,
                0.309
            ],
            "angle": 0,
            "content": "\\[\n\\left(\\nabla f \\left(x ^ {k + 1}\\right) - \\nabla f \\left(x ^ {k}\\right)\\right) ^ {\\mathrm {T}} d ^ {k} \\leqslant \\| \\nabla f \\left(x ^ {k + 1}\\right) - \\nabla f \\left(x ^ {k}\\right) \\| \\| d ^ {k} \\| \\leqslant \\alpha_ {k} L \\| d ^ {k} \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.32,
                0.314,
                0.336
            ],
            "angle": 0,
            "content": "结合上述两式可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.364,
                0.333,
                0.543,
                0.37
            ],
            "angle": 0,
            "content": "\\[\n\\alpha_ {k} \\geqslant \\frac {c _ {2} - 1}{L} \\frac {\\nabla f (x ^ {k}) ^ {\\mathrm {T}} d ^ {k}}{\\| d ^ {k} \\| ^ {2}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.375,
                0.569,
                0.393
            ],
            "angle": 0,
            "content": "注意到 \\(\\nabla f(x^{k})^{\\mathrm{T}}d^{k} < 0\\) ，将上式代入条件(6.1.4a)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.294,
                0.402,
                0.612,
                0.443
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k + 1}\\right) \\leqslant f \\left(x ^ {k}\\right) + c _ {1} \\frac {c _ {2} - 1}{L} \\frac {\\left(\\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} d ^ {k}\\right) ^ {2}}{\\| d ^ {k} \\| ^ {2}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.453,
                0.475,
                0.469
            ],
            "angle": 0,
            "content": "根据 \\(\\theta_{k}\\) 的定义，此不等式可等价表述为"
        },
        {
            "type": "equation",
            "bbox": [
                0.283,
                0.478,
                0.623,
                0.51
            ],
            "angle": 0,
            "content": "\\[\nf (x ^ {k + 1}) \\leqslant f (x ^ {k}) + c _ {1} \\frac {c _ {2} - 1}{L} \\cos^ {2} \\theta_ {k} \\| \\nabla f (x ^ {k}) \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.519,
                0.347,
                0.535
            ],
            "angle": 0,
            "content": "再关于 \\(k\\) 求和，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.274,
                0.545,
                0.635,
                0.585
            ],
            "angle": 0,
            "content": "\\[\nf (x ^ {k + 1}) \\leqslant f (x ^ {0}) - c _ {1} \\frac {1 - c _ {2}}{L} \\sum_ {j = 0} ^ {k} \\cos^ {2} \\theta_ {j} \\| \\nabla f (x ^ {j}) \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.596,
                0.738,
                0.633
            ],
            "angle": 0,
            "content": "又因为函数 \\(f\\) 是下有界的，且由 \\(0 < c_{1} < c_{2} < 1\\) 可知 \\(c_{1}(1 - c_{2}) > 0\\) ，因此当 \\(k \\to \\infty\\) 时，"
        },
        {
            "type": "equation",
            "bbox": [
                0.348,
                0.633,
                0.557,
                0.671
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {j = 0} ^ {\\infty} \\cos^ {2} \\theta_ {j} \\| \\nabla f (x ^ {j}) \\| ^ {2} <   + \\infty .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.679,
                0.738,
                0.759
            ],
            "angle": 0,
            "content": "定理6.1指出，只要迭代点满足Wolfe准则，对梯度利普希茨连续且下有界函数总能推出(6.1.7)式成立．实际上采用Goldstein准则也可推出类似的条件．Zoutendijk定理刻画了线搜索准则的性质，配合下降方向 \\(d^{k}\\) 的选取方式我们可以得到最基本的收敛性."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.772,
                0.738,
                0.828
            ],
            "angle": 0,
            "content": "推论6.1(线搜索算法的收敛性）对于迭代法(6.1.1)，设 \\(\\theta_{k}\\) 为每一步负梯度 \\(-\\nabla f(x^{k})\\) 与下降方向 \\(d^{k}\\) 的夹角，并假设对任意的 \\(k\\) ，存在常数 \\(\\gamma >0\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.405,
                0.828,
                0.737,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\theta_ {k} <   \\frac {\\pi}{2} - \\gamma , \\tag {6.1.8}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "224"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.157,
                0.49,
                0.174
            ],
            "angle": 0,
            "content": "则在定理6.1成立的条件下，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.478,
                0.183,
                0.604,
                0.208
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\nabla f (x ^ {k}) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.212,
                0.755,
                0.23
            ],
            "angle": 0,
            "content": "证明．假设结论不成立，即存在子列 \\(\\{k_l\\}\\) 和正常数 \\(\\delta >0\\) ，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.239,
                0.652,
                0.257
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| \\nabla f \\left(x ^ {k _ {l}}\\right) \\right\\| \\geqslant \\delta , l = 1, 2, \\dots .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.268,
                0.478,
                0.284
            ],
            "angle": 0,
            "content": "根据 \\(\\theta_{k}\\) 的假设，对任意的 \\(k\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.472,
                0.296,
                0.61,
                0.312
            ],
            "angle": 0,
            "content": "\\[\n\\cos \\theta_ {k} > \\sin \\gamma > 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.323,
                0.538,
                0.34
            ],
            "angle": 0,
            "content": "我们仅考虑和式(6.1.7)的第 \\(k_{l}\\) 项，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.345,
                0.712,
                0.422
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\sum_ {k = 0} ^ {\\infty} \\cos^ {2} \\theta_ {k} \\| \\nabla f (x ^ {k}) \\| ^ {2} \\geqslant \\sum_ {l = 1} ^ {\\infty} \\cos^ {2} \\theta_ {k _ {l}} \\| \\nabla f (x ^ {k _ {l}}) \\| ^ {2} \\\\ \\geqslant \\sum_ {l = 1} ^ {\\infty} (\\sin^ {2} \\gamma) \\cdot \\delta^ {2} \\rightarrow + \\infty , \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.428,
                0.542,
                0.443
            ],
            "angle": 0,
            "content": "这显然和定理6.1矛盾。因此必有"
        },
        {
            "type": "equation",
            "bbox": [
                0.478,
                0.454,
                0.604,
                0.478
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\nabla f (x ^ {k}) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.482,
                0.825,
                0.582
            ],
            "angle": 0,
            "content": "推论6.1建立在Zoutendijk条件之上，它的本质要求是关系(6.1.8)，即每一步的下降方向 \\(d^k\\) 和负梯度方向不能趋于正交．这个条件的几何直观明显：当下降方向 \\(d^k\\) 和梯度正交时，根据泰勒展开的一阶近似，目标函数值\\(f(x^{k})\\) 几乎不发生改变．因此我们要求 \\(d^k\\) 与梯度正交方向夹角有一致的下界．后面会介绍多种 \\(d^k\\) 的选取方法，在选取 \\(d^k\\) 时条件(6.1.8)总得到满足."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.586,
                0.825,
                0.644
            ],
            "angle": 0,
            "content": "总的来说，推论6.1仅仅给出了最基本的收敛性，而没有更进一步回答算法的收敛速度。这是由于算法收敛速度极大地取决于 \\(d^{k}\\) 的选取。接下来我们将着重介绍如何选取下降方向 \\(d^{k}\\)。"
        },
        {
            "type": "title",
            "bbox": [
                0.449,
                0.673,
                0.634,
                0.695
            ],
            "angle": 0,
            "content": "6.2 梯度类算法"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.712,
                0.827,
                0.853
            ],
            "angle": 0,
            "content": "本节介绍梯度类算法，其本质是仅仅使用函数的一阶导数信息选取下降方向 \\(d^k\\)。这其中最基本的算法是梯度下降法，即直接选择负梯度作为下降方向 \\(d^k\\)。梯度下降法的方向选取非常直观，实际应用范围非常广，因此它在优化算法中的地位可相当于高斯消元法在线性方程组算法中的地位。此外我们也会介绍BB方法。该方法作为一种梯度法的变形，虽然理论性质目前仍不完整，但由于它有优秀的数值表现，也是在实际应用中使用较多的一种算法。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.2 梯度类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "225"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.156,
                0.334,
                0.175
            ],
            "angle": 0,
            "content": "6.2.1 梯度下降法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.187,
                0.739,
                0.225
            ],
            "angle": 0,
            "content": "对于光滑函数 \\(f(x)\\)，在迭代点 \\(x^k\\) 处，我们需要选择一个较为合理的 \\(d^k\\) 作为下降方向。注意到 \\(\\phi(\\alpha) = f(x^k + \\alpha d^k)\\) 有泰勒展开"
        },
        {
            "type": "equation",
            "bbox": [
                0.29,
                0.236,
                0.619,
                0.256
            ],
            "angle": 0,
            "content": "\\[\n\\phi (\\alpha) = f (x ^ {k}) + \\alpha \\nabla f (x ^ {k}) ^ {\\mathrm {T}} d ^ {k} + \\mathcal {O} (\\alpha^ {2} \\| d ^ {k} \\| ^ {2}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.268,
                0.741,
                0.306
            ],
            "angle": 0,
            "content": "根据柯西不等式，当 \\(\\alpha\\) 足够小时，取 \\(d^{k} = -\\nabla f(x^{k})\\) 会使得函数下降最快因此梯度法就是选取 \\(d^{k} = -\\nabla f(x^{k})\\) 的算法，它的迭代格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.367,
                0.317,
                0.737,
                0.337
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\alpha_ {k} \\nabla f (x ^ {k}). \\tag {6.2.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.35,
                0.706,
                0.366
            ],
            "angle": 0,
            "content": "步长 \\(\\alpha_{k}\\) 的选取可依赖于上一节的线搜索算法，也可直接选取固定的 \\(\\alpha_{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.37,
                0.739,
                0.407
            ],
            "angle": 0,
            "content": "为了直观地理解梯度法的迭代过程，我们以二次函数为例来展示该过程."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.419,
                0.741,
                0.477
            ],
            "angle": 0,
            "content": "例6.2（二次函数的梯度法）设二次函数 \\( f(x,y) = x^{2} + 10y^{2} \\)，初始点 \\((x^0, y^0)\\) 取为(10,1)，取固定步长 \\(\\alpha_k = 0.085\\)。我们使用梯度法(6.2.1)进行15次迭代，结果如图6.4所示。"
        },
        {
            "type": "image",
            "bbox": [
                0.254,
                0.506,
                0.667,
                0.632
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.339,
                0.661,
                0.571,
                0.677
            ],
            "angle": 0,
            "content": "图6.4 梯度法的前15次迭代"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.707,
                0.644,
                0.723
            ],
            "angle": 0,
            "content": "实际上，对正定二次函数有如下收敛定理（证明见[128]）："
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.735,
                0.421,
                0.75
            ],
            "angle": 0,
            "content": "定理6.2 考虑正定二次函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.758,
                0.537,
                0.789
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\frac {1}{2} x ^ {\\mathrm {T}} A x - b ^ {\\mathrm {T}} x,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.796,
                0.714,
                0.813
            ],
            "angle": 0,
            "content": "其最优值点为 \\(x^{*}\\) 。若使用梯度法(6.2.1)并选取 \\(\\alpha_{k}\\) 为精确线搜索步长，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.82,
                0.737,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\alpha_ {k} = \\frac {\\left\\| \\nabla f \\left(x ^ {k}\\right) \\right\\| ^ {2}}{\\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} A \\nabla f \\left(x ^ {k}\\right)}, \\tag {6.2.2}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "226"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.641,
                0.174
            ],
            "angle": 0,
            "content": "则梯度法关于迭代点列 \\(\\{x^k\\}\\) 是Q-线性收敛的，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.389,
                0.185,
                0.697,
                0.224
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| x ^ {k + 1} - x ^ {*} \\right\\| _ {A} ^ {2} \\leqslant \\left(\\frac {\\lambda_ {1} - \\lambda_ {n}}{\\lambda_ {1} + \\lambda_ {n}}\\right) ^ {2} \\left\\| x ^ {k} - x ^ {*} \\right\\| _ {A} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.238,
                0.825,
                0.277
            ],
            "angle": 0,
            "content": "其中 \\(\\lambda_1, \\lambda_n\\) 分别为 \\(A\\) 的最大、最小特征值，\\(\\|x\\|_A \\stackrel{\\mathrm{def}}{=} \\sqrt{x^{\\mathrm{T}}Ax}\\) 为由正定矩阵 \\(A\\) 诱导的范数."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.292,
                0.827,
                0.413
            ],
            "angle": 0,
            "content": "定理6.2指出使用精确线搜索的梯度法在正定二次问题上有 Q-线性收敛速度。线性收敛速度的常数和矩阵 \\(A\\) 最大特征值与最小特征值之比有关。从等高线角度来看，这个比例越大则 \\(f(x)\\) 的等高线越扁平，图6.4中迭代路径折返频率会随之变高，梯度法收敛也就越慢。这个结果其实说明了梯度法的一个很重大的缺陷：当目标函数的海瑟矩阵条件数较大时，它的收敛速度会非常缓慢。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.417,
                0.838,
                0.455
            ],
            "angle": 0,
            "content": "接下来我们介绍当 \\(f(x)\\) 为梯度利普希茨连续的凸函数时，梯度法(6.2.1)的收敛性质."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.47,
                0.827,
                0.577
            ],
            "angle": 0,
            "content": "定理6.3（梯度法在凸函数上的收敛性）设函数 \\( f(x) \\) 为凸的梯度 \\( L \\)-利普希茨连续函数，\\( f^{*} = f(x^{*}) = \\inf_{x}f(x) \\) 存在且可达．如果步长 \\( \\alpha_{k} \\) 取为常数 \\( \\alpha \\) 且满足 \\( 0 < \\alpha < \\frac{1}{L} \\)，那么由迭代(6.2.1)得到的点列 \\( \\{x^k\\} \\) 的函数值收敛到最优值，且在函数值的意义下收敛速度为 \\( \\mathcal{O}\\left(\\frac{1}{k}\\right) \\)."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.585,
                0.791,
                0.602
            ],
            "angle": 0,
            "content": "证明. 因为函数 \\(f\\) 是利普希茨可微函数，对任意的 \\(x\\) ，根据(2.2.3)式，"
        },
        {
            "type": "equation",
            "bbox": [
                0.356,
                0.614,
                0.825,
                0.649
            ],
            "angle": 0,
            "content": "\\[\nf (x - \\alpha \\nabla f (x)) \\leqslant f (x) - \\alpha \\left(1 - \\frac {L \\alpha}{2}\\right) \\| \\nabla f (x) \\| ^ {2}. \\tag {6.2.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.661,
                0.647,
                0.692
            ],
            "angle": 0,
            "content": "现在记 \\(\\tilde{x} = x - \\alpha \\nabla f(x)\\) 并限制 \\(0 < \\alpha < \\frac{1}{L}\\)，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.348,
                0.702,
                0.732,
                0.826
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (\\tilde {x}) \\leqslant f (x) - \\frac {\\alpha}{2} \\| \\nabla f (x) \\| ^ {2} \\\\ \\leqslant f ^ {*} + \\nabla f (x) ^ {\\mathrm {T}} (x - x ^ {*}) - \\frac {\\alpha}{2} \\| \\nabla f (x) \\| ^ {2} \\\\ = f ^ {*} + \\frac {1}{2 \\alpha} \\left(\\| x - x ^ {*} \\| ^ {2} - \\| x - x ^ {*} - \\alpha \\nabla f (x) \\| ^ {2}\\right) \\\\ = f ^ {*} + \\frac {1}{2 \\alpha} (\\| x - x ^ {*} \\| ^ {2} - \\| \\tilde {x} - x ^ {*} \\| ^ {2}), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.836,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "其中第一个不等式是由于(6.2.3)式，第二个不等式为 \\(f\\) 的凸性。在上式中取"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.2 梯度类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "227"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.569,
                0.174
            ],
            "angle": 0,
            "content": "\\(x = x^{i - 1},\\tilde{x} = x^i\\) 并将不等式对 \\(i = 1,2,\\dots ,k\\) 求和得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.262,
                0.185,
                0.644,
                0.287
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\sum_ {i = 1} ^ {k} \\left(f \\left(x ^ {i}\\right) - f ^ {*}\\right) \\leqslant \\frac {1}{2 \\alpha} \\sum_ {i = 1} ^ {k} \\left(\\left\\| x ^ {i - 1} - x ^ {*} \\right\\| ^ {2} - \\left\\| x ^ {i} - x ^ {*} \\right\\| ^ {2}\\right) \\\\ = \\frac {1}{2 \\alpha} \\left(\\| x ^ {0} - x ^ {*} \\| ^ {2} - \\| x ^ {k} - x ^ {*} \\| ^ {2}\\right) \\\\ \\leqslant \\frac {1}{2 \\alpha} \\| x ^ {0} - x ^ {*} \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.299,
                0.475,
                0.316
            ],
            "angle": 0,
            "content": "根据(6.2.3)式得知 \\(f(x^{i})\\) 是非增的，所以"
        },
        {
            "type": "equation",
            "bbox": [
                0.27,
                0.327,
                0.635,
                0.365
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k}\\right) - f ^ {*} \\leqslant \\frac {1}{k} \\sum_ {i = 1} ^ {k} \\left(f \\left(x ^ {i}\\right) - f ^ {*}\\right) \\leqslant \\frac {1}{2 k \\alpha} \\| x ^ {0} - x ^ {*} \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.419,
                0.736,
                0.455
            ],
            "angle": 0,
            "content": "如果函数 \\(f\\) 还是 \\(m\\)-强凸函数，则梯度法的收敛速度会进一步提升为 Q-线性收敛."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.46,
                0.737,
                0.497
            ],
            "angle": 0,
            "content": "在给出收敛性证明之前，我们需要以下的引理来揭示凸的梯度 \\(L\\) -利普希茨连续函数的另一个重要性质."
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.51,
                0.688,
                0.527
            ],
            "angle": 0,
            "content": "引理6.1 设函数 \\(f(x)\\) 是 \\(\\mathbb{R}^n\\) 上的凸可微函数，则以下结论等价："
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.54,
                0.449,
                0.557
            ],
            "angle": 0,
            "content": "(1) \\(f\\) 的梯度为 \\(L_{-}\\) 利普希茨连续的；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.565,
                0.499,
                0.596
            ],
            "angle": 0,
            "content": "(2) 函数 \\(g(x) \\stackrel{\\mathrm{def}}{=} \\frac{L}{2} x^{\\mathrm{T}} x - f(x)\\) 是凸函数；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.603,
                0.562,
                0.621
            ],
            "angle": 0,
            "content": "(3) \\(\\nabla f(x)\\) 有余强制性, 即对任意的 \\(x, y \\in \\mathbb{R}^{n}\\), 有"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.54,
                0.562,
                0.621
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.259,
                0.631,
                0.737,
                0.662
            ],
            "angle": 0,
            "content": "\\[\n\\left(\\nabla f (x) - \\nabla f (y)\\right) ^ {\\mathrm {T}} (x - y) \\geqslant \\frac {1}{L} \\| \\nabla f (x) - \\nabla f (y) \\| ^ {2}. \\tag {6.2.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.675,
                0.639,
                0.693
            ],
            "angle": 0,
            "content": "证明. (1) \\(\\Rightarrow\\) (2) 即证 \\(g(x)\\) 的单调性. 对任意 \\(x, y \\in \\mathbb{R}^n\\),"
        },
        {
            "type": "equation",
            "bbox": [
                0.184,
                0.706,
                0.727,
                0.748
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left(\\nabla g (x) - \\nabla g (y)\\right) ^ {\\mathrm {T}} (x - y) = L \\| x - y \\| ^ {2} - \\left(\\nabla f (x) - \\nabla f (y)\\right) ^ {\\mathrm {T}} (x - y) \\\\ \\geqslant L \\| x - y \\| ^ {2} - \\| x - y \\| \\| \\nabla f (x) - \\nabla f (y) \\| \\geqslant 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.76,
                0.325,
                0.777
            ],
            "angle": 0,
            "content": "因此 \\(g(x)\\) 为凸函数"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.781,
                0.422,
                0.798
            ],
            "angle": 0,
            "content": "(2) \\(\\Rightarrow\\) (3) 构造辅助函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.362,
                0.812,
                0.548,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f _ {x} (z) = f (z) - \\nabla f (x) ^ {\\mathrm {T}} z, \\\\ f _ {y} (z) = f (z) - \\nabla f (y) ^ {\\mathrm {T}} z, \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.289,
                0.13
            ],
            "angle": 0,
            "content": "228"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.155,
                0.826,
                0.201
            ],
            "angle": 0,
            "content": "容易验证 \\(f_{x}\\) 和 \\(f_{y}\\) 均为凸函数．根据已知条件，\\(g_{x}(z) = \\frac{L}{2} z^{\\mathrm{T}}z - f_{x}(z)\\) 关于 \\(z\\) 是凸函数．根据凸函数的性质，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.211,
                0.737,
                0.23
            ],
            "angle": 0,
            "content": "\\[\ng _ {x} \\left(z _ {2}\\right) \\geqslant g _ {x} \\left(z _ {1}\\right) + \\nabla g _ {x} \\left(z _ {1}\\right) ^ {\\mathrm {T}} \\left(z _ {2} - z _ {1}\\right), \\quad \\forall z _ {1}, z _ {2} \\in \\mathbb {R} ^ {n}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.241,
                0.826,
                0.28
            ],
            "angle": 0,
            "content": "整理可推出 \\(f_{x}(z)\\) 有二次上界，且对应的系数也为 \\(L\\)。注意到 \\(\\nabla f_{x}(x) = 0\\)，这说明 \\(x\\) 是 \\(f_{x}(z)\\) 的最小值点。再由推论2.1的证明过程，"
        },
        {
            "type": "equation",
            "bbox": [
                0.329,
                0.289,
                0.753,
                0.343
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f _ {x} (y) - f _ {x} (x) = f (y) - f (x) - \\nabla f (x) ^ {\\mathrm {T}} (y - x) \\\\ \\geqslant \\frac {1}{2 L} \\| \\nabla f _ {x} (y) \\| ^ {2} = \\frac {1}{2 L} \\| \\nabla f (y) - \\nabla f (x) \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.349,
                0.533,
                0.366
            ],
            "angle": 0,
            "content": "同理，对 \\(f_{y}(z)\\) 进行类似的分析可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.374,
                0.75,
                0.405
            ],
            "angle": 0,
            "content": "\\[\nf (x) - f (y) - \\nabla f (y) ^ {\\mathrm {T}} (x - y) \\geqslant \\frac {1}{2 L} \\| \\nabla f (y) - \\nabla f (x) \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.41,
                0.674,
                0.427
            ],
            "angle": 0,
            "content": "将以上两式不等号左右分别相加，可得余强制性(6.2.4)."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.431,
                0.602,
                0.448
            ],
            "angle": 0,
            "content": "(3) \\(\\Longrightarrow\\) (1) 由余强制性和柯西不等式，"
        },
        {
            "type": "equation",
            "bbox": [
                0.348,
                0.454,
                0.735,
                0.51
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\frac {1}{L} \\| \\nabla f (x) - \\nabla f (y) \\| ^ {2} \\leqslant (\\nabla f (x) - \\nabla f (y)) ^ {\\mathrm {T}} (x - y) \\\\ \\leqslant \\| \\nabla f (x) - \\nabla f (y) \\| \\| x - y \\|, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.517,
                0.622,
                0.534
            ],
            "angle": 0,
            "content": "整理后即可得到 \\(f(x)\\) 是梯度 \\(L\\)-利普希茨连续的"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.519,
                0.824,
                0.53
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.547,
                0.825,
                0.606
            ],
            "angle": 0,
            "content": "引理6.1说明在 \\(f\\) 为凸函数的条件下，梯度 \\(L\\)-利普希茨连续、二次上界、余强制性三者是等价的，知道其中一个性质就可推出剩下两个．接下来给出梯度法在强凸函数下的收敛性."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.616,
                0.826,
                0.704
            ],
            "angle": 0,
            "content": "定理6.4（梯度法在强凸函数上的收敛性）设函数 \\( f(x) \\) 为 \\( m \\)-强凸的梯度 \\( L \\)-利普希茨连续函数，\\( f^{*} = f(x^{*}) = \\inf_{x} f(x) \\) 存在且可达．如果步长 \\( \\alpha \\) 满足 \\( 0 < \\alpha < \\frac{2}{m + L} \\)，那么由梯度下降法(6.2.1)迭代得到的点列 \\( \\{x^k\\} \\) 收敛到 \\( x^{*} \\)，且为 \\( Q \\)-线性收敛."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.714,
                0.655,
                0.731
            ],
            "angle": 0,
            "content": "证明．首先根据 \\(f\\) 强凸且 \\(\\nabla f\\) 利普希茨连续，可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.46,
                0.737,
                0.623,
                0.767
            ],
            "angle": 0,
            "content": "\\[\ng (x) = f (x) - \\frac {m}{2} x ^ {\\mathrm {T}} x\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.773,
                0.825,
                0.819
            ],
            "angle": 0,
            "content": "为凸函数且 \\(\\frac{L - m}{2} x^{\\mathrm{T}}x - g(x)\\) 为凸函数．由引理6.1可得 \\(g(x)\\) 是梯度 \\((L - m)\\)-利普希茨连续的．再次利用引理6.1可得关于 \\(g(x)\\) 的余强制性"
        },
        {
            "type": "equation",
            "bbox": [
                0.308,
                0.825,
                0.826,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\left(\\nabla g (x) - \\nabla g (y)\\right) ^ {\\mathrm {T}} (x - y) \\geqslant \\frac {1}{L - m} \\| \\nabla g (x) - \\nabla g (y) \\| ^ {2}. \\tag {6.2.5}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.2 梯度类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "229"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.373,
                0.174
            ],
            "angle": 0,
            "content": "代入 \\(g(x)\\) 的表达式，可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.282,
                0.187,
                0.736,
                0.242
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left(\\nabla f (x) - \\nabla f (y)\\right) ^ {\\mathrm {T}} (x - y) \\\\ \\geqslant \\frac {m L}{m + L} \\| x - y \\| ^ {2} + \\frac {1}{m + L} \\| \\nabla f (x) - \\nabla f (y) \\| ^ {2}. \\tag {6.2.6} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.25,
                0.739,
                0.298
            ],
            "angle": 0,
            "content": "然后我们估计在固定步长下梯度法的收敛速度.设步长 \\(\\alpha \\in \\left(0,\\frac{2}{m + L}\\right)\\) 则"
        },
        {
            "type": "equation",
            "bbox": [
                0.189,
                0.309,
                0.72,
                0.428
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| x ^ {k + 1} - x ^ {*} \\right\\| _ {2} ^ {2} = \\left\\| x ^ {k} - \\alpha \\nabla f \\left(x ^ {k}\\right) - x ^ {*} \\right\\| ^ {2} \\\\ = \\| x ^ {k} - x ^ {*} \\| ^ {2} - 2 \\alpha \\nabla f (x ^ {k}) ^ {\\mathrm {T}} (x ^ {k} - x ^ {*}) + \\alpha^ {2} \\| \\nabla f (x ^ {k}) \\| ^ {2} \\\\ \\leqslant \\left(1 - \\alpha \\frac {2 m L}{m + L}\\right) \\| x ^ {k} - x ^ {*} \\| ^ {2} + \\alpha \\left(\\alpha - \\frac {2}{m + L}\\right) \\| \\nabla f (x ^ {k}) \\| ^ {2} \\\\ \\leqslant \\left(1 - \\alpha \\frac {2 m L}{m + L}\\right) \\| x ^ {k} - x ^ {*} \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.437,
                0.71,
                0.456
            ],
            "angle": 0,
            "content": "其中第一个不等式是对 \\(x^{k}, x^{*}\\) 应用(6.2.6)式并注意到 \\(\\nabla f(x^{*}) = 0\\)。因此，"
        },
        {
            "type": "equation",
            "bbox": [
                0.267,
                0.463,
                0.641,
                0.495
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| x ^ {k} - x ^ {*} \\right\\| ^ {2} \\leqslant c ^ {k} \\left\\| x ^ {0} - x ^ {*} \\right\\| ^ {2}, \\quad c = 1 - \\alpha \\frac {2 m L}{m + L} <   1,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.502,
                0.547,
                0.518
            ],
            "angle": 0,
            "content": "即在强凸函数的条件下，梯度法是Q-线性收敛的"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.544,
                0.432,
                0.562
            ],
            "angle": 0,
            "content": "6.2.2 Barzilar-Borwein 方法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.575,
                0.738,
                0.675
            ],
            "angle": 0,
            "content": "由上一小节可以知道，当问题的条件数很大，也即问题比较病态时，梯度下降法的收敛性质会受到很大影响。Barzilar-Borwein (BB) 方法是一种特殊的梯度法，经常比一般的梯度法有着更好的效果。从形式上来看，BB 方法的下降方向仍是点 \\( x^k \\) 处的负梯度方向 \\(-\\nabla f(x^k)\\)，但步长 \\(\\alpha_k\\) 并不是直接由线搜索算法给出的。考虑梯度下降法的格式："
        },
        {
            "type": "equation",
            "bbox": [
                0.368,
                0.687,
                0.54,
                0.707
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\alpha_ {k} \\nabla f (x ^ {k}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.72,
                0.332,
                0.736
            ],
            "angle": 0,
            "content": "这种格式也可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.738,
                0.543,
                0.758
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - D ^ {k} \\nabla f (x ^ {k}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.767,
                0.663,
                0.785
            ],
            "angle": 0,
            "content": "其中 \\(D^{k} = \\alpha_{k}I\\) .BB方法选取的 \\(\\alpha_{k}\\) 是如下两个最优问题之一的解："
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.796,
                0.737,
                0.82
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {\\alpha} \\quad \\| \\alpha y ^ {k - 1} - s ^ {k - 1} \\| ^ {2}, \\tag {6.2.7}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.355,
                0.826,
                0.737,
                0.849
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {\\alpha} \\quad \\| y ^ {k - 1} - \\alpha^ {- 1} s ^ {k - 1} \\| ^ {2}, \\tag {6.2.8}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "230"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.826,
                0.217
            ],
            "angle": 0,
            "content": "其中引入记号 \\(s^{k-1} \\stackrel{\\mathrm{def}}{=} x^k - x^{k-1}\\) 以及 \\(y^{k-1} \\stackrel{\\mathrm{def}}{=} \\nabla f(x^k) - \\nabla f(x^{k-1})\\). 在这里先直接写出问题(6.2.7)和(6.2.8)，它们的实际含义将在第6.5小节中给出合理的解释."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.223,
                0.596,
                0.239
            ],
            "angle": 0,
            "content": "容易验证问题(6.2.7)和(6.2.8)的解分别为"
        },
        {
            "type": "equation",
            "bbox": [
                0.367,
                0.251,
                0.825,
                0.289
            ],
            "angle": 0,
            "content": "\\[\n\\alpha_ {\\mathrm {B B 1}} ^ {k} \\stackrel {\\mathrm {d e f}} {=} \\frac {(s ^ {k - 1}) ^ {\\mathrm {T}} y ^ {k - 1}}{(y ^ {k - 1}) ^ {\\mathrm {T}} y ^ {k - 1}} \\quad \\text {和} \\quad \\alpha_ {\\mathrm {B B 2}} ^ {k} \\stackrel {\\mathrm {d e f}} {=} \\frac {(s ^ {k - 1}) ^ {\\mathrm {T}} s ^ {k - 1}}{(s ^ {k - 1}) ^ {\\mathrm {T}} y ^ {k - 1}}, \\qquad \\qquad (6. 2. 9)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.3,
                0.557,
                0.316
            ],
            "angle": 0,
            "content": "因此可以得到 BB 方法的两种迭代格式："
        },
        {
            "type": "equation",
            "bbox": [
                0.448,
                0.333,
                0.826,
                0.351
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - a _ {\\mathrm {B B 1}} ^ {k} \\nabla f (x ^ {k}), \\tag {6.2.10a}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.449,
                0.357,
                0.825,
                0.377
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\alpha_ {\\mathrm {B B 2}} ^ {k} \\nabla f \\left(x ^ {k}\\right). \\tag {6.2.10b}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.394,
                0.825,
                0.493
            ],
            "angle": 0,
            "content": "我们从表达式(6.2.9)注意到，计算两种 BB 步长的任何一种仅仅需要函数相邻两步的梯度信息和迭代点信息，不需要任何线搜索算法即可选取算法步长。因为这个特点，BB 方法的使用范围特别广泛。对于一般的问题，通过(6.2.9)式计算出的步长可能过大或过小，因此我们还需要将步长做上界和下界的截断，即选取 \\(0 < \\alpha_{m} < \\alpha_{M}\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.485,
                0.513,
                0.597,
                0.528
            ],
            "angle": 0,
            "content": "\\[\n\\alpha_ {m} \\leqslant \\alpha_ {k} \\leqslant \\alpha_ {M}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.546,
                0.825,
                0.584
            ],
            "angle": 0,
            "content": "还需注意的是，BB 方法本身是非单调方法，有时也配合非单调收敛准则使用以获得更好的实际效果。算法6.2给出了一种 BB 方法的框架。"
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.602,
                0.512,
                0.617
            ],
            "angle": 0,
            "content": "算法6.2非单调线搜索的BB方法"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.622,
                0.761,
                0.641
            ],
            "angle": 0,
            "content": "1. 给定 \\(x^0\\) ，选取初值 \\(\\alpha > 0\\) ，整数 \\(M \\geqslant 0\\) ，\\(c_{1}, \\beta, \\varepsilon \\in (0,1)\\) ，\\(k = 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.643,
                0.474,
                0.661
            ],
            "angle": 0,
            "content": "2. while \\(\\| \\nabla f(x^k)\\| >\\varepsilon\\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.663,
                0.789,
                0.689
            ],
            "angle": 0,
            "content": "3. while \\( f(x^{k} - \\alpha \\nabla f(x^{k})) \\geqslant \\max_{0 \\leqslant j \\leqslant \\min(k, M)} f(x^{k-j}) - c_{1}\\alpha \\| \\nabla f(x^{k})\\|^{2} \\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.69,
                0.408,
                0.706
            ],
            "angle": 0,
            "content": "4. 令 \\(\\alpha \\gets \\beta \\alpha\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.711,
                0.389,
                0.724
            ],
            "angle": 0,
            "content": "5. end while"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.729,
                0.49,
                0.748
            ],
            "angle": 0,
            "content": "6. 令 \\(x^{k + 1} = x^k -\\alpha \\nabla f(x^k)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.751,
                0.723,
                0.769
            ],
            "angle": 0,
            "content": "7. 根据公式(6.2.9)之一计算 \\(\\alpha\\) ，并做截断使得 \\(\\alpha \\in [\\alpha_{m},\\alpha_{M}]\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.772,
                0.385,
                0.787
            ],
            "angle": 0,
            "content": "8. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.793,
                0.371,
                0.807
            ],
            "angle": 0,
            "content": "9. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.622,
                0.789,
                0.807
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.836,
                0.654,
                0.852
            ],
            "angle": 0,
            "content": "我们仍然使用例6.2来说明BB方法的迭代过程"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.2 梯度类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "231"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.744,
                0.257
            ],
            "angle": 0,
            "content": "例6.3（二次函数的BB方法）设二次函数 \\( f(x,y) = x^{2} + 10y^{2} \\)，并使用BB方法进行迭代，初始点为 \\((-10, -1)\\)，结果如图6.5所示。为了方便对比，我们也在此图中描绘了梯度法的迭代过程。可以很明显看出BB方法的收敛速度较快，在经历15次迭代后已经接近最优值点。从等高线也可观察到BB方法是非单调方法。"
        },
        {
            "type": "image",
            "bbox": [
                0.256,
                0.291,
                0.667,
                0.424
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.299,
                0.454,
                0.611,
                0.471
            ],
            "angle": 0,
            "content": "图6.5 梯度法与BB方法的前15次迭代"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.504,
                0.741,
                0.584
            ],
            "angle": 0,
            "content": "实际上，对于正定二次函数，BB方法有Q-超线性收敛速度．对于一般问题，BB方法的收敛性还需要进一步研究．但即便如此，使用BB方法的步长通常都会减少算法的迭代次数．因此在编写算法时，选取BB方法的步长是常用加速策略之一."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.625,
                0.315,
                0.644
            ],
            "angle": 0,
            "content": "6.2.3 应用举例"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.662,
                0.339,
                0.679
            ],
            "angle": 0,
            "content": "1. LASSO问题求解"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.697,
                0.687,
                0.714
            ],
            "angle": 0,
            "content": "本小节利用梯度法来求解 LASSO 问题. 这个问题的原始形式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.317,
                0.727,
                0.591,
                0.76
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad f (x) = \\frac {1}{2} \\| A x - b \\| ^ {2} + \\mu \\| x \\| _ {1}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "LASSO 问题的目标函数 \\( f(x) \\) 不光滑，在某些点处无法求出梯度，因此不能直接对原始问题使用梯度法求解。考虑到目标函数的不光滑项为 \\( \\| x \\|_1 \\)，它实际上是 \\( x \\) 各个分量绝对值的和，如果能找到一个光滑函数来近似绝对值函数，那么梯度法就可以被用在 LASSO 问题的求解上。在实际应用中，我"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "232"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.494,
                0.174
            ],
            "angle": 0,
            "content": "们可以考虑如下一维光滑函数："
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.184,
                0.826,
                0.245
            ],
            "angle": 0,
            "content": "\\[\nl _ {\\delta} (x) = \\left\\{ \\begin{array}{l l} { \\frac {1}{2 \\delta} x ^ {2},} & {| x | <   \\delta ,} \\\\ {| x | - \\frac {\\delta}{2},} & {\\text {其 他}.} \\end{array} \\right. \\tag {6.2.11}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.254,
                0.828,
                0.311
            ],
            "angle": 0,
            "content": "定义(6.2.11)实际上是Huber损失函数的一种变形，当 \\(\\delta \\rightarrow 0\\) 时，光滑函数\\(l_{\\delta}(x)\\) 和绝对值函数 \\(|x|\\) 会越来越接近．图6.6展示了当 \\(\\delta\\) 取不同值时 \\(l_{\\delta}(x)\\) 的图形."
        },
        {
            "type": "image",
            "bbox": [
                0.414,
                0.33,
                0.68,
                0.495
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.401,
                0.517,
                0.684,
                0.534
            ],
            "angle": 0,
            "content": "图6.6 当 \\(\\delta\\) 取不同值时 \\(l_{\\delta}(x)\\) 的图形"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.557,
                0.585,
                0.574
            ],
            "angle": 0,
            "content": "因此，我们构造光滑化 LASSO 问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.401,
                0.584,
                0.826,
                0.617
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad f _ {\\delta} (x) = \\frac {1}{2} \\| A x - b \\| ^ {2} + \\mu L _ {\\delta} (x), \\tag {6.2.12}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.626,
                0.54,
                0.642
            ],
            "angle": 0,
            "content": "其中 \\(\\delta\\) 为给定的光滑化参数，在这里"
        },
        {
            "type": "equation",
            "bbox": [
                0.474,
                0.654,
                0.61,
                0.69
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\delta} (x) = \\sum_ {i = 1} ^ {n} l _ {\\delta} \\left(x _ {i}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.702,
                0.825,
                0.739
            ],
            "angle": 0,
            "content": "即对 \\(x\\) 的每个分量作用光滑函数 (6.2.11) 再整体求和。容易计算出 \\(f_{\\delta}(x)\\) 的梯度为"
        },
        {
            "type": "equation",
            "bbox": [
                0.41,
                0.743,
                0.673,
                0.763
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f _ {\\delta} (x) = A ^ {\\mathrm {T}} (A x - b) + \\mu \\nabla L _ {\\delta} (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.773,
                0.507,
                0.79
            ],
            "angle": 0,
            "content": "其中 \\(\\nabla L_{\\delta}(x)\\) 是逐个分量定义的："
        },
        {
            "type": "equation",
            "bbox": [
                0.411,
                0.803,
                0.672,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n(\\nabla L _ {\\delta} (x)) _ {i} = \\left\\{ \\begin{array}{l l} \\operatorname {s i g n} (x _ {i}), & | x _ {i} | > \\delta , \\\\ \\frac {x _ {i}}{\\delta}, & | x _ {i} | \\leqslant \\delta . \\end{array} \\right.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.2 梯度类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "233"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.155,
                0.744,
                0.228
            ],
            "angle": 0,
            "content": "显然 \\(f_{\\delta}(x)\\) 的梯度是利普希茨连续的，且相应常数为 \\(L = \\| A^{\\mathrm{T}}A\\| _2 + \\frac{\\mu}{\\delta}\\) 根据定理6.3，固定步长需不超过 \\(\\frac{1}{L}\\) 才能保证算法收敛，如果 \\(\\delta\\) 过小，那么我们需要选取充分小的步长 \\(\\alpha_{k}\\) 使得梯度法收敛"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.239,
                0.744,
                0.278
            ],
            "angle": 0,
            "content": "图6.7和图6.8展示了光滑化LASSO问题的求解结果．在MATLAB环境中，我们用如下方式生成 \\(A,b\\)"
        },
        {
            "type": "code",
            "bbox": [
                0.159,
                0.301,
                0.414,
                0.387
            ],
            "angle": 0,
            "content": "1 m = 512; n = 1024;  \n2 A = randn(m, n);  \n3 u = sprandn(n, 1, r);  \n4 b = A * u;"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.427,
                0.741,
                0.466
            ],
            "angle": 0,
            "content": "其中 \\(r\\) 用来控制真解 \\(u\\) 的稀疏度（\\(u\\) 中非零元个数与总元素个数的比值为 \\(r\\)）。这里取稀疏度 \\(r = 0.1\\)，正则化参数 \\(\\mu = 10^{-3}\\)。只要"
        },
        {
            "type": "equation",
            "bbox": [
                0.261,
                0.498,
                0.647,
                0.518
            ],
            "angle": 0,
            "content": "\\[\n| f _ {\\delta} (x ^ {k}) - f _ {\\delta} (x ^ {k - 1}) | <   1 0 ^ {- 8}, \\text {或 者} \\| \\nabla f _ {\\delta} (x) \\| <   1 0 ^ {- 6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.55,
                0.741,
                0.63
            ],
            "angle": 0,
            "content": "或者最大迭代步数达到 3000, 则算法停止. 为了加快算法的收敛速度, 可以采用连续化策略来从较大的正则化参数 \\(\\mu_0\\) 逐渐减小到 \\(\\mu\\). 具体地, 对于每一个 \\(\\mu_t\\), 我们调用带 BB 步长的光滑化梯度法 (这里光滑化参数 \\(\\delta_t = 10^{-3} \\mu_t\\)) 来求解对应的子问题. 每个子问题的终止条件设为"
        },
        {
            "type": "equation",
            "bbox": [
                0.25,
                0.661,
                0.658,
                0.681
            ],
            "angle": 0,
            "content": "\\[\n| f _ {\\delta} (x ^ {k}) - f _ {\\delta} (x ^ {k - 1}) | <   1 0 ^ {- 4 - t}, \\text {或 者} \\| \\nabla f _ {\\delta} (x) \\| <   1 0 ^ {- 1 - t}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.714,
                0.424,
                0.731
            ],
            "angle": 0,
            "content": "当 \\(\\mu_t\\) 的子问题求解完之后，设置"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.765,
                0.737,
                0.782
            ],
            "angle": 0,
            "content": "\\[\n\\mu_ {t + 1} = \\max  \\left\\{\\mu_ {t} \\eta , \\mu \\right\\}, \\tag {6.2.13}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.738,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(\\eta\\) 为缩小因子，这里取为0.1。第7.1.4小节将给出连续化策略合理性的解释。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "234"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "image",
            "bbox": [
                0.315,
                0.154,
                0.77,
                0.43
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.382,
                0.441,
                0.7,
                0.459
            ],
            "angle": 0,
            "content": "图6.7 光滑化LASSO问题求解迭代过程"
        },
        {
            "type": "image",
            "bbox": [
                0.263,
                0.531,
                0.538,
                0.706
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.371,
                0.714,
                0.431,
                0.727
            ],
            "angle": 0,
            "content": "(a) 精确解"
        },
        {
            "type": "image",
            "bbox": [
                0.547,
                0.532,
                0.822,
                0.706
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.648,
                0.714,
                0.721,
                0.727
            ],
            "angle": 0,
            "content": "(b) 梯度法解"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.4,
                0.746,
                0.684,
                0.763
            ],
            "angle": 0,
            "content": "图6.8 光滑化LASSO问题求解结果"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "可以看到，在连续化策略的帮助下，光滑化梯度法在400步左右收敛到LASSO问题的解."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.2 梯度类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "235"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.157,
                0.407,
                0.174
            ],
            "angle": 0,
            "content": "2. Tikhonov正则化模型求解"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.188,
                0.739,
                0.248
            ],
            "angle": 0,
            "content": "Tikhonov 正则化方法是 Tikhonov 等人在 1977 年提出的求解病态问题的方法 [180]. 在这里我们讨论其在图像处理问题上的应用. 假设用 \\( x \\) 来表示真实图像, 它是一个 \\( m \\times n \\) 点阵, 用 \\( y \\) 来表示带噪声的图像, 即"
        },
        {
            "type": "equation",
            "bbox": [
                0.414,
                0.266,
                0.495,
                0.282
            ],
            "angle": 0,
            "content": "\\[\ny = x + e,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.298,
                0.737,
                0.336
            ],
            "angle": 0,
            "content": "其中 \\( e \\) 是高斯白噪声。为了从有噪声的图像 \\( y \\) 中还原出原始图像 \\( x \\)，利用Tikhonov正则化的思想可以建立如下模型："
        },
        {
            "type": "equation",
            "bbox": [
                0.24,
                0.345,
                0.738,
                0.378
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} f (x) = \\frac {1}{2} \\| x - y \\| _ {F} ^ {2} + \\lambda \\left(\\| D _ {1} x \\| _ {F} ^ {2} + \\| D _ {2} x \\| _ {F} ^ {2}\\right), \\tag {6.2.14}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.388,
                0.698,
                0.406
            ],
            "angle": 0,
            "content": "其中 \\(D_{1}x, D_{2}x\\) 分别表示对 \\(x\\) 在水平方向和竖直方向上做向前差分，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.261,
                0.415,
                0.649,
                0.448
            ],
            "angle": 0,
            "content": "\\[\n\\left(D _ {1} x\\right) _ {i j} = \\frac {1}{h} (x _ {i + 1, j} - x _ {i j}), \\quad \\left(D _ {2} x\\right) _ {i j} = \\frac {1}{h} (x _ {i, j + 1} - x _ {i j}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.458,
                0.739,
                0.578
            ],
            "angle": 0,
            "content": "其中 \\(h\\) 为给定的离散间隔．模型(6.2.14)由两项组成：第一项为保真项，即要求真实图像 \\(x\\) 和带噪声的图像 \\(y\\) 不要相差太大，这里使用 \\(F\\) 范数的原因是我们假设噪声是高斯白噪声；第二项为Tikhonov正则项，它实际上是对 \\(x\\) 本身的性质做出限制，在这里的含义是希望原始图像 \\(x\\) 各个部分的变化不要太剧烈（即水平和竖直方向上差分的平方和不要太大），这种正则项会使得恢复出的 \\(x\\) 有比较好的光滑性."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.583,
                0.738,
                0.621
            ],
            "angle": 0,
            "content": "模型(6.2.14)的目标函数是光滑的，因此可以利用梯度法来求解。容易求出 \\( f(x) \\) 的梯度"
        },
        {
            "type": "equation",
            "bbox": [
                0.362,
                0.626,
                0.547,
                0.644
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x) = x - y - 2 \\lambda \\Delta x,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.655,
                0.49,
                0.672
            ],
            "angle": 0,
            "content": "其中 \\(\\Delta\\) 是图像 \\(x\\) 的离散拉普拉斯算子，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.288,
                0.683,
                0.62,
                0.715
            ],
            "angle": 0,
            "content": "\\[\n(\\Delta x) _ {i j} = \\frac {x _ {i + 1 , j} + x _ {i - 1 , j} + x _ {i , j + 1} + x _ {i , j - 1} - 4 x _ {i j}}{h ^ {2}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.727,
                0.367,
                0.743
            ],
            "angle": 0,
            "content": "因此梯度法的迭代格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.334,
                0.759,
                0.573,
                0.779
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - t (I - 2 \\lambda \\Delta) x ^ {k} + t y ^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.795,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "我们注意到离散拉普拉斯算子是一个线性算子，因此求解模型(6.2.14)本质上是求解一个线性方程组．由于该线性方程组的系数矩阵不方便写出，所以可以使用梯度法求解．实际上该梯度法和Landweber迭代[113]是等价的."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "236"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "image",
            "bbox": [
                0.283,
                0.162,
                0.538,
                0.382
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.386,
                0.391,
                0.434,
                0.404
            ],
            "angle": 0,
            "content": "(a) 原图"
        },
        {
            "type": "image",
            "bbox": [
                0.547,
                0.162,
                0.804,
                0.381
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.639,
                0.391,
                0.712,
                0.404
            ],
            "angle": 0,
            "content": "(b) 高斯噪声"
        },
        {
            "type": "image",
            "bbox": [
                0.283,
                0.415,
                0.538,
                0.635
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.379,
                0.646,
                0.441,
                0.657
            ],
            "angle": 0,
            "content": "(c) \\(\\lambda = 0.5\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.547,
                0.415,
                0.804,
                0.635
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.649,
                0.646,
                0.704,
                0.658
            ],
            "angle": 0,
            "content": "(d) \\(\\lambda = 2\\)"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.393,
                0.677,
                0.692,
                0.694
            ],
            "angle": 0,
            "content": "图6.9 Tikhonov正则化模型求解结果"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.754,
                0.828,
                0.853
            ],
            "angle": 0,
            "content": "图6.9展示了模型(6.2.14)的求解结果，可以看到Tikhonov正则化模型可以有效地去除图像中的噪声，但它的缺点也很明显：经过Tikhonov正则化处理的图像会偏光滑，原图中物体之间的边界变得很模糊，当 \\(\\lambda\\) 较大时尤为明显。出现这一现象的原因是模型(6.2.14)中正则项选取不当，在后面的章节中我们将会考虑选取其他正则项的求解。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.3 次梯度算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "237"
        },
        {
            "type": "title",
            "bbox": [
                0.361,
                0.155,
                0.548,
                0.177
            ],
            "angle": 0,
            "content": "6.3 次梯度算法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.194,
                0.741,
                0.273
            ],
            "angle": 0,
            "content": "上一节讨论了梯度下降法，使用该方法的前提为目标函数 \\( f(x) \\) 是一阶可微的。在实际应用中经常会遇到不可微的函数，对于这类函数我们无法在每个点处求出梯度，但往往它们的最优值都是在不可微点处取到的。为了能处理这种情形，这一节介绍次梯度算法。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.298,
                0.373,
                0.317
            ],
            "angle": 0,
            "content": "6.3.1 次梯度算法结构"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.329,
                0.739,
                0.388
            ],
            "angle": 0,
            "content": "现在我们在问题(6.0.1)中假设 \\(f(x)\\) 为凸函数，但不一定可微．对凸函数可以在定义域的内点处定义次梯度 \\(g \\in \\partial f(x)\\)．类比梯度法的构造，我们有如下次梯度算法的迭代格式："
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.396,
                0.738,
                0.417
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\alpha_ {k} g ^ {k}, \\quad g ^ {k} \\in \\partial f (x ^ {k}), \\tag {6.3.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.427,
                0.513,
                0.445
            ],
            "angle": 0,
            "content": "其中 \\(\\alpha_{k} > 0\\) 为步长. 它通常有如下四种选择:"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.454,
                0.345,
                0.47
            ],
            "angle": 0,
            "content": "(1) 固定步长 \\(\\alpha_{k} = \\alpha\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.481,
                0.493,
                0.5
            ],
            "angle": 0,
            "content": "(2) 固定 \\(\\| x^{k + 1} - x^k\\|\\)，即 \\(\\alpha_{k}\\| g^{k}\\|\\) 为常数；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.507,
                0.462,
                0.543
            ],
            "angle": 0,
            "content": "(3) 消失步长 \\(\\alpha_{k} \\rightarrow 0\\) 且 \\(\\sum_{k=0}^{\\infty} \\alpha_{k} = +\\infty\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.55,
                0.468,
                0.567
            ],
            "angle": 0,
            "content": "(4) 选取 \\(\\alpha_{k}\\) 使其满足某种线搜索准则"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.454,
                0.493,
                0.567
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.578,
                0.74,
                0.761
            ],
            "angle": 0,
            "content": "次梯度算法(6.3.1)的构造虽然是受梯度法(6.2.1)的启发，但在很多方面次梯度算法有其独特性质。首先，我们知道次微分 \\(\\partial f(x)\\) 是一个集合，在次梯度算法的构造中只要求从这个集合中选出一个次梯度即可，但在实际中不同的次梯度取法可能会产生截然不同的效果；其次，对于梯度法，判断一阶最优性条件只需要验证 \\(\\|\\nabla f(x^{*})\\|\\) 是否充分小即可，但对于次梯度算法，根据第五章的定理5.5，有 \\(0 \\in \\partial f(x^{*})\\) ，而这个条件在实际应用中往往是不易直接验证的，这导致我们不能使用定理5.5作为次梯度算法的停机条件；此外，步长选取在次梯度法中的影响非常大，下一小节将讨论在不同步长取法下次梯度算法的收敛性质。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.785,
                0.334,
                0.803
            ],
            "angle": 0,
            "content": "6.3.2 收敛性分析"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.738,
                0.854
            ],
            "angle": 0,
            "content": "本小节讨论次梯度算法的收敛性．首先我们列出 \\(f(x)\\) 所要满足的基本假设."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "238"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.291,
                0.157,
                0.724,
                0.174
            ],
            "angle": 0,
            "content": "假设6.1 对无约束优化问题(6.0.1)，目标函数 \\( f(x) \\) 满足："
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.186,
                0.394,
                0.202
            ],
            "angle": 0,
            "content": "(1) \\(f\\) 为凸函数；"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.216,
                0.701,
                0.233
            ],
            "angle": 0,
            "content": "(2) \\(f\\) 至少存在一个有限的极小值点 \\(x^{*}\\), 且 \\(f(x^{*}) > -\\infty\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.246,
                0.49,
                0.263
            ],
            "angle": 0,
            "content": "(3) \\(f\\) 为利普希茨连续的, 即"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.186,
                0.701,
                0.263
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.409,
                0.277,
                0.715,
                0.295
            ],
            "angle": 0,
            "content": "\\[\n| f (x) - f (y) | \\leqslant G \\| x - y \\|, \\quad \\forall x, y \\in \\mathbb {R} ^ {n},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.309,
                0.515,
                0.325
            ],
            "angle": 0,
            "content": "其中 \\(G > 0\\) 为利普希茨常数"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.338,
                0.825,
                0.376
            ],
            "angle": 0,
            "content": "对于次梯度算法，我们假设 \\( f(x) \\) 本身是利普希茨连续的，这等价于 \\( f(x) \\) 的次梯度有界．实际上有如下引理："
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.388,
                0.825,
                0.425
            ],
            "angle": 0,
            "content": "引理6.2 设 \\(f(x)\\) 为凸函数，则 \\(f(x)\\) 是 \\(G\\)-利普希茨连续的当且仅当 \\(f(x)\\) 的次梯度是有界的，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.422,
                0.439,
                0.662,
                0.459
            ],
            "angle": 0,
            "content": "\\[\n\\| g \\| \\leqslant G, \\quad \\forall g \\in \\partial f (x), x \\in \\mathbb {R} ^ {n}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.471,
                0.831,
                0.51
            ],
            "angle": 0,
            "content": "证明. 先证充分性. 假设对任意次梯度 \\(g\\) 都有 \\(\\| g\\| \\leqslant G\\) ，选取 \\(g_{y}\\in \\partial f(y),g_{x}\\in\\) \\(\\partial f(x)\\) ，由次梯度的定义不难得出"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.522,
                0.684,
                0.543
            ],
            "angle": 0,
            "content": "\\[\ng _ {x} ^ {\\mathrm {T}} (x - y) \\geqslant f (x) - f (y) \\geqslant g _ {y} ^ {\\mathrm {T}} (x - y).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.555,
                0.391,
                0.571
            ],
            "angle": 0,
            "content": "再由柯西不等式，"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.582,
                0.672,
                0.601
            ],
            "angle": 0,
            "content": "\\[\ng _ {x} ^ {\\mathrm {T}} (x - y) \\leqslant \\| g _ {x} \\| \\| x - y \\| \\leqslant G \\| x - y \\|,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.605,
                0.7,
                0.628
            ],
            "angle": 0,
            "content": "\\[\ng _ {y} ^ {\\mathrm {T}} (x - y) \\geqslant - \\| g _ {y} \\| \\| x - y \\| \\geqslant - G \\| x - y \\|.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.637,
                0.471,
                0.653
            ],
            "angle": 0,
            "content": "结合上面两个不等式最终有"
        },
        {
            "type": "equation",
            "bbox": [
                0.44,
                0.668,
                0.644,
                0.687
            ],
            "angle": 0,
            "content": "\\[\n| f (x) - f (y) | \\leqslant G \\| x - y \\|.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.699,
                0.825,
                0.748
            ],
            "angle": 0,
            "content": "再证必要性. 设 \\(f(x)\\) 是 \\(G\\)-利普希茨连续的，反设存在 \\(x\\) 和 \\(g \\in \\partial f(x)\\) 使得 \\(\\| g \\| > G\\)，取 \\(y = x + \\frac{g}{\\|g\\|}\\)，则根据次梯度的定义，"
        },
        {
            "type": "equation",
            "bbox": [
                0.447,
                0.757,
                0.635,
                0.825
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (y) \\geqslant f (x) + g ^ {\\mathrm {T}} (y - x) \\\\ = f (x) + \\| g \\| \\\\ > f (x) + G, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.836,
                0.677,
                0.853
            ],
            "angle": 0,
            "content": "这与 \\(f(x)\\) 是 \\(G\\) -利普希茨连续的矛盾，因此必要性成立"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.838,
                0.826,
                0.851
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.3 次梯度算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "239"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.157,
                0.36,
                0.174
            ],
            "angle": 0,
            "content": "1. 不同步长下的收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.187,
                0.739,
                0.247
            ],
            "angle": 0,
            "content": "对于次梯度算法，一个重要的观察就是它并不是一个下降方法，即无法保证 \\( f(x^{k + 1}) < f(x^k) \\)，这给收敛性的证明带来了困难。不过我们可以分析 \\( f(x) \\) 历史迭代的最优点所满足的性质，实际上有如下定理。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.257,
                0.739,
                0.296
            ],
            "angle": 0,
            "content": "定理6.5（次梯度算法的收敛性）在假设6.1的条件下，设 \\(\\{\\alpha_{k} > 0\\}\\) 为任意步长序列，\\(\\{x^{k}\\}\\) 是由算法(6.3.1)产生的迭代序列，则对任意的 \\(k \\geqslant 0\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.287,
                0.303,
                0.738,
                0.348
            ],
            "angle": 0,
            "content": "\\[\n2 \\left(\\sum_ {i = 0} ^ {k} \\alpha_ {i}\\right) \\left(\\hat {f} ^ {k} - f ^ {*}\\right) \\leqslant \\left\\| x ^ {0} - x ^ {*} \\right\\| ^ {2} + \\sum_ {i = 0} ^ {k} \\alpha_ {i} ^ {2} G ^ {2}, \\tag {6.3.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.356,
                0.737,
                0.393
            ],
            "angle": 0,
            "content": "其中 \\(x^{*}\\) 是 \\(f(x)\\) 的一个全局极小值点，\\(f^{*} = f(x^{*})\\)，\\(\\hat{f}^{k}\\) 为前 \\(k\\) 次迭代 \\(f(x)\\) 的最小值，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.392,
                0.396,
                0.515,
                0.422
            ],
            "angle": 0,
            "content": "\\[\n\\hat {f} ^ {k} = \\min  _ {0 \\leqslant i \\leqslant k} f (x ^ {i}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.431,
                0.737,
                0.469
            ],
            "angle": 0,
            "content": "证明．该证明的关键是估计迭代点 \\(x^{k}\\) 与最小值点 \\(x^{*}\\) 之间的距离满足的关系．根据迭代格式(6.3.1)，"
        },
        {
            "type": "equation",
            "bbox": [
                0.23,
                0.479,
                0.737,
                0.548
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| x ^ {i + 1} - x ^ {*} \\right\\| ^ {2} = \\left\\| x ^ {i} - \\alpha_ {i} g ^ {i} - x ^ {*} \\right\\| ^ {2} \\\\ = \\left\\| x ^ {i} - x ^ {*} \\right\\| ^ {2} - 2 \\alpha_ {i} \\left\\langle g ^ {i}, x ^ {i} - x ^ {*} \\right\\rangle + \\alpha_ {i} ^ {2} \\| g ^ {i} \\| ^ {2} \\tag {6.3.3} \\\\ \\leqslant \\left\\| x ^ {i} - x ^ {*} \\right\\| ^ {2} - 2 \\alpha_ {i} \\left(f \\left(x ^ {i}\\right) - f ^ {*}\\right) + \\alpha_ {i} ^ {2} G ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.559,
                0.737,
                0.596
            ],
            "angle": 0,
            "content": "这里最后一个不等式是根据次梯度的定义和 \\(\\| g^{i}\\| \\leqslant G\\)。将(6.3.3)式移项，等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.235,
                0.609,
                0.738,
                0.629
            ],
            "angle": 0,
            "content": "\\[\n2 \\alpha_ {i} \\left(f \\left(x ^ {i}\\right) - f ^ {*}\\right) \\leqslant \\left\\| x ^ {i} - x ^ {*} \\right\\| ^ {2} - \\left\\| x ^ {i + 1} - x ^ {*} \\right\\| ^ {2} + \\alpha_ {i} ^ {2} G ^ {2}. \\tag {6.3.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.642,
                0.497,
                0.659
            ],
            "angle": 0,
            "content": "对(6.3.4)式两边关于 \\(i\\) 求和（从0到 \\(k\\)），有"
        },
        {
            "type": "equation",
            "bbox": [
                0.235,
                0.667,
                0.672,
                0.749
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} 2 \\sum_ {i = 0} ^ {k} \\alpha_ {i} \\left(f \\left(x ^ {i}\\right) - f ^ {*}\\right) \\leqslant \\left\\| x ^ {0} - x ^ {*} \\right\\| ^ {2} - \\left\\| x ^ {k + 1} - x ^ {*} \\right\\| ^ {2} + G ^ {2} \\sum_ {i = 0} ^ {k} \\alpha_ {i} ^ {2} \\\\ \\leqslant \\left\\| x ^ {0} - x ^ {*} \\right\\| ^ {2} + G ^ {2} \\sum_ {i = 0} ^ {k} \\alpha_ {i} ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.758,
                0.355,
                0.777
            ],
            "angle": 0,
            "content": "根据 \\(\\hat{f}^k\\) 的定义容易得出"
        },
        {
            "type": "equation",
            "bbox": [
                0.307,
                0.785,
                0.6,
                0.829
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i = 0} ^ {k} \\alpha_ {i} (f (x ^ {i}) - f ^ {*}) \\geqslant \\left(\\sum_ {i = 0} ^ {k} \\alpha_ {i}\\right) (\\hat {f} ^ {k} - f ^ {*}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.416,
                0.853
            ],
            "angle": 0,
            "content": "结合以上两式可得到结论(6.3.2)."
        },
        {
            "type": "image",
            "bbox": [
                0.72,
                0.838,
                0.738,
                0.851
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "240"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.236
            ],
            "angle": 0,
            "content": "定理 6.5 揭示了次梯度算法的一些关键性质：次梯度算法的收敛性非常依赖于步长的选取；次梯度算法是非单调算法，可以配套非单调线搜索准则(6.1.5)和(6.1.6)一起使用。根据定理 6.5 可以直接得到不同步长取法下次梯度算法的收敛性，证明留给读者完成。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.248,
                0.825,
                0.286
            ],
            "angle": 0,
            "content": "推论6.2 在假设6.1的条件下，次梯度算法的收敛性满足（\\(\\hat{f}^k\\) 的定义和定理6.5中的定义相同）："
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.298,
                0.492,
                0.314
            ],
            "angle": 0,
            "content": "(1) 取 \\(\\alpha_{i} = t\\) 为固定步长，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.451,
                0.322,
                0.673,
                0.357
            ],
            "angle": 0,
            "content": "\\[\n\\hat {f} ^ {k} - f ^ {*} \\leqslant \\frac {\\| x ^ {0} - x ^ {*} \\| ^ {2}}{2 k t} + \\frac {G ^ {2} t}{2};\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.369,
                0.707,
                0.386
            ],
            "angle": 0,
            "content": "(2) 取 \\(\\alpha_{i}\\) 使得 \\(\\| x^{i + 1} - x^i\\|\\) 固定，即 \\(\\alpha_{i}\\| g^{i}\\| = s\\) 为常数，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.447,
                0.395,
                0.677,
                0.43
            ],
            "angle": 0,
            "content": "\\[\n\\hat {f} ^ {k} - f ^ {*} \\leqslant \\frac {G \\| x ^ {0} - x ^ {*} \\| ^ {2}}{2 k s} + \\frac {G s}{2};\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.442,
                0.662,
                0.479
            ],
            "angle": 0,
            "content": "(3) 取 \\(\\alpha_{i}\\) 为消失步长，即 \\(\\alpha_{i} \\rightarrow 0\\) 且 \\(\\sum_{i=0}^{\\infty} \\alpha_{i} = +\\infty\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.437,
                0.489,
                0.687,
                0.569
            ],
            "angle": 0,
            "content": "\\[\n\\hat {f} ^ {k} - f ^ {*} \\leqslant \\frac {\\| x ^ {0} - x ^ {*} \\| ^ {2} + G ^ {2} \\sum_ {i = 0} ^ {k} \\alpha_ {i} ^ {2}}{2 \\sum_ {i = 0} ^ {k} \\alpha_ {i}};\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.579,
                0.492,
                0.597
            ],
            "angle": 0,
            "content": "进一步可得 \\(\\hat{f}^k\\) 收敛到 \\(f^{*}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.607,
                0.825,
                0.71
            ],
            "angle": 0,
            "content": "从推论6.2可以看到，无论是固定步长还是固定 \\(\\| x^{k + 1} - x^k\\|\\) ，次梯度算法均没有收敛性，只能收敛到一个次优的解，这和梯度法的结论有很大的不同；只有当 \\(\\alpha_{k}\\) 取消失步长时 \\(\\hat{f}^k\\) 才具有收敛性．一个常用的取法是 \\(\\alpha_{k} = \\frac{1}{k}\\) 这样不但可以保证其为消失步长，还可以保证 \\(\\sum_{i = 0}^{\\infty}\\alpha_{i}^{2}\\) 有界."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.728,
                0.465,
                0.745
            ],
            "angle": 0,
            "content": "2. 收敛速度和步长的关系"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.758,
                0.825,
                0.817
            ],
            "angle": 0,
            "content": "在推论6.2中，通过适当选取步长 \\(\\alpha_{i}\\) 可以获得对应次梯度算法的收敛速度．在这里我们假设 \\(\\| x^0 -x^*\\| \\leqslant R\\) ，即初值和最优解之间的距离有上界.假设总迭代步数 \\(k\\) 是给定的，根据推论6.2的第一个结论，"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.823,
                0.701,
                0.858
            ],
            "angle": 0,
            "content": "\\[\n\\hat {f} ^ {k} - f ^ {*} \\leqslant \\frac {\\| x ^ {0} - x ^ {*} \\| ^ {2}}{2 k t} + \\frac {G ^ {2} t}{2} \\leqslant \\frac {R ^ {2}}{2 k t} + \\frac {G ^ {2} t}{2}.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.3 次梯度算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.737,
                0.132
            ],
            "angle": 0,
            "content": "241"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.519,
                0.175
            ],
            "angle": 0,
            "content": "在固定步长下，由平均值不等式得知当 \\(t\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.355,
                0.187,
                0.554,
                0.226
            ],
            "angle": 0,
            "content": "\\[\n\\frac {R ^ {2}}{2 k t} = \\frac {G ^ {2} t}{2}, \\quad \\text {即}   t = \\frac {R}{G \\sqrt {k}}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.24,
                0.296,
                0.258
            ],
            "angle": 0,
            "content": "时，我们有估计"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.26,
                0.513,
                0.297
            ],
            "angle": 0,
            "content": "\\[\n\\hat {f} ^ {k} - f ^ {*} \\leqslant \\frac {G R}{\\sqrt {k}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.309,
                0.739,
                0.417
            ],
            "angle": 0,
            "content": "以上分析表明要使得目标函数值达到 \\(\\varepsilon\\) 的精度，即 \\(\\hat{f}^k - f^* \\leqslant \\varepsilon\\)，必须取迭代步数 \\(k = \\mathcal{O}\\left(\\frac{1}{\\varepsilon^2}\\right)\\) 且固定步长 \\(\\alpha_k\\) 要满足 \\(t = \\mathcal{O}\\left(\\frac{1}{\\sqrt{k}}\\right)\\)。注意这里的固定步长依赖于最大迭代步数，这和之前构造梯度法的步长是不太一样的。从上面的取法中还可以看出对于满足假设6.1的函数 \\(f\\)，最大迭代步数可以作为判定迭代点是否最优的一个终止准则。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.422,
                0.737,
                0.473
            ],
            "angle": 0,
            "content": "类似地，根据推论6.2的第二个结论以及平均值不等式，在固定 \\(\\| x^{i + 1} - x^i\\|\\) 的条件下可以取 \\(s = \\frac{R}{\\sqrt{k}}\\) ，同样会得到估计"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.487,
                0.513,
                0.523
            ],
            "angle": 0,
            "content": "\\[\n\\hat {f} ^ {k} - f ^ {*} \\leqslant \\frac {G R}{\\sqrt {k}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.539,
                0.741,
                0.596
            ],
            "angle": 0,
            "content": "如果我们知道 \\( f(x) \\) 的更多信息，则可以利用这些信息来选取步长。例如在某些应用中可预先知道 \\( f^* \\) 的值（但不知道最小值点），根据(6.3.3)式，当"
        },
        {
            "type": "equation",
            "bbox": [
                0.392,
                0.601,
                0.515,
                0.638
            ],
            "angle": 0,
            "content": "\\[\n\\alpha_ {i} = \\frac {f (x ^ {i}) - f ^ {*}}{\\| g ^ {i} \\| ^ {2}}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.649,
                0.454,
                0.667
            ],
            "angle": 0,
            "content": "时，不等式右侧达到极小，这等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.294,
                0.68,
                0.615,
                0.719
            ],
            "angle": 0,
            "content": "\\[\n\\frac {(f (x ^ {i}) - f ^ {*}) ^ {2}}{\\| g ^ {i} \\| ^ {2}} \\leqslant \\| x ^ {i} - x ^ {*} \\| ^ {2} - \\| x ^ {i + 1} - x ^ {*} \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.733,
                0.638,
                0.752
            ],
            "angle": 0,
            "content": "递归地利用上式并结合 \\(\\| x^0 -x^*\\| \\leqslant R\\) 和 \\(\\| g^i\\| \\leqslant G\\) ，可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.766,
                0.513,
                0.802
            ],
            "angle": 0,
            "content": "\\[\n\\hat {f} ^ {k} - f ^ {*} \\leqslant \\frac {G R}{\\sqrt {k}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "注意，此时步长的选取已经和最大迭代数无关，它仅仅依赖于当前点处的函数值与最优值的差和次梯度模长."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.291,
                0.13
            ],
            "angle": 0,
            "content": "242"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.156,
                0.403,
                0.175
            ],
            "angle": 0,
            "content": "6.3.3 应用举例"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.188,
                0.427,
                0.205
            ],
            "angle": 0,
            "content": "1. LASSO 问题求解"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.22,
                0.432,
                0.236
            ],
            "angle": 0,
            "content": "考虑 LASSO 问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.246,
                0.826,
                0.279
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad f (x) = \\frac {1}{2} \\| A x - b \\| ^ {2} + \\mu \\| x \\| _ {1}, \\tag {6.3.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.29,
                0.496,
                0.306
            ],
            "angle": 0,
            "content": "容易得知 \\(f(x)\\) 的一个次梯度为"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.322,
                0.652,
                0.343
            ],
            "angle": 0,
            "content": "\\[\ng = A ^ {\\mathrm {T}} (A x - b) + \\mu \\mathrm {s i g n} (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.358,
                0.825,
                0.395
            ],
            "angle": 0,
            "content": "其中 \\(\\operatorname{sign}(x)\\) 是关于 \\(x\\) 逐分量的符号函数。因此 LASSO 问题的次梯度算法为"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.4,
                0.701,
                0.42
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\alpha_ {k} \\left(A ^ {\\mathrm {T}} \\left(A x ^ {k} - b\\right) + \\mu \\operatorname {s i g n} \\left(x ^ {k}\\right)\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.433,
                0.534,
                0.449
            ],
            "angle": 0,
            "content": "步长 \\(\\alpha_{k}\\) 可选为固定步长或消失步长"
        },
        {
            "type": "image",
            "bbox": [
                0.262,
                0.469,
                0.523,
                0.624
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.319,
                0.632,
                0.466,
                0.646
            ],
            "angle": 0,
            "content": "(a) \\(f(x^{k}) - f^{*}\\) 的相对变化"
        },
        {
            "type": "image",
            "bbox": [
                0.564,
                0.467,
                0.822,
                0.623
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.628,
                0.631,
                0.756,
                0.646
            ],
            "angle": 0,
            "content": "(b) \\(\\hat{f}^k - f^*\\) 的相对变化"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.378,
                0.665,
                0.706,
                0.682
            ],
            "angle": 0,
            "content": "图6.10 次梯度算法求解LASSO问题结果"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.706,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "图6.10展示了使用不同步长的次梯度算法求解 LASSO 问题的迭代过程,其中测试数据的生成方式和第6.2节中一致, 正则化参数 \\(\\mu = 1\\). 我们分别选取固定步长 \\(\\alpha_{k} = 0.0005, 0.0002, 0.0001\\) 以及消失步长 \\(\\alpha_{k} = \\frac{0.002}{\\sqrt{k}}\\) 来测试求解效果. 从图6.10可以看出, 在3000步迭代内, 使用不同的固定步长最终会到达次优解, 函数值下降到一定程度便稳定在某个值附近; 而使用消失步长算法最终将会收敛. 从图中还可以看出次梯度算法本身是非单调方法, 因此在求解过程中我们需要记录历史中最好的一次迭代来作为算法的最终输出."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.3 次梯度算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "243"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.739,
                0.228
            ],
            "angle": 0,
            "content": "对于 \\(\\mu = 10^{-2}, 10^{-3}\\), 我们采用连续化次梯度算法进行求解. 停机准则和参数 \\(\\mu\\) 的连续化设置与第6.2节中的光滑化梯度法一致, 且若 \\(\\mu_{t} > \\mu\\), 则取固定步长 \\(\\frac{1}{\\lambda_{\\max}(A^{\\mathrm{T}}A)}\\); 若 \\(\\mu_{t} = \\mu\\), 则取步长"
        },
        {
            "type": "equation",
            "bbox": [
                0.328,
                0.234,
                0.581,
                0.268
            ],
            "angle": 0,
            "content": "\\[\n\\frac {1}{\\lambda_ {\\max} \\left(A ^ {\\mathrm {T}} A\\right) \\cdot \\left(\\max  \\{k , 1 0 0 \\} - 9 9\\right)},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.275,
                0.741,
                0.292
            ],
            "angle": 0,
            "content": "其中 \\(k\\) 为迭代步数．迭代收敛过程见图6.11．可以看到，次梯度算法在1000"
        },
        {
            "type": "image",
            "bbox": [
                0.268,
                0.302,
                0.641,
                0.52
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.249,
                0.532,
                0.658,
                0.549
            ],
            "angle": 0,
            "content": "图6.11 LASSO问题在不同正则化参数下的求解结果"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.57,
                0.541,
                0.587
            ],
            "angle": 0,
            "content": "步左右收敛，比上一节中的光滑化梯度法慢一些。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.61,
                0.345,
                0.627
            ],
            "angle": 0,
            "content": "2. 正定矩阵补全问题"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.64,
                0.692,
                0.657
            ],
            "angle": 0,
            "content": "正定矩阵补全问题是一种特殊的矩阵恢复问题，它的具体形式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.667,
                0.457,
                0.684
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\text {f i n d} \\quad X \\in S ^ {n}, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.357,
                0.687,
                0.737,
                0.706
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & X _ {i j} = M _ {i j}, (i, j) \\in \\Omega , \\end{array} \\tag {6.3.6}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.709,
                0.446,
                0.725
            ],
            "angle": 0,
            "content": "\\[\nX \\succeq 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.735,
                0.739,
                0.793
            ],
            "angle": 0,
            "content": "其中 \\(\\Omega\\) 是已经观测的分量位置集合. 问题(6.3.6)本质上是一个目标函数为常数的半定规划问题, 但由于其特殊性我们可以使用次梯度算法求解. 考虑两个集合"
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.805,
                0.574,
                0.825
            ],
            "angle": 0,
            "content": "\\[\nC _ {1} = \\left\\{X \\mid X _ {i j} = M _ {i j}, (i, j) \\in \\Omega \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.336,
                0.829,
                0.476,
                0.849
            ],
            "angle": 0,
            "content": "\\[\nC _ {2} = \\left\\{X \\mid X \\succeq 0 \\right\\},\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "244"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.825,
                0.174
            ],
            "angle": 0,
            "content": "求解问题(6.3.6)等价于寻找闭凸集 \\(C_1\\) 和 \\(C_2\\) 的交集．定义欧几里得距离函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.452,
                0.188,
                0.63,
                0.214
            ],
            "angle": 0,
            "content": "\\[\nd _ {j} (X) = \\inf  _ {Y \\in C _ {j}} \\| X - Y \\| _ {F},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.225,
                0.607,
                0.242
            ],
            "angle": 0,
            "content": "则可将这个问题转化为无约束非光滑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.41,
                0.256,
                0.826,
                0.275
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad f (X) = \\max  \\left\\{d _ {1} (X), d _ {2} (X) \\right\\}. \\tag {6.3.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.29,
                0.351,
                0.306
            ],
            "angle": 0,
            "content": "由定理2.23，"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.317,
                0.744,
                0.392
            ],
            "angle": 0,
            "content": "\\[\n\\partial f (X) = \\left\\{ \\begin{array}{l l} \\partial d _ {1} (X), & d _ {1} (X) > d _ {2} (X), \\\\ \\partial d _ {2} (X), & d _ {1} (X) <   d _ {2} (X), \\\\ \\mathbf {c o n v} (\\partial d _ {1} (X) \\cup \\partial d _ {2} (X)), & d _ {1} (X) = d _ {2} (X). \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.402,
                0.689,
                0.419
            ],
            "angle": 0,
            "content": "而又根据例2.19，我们可以求得距离函数的一个次梯度为"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.429,
                0.826,
                0.488
            ],
            "angle": 0,
            "content": "\\[\nG _ {j} = \\left\\{ \\begin{array}{l l} 0, & X \\in C _ {j}, \\\\ \\frac {1}{d _ {j} (X)} (X - \\mathcal {P} _ {C _ {j}} (X)), & X \\notin C _ {j}, \\end{array} \\right. \\tag {6.3.8}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.497,
                0.825,
                0.54
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{P}_{C_j}(X) = \\underset {Y\\in C_j}{\\arg \\min}\\| Y - X\\| _F\\) 为 \\(X\\) 到 \\(C_j\\) 的投影．对于集合 \\(C_1\\) ， \\(X\\) 在它上面的投影为"
        },
        {
            "type": "equation",
            "bbox": [
                0.422,
                0.541,
                0.826,
                0.592
            ],
            "angle": 0,
            "content": "\\[\n\\left(\\mathcal {P} _ {C _ {1}} (X)\\right) _ {i j} = \\left\\{ \\begin{array}{l l} M _ {i j}, & (i, j) \\in \\Omega , \\\\ X _ {i j}, & (i, j) \\notin \\Omega . \\end{array} \\right. \\tag {6.3.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.597,
                0.528,
                0.613
            ],
            "angle": 0,
            "content": "对于集合 \\(C_2\\)，\\(X\\) 在它上面的投影为"
        },
        {
            "type": "equation",
            "bbox": [
                0.435,
                0.623,
                0.826,
                0.659
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {P} _ {C _ {2}} (X) = \\sum_ {i = 1} ^ {n} \\max  \\left(0, \\lambda_ {i}\\right) q _ {i} q _ {i} ^ {\\mathrm {T}}, \\tag {6.3.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.669,
                0.827,
                0.729
            ],
            "angle": 0,
            "content": "其中 \\((\\lambda_{i},q_{i})\\) 是 \\(X\\) 的第 \\(i\\) 个特征对．在这里注意，为了比较 \\(d_{1}(X)\\) 和 \\(d_{2}(X)\\) 的大小关系，我们在计算次梯度时还是要将 \\(X\\) 到两个集合的投影分别求出，之后再选取距离较大的一个计算出次梯度，因此完整的次梯度计算过程为："
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.741,
                0.825,
                0.779
            ],
            "angle": 0,
            "content": "(1) 给定点 \\(X\\)，根据(6.3.9)式和(6.3.10)式计算出 \\(X\\) 到 \\(C_1\\) 和 \\(C_2\\) 的投影，分别记为 \\(P_1\\) 和 \\(P_2\\)；"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.791,
                0.643,
                0.81
            ],
            "angle": 0,
            "content": "(2) 比较 \\(d_{j}(X) = \\| X - P_{j}\\|_{F}, j = 1,2\\) ，较大者为 \\(\\hat{j}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.817,
                0.487,
                0.856
            ],
            "angle": 0,
            "content": "(3) 计算次梯度 \\(G = \\frac{X - P_{j}}{d_{j}^{s}(X)}\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.741,
                0.825,
                0.856
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.4 牛顿类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "245"
        },
        {
            "type": "title",
            "bbox": [
                0.361,
                0.155,
                0.548,
                0.177
            ],
            "angle": 0,
            "content": "6.4 牛顿类算法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.194,
                0.744,
                0.296
            ],
            "angle": 0,
            "content": "梯度法仅仅依赖函数值和梯度的信息（即一阶信息），如果函数 \\( f(x) \\) 充分光滑，则可以利用二阶导数信息构造下降方向 \\( d^{k} \\)。牛顿类算法就是利用二阶导数信息来构造迭代格式的算法。由于利用的信息变多，牛顿法的实际表现可以远好于梯度法，但是它对函数 \\( f(x) \\) 的要求也相应变高。本节首先介绍经典牛顿法的构造和性质，然后介绍一些修正的牛顿法和实际应用。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.318,
                0.334,
                0.337
            ],
            "angle": 0,
            "content": "6.4.1 经典牛顿法"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.349,
                0.731,
                0.369
            ],
            "angle": 0,
            "content": "对二次连续可微函数 \\(f(x)\\), 考虑 \\(f(x)\\) 在迭代点 \\(x^{k}\\) 处的二阶泰勒展开"
        },
        {
            "type": "equation",
            "bbox": [
                0.188,
                0.374,
                0.741,
                0.406
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k} + d ^ {k}\\right) = f \\left(x ^ {k}\\right) + \\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} d ^ {k} + \\frac {1}{2} \\left(d ^ {k}\\right) ^ {\\mathrm {T}} \\nabla^ {2} f \\left(x ^ {k}\\right) d ^ {k} + o \\left(\\| d ^ {k} \\| ^ {2}\\right). \\tag {6.4.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.413,
                0.746,
                0.452
            ],
            "angle": 0,
            "content": "我们的目的是根据这个二阶近似来选取合适的下降方向 \\(d^{k}\\). 如果忽略(6.4.1)式中的高阶项，并将等式右边看成关于 \\(d^{k}\\) 的函数求其稳定点，可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.364,
                0.462,
                0.739,
                0.483
            ],
            "angle": 0,
            "content": "\\[\n\\nabla^ {2} f (x ^ {k}) d ^ {k} = - \\nabla f (x ^ {k}). \\tag {6.4.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.492,
                0.741,
                0.552
            ],
            "angle": 0,
            "content": "方程(6.4.2)也被称为牛顿方程，容易得出当 \\(\\nabla^2 f(x^k)\\) 非奇异时，更新方向\\(d^{k} = -\\nabla^{2}f(x^{k})^{-1}\\nabla f(x^{k})\\) ．一般称满足方程(6.4.2)的 \\(d^{k}\\) 为牛顿方向．因此经典牛顿法的更新格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.336,
                0.562,
                0.739,
                0.583
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\nabla^ {2} f (x ^ {k}) ^ {- 1} \\nabla f (x ^ {k}). \\tag {6.4.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.593,
                0.741,
                0.632
            ],
            "angle": 0,
            "content": "注意，在格式(6.4.3)中，步长 \\(\\alpha_{k}\\) 恒为1，即可以不额外考虑步长的选取．我们也称步长为1的牛顿法为经典牛顿法."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.656,
                0.334,
                0.674
            ],
            "angle": 0,
            "content": "6.4.2 收敛性分析"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.687,
                0.711,
                0.705
            ],
            "angle": 0,
            "content": "经典牛顿法(6.4.3)有很好的局部收敛性质。实际上我们有如下定理："
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.715,
                0.739,
                0.773
            ],
            "angle": 0,
            "content": "定理6.6(经典牛顿法的收敛性）假设目标函数 \\(f\\) 是二阶连续可微的函数，且海瑟矩阵在最优值点 \\(x^{*}\\) 的一个邻域 \\(N_{\\delta}(x^{*})\\) 内是利普希茨连续的，即存在常数 \\(L > 0\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.264,
                0.785,
                0.646,
                0.806
            ],
            "angle": 0,
            "content": "\\[\n\\| \\nabla^ {2} f (x) - \\nabla^ {2} f (y) \\| \\leqslant L \\| x - y \\|, \\quad \\forall x, y \\in N _ {\\delta} (x ^ {*}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.815,
                0.755,
                0.854
            ],
            "angle": 0,
            "content": "如果函数 \\(f(x)\\) 在点 \\(x^{*}\\) 处满足 \\(\\nabla f(x^{*}) = 0,\\nabla^{2}f(x^{*})\\succ 0\\) ，则对于迭代法(6.4.3)有如下结论："
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "246"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.157,
                0.812,
                0.174
            ],
            "angle": 0,
            "content": "(1) 如果初始点离 \\(x^{*}\\) 足够近，则牛顿法产生的迭代点列 \\(\\left\\{x^{k}\\right\\}\\) 收敛到 \\(x^{*}\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.192,
                0.57,
                0.21
            ],
            "angle": 0,
            "content": "(2) \\(\\{x^k\\}\\) 收敛到 \\(x^{*}\\) 的速度是Q-二次的；"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.227,
                0.52,
                0.246
            ],
            "angle": 0,
            "content": "(3) \\(\\{\\| \\nabla f(x^k)\\| \\} Q\\)-二次收敛到0."
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.157,
                0.812,
                0.246
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.262,
                0.785,
                0.28
            ],
            "angle": 0,
            "content": "证明. 从牛顿法的定义(6.4.3)和最优值点 \\(x^{*}\\) 的性质 \\(\\nabla f(x^{*}) = 0\\) 可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.274,
                0.294,
                0.824,
                0.34
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} x ^ {k + 1} - x ^ {*} = x ^ {k} - \\nabla^ {2} f (x ^ {k}) ^ {- 1} \\nabla f (x ^ {k}) - x ^ {*} \\\\ = \\nabla^ {2} f (x ^ {k}) ^ {- 1} [ \\nabla^ {2} f (x ^ {k}) (x ^ {k} - x ^ {*}) - (\\nabla f (x ^ {k}) - \\nabla f (x ^ {*})) ]. \\tag {6.4.4} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.354,
                0.418,
                0.371
            ],
            "angle": 0,
            "content": "根据泰勒公式，可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.327,
                0.383,
                0.758,
                0.416
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x ^ {k}) - \\nabla f (x ^ {*}) = \\int_ {0} ^ {1} \\nabla^ {2} f (x ^ {k} + t (x ^ {*} - x ^ {k})) (x ^ {k} - x ^ {*}) d t,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.429,
                0.383,
                0.446
            ],
            "angle": 0,
            "content": "因此我们有估计"
        },
        {
            "type": "equation",
            "bbox": [
                0.32,
                0.461,
                0.825,
                0.62
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| \\nabla^ {2} f (x ^ {k}) \\left(x ^ {k} - x ^ {*}\\right) - \\left(\\nabla f (x ^ {k}) - \\nabla f (x ^ {*})\\right) \\right\\| \\\\ = \\left\\| \\int_ {0} ^ {1} \\left[ \\nabla^ {2} f \\left(x ^ {k} + t \\left(x ^ {*} - x ^ {k}\\right)\\right) - \\nabla^ {2} f \\left(x ^ {k}\\right) \\right] \\left(x ^ {k} - x ^ {*}\\right) d t \\right\\| \\\\ \\leqslant \\int_ {0} ^ {1} \\| \\nabla^ {2} f \\left(x ^ {k} + t \\left(x ^ {*} - x ^ {k}\\right)\\right) - \\nabla^ {2} f \\left(x ^ {k}\\right) \\| \\| x ^ {k} - x ^ {*} \\| \\mathrm {d} t \\tag {6.4.5} \\\\ \\leqslant \\| x ^ {k} - x ^ {*} \\| ^ {2} \\int_ {0} ^ {1} L t d t \\\\ = \\frac {L}{2} \\| x ^ {k} - x ^ {*} \\| ^ {2}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.633,
                0.827,
                0.693
            ],
            "angle": 0,
            "content": "其中第二个不等式是由于海瑟矩阵的局部利普希茨连续性．又因为 \\(\\nabla^2 f(x^*)\\) 是非奇异的且 \\(f\\) 二阶连续可微，因此存在 \\(r\\) ，使得对任意满足 \\(\\| x - x^{*}\\| \\leqslant r\\) 的点 \\(x\\) 均有 \\(\\| \\nabla^2 f(x)^{-1}\\| \\leqslant 2\\| \\nabla^2 f(x^*)^{-1}\\|\\) ，结合(6.4.4)式与(6.4.5)式可得："
        },
        {
            "type": "equation",
            "bbox": [
                0.305,
                0.707,
                0.825,
                0.776
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| x ^ {k + 1} - x ^ {*} \\right\\| \\\\ \\leqslant \\| \\nabla^ {2} f \\left(x ^ {k}\\right) ^ {- 1} \\| \\| \\nabla^ {2} f \\left(x ^ {k}\\right) \\left(x ^ {k} - x ^ {*}\\right) - \\left(\\nabla f \\left(x ^ {k}\\right) - \\nabla f \\left(x ^ {*}\\right)\\right) \\| \\tag {6.4.6} \\\\ \\leqslant L \\| \\nabla^ {2} f (x ^ {*}) ^ {- 1} \\| \\| x ^ {k} - x ^ {*} \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.791,
                0.443,
                0.809
            ],
            "angle": 0,
            "content": "因此，当初始点 \\(x^0\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.368,
                0.821,
                0.717,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| x ^ {0} - x ^ {*} \\right\\| \\leqslant \\min  \\left\\{\\delta , r, \\frac {1}{2 L \\| \\nabla^ {2} f (x ^ {*}) ^ {- 1} \\|} \\right\\} \\stackrel {\\text {d e f}} {=} \\hat {\\delta}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.4 牛顿类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "247"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.741,
                0.195
            ],
            "angle": 0,
            "content": "时，可保证迭代点列一直处于邻域 \\(N_{\\delta}(x^{*})\\) 中，因此 \\(\\{x^k\\}\\) Q-二次收敛到 \\(x^{*}\\) 由牛顿方程(6.4.2)可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.236,
                0.209,
                0.736,
                0.354
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\| \\nabla f (x ^ {k + 1}) \\| = \\| \\nabla f (x ^ {k + 1}) - \\nabla f (x ^ {k}) - \\nabla^ {2} f (x ^ {k}) d ^ {k} \\| \\\\ = \\left\\| \\int_ {0} ^ {1} \\nabla^ {2} f \\left(x ^ {k} + t d ^ {k}\\right) d ^ {k} d t - \\nabla^ {2} f \\left(x ^ {k}\\right) d ^ {k} \\right\\| \\\\ \\leqslant \\int_ {0} ^ {1} \\| \\nabla^ {2} f \\left(x ^ {k} + t d ^ {k}\\right) - \\nabla^ {2} f \\left(x ^ {k}\\right) \\| \\| d ^ {k} \\| d t \\tag {6.4.7} \\\\ \\leqslant \\frac {L}{2} \\| d ^ {k} \\| ^ {2} \\leqslant \\frac {1}{2} L \\| \\nabla^ {2} f (x ^ {k}) ^ {- 1} \\| ^ {2} \\| \\nabla f (x ^ {k}) \\| ^ {2} \\\\ \\leqslant 2 L \\| \\nabla^ {2} f (x ^ {*}) ^ {- 1} \\| ^ {2} \\| \\nabla f (x ^ {k}) \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.369,
                0.457,
                0.386
            ],
            "angle": 0,
            "content": "这证明了梯度的范数Q-二次收敛到0."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.404,
                0.744,
                0.567
            ],
            "angle": 0,
            "content": "定理6.6表明经典牛顿法是收敛速度很快的算法，但它的收敛是有条件的：第一，初始点 \\( x^0 \\) 必须距离问题的解充分近，即牛顿法只有局部收敛性，当 \\( x^0 \\) 距问题的解较远时，牛顿算法在多数情况下会失效；第二，海瑟矩阵 \\( \\nabla^2 f(x^*) \\) 需要为正定矩阵，有例子表明，若 \\( \\nabla^2 f(x^*) \\) 是奇异的半正定矩阵，牛顿算法的收敛速度可能仅达到Q-线性．在定理6.6的证明中还可以看出，问题的条件数并不会在很大程度上影响牛顿法的收敛速度，利普希茨常数 \\( L \\) 在迭代后期通常会被 \\( \\| x^k - x^* \\| \\) 抵消．但对于病态问题，牛顿法的收敛域可能会变小，这对初值选取有了更高的要求."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.57,
                0.741,
                0.629
            ],
            "angle": 0,
            "content": "以上总结了牛顿法的特点，我们可以知道牛顿法适用于优化问题的高精度求解，但它没有全局收敛性质。因此在实际应用中，人们通常会使用梯度类算法先求得较低精度的解，而后调用牛顿法来获得高精度的解。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.657,
                0.334,
                0.674
            ],
            "angle": 0,
            "content": "6.4.3 修正牛顿法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.689,
                0.738,
                0.726
            ],
            "angle": 0,
            "content": "尽管我们提出了算法 (6.4.3) 并分析了其理论性质，在实际应用中此格式几乎是不能使用的。经典牛顿法有如下缺陷："
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.741,
                0.737,
                0.78
            ],
            "angle": 0,
            "content": "(1) 每一步迭代需要求解一个 \\(n\\) 维线性方程组, 这导致在高维问题中计算量很大. 海瑟矩阵 \\(\\nabla^2 f(x^k)\\) 既不容易计算又不容易储存."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.794,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "(2) 当 \\(\\nabla^2 f(x^k)\\) 不正定时, 由牛顿方程 (6.4.2) 给出的解 \\(d^k\\) 的性质通常比较差. 例如可以验证当海瑟矩阵正定时, \\(d^k\\) 是一个下降方向, 而在其他情况下 \\(d^k\\) 不一定为下降方向."
        },
        {
            "type": "list",
            "bbox": [
                0.181,
                0.741,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "248"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.157,
                0.825,
                0.195
            ],
            "angle": 0,
            "content": "(3) 当迭代点距最优值较远时，直接选取步长 \\(\\alpha_{k} = 1\\) 会使得迭代极其不稳定，在有些情况下迭代点列会发散."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.207,
                0.826,
                0.286
            ],
            "angle": 0,
            "content": "为了克服这些缺陷，我们必须对经典牛顿法做出某种修正或变形，使其成为真正可以使用的算法。这里介绍带线搜索的修正牛顿法，其基本思想是对牛顿方程中的海瑟矩阵 \\(\\nabla^2 f(x^k)\\) 进行修正，使其变成正定矩阵；同时引入线搜索以改善算法稳定性。它的一般框架见算法 6.3。该算法的关键在于修"
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.3,
                0.5,
                0.316
            ],
            "angle": 0,
            "content": "算法6.3带线搜索的修正牛顿法"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.321,
                0.404,
                0.338
            ],
            "angle": 0,
            "content": "1. 给定初始点 \\(x^0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.343,
                0.442,
                0.358
            ],
            "angle": 0,
            "content": "2. for \\( k = 0,1,2,\\dots \\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.361,
                0.774,
                0.38
            ],
            "angle": 0,
            "content": "3. 确定矩阵 \\(E^k\\) 使得矩阵 \\(B^{k}\\stackrel {\\mathrm{def}}{=}\\nabla^{2}f(x^{k}) + E^{k}\\) 正定且条件数较小"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.383,
                0.672,
                0.401
            ],
            "angle": 0,
            "content": "4. 求解修正的牛顿方程 \\(B^{k}d^{k} = -\\nabla f(x^{k})\\) 得方向 \\(d^k\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.405,
                0.591,
                0.421
            ],
            "angle": 0,
            "content": "5. 使用任意一种线搜索准则确定步长 \\(\\alpha_{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.424,
                0.473,
                0.442
            ],
            "angle": 0,
            "content": "6. 更新 \\(x^{k + 1} = x^k +\\alpha_kd^k\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.447,
                0.351,
                0.461
            ],
            "angle": 0,
            "content": "7. end for"
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.321,
                0.774,
                0.461
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.484,
                0.825,
                0.583
            ],
            "angle": 0,
            "content": "正矩阵 \\(E^k\\) 如何选取。一个最直接的取法是取 \\(E^k = \\tau_k I\\)，即取 \\(E^k\\) 为单位矩阵的常数倍。根据矩阵理论可以知道，当 \\(\\tau_k\\) 充分大时，总可以保证 \\(B^k\\) 是正定矩阵。然而 \\(\\tau_k\\) 不宜取得过大，这是因为当 \\(\\tau_k\\) 趋于无穷时，\\(d^k\\) 的方向会接近负梯度方向。比较合适的取法是先估计 \\(\\nabla^2 f(x^k)\\) 的最小特征值，再适当选择 \\(\\tau_k\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.587,
                0.827,
                0.709
            ],
            "angle": 0,
            "content": "另一种 \\(E^k\\) 的选取是隐式的，它是通过修正Cholesky分解的方式来求解牛顿方程(6.4.2).我们知道当海瑟矩阵正定时，方程组(6.4.2)可以用Cholesky分解快速求解．当海瑟矩阵不定或条件数较大时，Cholesky分解会失败．而修正Cholesky分解算法对基本Cholesky分解算法进行修正，且修正后的分解和原矩阵相差不大．我们首先回顾Cholesky分解的定义．对任意对称正定矩阵 \\(A = (a_{ij})\\) ，它的Cholesky分解可写作"
        },
        {
            "type": "equation",
            "bbox": [
                0.498,
                0.721,
                0.586,
                0.739
            ],
            "angle": 0,
            "content": "\\[\nA = L D L ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.753,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(L = (l_{ij})\\) 是对角线元素均为 1 的下三角矩阵, \\(D = \\operatorname{Diag}(d_1, d_2, \\dots, d_n)\\) 是对角矩阵且对角线元素均为正. 我们在算法 6.4 中直接给出基本的 Cholesky 分解算法. 根据 Cholesky 分解的形式, 如果 \\(A\\) 正定且条件数较小, 矩阵 \\(D\\) 的对角线元素不应该太小. 如果计算过程中发现 \\(d_j\\) 过小就应该及时修正. 同时我们需要保证该修正是有界的, 因此对修正后的矩阵元素也需要有上界"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.4 牛顿类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "249"
        },
        {
            "type": "title",
            "bbox": [
                0.172,
                0.158,
                0.351,
                0.176
            ],
            "angle": 0,
            "content": "算法6.4Cholesky分解"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.181,
                0.384,
                0.199
            ],
            "angle": 0,
            "content": "1. 给定对称矩阵 \\(A = (a_{ij})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.202,
                0.354,
                0.218
            ],
            "angle": 0,
            "content": "2. for \\(j = 1,2,\\dots ,n\\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.219,
                0.39,
                0.257
            ],
            "angle": 0,
            "content": "3. 计算 \\( c_{jj} = a_{jj} - \\sum_{s=1}^{j-1} d_s l_{js}^2 \\)."
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.257,
                0.299,
                0.274
            ],
            "angle": 0,
            "content": "4. 令 \\(d_{j} = c_{jj}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.276,
                0.424,
                0.293
            ],
            "angle": 0,
            "content": "5. for \\(i = j + 1,j + 2,\\dots ,n\\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.293,
                0.422,
                0.331
            ],
            "angle": 0,
            "content": "6. 计算 \\(c_{ij} = a_{ij} - \\sum_{s=1}^{j-1} d_s l_{is} l_{js}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.332,
                0.336,
                0.363
            ],
            "angle": 0,
            "content": "7. 计算 \\( l_{ij} = \\frac{c_{ij}}{d_j} \\)."
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.363,
                0.28,
                0.375
            ],
            "angle": 0,
            "content": "8. end for"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.383,
                0.262,
                0.396
            ],
            "angle": 0,
            "content": "9. end for"
        },
        {
            "type": "list",
            "bbox": [
                0.183,
                0.181,
                0.424,
                0.396
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.425,
                0.536,
                0.442
            ],
            "angle": 0,
            "content": "约束．具体来说，我们选取两个正参数 \\(\\delta, \\beta\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.29,
                0.453,
                0.617,
                0.48
            ],
            "angle": 0,
            "content": "\\[\nd _ {j} \\geqslant \\delta , \\quad l _ {i j} \\sqrt {d _ {j}} \\leqslant \\beta , \\quad i = j + 1, j + 2, \\dots , n.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.491,
                0.737,
                0.528
            ],
            "angle": 0,
            "content": "在算法6.4中，我们只需要修改对 \\(d_{j}\\) 的更新即可保证上述条件成立。具体更新方式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.289,
                0.528,
                0.617,
                0.57
            ],
            "angle": 0,
            "content": "\\[\nd _ {j} = \\max  \\left\\{\\left| c _ {j j} \\right|, \\left(\\frac {\\theta_ {j}}{\\beta}\\right) ^ {2}, \\delta \\right\\}, \\quad \\theta_ {j} = \\max  _ {i > j} \\left| c _ {i j} \\right|.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.575,
                0.737,
                0.635
            ],
            "angle": 0,
            "content": "可以证明，修正的Cholesky分解算法实际上是计算修正矩阵 \\(\\nabla^2 f(x^k) + E^k\\) 的Cholesky分解，其中 \\(E^k\\) 是对角矩阵且对角线元素非负．当 \\(\\nabla^2 f(x^k)\\) 正定且条件数足够小时有 \\(E^k = 0\\)"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.66,
                0.353,
                0.679
            ],
            "angle": 0,
            "content": "6.4.4 非精确牛顿法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.691,
                0.737,
                0.791
            ],
            "angle": 0,
            "content": "在经典牛顿法中，计算牛顿方向 \\(d^k\\) 依赖于求解线性方程组．当 \\(n\\) 较大但 \\(\\nabla^2 f(x^k)\\) 有稀疏结构时，需要通过迭代法来求解牛顿方程(6.4.2).我们知道迭代法求解线性方程组总有精度误差，那么牛顿方程解的误差对牛顿法收敛性有何影响？如何控制解的精度使得牛顿法依然能够收敛？下面将简要回答这一问题."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.795,
                0.737,
                0.832
            ],
            "angle": 0,
            "content": "考虑牛顿方程(6.4.2)的非精确解 \\(d^{k}\\), 我们引入向量 \\(r^{k}\\) 来表示残差, 则非精确牛顿方向满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.835,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\nabla^ {2} f \\left(x ^ {k}\\right) d ^ {k} = - \\nabla f \\left(x ^ {k}\\right) + r ^ {k}. \\tag {6.4.8}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "250"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.46,
                0.174
            ],
            "angle": 0,
            "content": "这里假设相对误差 \\(\\eta_{k}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.466,
                0.189,
                0.825,
                0.209
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| r ^ {k} \\right\\| \\leqslant \\eta_ {k} \\| \\nabla f \\left(x ^ {k}\\right) \\|. \\tag {6.4.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.224,
                0.825,
                0.282
            ],
            "angle": 0,
            "content": "显然，牛顿法的收敛性依赖于相对误差 \\(\\eta_{k}\\) 的选取，直观上牛顿方程求解得越精确，非精确牛顿法的收敛性就越好。为此我们直接给出如下的定理（证明见 [175]）："
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.297,
                0.825,
                0.334
            ],
            "angle": 0,
            "content": "定理6.7（非精确牛顿法的收敛性）设函数 \\(f(x)\\) 二阶连续可微，且在最小值点 \\(x^{*}\\) 处的海瑟矩阵正定，则在非精确牛顿法中，"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.349,
                0.822,
                0.386
            ],
            "angle": 0,
            "content": "(1) 若存在常数 \\(t < 1\\) 使得 \\(\\eta_{k}\\) 满足 \\(0 < \\eta_{k} < t, k = 1,2,\\dots\\), 则该算法收敛速度是 \\(Q\\)-线性的;"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.402,
                0.673,
                0.425
            ],
            "angle": 0,
            "content": "(2) 若 \\(\\lim_{k\\to \\infty}\\eta_k = 0\\) ，则该算法收敛速度是Q-超线性的；"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.435,
                0.718,
                0.454
            ],
            "angle": 0,
            "content": "(3) 若 \\(\\eta_{k} = \\mathcal{O}\\left(\\left\\| \\nabla f(x^{k})\\right\\|\\right)\\), 则该算法收敛速度是 Q-二次的."
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.349,
                0.822,
                0.454
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.467,
                0.827,
                0.608
            ],
            "angle": 0,
            "content": "定理6.7的直观含义是如果要达到更好的收敛性就必须使得牛顿方程求解更加精确。在一般迭代法中，算法的停机准则通常都会依赖于相对误差的大小。定理6.7的第一条表明我们完全可以将这个相对误差设置为固定值，算法依然有收敛性。和经典牛顿法相比，固定误差的非精确牛顿法仅仅有Q-线性收敛性，但在病态问题上的表现很可能好于传统的梯度法。如果希望非精确牛顿法能有Q-二次收敛速度，则在迭代后期牛顿方程必须求解足够精确，这在本质上和牛顿法并无差别。"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.612,
                0.827,
                0.712
            ],
            "angle": 0,
            "content": "常用的非精确牛顿法是牛顿共轭梯度法，即使用共轭梯度法求解(6.4.2)式．由于共轭梯度法在求解线性方程组方面有不错的表现，因此只需少数几步（有时可能只需要一步）迭代就可以达到定理6.7中第一条结论需要的条件．在多数问题上牛顿共轭梯度法都有较好的数值表现，该方法已经是求解优化问题不可少的优化工具."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.739,
                0.402,
                0.757
            ],
            "angle": 0,
            "content": "6.4.5 应用举例"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.77,
                0.825,
                0.807
            ],
            "angle": 0,
            "content": "本小节给出牛顿法的一个具体例子，从而说明牛顿法具体的迭代过程。我们在第三章中建立了二分类的逻辑回归模型 (3.3.3)："
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.819,
                0.825,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad \\ell (x) \\stackrel {\\text {d e f}} {=} \\frac {1}{m} \\sum_ {i = 1} ^ {m} \\ln \\left(1 + \\exp \\left(- b _ {i} a _ {i} ^ {\\mathrm {T}} x\\right)\\right) + \\lambda \\| x \\| _ {2} ^ {2}. \\tag {6.4.10}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.4 牛顿类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "251"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.737,
                0.201
            ],
            "angle": 0,
            "content": "在实际中，\\(\\lambda\\) 经常取为 \\(\\frac{1}{100m}\\)。接下来推导牛顿法，这又化为了计算目标函数 \\(\\ell(x)\\) 的梯度和海瑟矩阵的问题。根据向量值函数求导法，容易算出"
        },
        {
            "type": "equation",
            "bbox": [
                0.193,
                0.213,
                0.737,
                0.289
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\nabla \\ell (x) = \\frac {1}{m} \\sum_ {i = 1} ^ {m} \\frac {1}{1 + \\exp \\left(- b _ {i} a _ {i} ^ {\\mathrm {T}} x\\right)} \\cdot \\exp \\left(- b _ {i} a _ {i} ^ {\\mathrm {T}} x\\right) \\cdot \\left(- b _ {i} a _ {i}\\right) + 2 \\lambda x \\tag {6.4.11} \\\\ = - \\frac {1}{m} \\sum_ {i = 1} ^ {m} (1 - p _ {i} (x)) b _ {i} a _ {i} + 2 \\lambda x, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.302,
                0.613,
                0.339
            ],
            "angle": 0,
            "content": "其中 \\(p_i(x) = \\frac{1}{1 + \\exp(-b_i a_i^{\\mathrm{T}} x)}\\). 再对(6.4.11)式求导可得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.193,
                0.348,
                0.738,
                0.484
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\nabla^ {2} \\ell (x) = \\frac {1}{m} \\sum_ {i = 1} ^ {m} b _ {i} \\cdot \\nabla p _ {i} (x) a _ {i} ^ {\\mathrm {T}} + 2 \\lambda I \\\\ = \\frac {1}{m} \\sum_ {i = 1} ^ {m} b _ {i} \\frac {- 1}{\\left(1 + \\exp \\left(- b _ {i} a _ {i} ^ {\\mathrm {T}} x\\right)\\right) ^ {2}} \\cdot \\exp \\left(- b _ {i} a _ {i} ^ {\\mathrm {T}} x\\right) \\cdot \\left(- b _ {i} a _ {i} a _ {i} ^ {\\mathrm {T}}\\right) + 2 \\lambda I \\\\ = \\frac {1}{m} \\sum_ {i = 1} ^ {m} \\left(1 - p _ {i} (x)\\right) p _ {i} (x) a _ {i} a _ {i} ^ {\\mathrm {T}} + 2 \\lambda I, \\tag {6.4.12} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.5,
                0.351,
                0.518
            ],
            "angle": 0,
            "content": "其中利用了 \\(b_{i}^{2} = 1\\) 以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.304,
                0.529,
                0.606,
                0.567
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\exp \\left(- b _ {i} a _ {i} ^ {\\mathrm {T}} x\\right)}{\\left(1 + \\exp \\left(- b _ {i} a _ {i} ^ {\\mathrm {T}} x\\right)\\right) ^ {2}} = p _ {i} (x) \\left(1 - p _ {i} (x)\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.578,
                0.737,
                0.618
            ],
            "angle": 0,
            "content": "实际上我们可以使用矩阵语言将以上结果用更紧凑的形式表达. 引入矩阵 \\(A = [a_{1}, a_{2}, \\dots, a_{m}]^{\\mathrm{T}} \\in \\mathbb{R}^{m \\times n}\\), 向量 \\(b = (b_{1}, b_{2}, \\dots, b_{m})^{\\mathrm{T}}\\), 以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.325,
                0.631,
                0.585,
                0.652
            ],
            "angle": 0,
            "content": "\\[\np (x) = (p _ {1} (x), p _ {2} (x), \\dots , p _ {m} (x)) ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.667,
                0.367,
                0.684
            ],
            "angle": 0,
            "content": "梯度和海瑟矩阵可重写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.31,
                0.695,
                0.737,
                0.734
            ],
            "angle": 0,
            "content": "\\[\n\\nabla \\ell (x) = - \\frac {1}{m} A ^ {\\mathrm {T}} (b - b \\odot p (x)) + 2 \\lambda x, \\tag {6.4.13}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.308,
                0.728,
                0.534,
                0.758
            ],
            "angle": 0,
            "content": "\\[\n\\nabla^ {2} \\ell (x) = \\frac {1}{m} A ^ {\\mathrm {T}} W (x) A + 2 \\lambda I,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.77,
                0.738,
                0.809
            ],
            "angle": 0,
            "content": "其中 \\(W(x)\\) 为由 \\(\\{p_i(x)(1 - p_i(x))\\}_{i = 1}^m\\) 生成的对角矩阵，\\(\\odot\\) 表示两个向量逐分量的乘积．因此牛顿法可以写作"
        },
        {
            "type": "equation",
            "bbox": [
                0.185,
                0.819,
                0.724,
                0.859
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} + \\left(\\frac {1}{m} (A ^ {\\mathrm {T}} W (x ^ {k}) A + 2 \\lambda I\\right) ^ {- 1} \\left(\\frac {1}{m} A ^ {\\mathrm {T}} (b - b \\odot p (x ^ {k})) - 2 \\lambda x ^ {k}\\right).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "252"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "table_caption",
            "bbox": [
                0.466,
                0.159,
                0.618,
                0.176
            ],
            "angle": 0,
            "content": "表 6.1 数据集信息"
        },
        {
            "type": "table",
            "bbox": [
                0.434,
                0.188,
                0.653,
                0.277
            ],
            "angle": 0,
            "content": "<table><tr><td>名称</td><td>m</td><td>n</td></tr><tr><td>a9a</td><td>16281</td><td>122</td></tr><tr><td>ijCNN1</td><td>91701</td><td>22</td></tr><tr><td>CINA</td><td>3206</td><td>132</td></tr></table>"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.302,
                0.825,
                0.339
            ],
            "angle": 0,
            "content": "当变量规模不是很大时，可以利用正定矩阵的Cholesky分解来求解牛顿方程；当变量规模较大时，可以使用共轭梯度法进行不精确求解."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.343,
                0.826,
                0.402
            ],
            "angle": 0,
            "content": "这里采用LIBSVM[42]网站的数据集，见表6.1．对于不同的数据集均调用牛顿法求解，其迭代过程见图6.12．其中，我们采用不精确的共轭梯度法求解牛顿方程，使得如下条件成立："
        },
        {
            "type": "equation",
            "bbox": [
                0.319,
                0.417,
                0.766,
                0.438
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| \\nabla^ {2} \\ell \\left(x ^ {k}\\right) d ^ {k} + \\nabla \\ell \\left(x ^ {k}\\right) \\right\\| _ {2} \\leqslant \\min  \\left\\{\\left\\| \\nabla \\ell \\left(x ^ {k}\\right) \\right\\| _ {2} ^ {2}, 0. 1 \\left\\| \\nabla \\ell \\left(x ^ {k}\\right) \\right\\| _ {2} \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.453,
                0.768,
                0.47
            ],
            "angle": 0,
            "content": "从图6.12中可以看到，在精确解附近梯度范数具有Q-超线性收敛性。"
        },
        {
            "type": "image",
            "bbox": [
                0.356,
                0.483,
                0.731,
                0.709
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.41,
                0.72,
                0.673,
                0.737
            ],
            "angle": 0,
            "content": "图6.12 梯度范数随迭代步的变化"
        },
        {
            "type": "title",
            "bbox": [
                0.437,
                0.776,
                0.646,
                0.798
            ],
            "angle": 0,
            "content": "6.5 拟牛顿类算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "牛顿法在理论上和实践中均取得很好的效果。然而对于大规模问题，函数的海瑟矩阵计算代价特别大或者难以得到，即便得到海瑟矩阵我们还需"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "6.5 拟牛顿类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "253"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.235
            ],
            "angle": 0,
            "content": "要求解一个大规模线性方程组. 那么能否使用海瑟矩阵或其逆矩阵的近似来进行牛顿迭代呢? 拟牛顿法便是这样的算法, 它能够在每一步以较小的计算代价生成近似矩阵, 并且使用近似矩阵代替海瑟矩阵而产生的迭代序列仍具有超线性收敛的性质."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.24,
                0.744,
                0.32
            ],
            "angle": 0,
            "content": "拟牛顿方法不计算海瑟矩阵 \\(\\nabla^2 f(x)\\)，而是构造其近似矩阵 \\(B^{k}\\) 或其逆的近似矩阵 \\(H^{k}\\)。我们希望 \\(B^{k}\\) 或 \\(H^{k}\\) 仍然保留海瑟矩阵的部分性质，例如使得 \\(d^{k}\\) 仍然为下降方向。那么拟牛顿矩阵应该满足一些什么性质？如何构造它们呢？"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.344,
                0.317,
                0.363
            ],
            "angle": 0,
            "content": "6.5.1 割线方程"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.375,
                0.741,
                0.414
            ],
            "angle": 0,
            "content": "首先回顾牛顿法的推导过程．设 \\(f(x)\\) 是二次连续可微函数，根据泰勒展开，向量值函数 \\(\\nabla f(x)\\) 在点 \\(x^{k + 1}\\) 处的近似为"
        },
        {
            "type": "equation",
            "bbox": [
                0.199,
                0.426,
                0.738,
                0.447
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x) = \\nabla f \\left(x ^ {k + 1}\\right) + \\nabla^ {2} f \\left(x ^ {k + 1}\\right) \\left(x - x ^ {k + 1}\\right) + \\mathcal {O} \\left(\\| x - x ^ {k + 1} \\| ^ {2}\\right). \\tag {6.5.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.458,
                0.62,
                0.478
            ],
            "angle": 0,
            "content": "令 \\(x = x^{k}, s^{k} = x^{k + 1} - x^{k}\\) 及 \\(y^{k} = \\nabla f(x^{k + 1}) - \\nabla f(x^{k})\\) ，得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.34,
                0.489,
                0.738,
                0.511
            ],
            "angle": 0,
            "content": "\\[\n\\nabla^ {2} f \\left(x ^ {k + 1}\\right) s ^ {k} + \\mathcal {O} \\left(\\left\\| s ^ {k} \\right\\| ^ {2}\\right) = y ^ {k}. \\tag {6.5.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.523,
                0.657,
                0.542
            ],
            "angle": 0,
            "content": "忽略高阶项 \\(\\| s^k\\|^2\\) ，我们希望海瑟矩阵的近似矩阵 \\(B^{k + 1}\\) 满足方程"
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.554,
                0.738,
                0.574
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = B ^ {k + 1} s ^ {k}, \\tag {6.5.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.587,
                0.447,
                0.605
            ],
            "angle": 0,
            "content": "或者其逆的近似矩阵 \\(H^{k + 1}\\) 满足方程"
        },
        {
            "type": "equation",
            "bbox": [
                0.405,
                0.618,
                0.738,
                0.638
            ],
            "angle": 0,
            "content": "\\[\ns ^ {k} = H ^ {k + 1} y ^ {k}, \\tag {6.5.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.651,
                0.444,
                0.668
            ],
            "angle": 0,
            "content": "并称(6.5.3)式和(6.5.4)式为割线方程"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.672,
                0.739,
                0.729
            ],
            "angle": 0,
            "content": "还可以从另一个角度理解割线方程(6.5.3). 我们知道, 牛顿法本质上是对目标函数 \\(f(x)\\) 在迭代点 \\(x^{k}\\) 处做二阶近似然后求解. 考虑在点 \\(x^{k+1}\\) 处的二阶近似"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.727,
                0.738,
                0.757
            ],
            "angle": 0,
            "content": "\\[\nm _ {k + 1} (d) = f \\left(x ^ {k + 1}\\right) + \\nabla f \\left(x ^ {k + 1}\\right) ^ {\\mathrm {T}} d + \\frac {1}{2} d ^ {\\mathrm {T}} B ^ {k + 1} d, \\tag {6.5.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.762,
                0.738,
                0.822
            ],
            "angle": 0,
            "content": "我们要求 \\(m_{k + 1}(d)\\) 在 \\(d = -s^k\\) 和 \\(d = 0\\) 处的梯度与 \\(f(x)\\) 在 \\(x = x^{k}\\) 和 \\(x = x^{k + 1}\\) 处的梯度分别保持一致. 注意到 \\(\\nabla m_{k + 1}(0) = \\nabla f(x^{k + 1})\\) 是自然满足的，为了使得第一个条件满足只需"
        },
        {
            "type": "equation",
            "bbox": [
                0.283,
                0.835,
                0.738,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\nabla m _ {k + 1} (- s ^ {k}) = \\nabla f (x ^ {k + 1}) - B ^ {k + 1} s ^ {k} = \\nabla f (x ^ {k}), \\tag {6.5.6}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "254"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.434,
                0.174
            ],
            "angle": 0,
            "content": "整理即可得到(6.5.3)式"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.18,
                0.825,
                0.218
            ],
            "angle": 0,
            "content": "另外，注意到近似矩阵 \\(B^{k}\\) 的正定性是一个很关键的因素，在(6.5.3)式两边同时左乘 \\((s^k)^{\\mathrm{T}}\\) 可得 \\((s^k)^{\\mathrm{T}}B^{k + 1}s^k = (s^k)^{\\mathrm{T}}y^k\\) ，因此条件"
        },
        {
            "type": "equation",
            "bbox": [
                0.499,
                0.238,
                0.826,
                0.256
            ],
            "angle": 0,
            "content": "\\[\n\\left(s ^ {k}\\right) ^ {\\mathrm {T}} y ^ {k} > 0 \\tag {6.5.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.276,
                0.825,
                0.357
            ],
            "angle": 0,
            "content": "为 \\(B^{k+1}\\) 正定的一个必要条件。我们额外要求条件(6.5.7)在迭代过程中始终满足，这个条件也称为曲率条件。对于一般的目标函数 \\(f(x)\\)，需要使用Wolfe准则线搜索来保证曲率条件(6.5.7)成立。实际上，根据Wolfe准则的条件(6.1.4b)有 \\(\\nabla f(x^{k+1})^{\\mathrm{T}} s^k \\geqslant c_2 \\nabla f(x^k)^{\\mathrm{T}} s^k\\)，两边同时减去 \\(\\nabla f(x^k)^{\\mathrm{T}} s^k\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.418,
                0.375,
                0.667,
                0.395
            ],
            "angle": 0,
            "content": "\\[\n\\left(y ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k} \\geqslant \\left(c _ {2} - 1\\right) \\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k} > 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.414,
                0.825,
                0.453
            ],
            "angle": 0,
            "content": "这是因为 \\(c_{2} < 1\\) 以及 \\(s^{k} = \\alpha_{k}d^{k}\\) 是下降方向．仅仅使用Armijo准则不能保证曲率条件成立."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.458,
                0.826,
                0.517
            ],
            "angle": 0,
            "content": "在通常情况下，近似矩阵 \\(B^{k + 1}\\) 或 \\(H^{k + 1}\\) 是由上一步迭代加上一个修正得到的，并且要求满足割线方程(6.5.3). 这一小节先给出拟牛顿方法的一般框架（算法6.5），下一小节将讨论一些具体的矩阵更新方式."
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.538,
                0.447,
                0.554
            ],
            "angle": 0,
            "content": "算法6.5拟牛顿算法框架"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.559,
                0.697,
                0.577
            ],
            "angle": 0,
            "content": "1. 给定 \\(x^0 \\in \\mathbb{R}^n\\), 初始矩阵 \\(B^0 \\in \\mathbb{R}^{n \\times n}\\) (或 \\(H^0\\)), 令 \\(k = 0\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.581,
                0.487,
                0.597
            ],
            "angle": 0,
            "content": "2. while 未达到停机准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.6,
                0.696,
                0.619
            ],
            "angle": 0,
            "content": "3. 计算方向 \\(d^{k} = -(B^{k})^{-1}\\nabla f(x^{k})\\) 或 \\(d^{k} = -H^{k}\\nabla f(x^{k}).\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.621,
                0.729,
                0.639
            ],
            "angle": 0,
            "content": "4. 通过线搜索找到合适的步长 \\(\\alpha_{k} > 0\\) ，令 \\(x^{k + 1} = x^{k} + \\alpha_{k}d^{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.642,
                0.722,
                0.66
            ],
            "angle": 0,
            "content": "5. 更新海瑟矩阵的近似矩阵 \\(B^{k + 1}\\) 或其逆矩阵的近似 \\(H^{k + 1}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.664,
                0.386,
                0.679
            ],
            "angle": 0,
            "content": "6. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.685,
                0.371,
                0.699
            ],
            "angle": 0,
            "content": "7. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.268,
                0.559,
                0.729,
                0.699
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.732,
                0.827,
                0.853
            ],
            "angle": 0,
            "content": "在实际应用中基于 \\(H^{k}\\) 的拟牛顿法更加实用，这是因为根据 \\(H^{k}\\) 计算下降方向 \\(d^{k}\\) 不需要求解线性方程组，而求解线性方程组在大规模问题上是非常耗时的．但基于 \\(B^{k}\\) 的拟牛顿法有比较好的理论性质，产生的迭代序列比较稳定．如果有的方法快速求解线性方程组，我们也可采用基于 \\(B^{k}\\) 的拟牛顿法．此外在某些场景下，比如有些带约束的优化问题的算法设计，由于需要用到海瑟矩阵的近似，\\(B^{k}\\) 的使用也很常见."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "6.5 拟牛顿类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "255"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.156,
                0.411,
                0.174
            ],
            "angle": 0,
            "content": "6.5.2 拟牛顿矩阵更新方式"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.187,
                0.576,
                0.205
            ],
            "angle": 0,
            "content": "这一小节介绍一些常见的拟牛顿矩阵的更新方式"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.228,
                0.322,
                0.244
            ],
            "angle": 0,
            "content": "1. 秩一更新 (SR1)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.257,
                0.739,
                0.316
            ],
            "angle": 0,
            "content": "秩一更新 (SR1) 公式是结构最简单的拟牛顿矩阵更新公式。设 \\(B^{k}\\) 是第 \\(k\\) 步的近似海瑟矩阵，我们通过对 \\(B^{k}\\) 进行秩一修正得到 \\(B^{k+1}\\)，使其满足割线方程(6.5.3)。为此使用待定系数法求出修正矩阵，并设"
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.327,
                0.737,
                0.347
            ],
            "angle": 0,
            "content": "\\[\nB ^ {k + 1} = B ^ {k} + a u u ^ {\\mathrm {T}}, \\tag {6.5.8}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.36,
                0.525,
                0.377
            ],
            "angle": 0,
            "content": "其中 \\(u\\in \\mathbb{R}^n,a\\in \\mathbb{R}\\) 待定．根据割线方程(6.5.3)，"
        },
        {
            "type": "equation",
            "bbox": [
                0.345,
                0.389,
                0.564,
                0.409
            ],
            "angle": 0,
            "content": "\\[\nB ^ {k + 1} s ^ {k} = \\left(B ^ {k} + a u u ^ {\\mathrm {T}}\\right) s ^ {k} = y ^ {k},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.422,
                0.245,
                0.437
            ],
            "angle": 0,
            "content": "进而得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.366,
                0.44,
                0.542,
                0.46
            ],
            "angle": 0,
            "content": "\\[\n\\left(a \\cdot u ^ {\\mathrm {T}} s ^ {k}\\right) u = y ^ {k} - B ^ {k} s ^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.468,
                0.737,
                0.507
            ],
            "angle": 0,
            "content": "注意到 \\(a \\cdot u^{\\mathrm{T}} s^{k}\\) 是一个标量，因此 \\(u\\) 和 \\(y^{k} - B^{k} s^{k}\\) 方向相同．不妨令 \\(u = y^{k} - B^{k} s^{k}\\)，代入原方程可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.299,
                0.518,
                0.607,
                0.539
            ],
            "angle": 0,
            "content": "\\[\na \\left(\\left(y ^ {k} - B ^ {k} s ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k}\\right) \\left(y ^ {k} - B ^ {k} s ^ {k}\\right) = y ^ {k} - B ^ {k} s ^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.546,
                0.737,
                0.592
            ],
            "angle": 0,
            "content": "如果假设 \\((y^{k} - B^{k}s^{k})^{\\mathrm{T}}s^{k}\\neq 0\\) ，可以得到 \\(a = \\frac{1}{(y^k - B^k s^k)^{\\mathrm{T}} s^k}\\) ，最终得到更新公式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.314,
                0.592,
                0.737,
                0.629
            ],
            "angle": 0,
            "content": "\\[\nB ^ {k + 1} = B ^ {k} + \\frac {\\left(y ^ {k} - B ^ {k} s ^ {k}\\right) \\left(y ^ {k} - B ^ {k} s ^ {k}\\right) ^ {\\mathrm {T}}}{\\left(y ^ {k} - B ^ {k} s ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k}}. \\tag {6.5.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.633,
                0.737,
                0.671
            ],
            "angle": 0,
            "content": "我们称(6.5.9)式为基于 \\(B^{k}\\) 的SR1公式．由完全一样的过程我们可以根据割线方程(6.5.4)得到基于 \\(H^{k}\\) 的SR1公式："
        },
        {
            "type": "equation",
            "bbox": [
                0.307,
                0.678,
                0.737,
                0.715
            ],
            "angle": 0,
            "content": "\\[\nH ^ {k + 1} = H ^ {k} + \\frac {\\left(s ^ {k} - H ^ {k} y ^ {k}\\right) \\left(s ^ {k} - H ^ {k} y ^ {k}\\right) ^ {\\mathrm {T}}}{\\left(s ^ {k} - H ^ {k} y ^ {k}\\right) ^ {\\mathrm {T}} y ^ {k}}. \\tag {6.5.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.723,
                0.739,
                0.782
            ],
            "angle": 0,
            "content": "SR1 公式虽然结构简单, 但是有一个重大缺陷: 它不能保证矩阵在迭代过程中保持正定. 容易验证 \\(\\left(y^{k} - B^{k}s^{k}\\right)^{\\mathrm{T}}s^{k} > 0\\) 是 \\(B^{k+1}\\) 正定的一个充分条件,但这个条件在迭代过程中未必得到满足. 因此在实际中较少使用 SR1 公式."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.785,
                0.738,
                0.823
            ],
            "angle": 0,
            "content": "另一个比较有趣的观察是公式(6.5.9)和(6.5.10)在形式上互为对偶。将公式(6.5.9)里的变量作如下替换："
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.835,
                0.53,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nB ^ {k} \\to H ^ {k}, \\quad s ^ {k} \\leftrightarrow y ^ {k},\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "256"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.156,
                0.824,
                0.194
            ],
            "angle": 0,
            "content": "即可得到公式(6.5.10). 实际上如果增加假设 \\(H^{k} = (B^{k})^{-1}\\), 公式(6.5.10)也可由SMW公式(B.1.2)推出."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.22,
                0.379,
                0.236
            ],
            "angle": 0,
            "content": "2. BFGS 公式"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.25,
                0.825,
                0.288
            ],
            "angle": 0,
            "content": "为了克服SR1公式的缺陷，现在考虑对 \\(B^{k}\\) 的秩二更新．同样地，采用待定系数法来推导此公式．设"
        },
        {
            "type": "equation",
            "bbox": [
                0.445,
                0.303,
                0.825,
                0.321
            ],
            "angle": 0,
            "content": "\\[\nB ^ {k + 1} = B ^ {k} + a u u ^ {\\mathrm {T}} + b v v ^ {\\mathrm {T}}, \\tag {6.5.11}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.338,
                0.654,
                0.355
            ],
            "angle": 0,
            "content": "其中 \\(u,v\\in \\mathbb{R}^n\\) ， \\(a,b\\in \\mathbb{R}\\) 待定．根据割线方程(6.5.3)，"
        },
        {
            "type": "equation",
            "bbox": [
                0.405,
                0.369,
                0.679,
                0.389
            ],
            "angle": 0,
            "content": "\\[\nB ^ {k + 1} s ^ {k} = \\left(B ^ {k} + a u u ^ {\\mathrm {T}} + b v v ^ {\\mathrm {T}}\\right) s ^ {k} = y ^ {k},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.405,
                0.334,
                0.421
            ],
            "angle": 0,
            "content": "整理可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.407,
                0.424,
                0.677,
                0.445
            ],
            "angle": 0,
            "content": "\\[\n\\left(a \\cdot u ^ {\\mathrm {T}} s ^ {k}\\right) u + \\left(b \\cdot v ^ {\\mathrm {T}} s ^ {k}\\right) v = y ^ {k} - B ^ {k} s ^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.455,
                0.825,
                0.493
            ],
            "angle": 0,
            "content": "我们通过选取 \\(u\\) 和 \\(v\\) 让以上等式成立即可．实际上，\\(u, v\\) 有非常多的取法，一种最直接的取法是让上面等式左右两边的两项分别对应相等，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.451,
                0.506,
                0.633,
                0.546
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} u = y ^ {k}, \\quad a \\cdot u ^ {\\mathrm {T}} s ^ {k} = 1, \\\\ v = B ^ {k} s ^ {k}, \\quad b \\cdot v ^ {T} s ^ {k} = - 1. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.562,
                0.401,
                0.578
            ],
            "angle": 0,
            "content": "因此得到更新方式"
        },
        {
            "type": "equation",
            "bbox": [
                0.407,
                0.588,
                0.825,
                0.625
            ],
            "angle": 0,
            "content": "\\[\nB ^ {k + 1} = B ^ {k} + \\frac {y ^ {k} \\left(y ^ {k}\\right) ^ {\\mathrm {T}}}{\\left(s ^ {k}\\right) ^ {\\mathrm {T}} y ^ {k}} - \\frac {B ^ {k} s ^ {k} \\left(B ^ {k} s ^ {k}\\right) ^ {\\mathrm {T}}}{\\left(s ^ {k}\\right) ^ {\\mathrm {T}} B ^ {k} s ^ {k}}. \\tag {6.5.12}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.637,
                0.825,
                0.674
            ],
            "angle": 0,
            "content": "格式(6.5.12)被称为基于 \\(B^k\\) 的BFGS公式，它是由Broyden, Fletcher, Goldfarb, Shanno四人名字的首字母组成."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.678,
                0.825,
                0.716
            ],
            "angle": 0,
            "content": "根据 SMW 公式(B.1.2)并假设 \\(H^{k} = (B^{k})^{-1}\\)，可立即推出基于 \\(H^{k}\\) 的 BFGS 公式："
        },
        {
            "type": "equation",
            "bbox": [
                0.314,
                0.731,
                0.825,
                0.751
            ],
            "angle": 0,
            "content": "\\[\nH ^ {k + 1} = \\left(I - \\rho_ {k} s ^ {k} \\left(y ^ {k}\\right) ^ {\\mathrm {T}}\\right) ^ {\\mathrm {T}} H ^ {k} \\left(I - \\rho_ {k} s ^ {k} \\left(y ^ {k}\\right) ^ {\\mathrm {T}}\\right) + \\rho_ {k} s ^ {k} \\left(s ^ {k}\\right) ^ {\\mathrm {T}}, \\tag {6.5.13}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.764,
                0.827,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(\\rho_{k} = \\frac{1}{(s^{k})^{\\mathrm{T}}y^{k}}\\) 容易看出，若要BFGS公式更新产生的矩阵 \\(H^{k + 1}\\) 正定，一个充分条件是不等式(6.5.7)成立且上一步更新矩阵 \\(H^{k}\\) 正定．在问题求解过程中，条件(6.5.7)不一定会得到满足，此时应该使用Wolfe准则的线搜索来迫使条件(6.5.7)成立."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "6.5 拟牛顿类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "257"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.739,
                0.195
            ],
            "angle": 0,
            "content": "BFGS 格式(6.5.13)还有更深刻的含义, 它其实满足了某种逼近的最优性. 具体来说, 由(6.5.13)式定义的 \\(H^{k+1}\\) 恰好是如下优化问题的解:"
        },
        {
            "type": "equation",
            "bbox": [
                0.38,
                0.204,
                0.528,
                0.228
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {H} \\| H - H ^ {k} \\| _ {W},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.391,
                0.228,
                0.737,
                0.246
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad H = H ^ {\\mathrm {T}}, \\tag {6.5.14}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.248,
                0.503,
                0.268
            ],
            "angle": 0,
            "content": "\\[\nH y ^ {k} = s ^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.279,
                0.739,
                0.317
            ],
            "angle": 0,
            "content": "这个优化问题的含义是在满足割线方程(6.5.4)的对称矩阵中找到离 \\(H^k\\) 最近的矩阵 \\(H\\). 这里 \\(\\| \\cdot \\|_W\\) 是加权范数，定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.358,
                0.328,
                0.551,
                0.349
            ],
            "angle": 0,
            "content": "\\[\n\\| H \\| _ {W} = \\| W ^ {1 / 2} H W ^ {1 / 2} \\| _ {F},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.359,
                0.556,
                0.378
            ],
            "angle": 0,
            "content": "其中 \\(W\\) 可以是任意满足割线方程 \\(Ws^{k} = y^{k}\\) 的矩阵"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.38,
                0.739,
                0.459
            ],
            "angle": 0,
            "content": "BFGS 公式是目前最有效的拟牛顿更新格式之一，它有比较好的理论性质，实现起来也并不复杂。对格式(6.5.13)进行改动可得到有限内存 BFGS 格式 (L-BFGS)，它是常用的处理大规模优化问题的拟牛顿类算法，我们将在本节的末尾介绍这一算法。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.483,
                0.28,
                0.499
            ],
            "angle": 0,
            "content": "3. DFP 公式"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.513,
                0.737,
                0.551
            ],
            "angle": 0,
            "content": "在BFGS公式的推导中，如果利用割线方程(6.5.4)对 \\(H^{k}\\) 推导秩二修正的拟牛顿修正，我们将得到基于 \\(H^{k}\\) 的拟牛顿矩阵更新"
        },
        {
            "type": "equation",
            "bbox": [
                0.31,
                0.556,
                0.737,
                0.595
            ],
            "angle": 0,
            "content": "\\[\nH ^ {k + 1} = H ^ {k} - \\frac {H ^ {k} y ^ {k} \\left(H ^ {k} y ^ {k}\\right) ^ {\\mathrm {T}}}{\\left(y ^ {k}\\right) ^ {\\mathrm {T}} H ^ {k} y ^ {k}} + \\frac {s ^ {k} \\left(s ^ {k}\\right) ^ {\\mathrm {T}}}{\\left(y ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k}}. \\tag {6.5.15}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.6,
                0.738,
                0.639
            ],
            "angle": 0,
            "content": "这种迭代格式首先由 Davidon 发现, 此后由 Fletcher 以及 Powell 进一步发展, 因此被称为 DFP 公式. 根据 SMW 公式可得其关于 \\(B^{k}\\) 的更新格式"
        },
        {
            "type": "equation",
            "bbox": [
                0.227,
                0.649,
                0.737,
                0.671
            ],
            "angle": 0,
            "content": "\\[\nB ^ {k + 1} = \\left(I - \\rho_ {k} y ^ {k} \\left(s ^ {k}\\right) ^ {\\mathrm {T}}\\right) ^ {\\mathrm {T}} B ^ {k} \\left(I - \\rho_ {k} y ^ {k} \\left(s ^ {k}\\right) ^ {\\mathrm {T}}\\right) + \\rho_ {k} y ^ {k} \\left(y ^ {k}\\right) ^ {\\mathrm {T}}, \\tag {6.5.16}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.681,
                0.379,
                0.698
            ],
            "angle": 0,
            "content": "其中 \\(\\rho_{k}\\) 的定义同(6.5.13)式"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.701,
                0.74,
                0.782
            ],
            "angle": 0,
            "content": "可以看到，DFP 公式(6.5.15)(6.5.16)和 BFGS 公式(6.5.12)(6.5.13)分别呈对偶关系．将BFGS格式(6.5.13)中的 \\(H^{k}\\) 换成 \\(B^{k}\\)，\\(s^{k}\\) 与 \\(y^{k}\\) 对换便得到了DFP 格式(6.5.16)．不仅如此，在逼近性上也有这样的对偶现象．实际上，由(6.5.16)式定义的 \\(B^{k+1}\\) 是如下优化问题的解："
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.79,
                0.522,
                0.814
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {B} \\| B - B ^ {k} \\| _ {W},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.814,
                0.737,
                0.832
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & B = B ^ {\\mathrm {T}}, \\end{array} \\tag {6.5.17}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.836,
                0.503,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nB s ^ {k} = y ^ {k},\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "258"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.156,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "其中 \\(\\| \\cdot \\|_{W}\\) 的含义和问题(6.5.14)中的基本相同，但 \\(W\\) 为任意满足 \\(Wy^{k} = s^{k}\\) 的矩阵．和BFGS格式类似，DFP格式要求 \\(B^{k + 1}\\) 为满足割线方程(6.5.3)的对称矩阵中离 \\(B^{k}\\) 最近的矩阵，它也暗含某种最优性."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.219,
                0.826,
                0.277
            ],
            "angle": 0,
            "content": "遗憾的是，尽管DFP格式在很多方面和BFGS格式存在对偶关系，但从实际效果来看，DFP格式整体上不如BFGS格式．因此在实际使用中人们更多使用BFGS格式."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.303,
                0.518,
                0.321
            ],
            "angle": 0,
            "content": "6.5.3 拟牛顿法的全局收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.334,
                0.825,
                0.372
            ],
            "angle": 0,
            "content": "本小节介绍拟牛顿法的收敛性质。首先我们利用Zoutendijk条件得到拟牛顿法基本的收敛性，之后简要介绍收敛速度。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.383,
                0.825,
                0.422
            ],
            "angle": 0,
            "content": "定理6.8(BFGS全局收敛性[145]定理6.5）假设初始矩阵 \\(B^0\\) 是对称正定矩阵，目标函数 \\(f(x)\\) 是二阶连续可微函数，且下水平集"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.435,
                0.652,
                0.455
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {L} = \\left\\{x \\in \\mathbb {R} ^ {n} \\mid f (x) \\leqslant f \\left(x ^ {0}\\right) \\right\\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.467,
                0.825,
                0.503
            ],
            "angle": 0,
            "content": "是凸的，并且存在正数 \\(m\\) 以及 \\(M\\) 使得对于任意的 \\(z \\in \\mathbb{R}^n\\) 以及任意的 \\(x \\in \\mathcal{L}\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.426,
                0.507,
                0.825,
                0.527
            ],
            "angle": 0,
            "content": "\\[\nm \\| z \\| ^ {2} \\leqslant z ^ {\\mathrm {T}} \\nabla^ {2} f (x) z \\leqslant M \\| z \\| ^ {2}, \\tag {6.5.18}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.536,
                0.825,
                0.573
            ],
            "angle": 0,
            "content": "则采用 BFGS 格式(6.5.12) 并结合 Wolfe 线搜索的拟牛顿算法全局收敛到 \\( f(x) \\) 的极小值点 \\( x^{*} \\)."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.585,
                0.484,
                0.601
            ],
            "angle": 0,
            "content": "证明．为了方便，我们定义"
        },
        {
            "type": "equation",
            "bbox": [
                0.424,
                0.609,
                0.66,
                0.647
            ],
            "angle": 0,
            "content": "\\[\nm _ {k} = \\frac {\\left(y ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k}}{\\left(s ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k}}, \\quad M _ {k} = \\frac {\\left(y ^ {k}\\right) ^ {\\mathrm {T}} y ^ {k}}{\\left(y ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.655,
                0.676,
                0.687
            ],
            "angle": 0,
            "content": "由(6.5.18)以及 \\(y^{k} = \\int_{0}^{1}\\nabla^{2}f(x^{k} + t(x^{k + 1} - x^{k}))s^{k}\\mathrm{d}t\\) 可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.466,
                0.695,
                0.618,
                0.711
            ],
            "angle": 0,
            "content": "\\[\nm _ {k} \\geqslant m, \\quad M _ {k} \\leqslant M.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.726,
                0.628,
                0.743
            ],
            "angle": 0,
            "content": "根据BFGS格式(6.5.12)，两边同时计算迹，得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.385,
                0.75,
                0.825,
                0.787
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {T r} \\left(B ^ {k + 1}\\right) = \\operatorname {T r} \\left(B ^ {k}\\right) - \\frac {\\| B ^ {k} s ^ {k} \\| ^ {2}}{\\left(s ^ {k}\\right) ^ {\\mathrm {T}} B ^ {k} s ^ {k}} + \\frac {\\| y ^ {k} \\| ^ {2}}{\\left(y ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k}}. \\tag {6.5.19}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.795,
                0.53,
                0.812
            ],
            "angle": 0,
            "content": "此外，容易验证（见课后习题6.11）"
        },
        {
            "type": "equation",
            "bbox": [
                0.439,
                0.82,
                0.825,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\det  B ^ {k + 1} = \\det  B ^ {k} \\frac {\\left(y ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k}}{\\left(s ^ {k}\\right) ^ {\\mathrm {T}} B ^ {k} s ^ {k}}. \\tag {6.5.20}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.317,
                0.133
            ],
            "angle": 0,
            "content": "6.5 拟牛顿类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "259"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.262,
                0.174
            ],
            "angle": 0,
            "content": "接下来定义"
        },
        {
            "type": "equation",
            "bbox": [
                0.309,
                0.17,
                0.599,
                0.208
            ],
            "angle": 0,
            "content": "\\[\n\\cos \\theta_ {k} = \\frac {\\left(s ^ {k}\\right) ^ {\\mathrm {T}} B ^ {k} s ^ {k}}{\\| s ^ {k} \\| \\| B ^ {k} s ^ {k} \\|}, \\quad q _ {k} = \\frac {\\left(s ^ {k}\\right) ^ {\\mathrm {T}} B ^ {k} s ^ {k}}{\\left(s ^ {k}\\right) ^ {\\mathrm {T}} S ^ {k}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.21,
                0.523,
                0.226
            ],
            "angle": 0,
            "content": "那么将(6.5.19)式等号右边第二项整理可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.283,
                0.231,
                0.627,
                0.268
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\| B ^ {k} s ^ {k} \\| ^ {2}}{(s ^ {k}) ^ {\\mathrm {T}} B ^ {k} s ^ {k}} = \\frac {\\| B ^ {k} s ^ {k} \\| ^ {2} \\| s ^ {k} \\| ^ {2}}{\\left((s ^ {k}) ^ {\\mathrm {T}} B ^ {k} s ^ {k}\\right) ^ {2}} \\frac {(s ^ {k}) ^ {\\mathrm {T}} B ^ {k} s ^ {k}}{\\| s ^ {k} \\| ^ {2}} = \\frac {q _ {k}}{\\cos^ {2} \\theta_ {k}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.273,
                0.438,
                0.29
            ],
            "angle": 0,
            "content": "同样，重新整理(6.5.20)式可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.279,
                0.294,
                0.629,
                0.332
            ],
            "angle": 0,
            "content": "\\[\n\\det  B ^ {k + 1} = \\det  B ^ {k} \\frac {\\left(y ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k}}{\\left(s ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k}} \\frac {\\left(s ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k}}{\\left(s ^ {k}\\right) ^ {\\mathrm {T}} B ^ {k} s ^ {k}} = \\det  B ^ {k} \\frac {m _ {k}}{q _ {k}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.337,
                0.332,
                0.354
            ],
            "angle": 0,
            "content": "再引进矩阵辅助函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.357,
                0.548,
                0.376
            ],
            "angle": 0,
            "content": "\\[\n\\psi (B) = \\operatorname {T r} (B) - \\ln \\det  B,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.383,
                0.279,
                0.399
            ],
            "angle": 0,
            "content": "那么，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.23,
                0.408,
                0.291,
                0.426
            ],
            "angle": 0,
            "content": "\\[\n\\psi (B ^ {k + 1})\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.216,
                0.431,
                0.589,
                0.46
            ],
            "angle": 0,
            "content": "\\[\n= \\operatorname {T r} \\left(B ^ {k}\\right) + M _ {k} - \\frac {q _ {k}}{\\cos^ {2} \\theta_ {k}} - \\ln \\det B ^ {k} - \\ln m _ {k} + \\ln q _ {k}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.216,
                0.457,
                0.737,
                0.519
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} = \\psi \\left(B ^ {k}\\right) + \\left(M _ {k} - \\ln m _ {k} - 1\\right) + \\left(1 - \\frac {q _ {k}}{\\cos^ {2} \\theta_ {k}} + \\ln \\frac {q _ {k}}{\\cos^ {2} \\theta_ {k}}\\right) \\tag {6.5.21} \\\\ + \\ln \\cos^ {2} \\theta_ {k}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.528,
                0.692,
                0.546
            ],
            "angle": 0,
            "content": "根据递推关系式(6.5.21)，以及 \\(\\psi(B) > 0, 1 - t + \\ln t \\leqslant 0, \\forall t\\) 可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.25,
                0.553,
                0.738,
                0.593
            ],
            "angle": 0,
            "content": "\\[\n0 <   \\psi \\left(B ^ {k + 1}\\right) \\leqslant \\psi \\left(B ^ {0}\\right) + (k + 1) c + \\sum_ {j = 0} ^ {k} \\ln \\cos^ {2} \\theta_ {j}. \\tag {6.5.22}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.6,
                0.764,
                0.702
            ],
            "angle": 0,
            "content": "这里， \\(c = M - \\ln m - 1\\) 并且不失一般性假设 \\(c > 0.\\) 注意到 \\(s^k = -\\alpha_k(B^k)^{-1}\\nabla f(x^k)\\) 是搜索方向，那么 \\(\\cos \\theta_{k}\\) 即是搜索方向与梯度方向夹角的余弦．根据定理6.1，可知 \\(\\| \\nabla f(x^{k})\\|\\) 大于某个非零常数仅当 \\(\\cos \\theta_{k}\\to 0\\) ，因此，为了证明\\(\\| \\nabla f(x^k)\\| \\to 0\\) ，我们仅需证明 \\(\\cos \\theta_{k}\\rightarrow 0\\) 不成立，下面用反证法证明这一结论．假设 \\(\\cos \\theta_{k}\\rightarrow 0\\) ，那么存在 \\(k_{1} > 0,\\) 对于任意的 \\(j > k_{1}\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.391,
                0.711,
                0.517,
                0.73
            ],
            "angle": 0,
            "content": "\\[\n\\ln \\cos^ {2} \\theta_ {j} <   - 2 c.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.74,
                0.394,
                0.757
            ],
            "angle": 0,
            "content": "结合(6.5.22)式，当 \\(k > k_{1}\\) 时，"
        },
        {
            "type": "equation",
            "bbox": [
                0.208,
                0.763,
                0.739,
                0.849
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} 0 <   \\psi (B ^ {k + 1}) \\leqslant \\psi (B ^ {0}) + (k + 1) c + \\sum_ {j = 0} ^ {k _ {1}} \\ln \\cos^ {2} \\theta_ {j} + \\sum_ {j = k _ {1}} ^ {k} (- 2 c) (6.5.23) \\\\ = \\psi \\left(B ^ {0}\\right) + \\sum_ {j = 0} ^ {k _ {1}} \\ln \\cos^ {2} \\theta_ {j} + 2 c k _ {1} + c - c k. (6.5.24) \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.291,
                0.13
            ],
            "angle": 0,
            "content": "260"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.827,
                0.239
            ],
            "angle": 0,
            "content": "上式的右边对于充分大的 \\(k\\) 是负的, 而不等式左边是 0 , 这导出矛盾, 因此假设不成立, 即存在一个子列 \\(\\{j_{k}\\}_{k = 1,2,\\dots}\\) 使得 \\(\\cos \\theta_{j_k} \\geqslant \\delta > 0\\). 根据 Zoutendijk 条件 (定理6.1), 我们可以得到 \\(\\lim _{k \\rightarrow \\infty} \\inf \\| \\nabla f(x^k) \\| \\rightarrow 0\\). 又因为问题对 \\(x \\in \\mathcal{L}\\) 是强凸的, 所以这可以导出 \\(x^k \\rightarrow x^*\\). □"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.254,
                0.825,
                0.314
            ],
            "angle": 0,
            "content": "定理6.8叙述了BFGS格式的全局收敛性，但没有说明以什么速度收敛。下面这个定理介绍了在一定条件下BFGS格式会达到Q-超线性收敛速度。这里只给出定理结果，详细的证明过程可参考[145]。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.327,
                0.825,
                0.385
            ],
            "angle": 0,
            "content": "定理6.9(BFGS收敛速度)设 \\(f(x)\\) 二阶连续可微，在最优点 \\(x^{*}\\) 的一个邻域内海瑟矩阵利普希茨连续，且使用BFGS迭代格式收敛到 \\(f\\) 的最优值点 \\(x^{*}\\) 。若迭代点列 \\(\\{x^k\\}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.463,
                0.398,
                0.825,
                0.433
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {k = 1} ^ {\\infty} \\left\\| x ^ {k} - x ^ {*} \\right\\| <   + \\infty , \\tag {6.5.25}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.446,
                0.492,
                0.464
            ],
            "angle": 0,
            "content": "则 \\(\\{x^{k}\\}\\) 以Q-超线性收敛到 \\(x^{*}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.478,
                0.827,
                0.6
            ],
            "angle": 0,
            "content": "正如我们预期的，由于仅仅使用了海瑟矩阵的近似矩阵，拟牛顿法只能达到 Q-超线性收敛速度，这个速度和牛顿法相比较慢。但由于拟牛顿法不需要每一步都计算海瑟矩阵，它在整体计算开销方面可能远远小于牛顿法，因此在实际问题中较为实用。同样地，定理6.9的结果建立在序列 \\(\\{x^k\\}\\) 本身收敛的前提之上，而对于强凸函数，定理6.8保证了序列 \\(\\{x^k\\}\\) 是全局收敛的。如果函数的性质稍差，则拟牛顿法可能只有局部的收敛性。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.626,
                0.502,
                0.645
            ],
            "angle": 0,
            "content": "6.5.4 有限内存BFGS方法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.657,
                0.825,
                0.757
            ],
            "angle": 0,
            "content": "拟牛顿法虽然克服了计算海瑟矩阵的困难，但是它仍然无法应用在大规模优化问题上。一般来说，拟牛顿矩阵 \\( B^{k} \\) 或 \\( H^{k} \\) 是稠密矩阵，而存储稠密矩阵要消耗 \\( \\mathcal{O}(n^{2}) \\) 的内存，这对于大规模问题显然是不可能实现的。在本小节介绍的有限内存BFGS方法（L-BFGS）解决了这一存储问题，从而使得人们在大规模问题上也可应用拟牛顿类方法加速迭代的收敛。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.761,
                0.825,
                0.82
            ],
            "angle": 0,
            "content": "L-BFGS 方法是根据 BFGS 公式(6.5.12)(6.5.13)变形而来的。为了推导方便，我们以 \\(H^{k}\\) 的更新公式(6.5.13)为基础来推导相应的 L-BFGS 公式。首先引入新的记号重写(6.5.13)式："
        },
        {
            "type": "equation",
            "bbox": [
                0.424,
                0.835,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nH ^ {k + 1} = \\left(V ^ {k}\\right) ^ {\\mathrm {T}} H ^ {k} V ^ {k} + \\rho_ {k} s ^ {k} \\left(s ^ {k}\\right) ^ {\\mathrm {T}}, \\tag {6.5.26}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.317,
                0.133
            ],
            "angle": 0,
            "content": "6.5 拟牛顿类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "261"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.157,
                0.21,
                0.173
            ],
            "angle": 0,
            "content": "其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.17,
                0.737,
                0.204
            ],
            "angle": 0,
            "content": "\\[\n\\rho_ {k} = \\frac {1}{\\left(y ^ {k}\\right) ^ {\\mathrm {T}} s ^ {k}}, \\quad V ^ {k} = I - \\rho_ {k} y ^ {k} \\left(s ^ {k}\\right) ^ {\\mathrm {T}}. \\tag {6.5.27}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.209,
                0.739,
                0.247
            ],
            "angle": 0,
            "content": "观察到(6.5.26)式有类似递推的性质，为此我们可将(6.5.26)式递归地展开 \\(m\\) 次，其中 \\(m\\) 是一个给定的整数："
        },
        {
            "type": "equation",
            "bbox": [
                0.199,
                0.26,
                0.736,
                0.376
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} H ^ {k} \\\\ = \\left(V ^ {k - m} \\dots V ^ {k - 1}\\right) ^ {\\mathrm {T}} H ^ {k - m} \\left(V ^ {k - m} \\dots V ^ {k - 1}\\right) + \\\\ \\rho_ {k - m} \\left(V ^ {k - m + 1} \\dots V ^ {k - 1}\\right) ^ {\\mathrm {T}} s ^ {k - m} \\left(s ^ {k - m}\\right) ^ {\\mathrm {T}} \\left(V ^ {k - m + 1} \\dots V ^ {k - 1}\\right) + \\tag {6.5.28} \\\\ \\rho_ {k - m + 1} \\left(V ^ {k - m + 2} \\dots V ^ {k - 1}\\right) ^ {\\mathrm {T}} s ^ {k - m + 1} \\left(s ^ {k - m + 1}\\right) ^ {\\mathrm {T}} \\left(V ^ {k - m + 2} \\dots V ^ {k - 1}\\right) + \\\\ \\dots + \\rho_ {k - 1} s ^ {k - 1} (s ^ {k - 1}) ^ {\\mathrm {T}}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.388,
                0.741,
                0.488
            ],
            "angle": 0,
            "content": "为了达到节省内存的目的, (6.5.26)式不能无限展开下去, 但这会产生一个问题: \\(H^{k - m}\\) 还是无法显式求出. 一个很自然的想法就是用 \\(H^{k - m}\\) 的近似矩阵来代替 \\(H^{k - m}\\) 进行计算, 近似矩阵的选取方式非常多, 但基本原则是要保证近似矩阵具有非常简单的结构. 假定我们给出了 \\(H^{k - m}\\) 的一个近似矩阵 \\(\\hat{H}^{k - m}\\), (6.5.28)式便可以用于计算拟牛顿迭代."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.491,
                0.739,
                0.551
            ],
            "angle": 0,
            "content": "在拟牛顿迭代中，实际上并不需要计算 \\(H^k\\) 的显式形式，只需要利用\\(H^{k}\\nabla f(x^{k})\\) 来计算迭代方向 \\(d^{k}\\) ．为此先直接给出一个利用展开式(6.5.28)直接求解 \\(H^{k}\\nabla f(x^{k})\\) 的算法，见算法6.6．该算法的设计非常巧妙，它充分利用"
        },
        {
            "type": "title",
            "bbox": [
                0.171,
                0.565,
                0.424,
                0.582
            ],
            "angle": 0,
            "content": "算法6.6L-BFGS双循环递归算法"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.587,
                0.354,
                0.606
            ],
            "angle": 0,
            "content": "1. 初始化 \\(q \\gets \\nabla f(x^k)\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.609,
                0.445,
                0.626
            ],
            "angle": 0,
            "content": "2. for \\(i = k - 1, k - 2, \\dots, k - m\\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.629,
                0.411,
                0.648
            ],
            "angle": 0,
            "content": "3. 计算并保存 \\(\\alpha_{i} \\gets \\rho_{i}(s^{i})^{\\mathrm{T}} q\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.65,
                0.355,
                0.668
            ],
            "angle": 0,
            "content": "4. 更新 \\(q \\gets q - \\alpha_i y^i\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.672,
                0.262,
                0.685
            ],
            "angle": 0,
            "content": "5. end for"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.69,
                0.597,
                0.709
            ],
            "angle": 0,
            "content": "6. 初始化 \\(r \\gets \\hat{H}^{k - m} q\\) ，其中 \\(\\hat{H}^{k - m}\\) 是 \\(H^{k - m}\\) 的近似矩阵"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.713,
                0.479,
                0.729
            ],
            "angle": 0,
            "content": "7. for \\(i = k - m, k - m + 1, \\dots, k - 1\\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.732,
                0.357,
                0.751
            ],
            "angle": 0,
            "content": "8. 计算 \\(\\beta \\leftarrow \\rho_{i}(y^{i})^{\\mathrm{T}}r\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.754,
                0.396,
                0.771
            ],
            "angle": 0,
            "content": "9. 更新 \\(r \\gets r + (\\alpha_i - \\beta)s^i\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.177,
                0.776,
                0.263,
                0.789
            ],
            "angle": 0,
            "content": "10. end for"
        },
        {
            "type": "text",
            "bbox": [
                0.176,
                0.794,
                0.369,
                0.813
            ],
            "angle": 0,
            "content": "11. 输出 \\(r\\)，即 \\(H^{k}\\nabla f(x^{k})\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.176,
                0.587,
                0.597,
                0.813
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.835,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "了(6.5.28)式的结构来尽量节省计算 \\(H^{k}\\nabla f(x^{k})\\) 的开销．由于其主体结构包"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "262"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.714,
                0.174
            ],
            "angle": 0,
            "content": "包含了方向相反的两个循环，因此它也被称为双循环递归算法。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.178,
                0.825,
                0.216
            ],
            "angle": 0,
            "content": "我们现在给出算法6.6的一个比较直观的执行过程。在(6.5.28)式中，等式左右两边同时右乘 \\(\\nabla f(x^{k})\\) ，若只观察等式右侧，则需要计算"
        },
        {
            "type": "equation",
            "bbox": [
                0.308,
                0.227,
                0.776,
                0.248
            ],
            "angle": 0,
            "content": "\\[\nV ^ {k - 1} \\nabla f (x ^ {k}), V ^ {k - 2} V ^ {k - 1} \\nabla f (x ^ {k}), \\dots , V ^ {k - m} \\dots V ^ {k - 2} V ^ {k - 1} \\nabla f (x ^ {k}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.26,
                0.825,
                0.34
            ],
            "angle": 0,
            "content": "这些结果可以递推地进行, 无需重复计算. 另一个比较重要的观察是, 在计算 \\(V^{k - l} \\cdots V^{k - 1} \\nabla f(x^k)\\) 的过程中恰好同时计算了上一步的 \\(\\rho_{k - l}(s^{k - l})^{\\mathrm{T}}[V^{k - l + 1} \\cdots V^{k - 1} \\nabla f(x^k)]\\), 这是一个标量, 对应着算法6.6的 \\(\\alpha_i\\). 因此执行完第一个循环后, 我们得到了 \\(\\alpha_i, q\\), 公式(6.5.28)变成了如下形式:"
        },
        {
            "type": "equation",
            "bbox": [
                0.285,
                0.349,
                0.825,
                0.418
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} H ^ {k} \\nabla f (x ^ {k}) = \\left(V ^ {k - m} \\dots V ^ {k - 1}\\right) ^ {\\mathrm {T}} H ^ {k - m} q + \\\\ \\left(V ^ {k - m + 1} \\dots V ^ {k - 1}\\right) ^ {\\mathrm {T}} s ^ {k - m} \\alpha_ {k - m} + \\tag {6.5.29} \\\\ \\left(V ^ {k - m + 2} \\dots V ^ {k - 1}\\right) ^ {\\mathrm {T}} s ^ {k - m + 1} \\alpha_ {k - m + 1} + \\dots + s ^ {k - 1} \\alpha_ {k - 1}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.429,
                0.825,
                0.487
            ],
            "angle": 0,
            "content": "公式(6.5.29)已经简化了不少，接下来算法6.6的第二个循环就是自上而下合并每一项。以合并前两项为例，它们有公共的因子 \\(\\left(V^{k - m + 1}\\dots V^{k - 1}\\right)^{\\mathrm{T}}\\) ，提取出来之后前两项的和可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.383,
                0.497,
                0.703,
                0.542
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} (V ^ {k - m + 1} \\dots V ^ {k - 1}) ^ {\\mathrm {T}} ((V ^ {k - m}) ^ {\\mathrm {T}} r + \\alpha_ {k - m} s ^ {k - m}) \\\\ = (V ^ {k - m + 1} \\dots V ^ {k - 1}) ^ {\\mathrm {T}} (r + (\\alpha_ {k - m} - \\beta) s ^ {k - m}), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.551,
                0.825,
                0.588
            ],
            "angle": 0,
            "content": "这正是第二个循环的迭代格式。注意合并后(6.5.29)式的结构仍不变，因此可递归地计算下去，最后变量 \\(r\\) 就是我们期望的结果 \\(H^{k}\\nabla f(x^{k})\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.592,
                0.825,
                0.67
            ],
            "angle": 0,
            "content": "算法6.6双循环约需要 \\(4mn\\) 次乘法运算与 \\(2mn\\) 次加法运算, 若近似矩阵 \\(\\hat{H}^{k - m}\\) 是对角矩阵, 则额外需要 \\(n\\) 次乘法运算. 由于 \\(m\\) 不会很大, 因此该算法的复杂度是 \\(\\mathcal{O}(mn)\\). 算法所需要的额外存储为临时变量 \\(\\alpha_{i}\\), 它的大小是 \\(\\mathcal{O}(m)\\). 综上所述, L-BFGS 双循环算法是非常高效的."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.673,
                0.729,
                0.691
            ],
            "angle": 0,
            "content": "近似矩阵 \\(\\hat{H}^{k - m}\\) 的取法可以是对角矩阵 \\(\\hat{H}^{k - m} = \\gamma_k I\\) ，其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.473,
                0.698,
                0.825,
                0.737
            ],
            "angle": 0,
            "content": "\\[\n\\gamma_ {k} = \\frac {\\left(s ^ {k - 1}\\right) ^ {\\mathrm {T}} y ^ {k - 1}}{\\left(y ^ {k - 1}\\right) ^ {\\mathrm {T}} y ^ {k - 1}}. \\tag {6.5.30}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.744,
                0.654,
                0.761
            ],
            "angle": 0,
            "content": "注意，这恰好是BB方法的第一个步长，见(6.2.9)式"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.765,
                0.825,
                0.823
            ],
            "angle": 0,
            "content": "至此我们基本介绍了 L-BFGS 方法的迭代格式（见算法 6.7），下面从另一个角度出发来重新理解这个格式，进而可以直接给出 L-BFGS 格式下拟牛顿矩阵的形式。为了讨论问题方便，我们引入新的记号"
        },
        {
            "type": "equation",
            "bbox": [
                0.373,
                0.834,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nS ^ {k} = \\left[ s ^ {0}, s ^ {1}, \\dots , s ^ {k - 1} \\right], \\quad Y ^ {k} = \\left[ y ^ {0}, y ^ {1}, \\dots , y ^ {k - 1} \\right]. \\tag {6.5.31}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.317,
                0.133
            ],
            "angle": 0,
            "content": "6.5 拟牛顿类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "263"
        },
        {
            "type": "code_caption",
            "bbox": [
                0.172,
                0.159,
                0.336,
                0.175
            ],
            "angle": 0,
            "content": "算法6.7L-BFGS方法"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.179,
                0.476,
                0.198
            ],
            "angle": 0,
            "content": "1. 选择初始点 \\(x^0\\) ，参数 \\(m > 0\\) ， \\(k\\gets 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.201,
                0.399,
                0.217
            ],
            "angle": 0,
            "content": "2. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.221,
                0.373,
                0.238
            ],
            "angle": 0,
            "content": "3. 选取近似矩阵 \\(\\hat{H}^{k - m}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.241,
                0.551,
                0.26
            ],
            "angle": 0,
            "content": "4. 使用算法6.6计算下降方向 \\(d^{k} = -H^{k}\\nabla f(x^{k})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.263,
                0.572,
                0.28
            ],
            "angle": 0,
            "content": "5. 使用线搜索算法计算满足Wolfe准则的步长 \\(\\alpha_{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.282,
                0.384,
                0.3
            ],
            "angle": 0,
            "content": "6. 更新 \\(x^{k + 1} = x^k +\\alpha_kd^k\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.305,
                0.322,
                0.32
            ],
            "angle": 0,
            "content": "7. if \\( k > m \\) then"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.324,
                0.456,
                0.342
            ],
            "angle": 0,
            "content": "8. 从内存空间中删除 \\(s^{k - m},y^{k - m}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.346,
                0.269,
                0.361
            ],
            "angle": 0,
            "content": "9. end if"
        },
        {
            "type": "text",
            "bbox": [
                0.179,
                0.365,
                0.62,
                0.384
            ],
            "angle": 0,
            "content": "10. 计算并保存 \\( s^k = x^{k+1} - x^k \\)，\\( y^k = \\nabla f(x^{k+1}) - \\nabla f(x^k) \\)."
        },
        {
            "type": "text",
            "bbox": [
                0.178,
                0.388,
                0.297,
                0.403
            ],
            "angle": 0,
            "content": "11. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.178,
                0.409,
                0.283,
                0.422
            ],
            "angle": 0,
            "content": "12. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.178,
                0.179,
                0.62,
                0.422
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.451,
                0.678,
                0.468
            ],
            "angle": 0,
            "content": "引入矩阵记号的目的是使得BFGS格式(6.5.13)有更紧凑的表达形式"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.482,
                0.737,
                0.54
            ],
            "angle": 0,
            "content": "定理6.10 设 \\(H^0\\) 为BFGS格式的初始矩阵，且是对称正定的，又设 \\(k\\) 个向量对 \\(\\{s^i, y^i\\}_{i=0}^{k-1}\\) 满足 \\((s^i)^{\\mathrm{T}}y^i > 0\\)，\\(H^k\\) 是BFGS格式(6.5.13)产生的拟牛顿矩阵，则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.207,
                0.549,
                0.737,
                0.595
            ],
            "angle": 0,
            "content": "\\[\nH ^ {k} = H ^ {0} + \\left[ \\begin{array}{l l} S ^ {k} & H ^ {0} Y ^ {k} \\end{array} \\right] \\left[ \\begin{array}{c c} W ^ {k} & - (R ^ {k}) ^ {- \\mathrm {T}} \\\\ - (R ^ {k}) ^ {- 1} & 0 \\end{array} \\right] \\left[ \\begin{array}{l} (S ^ {k}) ^ {\\mathrm {T}} \\\\ (Y ^ {k}) ^ {\\mathrm {T}} H ^ {0} \\end{array} \\right], \\tag {6.5.32}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.604,
                0.208,
                0.619
            ],
            "angle": 0,
            "content": "其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.295,
                0.621,
                0.611,
                0.643
            ],
            "angle": 0,
            "content": "\\[\nW ^ {k} = \\left(\\left(R ^ {k}\\right) ^ {- 1}\\right) ^ {\\mathrm {T}} \\left(D ^ {k} + \\left(Y ^ {k}\\right) ^ {\\mathrm {T}} H ^ {0} Y ^ {k}\\right) \\left(R ^ {k}\\right) ^ {- 1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.652,
                0.471,
                0.669
            ],
            "angle": 0,
            "content": "矩阵 \\(R^{k}\\) 是 \\(k \\times k\\) 上三角矩阵，其元素为"
        },
        {
            "type": "equation",
            "bbox": [
                0.342,
                0.68,
                0.564,
                0.732
            ],
            "angle": 0,
            "content": "\\[\n(R ^ {k}) _ {i j} = \\left\\{ \\begin{array}{l l} (s ^ {i - 1}) ^ {\\mathrm {T}} y ^ {i - 1}, & i \\leqslant j, \\\\ 0, & i > j. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.742,
                0.591,
                0.762
            ],
            "angle": 0,
            "content": "\\(D^{k} = \\mathrm{Diag}\\big((s^{0})^{\\mathrm{T}}y^{0},(s^{1})^{\\mathrm{T}}y^{1},\\dots ,(s^{k - 1})^{\\mathrm{T}}y^{k - 1}\\big)\\) 为对角矩阵."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.773,
                0.737,
                0.853
            ],
            "angle": 0,
            "content": "该定理证明比较烦琐，见 [37]。它的重要意义在于可在给定 \\(H^0, S^k, Y^k\\) 的条件下直接计算出 BFGS 迭代矩阵 \\(H^k\\)。如果 \\(H^0\\) 是一个近似矩阵，那么 (6.5.32)式将给出 L-BFGS 格式迭代矩阵的显式格式。我们注意到格式(6.5.32)非常紧凑，直接使用矩阵的语言表达，因此从实现和理解上也比双循环算法"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "264"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "直观. 格式(6.5.32)也可直接用于L-BFGS的计算, 但总计算量要比算法6.6多一个常数量级. 然而(6.5.32)涉及矩阵操作, 在现代计算机实现下可以使用 BLAS2 (BLAS3) 操作更高效地执行, 实际效果很可能赶超双循环算法."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.219,
                0.752,
                0.235
            ],
            "angle": 0,
            "content": "利用 SMW 公式(B.1.2)可以求出 BFGS 基于 \\(B^{k}\\) 的块迭代格式"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.248,
                0.825,
                0.306
            ],
            "angle": 0,
            "content": "定理6.11 设 \\(B^0\\) 为BFGS格式的初始矩阵，且是对称正定的，又设 \\(k\\) 个向量对 \\(\\{s^i,y^i\\}_{i = 0}^{k - 1}\\) 满足 \\((s^i)^{\\mathrm{T}}y^i >0\\) ， \\(B^{k}\\) 是BFGS格式(6.5.12)产生的拟牛顿矩阵，则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.305,
                0.315,
                0.826,
                0.363
            ],
            "angle": 0,
            "content": "\\[\nB ^ {k} = B ^ {0} - \\left[ \\begin{array}{l l} B ^ {0} S ^ {k} & Y ^ {k} \\end{array} \\right] \\left[ \\begin{array}{c c} (S ^ {k}) ^ {\\mathrm {T}} B ^ {0} S ^ {k} & L ^ {k} \\\\ (L ^ {k}) ^ {\\mathrm {T}} & - D ^ {k} \\end{array} \\right] ^ {- 1} \\left[ \\begin{array}{c} (S ^ {k}) ^ {\\mathrm {T}} B ^ {0} \\\\ (Y ^ {k}) ^ {\\mathrm {T}} \\end{array} \\right], \\tag {6.5.33}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.371,
                0.591,
                0.387
            ],
            "angle": 0,
            "content": "其中 \\(L^{k}\\) 是 \\(k \\times k\\) 严格下三角矩阵，其元素为"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.397,
                0.652,
                0.448
            ],
            "angle": 0,
            "content": "\\[\n(L ^ {k}) _ {i j} = \\left\\{ \\begin{array}{l l} (s ^ {i - 1}) ^ {\\mathrm {T}} y ^ {i - 1}, & i > j, \\\\ 0, & i \\leqslant j. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.457,
                0.621,
                0.476
            ],
            "angle": 0,
            "content": "\\(D^{k} = \\mathrm{Diag}\\big((s^{0})^{\\mathrm{T}}y^{0},\\dots ,(s^{k - 1})^{\\mathrm{T}}y^{k - 1}\\big)\\) 为对角矩阵."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.488,
                0.825,
                0.587
            ],
            "angle": 0,
            "content": "正因为L-BFGS方法的出现，人们可以使用拟牛顿类算法求解优化问题。虽然有关L-BFGS方法的收敛性质依然很有限，但在实际应用中L-BFGS方法很快成为了应用最广泛的拟牛顿类算法。比较有趣的是，尽管DFP公式和BFGS公式呈对偶关系，但极少有人研究有限内存的DFP格式，这也使得BFGS格式在地位上比DFP格式略胜一筹。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.613,
                0.401,
                0.631
            ],
            "angle": 0,
            "content": "6.5.5 应用举例"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.645,
                0.38,
                0.661
            ],
            "angle": 0,
            "content": "1. 基追踪问题"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.675,
                0.474,
                0.691
            ],
            "angle": 0,
            "content": "考虑基追踪问题(4.1.8)："
        },
        {
            "type": "equation",
            "bbox": [
                0.437,
                0.705,
                0.825,
                0.729
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| x \\| _ {1}, \\quad \\text {s . t .} \\quad A x = b, \\tag {6.5.34}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.738,
                0.825,
                0.817
            ],
            "angle": 0,
            "content": "其中 \\(A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^{m}\\) 为给定的矩阵和向量. 这是一个约束优化问题, 如何将其转化为一个无约束优化问题呢? 自然地, 我们可以考虑其对偶问题. 由于问题(6.5.34)的对偶问题的无约束优化形式不是可微的, 即无法计算梯度 (读者可以自行验证), 我们考虑如下正则化问题:"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.825,
                0.825,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| x \\| _ {1} + \\frac {1}{2 \\alpha} \\| x \\| _ {2} ^ {2}, \\quad \\text {s . t .} \\quad A x = b, \\tag {6.5.35}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "6.5 拟牛顿类算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "265"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.219
            ],
            "angle": 0,
            "content": "这里 \\(\\alpha > 0\\) 为正则化参数。显然，当 \\(\\alpha\\) 趋于无穷大时，问题(6.5.35)的解会逼近(6.5.34)的解。由于问题(6.5.35)的目标函数是强凸的，其对偶问题的无约束优化形式的目标函数是可微的。具体地，问题(6.5.35)的对偶问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.25,
                0.232,
                0.739,
                0.262
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {y \\in \\mathbb {R} ^ {m}} f (y) = - b ^ {\\mathrm {T}} y + \\frac {\\alpha}{2} \\| A ^ {\\mathrm {T}} y - \\mathcal {P} _ {[ - 1, 1 ] ^ {n}} \\left(A ^ {\\mathrm {T}} y\\right) \\| _ {2} ^ {2}, \\tag {6.5.36}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.28,
                0.67,
                0.298
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{P}_{[-1,1]^n}(x)\\) 为 \\(x\\) 到集合 \\([-1,1]^n\\) 的投影．通过简单计算，可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.299,
                0.317,
                0.611,
                0.337
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (y) = - b + \\alpha A \\left(A ^ {\\mathrm {T}} y - \\mathcal {P} _ {[ - 1, 1 ] ^ {n}} \\left(A ^ {\\mathrm {T}} y\\right)\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.358,
                0.737,
                0.397
            ],
            "angle": 0,
            "content": "那么，我们可以利用L-BFGS方法来求解问题(6.5.36).在得到该问题的解 \\(y^{*}\\) 之后，问题(6.5.35)的解 \\(x^{*}\\) 可通过下式近似得到："
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.416,
                0.574,
                0.436
            ],
            "angle": 0,
            "content": "\\[\nx ^ {*} \\approx \\alpha \\left(A ^ {\\mathrm {T}} y ^ {*} - \\mathcal {P} _ {[ - 1, 1 ] ^ {n}} \\left(A ^ {\\mathrm {T}} y ^ {*}\\right)\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.456,
                0.739,
                0.515
            ],
            "angle": 0,
            "content": "进一步地，文章[205]中证明，当 \\(\\alpha\\) 充分大时，问题(6.5.35)的解就是原问题(6.5.34)的解．因此，我们可以通过选取合适的 \\(\\alpha\\) ，通过求解问题(6.5.36)来得到问题(6.5.34)的解"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.522,
                0.74,
                0.58
            ],
            "angle": 0,
            "content": "我们用第 6.2 小节中的 \\(A\\) 和 \\(b\\)，分别选取 \\(\\alpha = 5,10\\)，调用 L-BFGS 方法求解问题(6.5.36)，其中内存长度取为 5。迭代收敛过程见图 6.13。从图中我们可以看到，当靠近最优解时，L-BFGS 方法的迭代点列呈 Q-线性收敛。"
        },
        {
            "type": "image",
            "bbox": [
                0.268,
                0.598,
                0.642,
                0.82
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.374,
                0.83,
                0.534,
                0.848
            ],
            "angle": 0,
            "content": "图6.13 基追踪问题"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "266"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.157,
                0.398,
                0.174
            ],
            "angle": 0,
            "content": "2. 逻辑回归问题"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.187,
                0.437,
                0.204
            ],
            "angle": 0,
            "content": "考虑逻辑回归问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.211,
                0.825,
                0.248
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad \\ell (x) \\stackrel {\\text {d e f}} {=} \\frac {1}{m} \\sum_ {i = 1} ^ {m} \\ln \\left(1 + \\exp \\left(- b _ {i} a _ {i} ^ {\\mathrm {T}} x\\right)\\right) + \\lambda \\| x \\| _ {2} ^ {2}, \\tag {6.5.37}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.258,
                0.826,
                0.323
            ],
            "angle": 0,
            "content": "这里，选取 \\(\\lambda = \\frac{1}{100m}\\)。目标函数的导数计算可以参考上一节。同样地，我们选取 LIBSVM 上的数据集，调用 L-BFGS（内存长度取为 5）求解代入数据集后的问题(6.5.37)，其迭代收敛过程见图 6.14。"
        },
        {
            "type": "image",
            "bbox": [
                0.357,
                0.333,
                0.727,
                0.557
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.453,
                0.568,
                0.631,
                0.585
            ],
            "angle": 0,
            "content": "图6.14 逻辑回归问题"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.606,
                0.825,
                0.685
            ],
            "angle": 0,
            "content": "从 ijCNN 数据集的迭代结果可以看到，当靠近最优解时，L-BFGS 方法的迭代点列呈 Q-线性收敛。对于 a9a 和 CINA 数据集，由于对应的海瑟矩阵的谱分布不同，图中的迭代还没有进入 LBFGS 算法的线性收敛区域，因而会产生一些抖动，但总体是呈下降趋势。"
        },
        {
            "type": "title",
            "bbox": [
                0.449,
                0.715,
                0.634,
                0.736
            ],
            "angle": 0,
            "content": "6.6 信赖域算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.753,
                0.827,
                0.853
            ],
            "angle": 0,
            "content": "本节介绍信赖域算法。它和线搜索算法都是借助泰勒展开来对目标函数进行局部近似，但它们看待近似函数的方式不同。在线搜索算法中，我们先利用近似模型求出下降方向，然后给定步长；而在信赖域类算法中，我们直接在一个有界区域内求解这个近似模型，而后迭代到下一个点。因此信赖域算法实际上是同时选择了方向和步长。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.299,
                0.133
            ],
            "angle": 0,
            "content": "6.6 信赖域算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "267"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.156,
                0.373,
                0.174
            ],
            "angle": 0,
            "content": "6.6.1 信赖域算法框架"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.188,
                0.737,
                0.225
            ],
            "angle": 0,
            "content": "我们对信赖域算法给一个直观的数学表达. 根据带拉格朗日余项的泰勒展开,"
        },
        {
            "type": "equation",
            "bbox": [
                0.255,
                0.226,
                0.652,
                0.256
            ],
            "angle": 0,
            "content": "\\[\nf (x ^ {k} + d) = f (x ^ {k}) + \\nabla f (x ^ {k}) ^ {\\mathrm {T}} d + \\frac {1}{2} d ^ {\\mathrm {T}} \\nabla^ {2} f (x ^ {k} + t d) d,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.264,
                0.738,
                0.302
            ],
            "angle": 0,
            "content": "其中 \\(t \\in (0,1)\\) 为和 \\(d\\) 有关的正数。和牛顿法相同，我们利用 \\(f(x)\\) 的一个二阶近似来刻画 \\(f(x)\\) 在点 \\(x = x^k\\) 处的性质："
        },
        {
            "type": "equation",
            "bbox": [
                0.309,
                0.314,
                0.737,
                0.345
            ],
            "angle": 0,
            "content": "\\[\nm _ {k} (d) = f \\left(x ^ {k}\\right) + \\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} d + \\frac {1}{2} d ^ {\\mathrm {T}} B ^ {k} d, \\tag {6.6.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.358,
                0.737,
                0.417
            ],
            "angle": 0,
            "content": "其中 \\(B^{k}\\) 是对称矩阵．这里要求 \\(B^{k}\\) 是海瑟矩阵的近似矩阵，如果 \\(B^{k}\\) 恰好是函数 \\(f(x)\\) 在点 \\(x = x^{k}\\) 处的海瑟矩阵，那么当 \\(f(x)\\) 充分光滑时，\\(m_{k}(d)\\) 的逼近误差是 \\(O(\\| d\\|^3)\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.421,
                0.738,
                0.5
            ],
            "angle": 0,
            "content": "我们使用了二阶泰勒展开来近似目标函数 \\( f(x) \\)，但还需要考虑到泰勒展开是函数的局部性质，它仅仅对模长较小的 \\( d \\) 有意义。当 \\( d \\) 过长时，模型(6.6.1)便不再能刻画 \\( f(x) \\) 的特征，为此需要对模型(6.6.1)添加约束。我们仅在如下球内考虑 \\( f(x) \\) 的近似："
        },
        {
            "type": "equation",
            "bbox": [
                0.352,
                0.517,
                0.555,
                0.536
            ],
            "angle": 0,
            "content": "\\[\n\\Omega_ {k} = \\left\\{x ^ {k} + d \\mid \\| d \\| \\leqslant \\Delta_ {k} \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.552,
                0.739,
                0.631
            ],
            "angle": 0,
            "content": "其中 \\(\\Delta_{k} > 0\\) 是一个和迭代有关的参数。我们称 \\(\\Omega_{k}\\) 为信赖域，\\(\\Delta_{k}\\) 为信赖域半径。从命名方式也可看出，信赖域就是我们相信 \\(m_{k}(d)\\) 能很好地近似 \\(f(x)\\) 的区域，而 \\(\\Delta_{k}\\) 表示了这个区域的大小。因此信赖域算法每一步都需要求解如下子问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.336,
                0.637,
                0.737,
                0.661
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {d \\in \\mathbb {R} ^ {n}} m _ {k} (d), \\quad \\text {s . t .} \\quad \\| d \\| \\leqslant \\Delta_ {k}. \\tag {6.6.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.67,
                0.739,
                0.79
            ],
            "angle": 0,
            "content": "图6.15显示了子问题(6.6.2)的求解过程。图中实线表示 \\(f(x)\\) 的等高线，虚线表示 \\(m_{k}(d)\\) 的等高线（这里有关系 \\(d = x - x^{k}\\)），\\(d_{N}^{k}\\) 表示求解无约束问题得到的下降方向（若 \\(B^{k}\\) 为海瑟矩阵则 \\(d_{N}^{k}\\) 为牛顿方向），\\(d_{TR}^{k}\\) 表示求解信赖域子问题(6.6.2)得到的下降方向，可以看到二者明显是不同的。信赖域算法正是限制了 \\(d\\) 的大小，使得迭代更加保守，因此可以在牛顿方向很差时发挥作用。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.795,
                0.738,
                0.854
            ],
            "angle": 0,
            "content": "在子问题(6.6.2)中仍需要确定信赖域半径 \\(\\Delta_{k}\\). 实际上, 选取信赖域半径非常关键, 它决定了算法的收敛性. 考虑到信赖域半径是 “对模型 \\(m_{k}(d)\\) 相信的程度”, 如果 \\(m_{k}(d)\\) 对函数 \\(f(x)\\) 近似较好, 就应该扩大信赖域半径, 在"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "268"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "image",
            "bbox": [
                0.337,
                0.171,
                0.75,
                0.421
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.428,
                0.449,
                0.657,
                0.466
            ],
            "angle": 0,
            "content": "图6.15 信赖域算法一步迭代"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.492,
                0.825,
                0.531
            ],
            "angle": 0,
            "content": "更大的区域内使用这个近似，反之就应该减小信赖域半径重新计算。我们引入如下定义来衡量 \\(m_{k}(d)\\) 近似程度的好坏："
        },
        {
            "type": "equation",
            "bbox": [
                0.446,
                0.54,
                0.826,
                0.577
            ],
            "angle": 0,
            "content": "\\[\n\\rho_ {k} = \\frac {f \\left(x ^ {k}\\right) - f \\left(x ^ {k} + d ^ {k}\\right)}{m _ {k} (0) - m _ {k} \\left(d ^ {k}\\right)}, \\tag {6.6.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.587,
                0.827,
                0.708
            ],
            "angle": 0,
            "content": "其中 \\(d^{k}\\) 为求解子问题(6.6.2)得到的迭代方向。根据 \\(\\rho_{k}\\) 的定义我们知道，它是函数值实际下降量与预估下降量（即二阶近似模型下降量）的比值。如果 \\(\\rho_{k}\\) 接近于1，说明用 \\(m_{k}(d)\\) 来近似 \\(f(x)\\) 是比较成功的，我们应该扩大 \\(\\Delta_{k}\\)；如果 \\(\\rho_{k}\\) 非常小甚至为负，就说明我们过分地相信了二阶模型 \\(m_{k}(d)\\)，此时应该缩小 \\(\\Delta_{k}\\)。使用这个机制可以动态调节 \\(\\Delta_{k}\\)，让二阶模型 \\(m_{k}(d)\\) 的定义域处于一个合适的范围。"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.712,
                0.827,
                0.832
            ],
            "angle": 0,
            "content": "算法 6.8 给出完整的信赖域方法。该算法虽然有一些参数, 但是它对这些参数的取值并不敏感。实际中可取 \\(\\bar{\\rho}_{1} = 0.25, \\bar{\\rho}_{2} = 0.75\\) 以及 \\(\\gamma_{1} = 0.25, \\gamma_{2} = 2\\)。注意, 信赖域半径 \\(\\Delta_{k}\\) 不会无限增长, 一是因为它有上界的控制, 二是如果信赖域约束不起作用 (即二次模型最优值处于信赖域内), 我们也无需增加信赖域半径。只有当 \\(m_{k}(d)\\) 近似足够好并且信赖域约束起作用时, 才需要增加 \\(\\Delta_{k}\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.291,
                0.836,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "在算法 6.8 中只剩下一个关键问题没有说明：如何求解信赖域子问"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "6.6 信赖域算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "269"
        },
        {
            "type": "title",
            "bbox": [
                0.172,
                0.159,
                0.326,
                0.175
            ],
            "angle": 0,
            "content": "算法6.8信赖域算法"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.18,
                0.61,
                0.198
            ],
            "angle": 0,
            "content": "1. 给定最大半径 \\(\\Delta_{\\mathrm{max}}\\) ，初始半径 \\(\\Delta_0\\) ，初始点 \\(x^0\\) ， \\(k\\gets 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.201,
                0.533,
                0.219
            ],
            "angle": 0,
            "content": "2. 给定参数 \\(0 \\leqslant \\eta < \\bar{\\rho}_1 < \\bar{\\rho}_2 < 1\\) ， \\(\\gamma_1 < 1 < \\gamma_2\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.223,
                0.4,
                0.238
            ],
            "angle": 0,
            "content": "3. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.242,
                0.482,
                0.259
            ],
            "angle": 0,
            "content": "4. 计算子问题(6.6.2)得到迭代方向 \\(d^{k}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.264,
                0.43,
                0.281
            ],
            "angle": 0,
            "content": "5. 根据(6.6.3)式计算下降率 \\(\\rho_{k}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.285,
                0.351,
                0.301
            ],
            "angle": 0,
            "content": "6. 更新信赖域半径："
        },
        {
            "type": "list",
            "bbox": [
                0.18,
                0.18,
                0.61,
                0.301
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.275,
                0.312,
                0.677,
                0.388
            ],
            "angle": 0,
            "content": "\\[\n\\Delta_ {k + 1} = \\left\\{ \\begin{array}{l l} {\\gamma_ {1} \\Delta_ {k},} & {\\rho_ {k} <   \\bar {\\rho} _ {1},} \\\\ {\\min  \\{\\gamma_ {2} \\Delta_ {k}, \\Delta_ {\\max } \\},} & {\\rho_ {k} > \\bar {\\rho} _ {2} \\text {以 及} \\| d ^ {k} \\| = \\Delta_ {k},} \\\\ {\\Delta_ {k},} & {\\text {其 他}.} \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.41,
                0.315,
                0.426
            ],
            "angle": 0,
            "content": "7. 更新自变量："
        },
        {
            "type": "equation",
            "bbox": [
                0.214,
                0.437,
                0.73,
                0.488
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\left\\{ \\begin{array}{l l} x ^ {k} + d ^ {k}, & \\rho_ {k} > \\eta , \\\\ x ^ {k}, & \\text {其 他 .} \\end{array} \\quad / \\star \\text {只 有 下 降 比 例 足 够 大 才 更 新} \\star / \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.51,
                0.298,
                0.525
            ],
            "angle": 0,
            "content": "8. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.531,
                0.284,
                0.545
            ],
            "angle": 0,
            "content": "9. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.51,
                0.298,
                0.545
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.571,
                0.467,
                0.587
            ],
            "angle": 0,
            "content": "题(6.6.2)? 下一个小节将给出两种方法"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.613,
                0.393,
                0.631
            ],
            "angle": 0,
            "content": "6.6.2 信赖域子问题求解"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.644,
                0.737,
                0.683
            ],
            "angle": 0,
            "content": "在多数实际应用中，信赖域子问题(6.6.2)的解是无法显式写出的。为了求出迭代方向 \\(d^{k}\\)，我们需要设计算法快速或近似求解子问题(6.6.2)。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.705,
                0.259,
                0.722
            ],
            "angle": 0,
            "content": "1. 迭代法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.735,
                0.737,
                0.793
            ],
            "angle": 0,
            "content": "信赖域子问题是一个仅仅涉及二次函数的约束优化问题，那么能否用约束优化问题的最优性条件来求解子问题的解呢？下面的定理给出了子问题的最优解 \\( p^* \\) 需要满足的条件："
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.804,
                0.432,
                0.82
            ],
            "angle": 0,
            "content": "定理6.12 \\(d^{*}\\) 是信赖域子问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.271,
                0.826,
                0.737,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad m (d) = f + g ^ {\\mathrm {T}} d + \\frac {1}{2} d ^ {\\mathrm {T}} B d, \\quad \\text {s . t .} \\quad \\| d \\| \\leqslant \\Delta \\tag {6.6.4}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "270"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.664,
                0.172
            ],
            "angle": 0,
            "content": "的全局极小解当且仅当 \\(d^{*}\\) 是可行的且存在 \\(\\lambda \\geqslant 0\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.483,
                0.189,
                0.824,
                0.205
            ],
            "angle": 0,
            "content": "\\[\n(B + \\lambda I) d ^ {*} = - g, \\tag {6.6.5a}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.469,
                0.213,
                0.824,
                0.23
            ],
            "angle": 0,
            "content": "\\[\n\\lambda (\\Delta - \\| d ^ {*} \\|) = 0, \\tag {6.6.5b}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.498,
                0.238,
                0.824,
                0.254
            ],
            "angle": 0,
            "content": "\\[\n(B + \\lambda I) \\succeq 0. \\tag {6.6.5c}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.268,
                0.824,
                0.305
            ],
            "angle": 0,
            "content": "证明．先证明必要性．实际上，我们可以利用KKT条件来直接写出 \\(d^{*}\\) 所满足的关系．问题(6.6.4)的拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.313,
                0.709,
                0.345
            ],
            "angle": 0,
            "content": "\\[\nL (d, \\lambda) = f + g ^ {\\mathrm {T}} d + \\frac {1}{2} d ^ {\\mathrm {T}} B d - \\frac {\\lambda}{2} (\\Delta^ {2} - \\| d \\| ^ {2}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.352,
                0.649,
                0.37
            ],
            "angle": 0,
            "content": "其中乘子 \\(\\lambda \\geqslant 0\\) ，根据KKT条件， \\(d^{*}\\) 是可行解，且"
        },
        {
            "type": "equation",
            "bbox": [
                0.414,
                0.383,
                0.668,
                0.401
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {d} L \\left(d ^ {*}, \\lambda\\right) = \\left(B + \\lambda I\\right) d ^ {*} + g = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.415,
                0.401,
                0.431
            ],
            "angle": 0,
            "content": "此外还有互补条件"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.429,
                0.617,
                0.46
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\lambda}{2} (\\varDelta^ {2} - \\| d ^ {*} \\| ^ {2}) = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.463,
                0.825,
                0.501
            ],
            "angle": 0,
            "content": "以上两式整理后就是(6.6.5a)式和(6.6.5b)式．为了证明(6.6.5c)式，我们任取\\(d\\) 满足 \\(\\| d\\| = \\Delta\\) ，根据最优性，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.378,
                0.51,
                0.705,
                0.541
            ],
            "angle": 0,
            "content": "\\[\nm (d) \\geqslant m (d ^ {*}) = m (d ^ {*}) + \\frac {\\lambda}{2} (\\| d ^ {*} \\| ^ {2} - \\| d \\| ^ {2}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.549,
                0.573,
                0.566
            ],
            "angle": 0,
            "content": "利用(6.6.5a)式消去 \\(g\\)，代入上式整理可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.424,
                0.579,
                0.66,
                0.597
            ],
            "angle": 0,
            "content": "\\[\n(d - d ^ {*}) ^ {\\mathrm {T}} (B + \\lambda I) (d - d ^ {*}) \\geqslant 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.611,
                0.482,
                0.627
            ],
            "angle": 0,
            "content": "由任意性可知 \\(B + \\lambda I\\) 半正定"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.632,
                0.521,
                0.648
            ],
            "angle": 0,
            "content": "再证明充分性. 定义辅助函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.655,
                0.728,
                0.688
            ],
            "angle": 0,
            "content": "\\[\n\\hat {m} (d) = f + g ^ {\\mathrm {T}} d + \\frac {1}{2} d ^ {\\mathrm {T}} (B + \\lambda I) d = m (d) + \\frac {\\lambda}{2} d ^ {\\mathrm {T}} d,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.695,
                0.825,
                0.753
            ],
            "angle": 0,
            "content": "由条件 (6.6.5c) 可知 \\(\\hat{m}(d)\\) 关于 \\(d\\) 是凸函数。根据条件 (6.6.5a)，\\(d^{*}\\) 满足凸函数一阶最优性条件，结合定理 5.5 可推出 \\(d^{*}\\) 是 \\(\\hat{m}(d)\\) 的全局极小值点，进而对任意可行解 \\(d\\)，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.41,
                0.761,
                0.672,
                0.793
            ],
            "angle": 0,
            "content": "\\[\nm (d) \\geqslant m (d ^ {*}) + \\frac {\\lambda}{2} (\\| d ^ {*} \\| ^ {2} - \\| d \\| ^ {2}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.798,
                0.777,
                0.817
            ],
            "angle": 0,
            "content": "由互补条件 (6.6.5b) 可知 \\(\\lambda (\\Delta^{2} - \\| d^{*}\\|^{2}) = 0\\) ，代入上式消去 \\(\\| d^{*}\\|^{2}\\) 得"
        },
        {
            "type": "equation",
            "bbox": [
                0.389,
                0.825,
                0.692,
                0.857
            ],
            "angle": 0,
            "content": "\\[\nm (d) \\geqslant m \\left(d ^ {*}\\right) + \\frac {\\lambda}{2} \\left(\\Delta^ {2} - \\| d \\| ^ {2}\\right) \\geqslant m \\left(d ^ {*}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.808,
                0.835,
                0.825,
                0.847
            ],
            "angle": 0,
            "content": "□"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.299,
                0.133
            ],
            "angle": 0,
            "content": "6.6 信赖域算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "271"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.195
            ],
            "angle": 0,
            "content": "定理6.12提供了问题维数 \\(n\\) 较小时寻找 \\(d^{*}\\) 的一个方法. 根据 (6.6.5a) 式, 最优解是以 \\(\\lambda\\) 为参数的一族向量. 我们定义"
        },
        {
            "type": "equation",
            "bbox": [
                0.367,
                0.211,
                0.737,
                0.23
            ],
            "angle": 0,
            "content": "\\[\nd (\\lambda) = - (B + \\lambda I) ^ {- 1} g, \\tag {6.6.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.246,
                0.744,
                0.41
            ],
            "angle": 0,
            "content": "则只需要寻找合适的 \\(\\lambda\\) 使得(6.6.5b)式和(6.6.5c)式成立即可．根据互补条件(6.6.5b)，当 \\(\\lambda >0\\) 时必有 \\(\\| d(\\lambda)\\| = \\Delta\\) ；根据半正定条件(6.6.5c)， \\(\\lambda\\) 须大于等于 \\(B\\) 的最小特征值的相反数．现在研究 \\(\\| d(\\lambda)\\|\\) 随 \\(\\lambda\\) 变化的性质．设\\(B\\) 有特征值分解 \\(B = Q\\Lambda Q^{\\mathrm{T}}\\) ，其中 \\(Q = [q_{1},q_{2},\\dots ,q_{n}]\\) 是正交矩阵， \\(\\Lambda =\\) \\(\\operatorname {Diag}(\\lambda_1,\\lambda_2,\\dots ,\\lambda_n)\\) 是对角矩阵， \\(\\lambda_1\\leqslant \\lambda_2\\leqslant \\dots \\leqslant \\lambda_n\\) 是 \\(B\\) 的特征值．为了方便，以下仅考虑 \\(\\lambda_1\\leqslant 0\\) 且 \\(\\lambda_{1}\\) 是单特征根的情形．其他情形可类似分析．显然， \\(B + \\lambda I\\) 有特征值分解 \\(B + \\lambda I = Q(\\Lambda +\\lambda I)Q^{\\mathrm{T}}\\) ：对 \\(\\lambda > - \\lambda_1\\geqslant 0\\) ，我们可直接写出 \\(d(\\lambda)\\) 的表达式："
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.42,
                0.737,
                0.458
            ],
            "angle": 0,
            "content": "\\[\nd (\\lambda) = - Q \\left(\\Lambda + \\lambda I\\right) ^ {- 1} Q ^ {\\mathrm {T}} g = - \\sum_ {i = 1} ^ {n} \\frac {q _ {i} ^ {\\mathrm {T}} g}{\\lambda_ {i} + \\lambda} q _ {i}. \\tag {6.6.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.47,
                0.531,
                0.487
            ],
            "angle": 0,
            "content": "这正是 \\(d(\\lambda)\\) 的正交分解，由正交性可容易求出"
        },
        {
            "type": "equation",
            "bbox": [
                0.361,
                0.499,
                0.737,
                0.538
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| d (\\lambda) \\right\\| ^ {2} = \\sum_ {i = 1} ^ {n} \\frac {\\left(q _ {i} ^ {\\mathrm {T}} g\\right) ^ {2}}{\\left(\\lambda_ {i} + \\lambda\\right) ^ {2}}. \\tag {6.6.8}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.55,
                0.741,
                0.588
            ],
            "angle": 0,
            "content": "根据(6.6.8)式可知当 \\(\\lambda > -\\lambda_{1}\\) 且 \\(q_{1}^{\\mathrm{T}}g\\neq 0\\) 时， \\(\\| d(\\lambda)\\| ^2\\) 是关于 \\(\\lambda\\) 的严格减函数，且有"
        },
        {
            "type": "equation",
            "bbox": [
                0.3,
                0.593,
                0.609,
                0.618
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {\\lambda \\to \\infty} \\| d (\\lambda) \\| = 0, \\quad \\lim  _ {\\lambda \\to - \\lambda_ {1} +} \\| d (\\lambda) \\| = + \\infty .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.626,
                0.738,
                0.663
            ],
            "angle": 0,
            "content": "根据连续函数介值定理，\\(\\|d(\\lambda)\\| = \\Delta\\) 的解必存在且唯一．一个典型的例子如图6.16所示."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.668,
                0.741,
                0.727
            ],
            "angle": 0,
            "content": "根据上面的分析，寻找 \\(\\lambda^{*}\\) 已经转化为一个一元方程求根问题，我们可以使用牛顿法求解．在得到最优解 \\(\\lambda^{*}\\) 后，根据(6.6.5a)式即可求出迭代方向\\(d^{*}\\) ，这里略去细节，感兴趣的读者可参考[145]."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.73,
                0.744,
                0.855
            ],
            "angle": 0,
            "content": "此外，在上面的分析中我们假定了 \\(q_{1}^{\\mathrm{T}}g\\neq 0\\) ，在实际中这个条件未必满足．当 \\(q_{1}^{\\mathrm{T}}g = 0\\) 时，(6.6.8)式将没有和 \\(\\lambda_{1}\\) 相关的项．此时未必存在 \\(\\lambda^{*} > - \\lambda_{1}\\) 使得 \\(\\| d(\\lambda^{*})\\| = \\Delta\\) 成立.记 \\(M = \\lim_{\\lambda \\to -\\lambda_1 + }\\| d(\\lambda)\\|\\) ，当 \\(M\\geqslant \\Delta\\) 时，仍然可以根据介值定理得出 \\(\\lambda^{*}(> - \\lambda_{1})\\) 的存在性；而当 \\(M <   \\Delta\\) 时，无法利用前面的分析求出 \\(\\lambda^{*}\\) 和 \\(d^{*}\\) ，此时信赖域子问题变得比较复杂．实际上， \\(q_{1}^{\\mathrm{T}}g = 0\\) 且 \\(M <   \\Delta\\) 的情形被人们称为“困难情形(hard case)”．此情形发生时，区间 \\((-\\lambda_{1}, + \\infty)\\)"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "272"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "image",
            "bbox": [
                0.336,
                0.167,
                0.756,
                0.379
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.411,
                0.405,
                0.673,
                0.423
            ],
            "angle": 0,
            "content": "图6.16 \\(\\| d(\\lambda)\\|^2\\) 随 \\(\\lambda\\) 的变化关系"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.451,
                0.825,
                0.488
            ],
            "angle": 0,
            "content": "中的点无法使得 (6.6.5b) 成立, 而定理 6.12 的结果说明 \\(\\lambda^{*} \\in \\left[-\\lambda_{1}, + \\infty\\right)\\), 因此必有 \\(\\lambda^{*} = -\\lambda_{1}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.492,
                0.825,
                0.529
            ],
            "angle": 0,
            "content": "为了求出 \\(d^{*}\\)，可以利用（奇异）线性方程组(6.6.5a)解的结构，其通解可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.53,
                0.684,
                0.567
            ],
            "angle": 0,
            "content": "\\[\nd (\\alpha) = - \\sum_ {i = 2} ^ {n} \\frac {q _ {i} ^ {\\mathrm {T}} g}{\\lambda_ {i} - \\lambda_ {1}} q _ {i} + \\alpha q _ {1}, \\quad \\alpha \\in \\mathbb {R}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.575,
                0.338,
                0.591
            ],
            "angle": 0,
            "content": "由正交性，"
        },
        {
            "type": "equation",
            "bbox": [
                0.429,
                0.59,
                0.655,
                0.629
            ],
            "angle": 0,
            "content": "\\[\n\\| d (\\alpha) \\| ^ {2} = \\alpha^ {2} + \\sum_ {i = 2} ^ {n} \\frac {\\left(q _ {i} ^ {\\mathrm {T}} g\\right) ^ {2}}{\\left(\\lambda_ {i} - \\lambda_ {1}\\right) ^ {2}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.637,
                0.838,
                0.695
            ],
            "angle": 0,
            "content": "注意在困难情形中有 \\(M = \\sqrt{\\sum_{i=2}^{n} \\frac{(q_i^{\\mathrm{T}}g)^2}{(\\lambda_i - \\lambda_1)^2}} < \\Delta\\)，因此必存在 \\(\\alpha^*\\) 使得 \\(\\|d(\\alpha^*)\\| = \\Delta\\)。这就求出了 \\(d^*\\) 的表达式。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.723,
                0.414,
                0.739
            ],
            "angle": 0,
            "content": "2. 截断共轭梯度法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.753,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "我们再介绍一种信赖域子问题的求解方法。既然问题(6.6.2)的解不易求出，能否写出它的一个近似解呢？Steihaug[172]在1983年对共轭梯度法进行了改造，使其成为能求解问题(6.6.2)的算法。此算法能够应用在大规模问题中，是一种非常有效的信赖域子问题的求解方法。我们知道，子问题(6.6.2)和一般的二次极小化问题相差一个约束，如果先不考虑其中的约束"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.299,
                0.133
            ],
            "angle": 0,
            "content": "6.6 信赖域算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "273"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.195
            ],
            "angle": 0,
            "content": "\\(\\| d \\| \\leqslant \\Delta\\) 而直接使用共轭梯度法求解，在迭代过程中应该能找到合适的迭代点作为信赖域子问题的近似解。这就是截断共轭梯度法的基本思想。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.199,
                0.737,
                0.236
            ],
            "angle": 0,
            "content": "为了介绍截断共轭梯度法，我们简要回顾一下标准共轭梯度法的迭代过程．对于二次极小化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.245,
                0.562,
                0.277
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {d} \\quad q (s) \\stackrel {\\text {d e f}} {=} g ^ {\\mathrm {T}} s + \\frac {1}{2} s ^ {\\mathrm {T}} B s,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.289,
                0.618,
                0.308
            ],
            "angle": 0,
            "content": "给定初值 \\(s^0 = 0, r^0 = g, p^0 = -g\\)，共轭梯度法的迭代过程为"
        },
        {
            "type": "equation",
            "bbox": [
                0.372,
                0.317,
                0.534,
                0.466
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\alpha_ {k + 1} = \\frac {\\| r ^ {k} \\| ^ {2}}{\\left(p ^ {k}\\right) ^ {\\mathrm {T}} B p ^ {k}}, \\\\ s ^ {k + 1} = s ^ {k} + \\alpha_ {k} p ^ {k}, \\\\ r ^ {k + 1} = r ^ {k} + \\alpha_ {k} B p ^ {k}, \\\\ \\beta_ {k} = \\frac {\\| r ^ {k + 1} \\| ^ {2}}{\\| r ^ {k} \\| ^ {2}}, \\\\ p ^ {k + 1} = - r ^ {k + 1} + \\beta_ {k} p ^ {k}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.481,
                0.738,
                0.581
            ],
            "angle": 0,
            "content": "其中迭代序列 \\(\\{s^k\\}\\) 最终的输出即为二次极小化问题的解，算法的终止准则是判断 \\(\\| r^k\\|\\) 是否足够小．截断共轭梯度法则是给标准的共轭梯度法增加了两条终止准则，并对最后一步的迭代点 \\(s^k\\) 进行修正来得到信赖域子问题的解．考虑到矩阵 \\(B\\) 不一定是正定矩阵，在迭代过程中可能会产生如下三种情况："
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.595,
                0.736,
                0.675
            ],
            "angle": 0,
            "content": "(1) \\((p^{k})^{\\mathrm{T}} B p^{k} \\leqslant 0\\), 即 \\(B\\) 不是正定矩阵. 我们知道共轭梯度法不能处理非正定的线性方程组, 遇到这种情况应该立即终止算法. 但根据这个条件也找到了一个负曲率方向, 此时只需要沿着这个方向走到信赖域边界即可."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.688,
                0.736,
                0.749
            ],
            "angle": 0,
            "content": "(2) \\((p^k)^{\\mathrm{T}} B p^k > 0\\) 但 \\(\\| s^{k+1} \\| \\geqslant \\Delta\\), 这表示若继续进行共轭梯度法迭代, 则点 \\(s^{k+1}\\) 将处于信赖域之外或边界上, 此时必须马上停止迭代, 并在 \\(s^k\\) 和 \\(s^{k+1}\\) 之间找一个近似解."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.762,
                0.736,
                0.801
            ],
            "angle": 0,
            "content": "(3) \\((p^{k})^{\\mathrm{T}} B p^{k} > 0\\) 且 \\(\\| r^{k+1}\\|\\) 充分小, 这表示若共轭梯度法成功收敛到信赖域内. 子问题(6.6.2)和不带约束的二次极小化问题是等价的."
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.595,
                0.736,
                0.801
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.815,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "从上述终止条件来看截断共轭梯度法仅仅产生了共轭梯度法的部分迭代点，这也是该方法名字的由来."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "274"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "code_caption",
            "bbox": [
                0.26,
                0.158,
                0.561,
                0.176
            ],
            "angle": 0,
            "content": "算法6.9截断共轭梯度法(Steihaug-CG)"
        },
        {
            "type": "algorithm",
            "bbox": [
                0.269,
                0.18,
                0.707,
                0.625
            ],
            "angle": 0,
            "content": "1. 给定精度 \\(\\varepsilon > 0\\) ，初始化 \\(s^0 = 0, r^0 = g, p^0 = -g\\) ，\\(k \\gets 0\\).\n2. if \\(\\|p^0\\| \\leqslant \\varepsilon\\) then\n3. 算法停止，输出 \\(s = 0\\).\n4. end if\n5. loop\n6. if \\((p^k)^{\\mathrm{T}} B p^k \\leqslant 0\\) then\n7. 计算 \\(\\tau > 0\\) 使得 \\(\\|s^k + \\tau p^k\\| = \\Delta\\)\n8. 算法停止，输出 \\(s = s^k + \\tau p^k\\).\n9. end if\n10. 计算 \\(\\alpha_k = \\frac{\\|r^k\\|^2}{(p^k)^{\\mathrm{T}} B p^k}\\)，更新 \\(s^{k+1} = s^k + \\alpha_k p^k\\).\n11. if \\(\\|s^{k+1}\\| \\geqslant \\Delta\\) then\n12. 计算 \\(\\tau > 0\\) 使得 \\(\\|s^k + \\tau p^k\\| = \\Delta\\)\n13. 算法停止，输出 \\(s = s^k + \\tau p^k\\).\n14. end if\n15. 计算 \\(r^{k+1} = r^k + \\alpha_k B p^k\\).\n16. if \\(\\|r^{k+1}\\| < \\varepsilon \\|r^0\\|\\) then\n17. 算法停止，输出 \\(s = s^{k+1}\\).\n18. end if\n19. 计算 \\(\\beta_k = \\frac{\\|r^{k+1}\\|^2}{\\|r^k\\|^2}\\)，更新 \\(p^{k+1} = -r^{k+1} + \\beta_k p^k\\).\n20. \\(k \\gets k + 1\\).\n21. end loop"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.66,
                0.825,
                0.761
            ],
            "angle": 0,
            "content": "算法6.9给出截断共轭梯度法的迭代过程，其中的三个判断分别对应了之前叙述的三种情况．注意，在不加限制时，下一步迭代点总是为 \\(s^{k + 1} = s^k +\\alpha_kp^k\\) ，当情况(1)发生时，只需要沿着 \\(p^k\\) 走到信赖域边界；当情况(2)发生时，由于 \\(\\| s^k\\| <  \\Delta ,\\| s^k +\\alpha_kp^k\\| \\geqslant \\Delta\\) ，由连续函数介值定理可得 \\(\\tau\\) 必定存在且处于区间 \\((0,\\alpha_{k}]\\) 内."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.769,
                0.825,
                0.806
            ],
            "angle": 0,
            "content": "截断共轭梯度法的迭代序列 \\(\\{s^k\\}\\) 有非常好的性质，实际上我们可以证明如下定理："
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.836,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "定理6.13 设 \\(q(s)\\) 是任意外迭代步信赖域子问题的目标函数，令 \\(\\{s^j\\}\\)"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.299,
                0.133
            ],
            "angle": 0,
            "content": "6.6 信赖域算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "275"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.737,
                0.175
            ],
            "angle": 0,
            "content": "是由算法6.9产生的迭代序列，则在算法终止前 \\(q(s^j)\\) 是严格单调递减的，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.187,
                0.737,
                0.208
            ],
            "angle": 0,
            "content": "\\[\nq \\left(s ^ {j + 1}\\right) <   q \\left(s ^ {j}\\right). \\tag {6.6.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.221,
                0.423,
                0.238
            ],
            "angle": 0,
            "content": "并且 \\(\\| s^j\\|\\) 是严格单调递增的，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.252,
                0.737,
                0.272
            ],
            "angle": 0,
            "content": "\\[\n0 = \\| s ^ {0} \\| <   \\| s ^ {1} \\| <   \\dots <   \\| s ^ {j + 1} \\| <   \\dots \\leqslant \\Delta . \\tag {6.6.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.285,
                0.739,
                0.366
            ],
            "angle": 0,
            "content": "证明．为了方便讨论我们不妨设迭代在第 \\(t\\) 步终止．根据算法6.9，在终止之前， \\((p^j)^\\mathrm{T}Bp^j >0,j < t\\) 是一直成立的．此时的算法就是共轭梯度法，而对共轭梯度法很容易证明(6.6.9)式和(6.6.10)式．注意到 \\(q(s)\\) 在点 \\(s^j\\) 处的梯度就是 \\(r^j\\) ，而根据共轭梯度迭代性质 \\((r^j)^\\mathrm{T}p^i = 0,i < j\\) ，所以"
        },
        {
            "type": "equation",
            "bbox": [
                0.284,
                0.378,
                0.627,
                0.398
            ],
            "angle": 0,
            "content": "\\[\n\\left(r ^ {j}\\right) ^ {\\mathrm {T}} p ^ {j} = \\left(r ^ {j}\\right) ^ {\\mathrm {T}} \\left(- r ^ {j} + \\beta_ {j - 1} p ^ {j - 1}\\right) = - \\left\\| r ^ {j} \\right\\| ^ {2} <   0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.411,
                0.737,
                0.45
            ],
            "angle": 0,
            "content": "即 \\(p^j\\) 是下降方向．而 \\(\\alpha_{j}\\) 的选取恰好为精确线搜索的步长，因此有 \\(q(s^{j + 1}) < q(s^{j})\\) ，此外由 \\(s^j\\) 的定义，"
        },
        {
            "type": "equation",
            "bbox": [
                0.373,
                0.46,
                0.537,
                0.5
            ],
            "angle": 0,
            "content": "\\[\ns ^ {j} = \\sum_ {i = 0} ^ {j - 1} \\alpha_ {i} p ^ {i}, \\quad \\alpha_ {i} > 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.51,
                0.468,
                0.526
            ],
            "angle": 0,
            "content": "再一次根据共轭梯度法的性质，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.287,
                0.536,
                0.622,
                0.576
            ],
            "angle": 0,
            "content": "\\[\n(p ^ {j}) ^ {\\mathrm {T}} s ^ {j} = \\sum_ {i = 0} ^ {j - 1} \\alpha_ {i} (p ^ {j}) ^ {\\mathrm {T}} p ^ {i} = \\sum_ {i = 0} ^ {j - 1} \\alpha_ {i} \\frac {\\| r ^ {j} \\| ^ {2}}{\\| r ^ {i} \\| ^ {2}} \\| p ^ {i} \\| ^ {2} > 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.586,
                0.332,
                0.603
            ],
            "angle": 0,
            "content": "结合以上表达式可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.225,
                0.616,
                0.738,
                0.637
            ],
            "angle": 0,
            "content": "\\[\n\\| s ^ {j + 1} \\| ^ {2} = \\| s ^ {j} + \\alpha_ {j} p ^ {j} \\| ^ {2} = \\| s ^ {j} \\| ^ {2} + 2 \\alpha_ {j} (p ^ {j}) ^ {\\mathrm {T}} s ^ {j} + \\alpha_ {j} ^ {2} \\| p ^ {j} \\| ^ {2} > \\| s ^ {j} \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.65,
                0.676,
                0.667
            ],
            "angle": 0,
            "content": "实际上，我们还可进一步说明算法6.9的输出 \\(s\\) 满足如下关系："
        },
        {
            "type": "equation",
            "bbox": [
                0.353,
                0.681,
                0.554,
                0.701
            ],
            "angle": 0,
            "content": "\\[\nq (s) \\leqslant q (s ^ {t}), \\quad \\| s ^ {t} \\| \\leqslant \\| s \\|,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.714,
                0.694,
                0.732
            ],
            "angle": 0,
            "content": "其中 \\(t\\) 为算法终止时的迭代数．这只需要分别讨论三种终止条件即可."
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.743,
                0.737,
                0.784
            ],
            "angle": 0,
            "content": "(1) 若 \\((p^t)^{\\mathrm{T}} B p^t \\leqslant 0\\)，则 \\(p^t\\) 是负曲率方向，沿着负曲率方向显然有 \\(q(s) \\leqslant q(s^t)\\). 注意到此时 \\(\\|s\\| = \\Delta\\)，因此有 \\(\\|s^t\\| \\leqslant \\|s\\| = \\Delta\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.793,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "(2) 若 \\((p^t)^{\\mathrm{T}} B p^t > 0\\) 但 \\(\\| s^{t+1} \\| \\geqslant \\Delta\\), 根据最速下降法的性质, \\(q(s^t + \\alpha p^t)\\) 关于 \\(\\alpha \\in (0, \\alpha_t]\\) 单调下降, 根据 \\(\\tau\\) 的取法显然有 \\(q(s) \\leqslant q(s^t)\\). 此时依然有 \\(\\|s\\| = \\Delta\\), 因此 \\(\\|s^t\\| \\leqslant \\|s\\| = \\Delta\\) 仍成立."
        },
        {
            "type": "list",
            "bbox": [
                0.181,
                0.743,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "276"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.156,
                0.823,
                0.194
            ],
            "angle": 0,
            "content": "(3) 若 \\(\\left(p^{t}\\right)^{\\mathrm{T}} B p^{t} > 0\\) 且 \\(\\| r^{t+1} \\| \\leqslant \\varepsilon \\| r^{0} \\|\\)，此时算法就是共轭梯度法，结论自然成立."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.215,
                0.827,
                0.398
            ],
            "angle": 0,
            "content": "定理6.13其实可以从以下的观点来看：记 \\(d(\\Delta)\\) 为信赖域子问题(6.6.2)当信赖域半径取 \\(\\Delta\\) 时的解，在这里我们让 \\(\\Delta\\) 从0开始变化，则 \\(d(\\Delta)\\) 的轨迹就是一条单参数曲线．算法6.9实际上是使用由共轭梯度法迭代点列 \\(\\{s^j\\}\\) 确定的折线来逼近这条曲线，并将该折线与信赖域交集中距信赖域中心的最远点作为信赖域子问题的近似解．实际上，许多方法都利用了这种逼近的思想，例如Dogleg方法[158]，双折Dogleg方法[58]．根据定理6.13，由于目标函数的单调递减性，截断共轭梯度法可以保证子问题的求解精度自动满足信赖域方法的收敛条件（收敛性分析将在下一小节中介绍），从而保证理论与数值上有更好的效果."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.437,
                0.42,
                0.455
            ],
            "angle": 0,
            "content": "6.6.3 收敛性分析"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.473,
                0.825,
                0.51
            ],
            "angle": 0,
            "content": "本小节简要介绍信赖域算法的收敛性结果. 我们将着重于介绍定理的条件和最终结论, 而略去较为烦琐的证明部分."
        },
        {
            "type": "title",
            "bbox": [
                0.259,
                0.548,
                0.345,
                0.565
            ],
            "angle": 0,
            "content": "1. 柯西点"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.583,
                0.825,
                0.62
            ],
            "angle": 0,
            "content": "为了估计求解每个信赖域子问题得到的函数值改善情况，我们引入柯西点的定义."
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.641,
                0.825,
                0.678
            ],
            "angle": 0,
            "content": "定义6.6(柯西点)设 \\(m_{k}(d)\\) 是 \\(f(x)\\) 在点 \\(x = x^{k}\\) 处的二阶近似，常数 \\(\\tau_{k}\\) 为如下优化问题的解："
        },
        {
            "type": "equation",
            "bbox": [
                0.424,
                0.695,
                0.657,
                0.736
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\quad m _ {k} (- \\tau \\nabla f (x ^ {k})), \\\\ \\begin{array}{l l} \\text {s . t .} & \\| \\tau \\nabla f (x ^ {k}) \\| \\leqslant \\Delta_ {k}, \\tau \\geqslant 0. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.755,
                0.666,
                0.776
            ],
            "angle": 0,
            "content": "则称 \\(x_{C}^{k}\\stackrel {\\mathrm{def}}{=}x^{k} + d_{C}^{k}\\) 为柯西点，其中 \\(d_C^k = -\\tau_k\\nabla f(x^k)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "根据柯西点的定义，它实际上是对 \\(m_{k}(d)\\) 进行了一次精确线搜索的梯度法，不过这个线搜索是考虑了信赖域约束的。图6.17直观地解释了柯西点的含义。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.299,
                0.133
            ],
            "angle": 0,
            "content": "6.6 信赖域算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "277"
        },
        {
            "type": "image",
            "bbox": [
                0.249,
                0.171,
                0.662,
                0.32
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.391,
                0.349,
                0.518,
                0.365
            ],
            "angle": 0,
            "content": "图6.17 柯西点"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.388,
                0.739,
                0.426
            ],
            "angle": 0,
            "content": "实际上，给定 \\(m_{k}(d)\\) ，柯西点可以显式计算出来．为了方便我们用 \\(g^{k}\\) 表示 \\(\\nabla f(x^k)\\) ，根据 \\(\\tau_{k}\\) 的定义，容易计算出其表达式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.27,
                0.432,
                0.637,
                0.5
            ],
            "angle": 0,
            "content": "\\[\n\\tau_ {k} = \\left\\{ \\begin{array}{l l} {\\frac {\\varDelta_ {k}}{\\| g ^ {k} \\|},} & {(g ^ {k}) ^ {\\mathrm {T}} B ^ {k} g ^ {k} \\leqslant 0,} \\\\ {\\min  \\left\\{\\frac {\\| g ^ {k} \\| ^ {2}}{(g ^ {k}) ^ {\\mathrm {T}} B ^ {k} g ^ {k}}, \\frac {\\varDelta_ {k}}{\\| g ^ {k} \\|} \\right\\},} & {\\text {其 他}.} \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.505,
                0.744,
                0.647
            ],
            "angle": 0,
            "content": "以上的分析表明，柯西点是信赖域子问题(6.6.2)的一个可行点，但在实际计算中人们一般不会将柯西点作为下一步迭代点的近似。这是由于求柯西点本质上为一个带截断步长的最速下降法，它并没有充分利用海瑟矩阵 \\( B^{k} \\) 的信息。即便如此，人们还是可以将柯西点作为信赖域子问题算法的一个评判标准，即要求子问题算法产生的迭代点至少比柯西点要好。而容易看出，若迭代点取为柯西点，二次模型的目标函数值依然是下降的。实际上，我们有如下引理（证明见[145]引理4.3）："
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.656,
                0.721,
                0.674
            ],
            "angle": 0,
            "content": "引理6.3 (柯西点的下降量) 设 \\(d_C^k\\) 为求解柯西点产生的下降方向，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.288,
                0.679,
                0.737,
                0.715
            ],
            "angle": 0,
            "content": "\\[\nm _ {k} (0) - m _ {k} \\left(d _ {C} ^ {k}\\right) \\geqslant \\frac {1}{2} \\| g ^ {k} \\| \\min  \\left\\{\\Delta_ {k}, \\frac {\\| g ^ {k} \\|}{\\| B ^ {k} \\| _ {2}} \\right\\}. \\tag {6.6.11}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.721,
                0.741,
                0.759
            ],
            "angle": 0,
            "content": "而前一小节介绍的子问题求解方法（如迭代法，截断共轭梯度法，Dogleg方法）得到的迭代方向 \\(d^k\\) 均满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.307,
                0.769,
                0.6,
                0.789
            ],
            "angle": 0,
            "content": "\\[\nm _ {k} (0) - m _ {k} \\left(d ^ {k}\\right) \\geqslant c _ {2} \\left(m _ {k} (0) - m _ {k} \\left(d _ {C} ^ {k}\\right)\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.799,
                0.332,
                0.815
            ],
            "angle": 0,
            "content": "这也就意味着估计式"
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.821,
                0.737,
                0.857
            ],
            "angle": 0,
            "content": "\\[\nm _ {k} (0) - m _ {k} \\left(d ^ {k}\\right) \\geqslant \\frac {1}{2} c _ {2} \\left\\| g ^ {k} \\right\\| \\min  \\left\\{\\Delta_ {k}, \\frac {\\left\\| g ^ {k} \\right\\|}{\\left\\| B ^ {k} \\right\\| _ {2}} \\right\\} \\tag {6.6.12}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "278"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.799,
                0.174
            ],
            "angle": 0,
            "content": "在很多情况下都会成立. 这为我们证明信赖域算法的收敛性提供了基础"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.198,
                0.38,
                0.215
            ],
            "angle": 0,
            "content": "2. 全局收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.228,
                0.825,
                0.327
            ],
            "angle": 0,
            "content": "现在介绍信赖域算法的全局收敛性。回顾信赖域算法6.8，我们引入了一个参数 \\(\\eta\\) 来确定是否应该更新迭代点。这分为两种情况：当 \\(\\eta = 0\\) 时，只要原目标函数有下降量就接受信赖域迭代步的更新；当 \\(\\eta > 0\\) 时，只有当改善量 \\(\\rho_{k}\\) 达到一定程度时再进行更新。在这两种情况下得到的收敛性结果是不同的，我们分别介绍这两种结果。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.331,
                0.71,
                0.349
            ],
            "angle": 0,
            "content": "根据[145]定理4.5，在 \\(\\eta = 0\\) 的条件下有如下收敛性定理："
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.362,
                0.826,
                0.44
            ],
            "angle": 0,
            "content": "定理6.14（全局收敛性1）设近似海瑟矩阵 \\(B^k\\) 有界，即 \\(\\| B^k\\| _2\\leqslant M,\\forall k\\) \\(f(x)\\) 在下水平集 \\(\\mathcal{L} = \\{x\\mid f(x)\\leqslant f(x^0)\\}\\) 上有下界，且 \\(\\nabla f(x)\\) 在 \\(\\mathcal{L}\\) 的一个开邻域内利普希茨连续．若 \\(d^{k}\\) 为信赖域子问题的近似解且满足(6.6.12)式，算法6.8选取参数 \\(\\eta = 0\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.458,
                0.456,
                0.624,
                0.48
            ],
            "angle": 0,
            "content": "\\[\n\\liminf_{k\\to \\infty}\\| \\nabla f(x^{k})\\| = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.493,
                0.463,
                0.509
            ],
            "angle": 0,
            "content": "即 \\(x^{k}\\) 的聚点中包含稳定点."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.524,
                0.825,
                0.582
            ],
            "angle": 0,
            "content": "定理 6.14 表明若无条件接受信赖域子问题的更新，则算法 6.8 仅仅有子序列的收敛性，迭代点序列本身不一定收敛。根据 [145] 定理 4.6，下面的定理则说明选取 \\(\\eta > 0\\) 可以改善收敛性结果。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.596,
                0.824,
                0.634
            ],
            "angle": 0,
            "content": "定理6.15(全局收敛性2）在定理6.14的条件下，若算法6.8选取参数\\(\\eta >0\\) ，且信赖域子问题近似解 \\(d^{k}\\) 满足(6.6.12)式，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.648,
                0.613,
                0.672
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\| \\nabla f (x ^ {k}) \\| = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.683,
                0.825,
                0.721
            ],
            "angle": 0,
            "content": "和牛顿类算法不同，信赖域算法具有全局收敛性，因此它对迭代初值选取的要求比较弱。而牛顿法的收敛性极大地依赖初值的选取。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.745,
                0.38,
                0.761
            ],
            "angle": 0,
            "content": "3. 局部收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.774,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "再来介绍信赖域算法的局部收敛性. 在构造信赖域子问题时利用了 \\(f(x)\\) 的二阶信息，它在最优点附近应该具有牛顿法的性质．特别地，当近似矩阵 \\(B^{k}\\) 取为海瑟矩阵 \\(\\nabla^2 f(x^k)\\) 时，根据信赖域子问题的更新方式，二次模型\\(m_{k}(d)\\) 将会越来越逼近原函数 \\(f(x)\\)，最终信赖域约束 \\(\\| d\\| \\leqslant \\Delta_k\\) 将会失效. 此"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.299,
                0.133
            ],
            "angle": 0,
            "content": "6.6 信赖域算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "279"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.195
            ],
            "angle": 0,
            "content": "时信赖域方法将会和牛顿法十分接近，而根据定理6.6，牛顿法有Q-二次收敛的性质，这个性质很自然地会继承到信赖域算法上。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.199,
                0.739,
                0.3
            ],
            "angle": 0,
            "content": "和牛顿法不同的是，在信赖域迭代算法中，我们并不知道迭代点列是否已经接近最优点。即使信赖域半径约束已经不起作用，如果 \\(B^k\\) 没有取为海瑟矩阵 \\(\\nabla^2 f(x^k)\\) 或者信赖域子问题没有精确求解，\\(d^k\\) 一般也不会等于牛顿方向 \\(d_N^k\\)，但是它们的误差往往是越来越小的。根据[145]定理4.9，我们有如下定理："
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.313,
                0.739,
                0.412
            ],
            "angle": 0,
            "content": "定理6.16 设 \\(f(x)\\) 在最优点 \\(x = x^{*}\\) 的一个邻域内二阶连续可微，且 \\(\\nabla f(x)\\) 利普希茨连续，在最优点 \\(x^{*}\\) 处二阶充分条件成立，即 \\(\\nabla^2 f(x) \\succ 0\\)。若迭代点列 \\(\\{x^k\\}\\) 收敛到 \\(x^{*}\\)，且在迭代中选取 \\(B^k\\) 为海瑟矩阵 \\(\\nabla^2 f(x^k)\\)，则对充分大的 \\(k\\)，任意满足(6.6.12)式的信赖域子问题算法产生的迭代方向 \\(d^k\\) 均满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.416,
                0.737,
                0.436
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| d ^ {k} - d _ {N} ^ {k} \\right\\| = o \\left(\\left\\| d _ {N} ^ {k} \\right\\|\\right), \\tag {6.6.13}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.442,
                0.604,
                0.473
            ],
            "angle": 0,
            "content": "其中 \\(d_N^k\\) 为第 \\(k\\) 步迭代的牛顿方向且满足假设 \\(\\| d_N^k\\| \\leqslant \\frac{\\Delta_k}{2}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.48,
                0.739,
                0.518
            ],
            "angle": 0,
            "content": "定理6.16说明若信赖域算法收敛，则当 \\(k\\) 充分大时，信赖域半径的约束终将失效，且算法产生的迭代方向将会越来越接近牛顿方向。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.522,
                0.529,
                0.54
            ],
            "angle": 0,
            "content": "根据定理6.16很容易得到收敛速度的估计"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.554,
                0.737,
                0.591
            ],
            "angle": 0,
            "content": "推论6.3（信赖域算法的局部收敛速度）在定理6.16的条件下，信赖域算法产生的迭代序列 \\(\\{x^k\\}\\) 具有Q-超线性收敛速度。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.606,
                0.441,
                0.623
            ],
            "angle": 0,
            "content": "证明. 根据定理6.6，对牛顿方向，"
        },
        {
            "type": "equation",
            "bbox": [
                0.328,
                0.638,
                0.582,
                0.658
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| x ^ {k} + d _ {N} ^ {k} - x ^ {*} \\right\\| = \\mathcal {O} \\big (\\left\\| x ^ {k} - x ^ {*} \\right\\| ^ {2} \\big),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.673,
                0.586,
                0.692
            ],
            "angle": 0,
            "content": "因此得到估计 \\(\\| d_N^k\\| = \\mathcal{O}(\\| x^k -x^*\\|)\\) .又根据定理6.16,"
        },
        {
            "type": "equation",
            "bbox": [
                0.221,
                0.706,
                0.687,
                0.726
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| x ^ {k} + d ^ {k} - x ^ {*} \\right\\| \\leqslant \\left\\| x ^ {k} + d _ {N} ^ {k} - x ^ {*} \\right\\| + \\left\\| d ^ {k} - d _ {N} ^ {k} \\right\\| = o \\big (\\left\\| x ^ {k} - x ^ {*} \\right\\| \\big).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.741,
                0.462,
                0.759
            ],
            "angle": 0,
            "content": "这说明信赖域算法是Q-超线性收敛的"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "容易看出，若在迭代后期 \\(d^{k} = d_{N}^{k}\\) 能得到满足，则信赖域算法是Q-二次收敛的．很多算法都会有这样的性质，例如前面提到的截断共轭梯度法和Dogleg方法．因此在实际应用中，截断共轭梯度法是最常用的信赖域子问题的求解方法，使用此方法能够同时兼顾全局收敛性和局部Q-二次收敛性."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.291,
                0.13
            ],
            "angle": 0,
            "content": "280"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.402,
                0.174
            ],
            "angle": 0,
            "content": "6.6.4 应用举例"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.187,
                0.437,
                0.204
            ],
            "angle": 0,
            "content": "考虑逻辑回归问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.208,
                0.826,
                0.245
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad \\frac {1}{m} \\sum_ {i = 1} ^ {m} \\ln \\left(1 + \\exp \\left(- b _ {i} a _ {i} ^ {\\mathrm {T}} x\\right)\\right) + \\lambda \\| x \\| _ {2} ^ {2}, \\tag {6.6.14}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.251,
                0.828,
                0.419
            ],
            "angle": 0,
            "content": "这里选取 \\(\\lambda = \\frac{1}{100m}\\). 其导数和海瑟矩阵计算参见第 6.4 节. 同样地, 我们选取 LIBSVM 上的数据集, 调用信赖域算法求解代入数据集后的问题(6.6.14), 其迭代收敛过程见图 6.18. 其中使用截断共轭梯度法来求解信赖域子问题, 精度设置同第 6.4 节的牛顿法一致. 从图中可以看到, 在精确解附近梯度范数具有 Q-超线性收敛性质. 由于这个问题是强凸的, 所以选取一个较大的初始信赖域半径 \\((\\sqrt{n})\\). 在数据集 a9a 和 ijCNN1 的求解中, 信赖域子问题的求解没有因为超出信赖域边界而停机, 因此和第 6.4 节中牛顿法的数值表现一致."
        },
        {
            "type": "image",
            "bbox": [
                0.356,
                0.429,
                0.73,
                0.653
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.453,
                0.666,
                0.631,
                0.683
            ],
            "angle": 0,
            "content": "图6.18 逻辑回归问题"
        },
        {
            "type": "title",
            "bbox": [
                0.376,
                0.728,
                0.707,
                0.751
            ],
            "angle": 0,
            "content": "6.7 非线性最小二乘问题算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.768,
                0.825,
                0.825
            ],
            "angle": 0,
            "content": "本节研究非线性最小二乘问题的算法. 非线性最小二乘问题是一类特殊的无约束优化问题, 它有非常广泛的实际应用背景. 例如在统计中, 我们经常建立如下带参数的模型:"
        },
        {
            "type": "equation",
            "bbox": [
                0.482,
                0.836,
                0.602,
                0.853
            ],
            "angle": 0,
            "content": "\\[\nb = \\phi (a; x) + \\varepsilon ,\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.404,
                0.133
            ],
            "angle": 0,
            "content": "6.7 非线性最小二乘问题算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "281"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.744,
                0.238
            ],
            "angle": 0,
            "content": "其中 \\(a\\) 为自变量，\\(b\\) 为响应变量，它们之间的关系由函数 \\(\\phi(\\cdot; x)\\) 决定且 \\(x\\) 是参数，\\(\\varepsilon\\) 是噪声项，即观测都是有误差的。我们的目的是要根据观测 \\((a_i, b_i)\\)，估计未知参数 \\(x\\) 的值。若 \\(\\varepsilon\\) 服从高斯分布，则使用 \\(\\ell_2\\) 范数平方是处理高斯噪声最好的方式：对 \\(\\ell_2\\) 范数平方损失函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.345,
                0.245,
                0.564,
                0.283
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\frac {1}{m} \\sum_ {i = 1} ^ {m} \\| b _ {i} - \\phi (a _ {i}; x) \\| ^ {2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.292,
                0.741,
                0.33
            ],
            "angle": 0,
            "content": "进行极小化即可求出未知参数 \\(x\\) 的估计．而对 \\(\\ell_2\\) 范数平方损失函数求解极小值就是一个最小二乘问题."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.334,
                0.744,
                0.445
            ],
            "angle": 0,
            "content": "最小二乘问题一般属于无约束优化问题，但由于问题特殊性，人们针对其结构设计了许多算法快速求解。一般地，设 \\(x^{*}\\) 为最小二乘问题的解，根据最优解处残量 \\(\\sum_{i = 1}^{m}\\| b_{i} - \\phi (a_{i};x)\\|^{2}\\) 的大小，可以将最小二乘问题分为小残量问题和大残量问题。本节针对小残量问题介绍两种方法：高斯-牛顿算法和LM方法；而针对大残量问题简要地引入带结构的拟牛顿法。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.467,
                0.414,
                0.487
            ],
            "angle": 0,
            "content": "6.7.1 非线性最小二乘问题"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.499,
                0.487,
                0.516
            ],
            "angle": 0,
            "content": "考虑非线性最小二乘问题的一般形式"
        },
        {
            "type": "equation",
            "bbox": [
                0.383,
                0.524,
                0.739,
                0.565
            ],
            "angle": 0,
            "content": "\\[\nf (x) = \\frac {1}{2} \\sum_ {j = 1} ^ {m} r _ {j} ^ {2} (x), \\tag {6.7.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.573,
                0.741,
                0.613
            ],
            "angle": 0,
            "content": "其中 \\(r_j: \\mathbb{R}^n \\to \\mathbb{R}\\) 是光滑函数，并且假设 \\(m \\geqslant n\\)。我们称 \\(r_j\\) 为残差。为了表述问题的方便，定义残差向量 \\(r: \\mathbb{R}^n \\to \\mathbb{R}^m\\) 为"
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.625,
                0.579,
                0.646
            ],
            "angle": 0,
            "content": "\\[\nr (x) = (r _ {1} (x), r _ {2} (x), \\dots , r _ {m} (x)) ^ {\\mathrm {T}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.654,
                0.576,
                0.681
            ],
            "angle": 0,
            "content": "使用这一定义，函数 \\(f(x)\\) 可以写为 \\(f(x) = \\frac{1}{2}\\| r(x)\\| _2^2\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.682,
                0.741,
                0.72
            ],
            "angle": 0,
            "content": "问题(6.7.1)是一个无约束优化问题，可以使用前面讲过的任何一种算法求解．为此我们直接给出 \\(f(x)\\) 的梯度和海瑟矩阵："
        },
        {
            "type": "equation",
            "bbox": [
                0.312,
                0.732,
                0.738,
                0.752
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x) = J (x) ^ {\\mathrm {T}} r (x), \\tag {6.7.2a}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.307,
                0.756,
                0.739,
                0.793
            ],
            "angle": 0,
            "content": "\\[\n\\nabla^ {2} f (x) = J (x) ^ {\\mathrm {T}} J (x) + \\sum_ {i = 1} ^ {m} r _ {i} (x) \\nabla^ {2} r _ {i} (x), \\tag {6.7.2b}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.803,
                0.744,
                0.857
            ],
            "angle": 0,
            "content": "其中 \\(J(x)\\in \\mathbb{R}^{m\\times n}\\) 是向量值函数 \\(r(x)\\) 在点 \\(x\\) 处的雅可比矩阵．这里指出，\\(\\nabla^2 f(x)\\) 在形式上分为两部分，分别为 \\(J(x)^{\\mathrm{T}}J(x)\\) 和 \\(\\sum_{i = 1}^{m}r_{i}(x)\\nabla^{2}r_{i}(x)\\) ，处理这"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "282"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.826,
                0.236
            ],
            "angle": 0,
            "content": "两部分的难度是截然不同的: 注意到计算 \\(\\nabla f(x)\\) 时需要 \\(r(x)\\) 的雅可比矩阵,因此海瑟矩阵的前一项是自然得到的, 不需要进行额外计算; 而海瑟矩阵的第二项则需要计算每个 \\(\\nabla^2 r_i(x)\\), 这会导致额外计算量, 因此很多最小二乘算法就是根据这个性质来设计的."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.262,
                0.46,
                0.28
            ],
            "angle": 0,
            "content": "6.7.2 高斯-牛顿算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.293,
                0.825,
                0.392
            ],
            "angle": 0,
            "content": "高斯－牛顿法是求解非线性最小二乘问题的经典方法，它可以看成是结合了线搜索的牛顿法的变形．既然海瑟矩阵中有关 \\(r_i(x)\\) 的二阶导数项不易求出，高斯－牛顿法不去计算这一部分，直接使用 \\(J(\\boldsymbol {x})^{\\mathrm{T}}J(\\boldsymbol {x})\\) 作为海瑟矩阵的近似矩阵来求解牛顿方程．我们用 \\(J^{k}\\) 简记 \\(J(x^{k})\\) ，高斯－牛顿法产生的下降方向 \\(d^{k}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.461,
                0.394,
                0.825,
                0.414
            ],
            "angle": 0,
            "content": "\\[\n\\left(J ^ {k}\\right) ^ {\\mathrm {T}} J ^ {k} d ^ {k} = - \\left(J ^ {k}\\right) ^ {\\mathrm {T}} r ^ {k}. \\tag {6.7.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.424,
                0.825,
                0.481
            ],
            "angle": 0,
            "content": "方程(6.7.3)正是法方程的形式，而由线性代数的知识可知，不管 \\(J^{k}\\) 是否是满秩矩阵，方程 (6.7.3)一定存在解。实际上，该方程是如下线性最小二乘问题的最优性条件："
        },
        {
            "type": "equation",
            "bbox": [
                0.464,
                0.479,
                0.619,
                0.509
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {d} \\frac {1}{2} \\| J ^ {k} d + r ^ {k} \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.516,
                0.825,
                0.554
            ],
            "angle": 0,
            "content": "在求解线性最小二乘问题时，我们只需要对 \\(J^{k}\\) 做QR分解，因此矩阵 \\((J^k)^\\mathrm{T}J^k\\) 无需计算出来."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.558,
                0.826,
                0.596
            ],
            "angle": 0,
            "content": "高斯－牛顿法的框架如算法6.10．为了方便理解，我们将求解线性最小二乘问题的方法进行了展开．高斯－牛顿法每一步的运算量是来自残差向量"
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.61,
                0.44,
                0.626
            ],
            "angle": 0,
            "content": "算法6.10 高斯-牛顿法"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.632,
                0.463,
                0.648
            ],
            "angle": 0,
            "content": "1. 给定初始值 \\(x^0\\) ， \\(k\\gets 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.653,
                0.487,
                0.668
            ],
            "angle": 0,
            "content": "2. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.673,
                0.555,
                0.689
            ],
            "angle": 0,
            "content": "3. 计算残差向量 \\(r^k\\) ，雅可比矩阵 \\(J^{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.693,
                0.765,
                0.71
            ],
            "angle": 0,
            "content": "4. 计算 \\(J^{k}\\) 的QR分解： \\(J^{k} = Q^{k}R^{k}\\) ，其中 \\(Q^{k}\\in \\mathbb{R}^{m\\times n},R^{k}\\in \\mathbb{R}^{n\\times n}.\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.714,
                0.622,
                0.731
            ],
            "angle": 0,
            "content": "5. 求解方程 \\(R^k d^k = -(Q^k)^{\\mathrm{T}}r^k\\) 得下降方向 \\(d^{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.735,
                0.523,
                0.751
            ],
            "angle": 0,
            "content": "6. 使用线搜索准则计算步长 \\(\\alpha_{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.755,
                0.485,
                0.771
            ],
            "angle": 0,
            "content": "7. 更新： \\(x^{k + 1} = x^k +\\alpha_kd^k\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.777,
                0.385,
                0.791
            ],
            "angle": 0,
            "content": "8. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.798,
                0.371,
                0.812
            ],
            "angle": 0,
            "content": "9. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.632,
                0.765,
                0.812
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.836,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "\\(r^k\\) 和雅可比矩阵 \\(J^{k}\\) ，和其他算法相比，它的计算量较小．我们还注意到，若"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.404,
                0.133
            ],
            "angle": 0,
            "content": "6.7 非线性最小二乘问题算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "283"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.156,
                0.736,
                0.194
            ],
            "angle": 0,
            "content": "\\(J^{k}\\) 是满秩矩阵，则高斯－牛顿法得到的方向 \\(d^{k}\\) 总是一个下降方向，这是因为"
        },
        {
            "type": "equation",
            "bbox": [
                0.287,
                0.197,
                0.623,
                0.217
            ],
            "angle": 0,
            "content": "\\[\n\\left(d ^ {k}\\right) ^ {\\mathrm {T}} \\nabla f (x ^ {k}) = \\left(d ^ {k}\\right) ^ {\\mathrm {T}} \\left(J ^ {k}\\right) ^ {\\mathrm {T}} r ^ {k} = - \\| J ^ {k} d ^ {k} \\| ^ {2} <   0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.224,
                0.739,
                0.282
            ],
            "angle": 0,
            "content": "这也是高斯－牛顿法的优点．在此之前我们介绍了牛顿法，但它并不总是保证 \\(d^k\\) 是下降方向．而高斯－牛顿法使用一个半正定矩阵来近似牛顿矩阵，可以获得较好的下降方向."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.286,
                0.74,
                0.428
            ],
            "angle": 0,
            "content": "一个很自然的问题是：高斯－牛顿法使用了近似矩阵来求解牛顿方程，那么在什么情况下这个近似是合理的？直观上看，根据海瑟矩阵(6.7.2b)的表达式，当 \\((J^k)^\\mathrm{T}J^k\\) 这一部分起主导时，所使用的近似是有意义的．一个充分条件就是在最优点 \\(x^{*}\\) 处 \\(r_i(x^*)\\) 的值都很小．此时高斯－牛顿法和牛顿法相近，它们也有很多相似的性质．如果残差向量 \\(r(x^{*})\\) 模长较大，则仅仅使用 \\((J^{k})^{\\mathrm{T}}J^{k}\\) 并不能很好地近似 \\(\\nabla^2 f(x^k)\\) ，此时高斯－牛顿法可能收敛很慢甚至发散."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.431,
                0.739,
                0.492
            ],
            "angle": 0,
            "content": "接下来给出高斯－牛顿法的收敛性质．通过上面的描述可以注意到，雅可比矩阵 \\(J^k\\) 的非奇异性是一个很关键的因素，因此我们在这个条件下建立收敛性．假设雅可比矩阵 \\(J(x)\\) 的奇异值一致地大于0，即存在 \\(\\gamma > 0\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.501,
                0.738,
                0.52
            ],
            "angle": 0,
            "content": "\\[\n\\| J (x) z \\| \\geqslant \\gamma \\| z \\|, \\quad \\forall x \\in \\mathcal {N}, \\tag {6.7.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.531,
                0.324,
                0.549
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{N}\\) 是下水平集"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.55,
                0.737,
                0.57
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {L} = \\{x \\mid f (x) \\leqslant f \\left(x ^ {0}\\right) \\} \\tag {6.7.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.577,
                0.583,
                0.594
            ],
            "angle": 0,
            "content": "的一个邻域，\\(x^0\\) 是算法的初始点，且假设 \\(\\mathcal{L}\\) 是有界的。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.598,
                0.492,
                0.614
            ],
            "angle": 0,
            "content": "在前面的假设下，有如下收敛性定理："
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.625,
                0.738,
                0.705
            ],
            "angle": 0,
            "content": "定理6.17(全局收敛性）如果每个残差函数 \\(r_j\\) 在有界下水平集(6.7.5)的一个邻域 \\(\\mathcal{N}\\) 内是利普希茨连续可微的，并且雅可比矩阵 \\(J(x)\\) 在 \\(\\mathcal{N}\\) 内满足一致满秩条件(6.7.4)，而步长满足Wolfe准则(6.1.4)，则对高斯－牛顿法得到的序列 \\(\\{x^k\\}\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.393,
                0.707,
                0.513,
                0.731
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} (J ^ {k}) ^ {\\mathrm {T}} r ^ {k} = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.735,
                0.739,
                0.795
            ],
            "angle": 0,
            "content": "证明．这里直接验证Zoutendijk条件(6.1.7)成立即可．首先，选择有界下水平集 \\(\\mathcal{L}\\) 的邻域 \\(\\mathcal{N}\\) 足够小，从而使得存在 \\(L > 0, \\beta > 0\\) ，对于任何 \\(x, \\tilde{x} \\in \\mathcal{N}\\) 以及任意的 \\(j = 1,2,\\dots,m\\) ，以下条件被满足："
        },
        {
            "type": "equation",
            "bbox": [
                0.3,
                0.805,
                0.738,
                0.825
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| r _ {j} (x) \\right\\| \\leqslant \\beta , \\quad \\left\\| \\nabla r _ {j} (x) \\right\\| \\leqslant \\beta , \\tag {6.7.6}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.194,
                0.829,
                0.737,
                0.849
            ],
            "angle": 0,
            "content": "\\[\n\\left| r _ {j} (x) - r _ {j} (\\tilde {x}) \\right| \\leqslant L \\| x - \\tilde {x} \\|, \\quad \\| \\nabla r _ {j} (x) - \\nabla r _ {j} (\\tilde {x}) \\| \\leqslant L \\| x - \\tilde {x} \\|. \\tag {6.7.7}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "284"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.156,
                0.829,
                0.195
            ],
            "angle": 0,
            "content": "易得对任意的 \\(x \\in \\mathcal{L}\\) 存在 \\(\\tilde{\\beta}\\) 使得 \\(\\|J(x)\\| = \\|J(x)^{\\mathrm{T}}\\| \\leqslant \\tilde{\\beta}\\), 及 \\(\\nabla f(x) = J(x)^{\\mathrm{T}}r(x)\\) 是利普希茨连续函数. 记 \\(\\theta_{k}\\) 是高斯-牛顿方向 \\(d^{k}\\) 与负梯度方向的夹角, 则"
        },
        {
            "type": "equation",
            "bbox": [
                0.29,
                0.207,
                0.794,
                0.246
            ],
            "angle": 0,
            "content": "\\[\n\\cos \\theta_ {k} = - \\frac {\\nabla f (x ^ {k}) ^ {\\mathrm {T}} d ^ {k}}{\\| d ^ {k} \\| \\| \\nabla f (x ^ {k}) \\|} = \\frac {\\| J ^ {k} d ^ {k} \\| ^ {2}}{\\| d ^ {k} \\| \\| (J ^ {k}) ^ {\\mathrm {T}} J ^ {k} d ^ {k} \\|} \\geqslant \\frac {\\gamma^ {2} \\| d ^ {k} \\| ^ {2}}{\\tilde {\\beta} ^ {2} \\| d ^ {k} \\|} = \\frac {\\gamma^ {2}}{\\tilde {\\beta} ^ {2}} > 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.26,
                0.513,
                0.278
            ],
            "angle": 0,
            "content": "根据推论6.1即可得 \\(\\nabla f(x^k)\\to 0\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.263,
                0.825,
                0.275
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.307,
                0.825,
                0.364
            ],
            "angle": 0,
            "content": "定理6.17的关键假设是一致满秩条件 (6.7.4). 实际上, 若 \\(J^{k}\\) 不满秩, 则线性方程组 (6.7.3) 有无穷多个解. 如果对解的性质不提额外要求, 则无法推出 \\(\\cos \\theta_{k}\\) 一致地大于零. 此时收敛性可能不成立."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.37,
                0.825,
                0.428
            ],
            "angle": 0,
            "content": "当 \\((J^{k})^{\\mathrm{T}}J^{k}\\) 在海瑟矩阵 (6.7.2b) 中占据主导部分时，高斯－牛顿算法可能会有更快的收敛速度。类似于牛顿法，我们给出高斯－牛顿法的局部收敛性。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.448,
                0.825,
                0.506
            ],
            "angle": 0,
            "content": "定理6.18（局部收敛性）设 \\(r_i(x)\\) 二阶连续可微，\\(x^*\\) 是最小二乘问题(6.7.1)的最优解，海瑟矩阵 \\(\\nabla^2 f(x)\\) 和其近似矩阵 \\(J(x)^{\\mathrm{T}}J(x)\\) 均在点 \\(x^*\\) 的一个邻域内利普希茨连续，则当高斯-牛顿算法步长 \\(\\alpha_k\\) 恒为1时，"
        },
        {
            "type": "equation",
            "bbox": [
                0.291,
                0.524,
                0.825,
                0.544
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| x ^ {k + 1} - x ^ {*} \\right\\| \\leqslant C \\| \\left(\\left(J ^ {*}\\right) ^ {\\mathrm {T}} J ^ {*}\\right) ^ {- 1} H ^ {*} \\| \\left\\| x ^ {k} - x ^ {*} \\right\\| + \\mathcal {O} \\left(\\left\\| x ^ {k} - x ^ {*} \\right\\| ^ {2}\\right), \\tag {6.7.8}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.56,
                0.827,
                0.609
            ],
            "angle": 0,
            "content": "其中 \\(H^{*} = \\sum_{i = 1}^{m}r(x^{*})\\nabla^{2}r(x^{*})\\) 为海瑟矩阵 \\(\\nabla^2 f(x^*)\\) 去掉 \\(J(x^{*})^{\\mathrm{T}}J(x^{*})\\) 的部分，\\(C > 0\\) 为常数."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.629,
                0.436,
                0.645
            ],
            "angle": 0,
            "content": "证明. 根据迭代公式,"
        },
        {
            "type": "equation",
            "bbox": [
                0.284,
                0.661,
                0.825,
                0.707
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} x ^ {k + 1} - x ^ {*} = x ^ {k} + d ^ {k} - x ^ {*} \\\\ = \\left(\\left(J ^ {k}\\right) ^ {\\mathrm {T}} J ^ {k}\\right) ^ {- 1} \\left(\\left(J ^ {k}\\right) ^ {\\mathrm {T}} J ^ {k} \\left(x ^ {k} - x ^ {*}\\right) + \\nabla f \\left(x ^ {*}\\right) - \\nabla f \\left(x ^ {k}\\right)\\right). \\tag {6.7.9} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.723,
                0.356,
                0.739
            ],
            "angle": 0,
            "content": "由泰勒展开，"
        },
        {
            "type": "equation",
            "bbox": [
                0.325,
                0.753,
                0.757,
                0.822
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\nabla f (x ^ {k}) - \\nabla f (x ^ {*}) = \\int_ {0} ^ {1} J ^ {\\mathrm {T}} J (x ^ {*} + t (x ^ {k} - x ^ {*})) (x ^ {k} - x ^ {*}) \\mathrm {d} t + \\\\ \\int_ {0} ^ {1} H (x ^ {*} + t (x ^ {k} - x ^ {*})) (x ^ {k} - x ^ {*}) \\mathrm {d} t, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.835,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(J^{\\mathrm{T}}J(x)\\) 是 \\(J^{\\mathrm{T}}(x)J(x)\\) 的简写，\\(H(x) = \\nabla^{2}f(x) - J^{\\mathrm{T}}J(x)\\) 为海瑟矩阵剩余"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.404,
                0.133
            ],
            "angle": 0,
            "content": "6.7 非线性最小二乘问题算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "285"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.634,
                0.174
            ],
            "angle": 0,
            "content": "部分．将泰勒展开式代入(6.7.9)式右边，取范数进行估计，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.257,
                0.188,
                0.652,
                0.311
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| \\left(J ^ {k}\\right) ^ {\\mathrm {T}} J ^ {k} \\left(x ^ {k} - x ^ {*}\\right) + \\nabla f \\left(x ^ {*}\\right) - \\nabla f \\left(x ^ {k}\\right) \\right\\| \\\\ \\leqslant \\int_ {0} ^ {1} \\| \\left(J ^ {\\mathrm {T}} J \\left(x ^ {k}\\right) - J ^ {\\mathrm {T}} J \\left(x ^ {*} + t \\left(x ^ {k} - x ^ {*}\\right)\\right)\\right) \\left(x ^ {k} - x ^ {*}\\right) \\| \\mathrm {d} t + \\\\ \\int_ {0} ^ {1} \\| H \\left(x ^ {*} + t \\left(x ^ {k} - x ^ {*}\\right)\\right) \\left(x ^ {k} - x ^ {*}\\right) \\| d t \\\\ \\leqslant \\frac {L}{2} \\| x ^ {k} - x ^ {*} \\| ^ {2} + C \\| H ^ {*} \\| \\| x ^ {k} - x ^ {*} \\|, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.323,
                0.739,
                0.403
            ],
            "angle": 0,
            "content": "其中 \\(L\\) 是 \\(J^{\\mathrm{T}} J(x)\\) 的利普希茨常数。最后一个不等式是因为我们使用 \\(H^{*}\\) 来近似 \\(H\\left(x^{*} + t\\left(x^{k} - x^{*}\\right)\\right)\\)，由连续性，存在 \\(C > 0\\) 以及点 \\(x^{*}\\) 的一个邻域 \\(\\mathcal{N}\\) 对任意的 \\(x \\in \\mathcal{N}\\) 有 \\(\\| H(x)\\| \\leqslant C\\| H(x^{*})\\|\\)。将上述估计代入 (6.7.9) 式即可得到 (6.7.8) 式。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.421,
                0.738,
                0.48
            ],
            "angle": 0,
            "content": "定理6.18指出，如果 \\(\\| H(x^{*})\\|\\) 充分小，则高斯-牛顿法可以达到Q-线性收敛速度；而当 \\(\\| H(x^{*})\\| = 0\\) 时，收敛速度是Q-二次的．如果 \\(\\| H(x^{*})\\|\\) 较大，则高斯-牛顿法很可能会失效"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.509,
                0.476,
                0.528
            ],
            "angle": 0,
            "content": "6.7.3 Levenberg-Marquardt 方法"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.541,
                0.345,
                0.557
            ],
            "angle": 0,
            "content": "1. 信赖域型LM方法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.572,
                0.739,
                0.651
            ],
            "angle": 0,
            "content": "Levenberg-Marquardt (LM) 方法是由 Levenberg 在 1944 年提出的求解非线性最小二乘问题的方法 [116]. 它本质上是一种信赖域型方法, 主要应用场合是当矩阵 \\(\\left(J^{k}\\right)^{\\mathrm{T}} J^{k}\\) 奇异时, 它仍然能给出一个下降方向. LM 方法每一步求解如下子问题:"
        },
        {
            "type": "equation",
            "bbox": [
                0.311,
                0.66,
                0.737,
                0.692
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {d} \\quad \\frac {1}{2} \\| J ^ {k} d + r ^ {k} \\| ^ {2}, \\quad \\text {s . t .} \\quad \\| d \\| \\leqslant \\Delta_ {k}. \\tag {6.7.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.703,
                0.58,
                0.72
            ],
            "angle": 0,
            "content": "事实上，LM方法将如下近似当作信赖域方法中的 \\(m_{k}\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.29,
                0.73,
                0.737,
                0.762
            ],
            "angle": 0,
            "content": "\\[\nm _ {k} (d) = \\frac {1}{2} \\| r ^ {k} \\| ^ {2} + d ^ {\\mathrm {T}} \\left(J ^ {k}\\right) ^ {\\mathrm {T}} r ^ {k} + \\frac {1}{2} d ^ {\\mathrm {T}} \\left(J ^ {k}\\right) ^ {\\mathrm {T}} J ^ {k} d. \\tag {6.7.11}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.773,
                0.739,
                0.853
            ],
            "angle": 0,
            "content": "该方法使用 \\(B^{k} = (J^{k})^{\\mathrm{T}} J^{k}\\) 来近似海瑟矩阵，这个取法是从高斯－牛顿法推广而来的．LM 方法并不直接使用海瑟矩阵来求解．以下为了方便，省去迭代指标 \\(k\\) ．子问题(6.7.10)是信赖域子问题，上一节讨论过这个子问题的一些好的性质．根据定理6.12，可直接得到如下推论："
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "286"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.157,
                0.551,
                0.174
            ],
            "angle": 0,
            "content": "推论6.4 向量 \\(d^{*}\\) 是信赖域子问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.41,
                0.179,
                0.674,
                0.21
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {d} \\quad \\frac {1}{2} \\| J d + r \\| ^ {2}, \\quad \\text {s . t .} \\quad \\| d \\| \\leqslant \\Delta\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.217,
                0.63,
                0.233
            ],
            "angle": 0,
            "content": "的解当且仅当 \\(d^{*}\\) 是可行解并且存在数 \\(\\lambda \\geqslant 0\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.461,
                0.245,
                0.825,
                0.265
            ],
            "angle": 0,
            "content": "\\[\n\\left(J ^ {\\mathrm {T}} J + \\lambda I\\right) d ^ {*} = - J ^ {\\mathrm {T}} r, \\tag {6.7.12}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.461,
                0.27,
                0.825,
                0.289
            ],
            "angle": 0,
            "content": "\\[\n\\lambda \\left(\\Delta - \\| d ^ {*} \\|\\right) = 0. \\tag {6.7.13}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.301,
                0.731,
                0.317
            ],
            "angle": 0,
            "content": "注意到 \\(J^{\\mathrm{T}} J\\) 是半正定矩阵，因此条件(6.6.5c)是自然成立的。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.321,
                0.827,
                0.421
            ],
            "angle": 0,
            "content": "下面简要说明如何求解LM子问题(6.7.10).实际上，和信赖域子问题中的迭代法相同，我们先通过求根的方式来确定 \\(\\lambda\\) 的选取，然后直接求得LM方程的迭代方向．由于LM子问题的特殊性，可以不显式求出矩阵 \\(J^{\\mathrm{T}}J + \\lambda I\\) 的Cholesky分解，而仍然是借助QR分解，进而无需算出 \\(J^{\\mathrm{T}}J + \\lambda I\\) ．注意，\\((J^{\\mathrm{T}}J + \\lambda I)d = -J^{\\mathrm{T}}r\\) 实际上是最小二乘问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.449,
                0.428,
                0.826,
                0.473
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {d} \\quad \\left\\| \\left[ \\begin{array}{l} J \\\\ \\sqrt {\\lambda} I \\end{array} \\right] d + [ r ] \\right\\| \\tag {6.7.14}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.48,
                0.825,
                0.539
            ],
            "angle": 0,
            "content": "的最优性条件。此问题的系数矩阵带有一定结构，每次改变 \\(\\lambda\\) 进行试探时，有关 \\(J\\) 的块是不变的，因此无需重复计算 \\(J\\) 的 QR 分解。具体来说，设 \\(J = QR\\) 为 \\(J\\) 的 QR 分解，其中 \\(Q \\in \\mathbb{R}^{m \\times n}, R \\in \\mathbb{R}^{n \\times n}\\)。我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.404,
                0.545,
                0.682,
                0.59
            ],
            "angle": 0,
            "content": "\\[\n\\left[ \\begin{array}{c} J \\\\ \\sqrt {\\lambda} I \\end{array} \\right] = \\left[ \\begin{array}{c} Q R \\\\ \\sqrt {\\lambda} I \\end{array} \\right] = \\left[ \\begin{array}{c c} Q & 0 \\\\ 0 & I \\end{array} \\right] \\left[ \\begin{array}{c} R \\\\ \\sqrt {\\lambda} I \\end{array} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.597,
                0.825,
                0.697
            ],
            "angle": 0,
            "content": "矩阵 \\(\\left[\\frac{R}{\\sqrt{\\lambda}I}\\right]\\) 含有较多零元素，利用这个特点我们可以使用Householder变换或Givens变换来完成此矩阵的QR分解．如果矩阵 \\(J\\) 没有显式形式，只能提供矩阵乘法，则仍然可以用截断共轭梯度法，即算法6.9来求解子问题(6.7.10)."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.701,
                0.825,
                0.739
            ],
            "angle": 0,
            "content": "LM方法的收敛性也可以直接从信赖域方法的收敛性得出，我们直接给出收敛性定理."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.744,
                0.827,
                0.814
            ],
            "angle": 0,
            "content": "定理6.19 假设常数 \\(\\eta \\in \\left(0, \\frac{1}{4}\\right)\\)，下水平集 \\(\\mathcal{L}\\) 是有界的且每个 \\(r_i(x)\\) 在下水平集 \\(\\mathcal{L}\\) 的一个邻域 \\(\\mathcal{N}\\) 内是利普希茨连续可微的。假设对于任意的 \\(k\\)，子问题(6.7.10)的近似解 \\(d_k\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.82,
                0.737,
                0.858
            ],
            "angle": 0,
            "content": "\\[\nm _ {k} (0) - m _ {k} \\left(d ^ {k}\\right) \\geqslant c _ {1} \\left\\| \\left(J ^ {k}\\right) ^ {\\mathrm {T}} r ^ {k} \\right\\| \\min  \\left\\{\\Delta_ {k}, \\frac {\\left\\| \\left(J ^ {k}\\right) ^ {\\mathrm {T}} r ^ {k} \\right\\|}{\\left\\| \\left(J ^ {k}\\right) ^ {\\mathrm {T}} J ^ {k} \\right\\|} \\right\\},\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.404,
                0.133
            ],
            "angle": 0,
            "content": "6.7 非线性最小二乘问题算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "287"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.452,
                0.174
            ],
            "angle": 0,
            "content": "其中 \\(c_{1} > 0\\) 且 \\(\\| d^{k}\\| \\leqslant \\gamma \\Delta_{k},\\gamma \\geqslant 1\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.34,
                0.192,
                0.567,
                0.218
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\nabla f (x ^ {k}) = \\lim  _ {k \\rightarrow \\infty} (J ^ {k}) ^ {\\mathrm {T}} r ^ {k} = 0.\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.246,
                0.282,
                0.263
            ],
            "angle": 0,
            "content": "2. LMF方法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.28,
                0.737,
                0.318
            ],
            "angle": 0,
            "content": "信赖域型LM方法本质上是固定信赖域半径 \\(\\Delta\\) ，通过迭代寻找满足条件(6.7.12)的乘子 \\(\\lambda\\) ，每一步迭代需要求解线性方程组"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.336,
                0.532,
                0.356
            ],
            "angle": 0,
            "content": "\\[\n(J ^ {T} J + \\lambda I) d = - J ^ {T} r.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.375,
                0.741,
                0.537
            ],
            "angle": 0,
            "content": "这个求解过程在 LM 方法中会占据相当大的计算量，能否简化这个计算呢？在学习信赖域子问题求解方法时，我们仔细讨论了迭代法，图6.16表明，当 \\(\\lambda > -\\lambda_{1}\\) 时，下降方向 \\(d\\) 的模长随着 \\(\\lambda\\) 的增加而减小。在 LM 方法中，一个显然的结论就是 \\(-\\lambda_{1} < 0\\)。这就意味着 \\(\\Delta\\) 的大小被 \\(\\lambda\\) 隐式地决定，直接调整 \\(\\lambda\\) 的大小就相当于调整了信赖域半径 \\(\\Delta\\) 的大小。因此，我们可构造基于 \\(\\lambda\\) 更新的 LM 方法。由于 LM 方程(6.7.12)和信赖域子问题的关系由 Fletcher 在 1981 年给出 [69]，因此基于 \\(\\lambda\\) 更新的 LM 方法也被称为 LMF 方法，即每一步求解子问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.538,
                0.56,
                0.57
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {d} \\quad \\frac {1}{2} \\| J d + r \\| _ {2} ^ {2} + \\lambda \\| d \\| _ {2} ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.584,
                0.739,
                0.706
            ],
            "angle": 0,
            "content": "在 LMF 方法中，设第 \\(k\\) 步产生的迭代方向为 \\(d^{k}\\)，根据信赖域算法的思想，我们需要计算目标函数的预估下降量和实际下降量的比值 \\(\\rho_{k}\\)，来确定下一步信赖域半径的大小。这一比值很容易通过公式(6.6.3)计算，其中 \\(f(x)\\) 和 \\(m_{k}(d)\\) 分别取为(6.7.1)式和(6.7.11)式。计算 \\(\\rho_{k}\\) 后，我们根据 \\(\\rho_{k}\\) 的大小来更新 \\(\\lambda_{k}\\)。当乘子 \\(\\lambda_{k}\\) 增大时，信赖域半径会变小，反之亦然。所以 \\(\\lambda_{k}\\) 的变化策略应该和信赖域算法中 \\(\\Delta_{k}\\) 的恰好相反。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.71,
                0.739,
                0.79
            ],
            "angle": 0,
            "content": "有了上面的叙述，接下来就可以给出 LMF 算法的框架了。通过比较得知 LMF 方法（算法 6.11）和信赖域算法 6.8 的结构非常相似。算法 6.11 对 \\(\\gamma_1, \\gamma_2\\) 等参数并不敏感。但根据信赖域方法的收敛定理 6.15，参数 \\(\\eta\\) 可以取大于 0 的值来改善收敛结果。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.795,
                0.738,
                0.854
            ],
            "angle": 0,
            "content": "和LM方法相比，LMF方法每一次迭代只需要求解一次LM方程，从而极大地减少了计算量，在编程方面也更容易实现．所以LMF方法在求解最小二乘问题中是很常见的做法."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "288"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.259,
                0.159,
                0.411,
                0.175
            ],
            "angle": 0,
            "content": "算法6.11LMF方法"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.18,
                0.571,
                0.198
            ],
            "angle": 0,
            "content": "1. 给定初始点 \\(x^0\\) ，初始乘子 \\(\\lambda_0\\) ， \\(k\\gets 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.201,
                0.618,
                0.218
            ],
            "angle": 0,
            "content": "2. 给定参数 \\(0 \\leqslant \\eta < \\bar{\\rho}_1 < \\bar{\\rho}_2 < 1\\) ， \\(\\gamma_1 < 1 < \\gamma_2\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.18,
                0.618,
                0.218
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.222,
                0.486,
                0.236
            ],
            "angle": 0,
            "content": "3. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.24,
                0.748,
                0.259
            ],
            "angle": 0,
            "content": "4. 求解LM方程 \\((\\left(J^{k}\\right)^{\\mathrm{T}}J^{k} + \\lambda I)d = -(J^{k})^{\\mathrm{T}}r^{k}\\) 得到迭代方向 \\(d^{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.263,
                0.516,
                0.28
            ],
            "angle": 0,
            "content": "5. 根据(6.6.3)式计算下降率 \\(\\rho_{k}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.284,
                0.386,
                0.3
            ],
            "angle": 0,
            "content": "6. 更新乘子："
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.24,
                0.748,
                0.3
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.305,
                0.311,
                0.816,
                0.387
            ],
            "angle": 0,
            "content": "\\[\n\\lambda_ {k + 1} = \\left\\{ \\begin{array}{l l} \\gamma_ {2} \\lambda_ {k}, & \\rho_ {k} <   \\bar {\\rho} _ {1}, \\\\ \\gamma_ {1} \\lambda_ {k}, & \\rho_ {k} > \\bar {\\rho} _ {2}, \\\\ \\lambda_ {k}, & \\text {其 他 .} \\end{array} \\quad / * \\text {扩 大 乘 子 (缩 小 信 赖 域 半 径)} * / * \\text {缩 小 乘 子 (扩 大 信 赖 域 半 径)} * / * \\text {乘 子 不 变} * / \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.409,
                0.403,
                0.425
            ],
            "angle": 0,
            "content": "7. 更新自变量："
        },
        {
            "type": "equation",
            "bbox": [
                0.304,
                0.437,
                0.816,
                0.487
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\left\\{ \\begin{array}{l l} x ^ {k} + d ^ {k}, & \\rho_ {k} > \\eta , \\\\ x ^ {k}, & \\text {其 他 .} \\end{array} \\quad / \\star \\right. \\text {只 有 下 降 比 例 足 够 大 才 更 新} * /\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.51,
                0.385,
                0.524
            ],
            "angle": 0,
            "content": "8. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.531,
                0.371,
                0.544
            ],
            "angle": 0,
            "content": "9. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.51,
                0.385,
                0.544
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.575,
                0.518,
                0.593
            ],
            "angle": 0,
            "content": "6.7.4 大残量问题的拟牛顿法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.607,
                0.825,
                0.79
            ],
            "angle": 0,
            "content": "前面两个小节分别介绍了高斯－牛顿法和LM方法，这两个算法针对小残量最小二乘问题十分有效．而在大残量问题中，海瑟矩阵 \\(\\nabla^2 f(x)\\) 的第二部分的作用不可忽视，仅仅考虑 \\((J^{k})^{\\mathrm{T}}J^{k}\\) 作为第 \\(k\\) 步的海瑟矩阵近似则会带来很大误差．在这种情况下高斯－牛顿法和LM方法很可能会失效．自然，我们可以将最小二乘问题(6.7.1)当成一般的无约束问题，并使用之前讨论过的牛顿法和拟牛顿法求解．但对于很多问题来说，各个残量分量的海瑟矩阵 \\(\\nabla^2 r_i(x)\\) 不易求出，使用牛顿法会有很大开销；而直接使用拟牛顿法对海瑟矩阵 \\(\\nabla^2 f(x)\\) 进行近似又似乎忽略了最小二乘问题的特殊结构．有没有一个两全其美的办法呢？"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "海瑟矩阵表达式(6.7.2b)说明了 \\(\\nabla^2 f(x)\\) 由两部分组成: 一部分容易得出, 但不精确; 另一部分较难求得, 但在计算中又必不可少. 对于容易部分可以直接保留高斯 - 牛顿矩阵 \\(J^{\\mathrm{T}} J\\), 而对于较难部分则可以利用拟牛顿法来"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.404,
                0.133
            ],
            "angle": 0,
            "content": "6.7 非线性最小二乘问题算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "289"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.195
            ],
            "angle": 0,
            "content": "进行近似。这就是我们求解大残量问题的基本思路，它同时考虑了最小二乘问题的海瑟矩阵结构和计算量，是一种混合的近似方法。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.202,
                0.633,
                0.22
            ],
            "angle": 0,
            "content": "具体来说，我们使用 \\(B^k\\) 来表示 \\(\\nabla^2 f(x^k)\\) 的近似矩阵，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.242,
                0.523,
                0.261
            ],
            "angle": 0,
            "content": "\\[\nB ^ {k} = \\left(J ^ {k}\\right) ^ {\\mathrm {T}} J ^ {k} + T ^ {k},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.281,
                0.741,
                0.42
            ],
            "angle": 0,
            "content": "其中 \\(T^k\\) 是海瑟矩阵第二部分 \\(\\sum_{j=1}^{m} r_j(x^k) \\nabla^2 r_j(x^k)\\) 的近似。问题的关键在于如何构造矩阵 \\(T^k\\)，回忆我们建立拟牛顿法时，构造拟牛顿格式主要分为两步：一是找出拟牛顿条件，二是根据拟牛顿条件来构造拟牛顿矩阵的低秩更新。在这里我们使用相似的过程，但注意 \\(T^k\\) 仅仅是拟牛顿矩阵 \\(B^k\\) 的一部分，它不太可能满足割线条件(6.5.3）（将其中的 \\(B^{k+1}\\) 替换成 \\(T^{k+1}\\)）。我们的目标是让 \\(T^{k+1}\\) 和海瑟矩阵的第二部分尽量相似，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.342,
                0.438,
                0.566,
                0.478
            ],
            "angle": 0,
            "content": "\\[\nT ^ {k + 1} \\approx \\sum_ {j = 1} ^ {m} r _ {j} (x ^ {k + 1}) \\nabla^ {2} r _ {j} (x ^ {k + 1}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.498,
                0.66,
                0.515
            ],
            "angle": 0,
            "content": "由一阶泰勒展开得知，\\(T^{k+1}\\) 应该尽量保留原海瑟矩阵的性质，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.295,
                0.533,
                0.611,
                0.686
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} T ^ {k + 1} s ^ {k} \\approx \\left(\\sum_ {j = 1} ^ {m} r _ {j} (x ^ {k + 1}) \\nabla^ {2} r _ {j} (x ^ {k + 1})\\right) s ^ {k} \\\\ = \\sum_ {j = 1} ^ {m} r _ {j} \\left(x ^ {k + 1}\\right) \\left(\\nabla^ {2} r _ {j} \\left(x ^ {k + 1}\\right)\\right) s ^ {k} \\\\ \\approx \\sum_ {j = 1} ^ {m} r _ {j} \\left(x ^ {k + 1}\\right) \\left(\\nabla r _ {j} \\left(x ^ {k + 1}\\right) - \\nabla r _ {j} \\left(x ^ {k}\\right)\\right) \\\\ = \\left(J ^ {k + 1}\\right) ^ {\\mathrm {T}} r ^ {k + 1} - \\left(J ^ {k}\\right) ^ {\\mathrm {T}} r ^ {k + 1}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.706,
                0.616,
                0.726
            ],
            "angle": 0,
            "content": "令 \\(\\hat{y}^k = (J^{k + 1})^\\mathrm{T}r^{k + 1} - (J^k)^\\mathrm{T}r^{k + 1}\\)，则 \\(T^k\\) 满足的拟牛顿条件为"
        },
        {
            "type": "equation",
            "bbox": [
                0.407,
                0.747,
                0.737,
                0.767
            ],
            "angle": 0,
            "content": "\\[\nT ^ {k + 1} s ^ {k} = \\hat {y} ^ {k}. \\tag {6.7.15}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.79,
                0.395,
                0.808
            ],
            "angle": 0,
            "content": "在这里注意 \\(\\hat{y}^k\\) 不是原有的 \\(y^{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.735,
                0.854
            ],
            "angle": 0,
            "content": "有了拟牛顿条件(6.7.15)，我们就可以使用之前讲过的方法来构造拟牛顿格式了，构造的过程完全相同，这里不再赘述。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "290"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.402,
                0.174
            ],
            "angle": 0,
            "content": "6.7.5 应用举例"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.188,
                0.825,
                0.225
            ],
            "angle": 0,
            "content": "第三章提到了相位恢复问题，它实际上是非线性最小二乘问题的重要应用。相位恢复的原始模型为"
        },
        {
            "type": "equation",
            "bbox": [
                0.411,
                0.238,
                0.824,
                0.275
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbf {C} ^ {n}} f (x) \\stackrel {\\text {d e f}} {=} \\frac {1}{2} \\sum_ {j = 1} ^ {m} \\left(\\left| \\bar {a} _ {j} ^ {\\mathrm {T}} x \\right| ^ {2} - b _ {j}\\right) ^ {2}, \\tag {6.7.16}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.288,
                0.673,
                0.306
            ],
            "angle": 0,
            "content": "其中 \\(a_{j}\\in \\mathbb{C}^{n}\\) 是已知的采样向量， \\(b_{j}\\in \\mathbb{R}\\) 是观测的模长"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.309,
                0.825,
                0.369
            ],
            "angle": 0,
            "content": "该问题的变量为复数，因此我们考虑使用 Wirtinger 导数 [39] 表示其梯度和雅可比矩阵。对于 \\( f(x) \\)，记 \\( \\mathbf{x} = \\begin{bmatrix} x \\\\ \\bar{x} \\end{bmatrix} \\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.422,
                0.381,
                0.662,
                0.42
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x) = \\sum_ {j = 1} ^ {m} \\left(\\left| \\bar {a} _ {j} ^ {\\mathrm {T}} x \\right| ^ {2} - b _ {j}\\right) a _ {j} \\bar {a} _ {j} ^ {\\mathrm {T}} x,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.432,
                0.299,
                0.448
            ],
            "angle": 0,
            "content": "以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.466,
                0.448,
                0.617,
                0.492
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (\\mathbf {x}) = \\left[ \\begin{array}{c} \\nabla f (x) \\\\ \\overline {{\\nabla f (x)}} \\end{array} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.499,
                0.59,
                0.516
            ],
            "angle": 0,
            "content": "那么，雅可比矩阵和高斯－牛顿矩阵分别为"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.527,
                0.713,
                0.577
            ],
            "angle": 0,
            "content": "\\[\nJ (\\mathbf {x}) = \\overline {{\\left[ \\begin{array}{c c c c} a _ {1} (\\bar {a} _ {1} ^ {\\mathrm {T}} x) , & a _ {2} (\\bar {a} _ {2} ^ {\\mathrm {T}} x) , & \\dots , & a _ {m} (\\bar {a} _ {m} ^ {\\mathrm {T}} x) \\\\ \\bar {a} _ {1} (a _ {1} ^ {\\mathrm {T}} \\bar {x}) , & \\bar {a} _ {2} (a _ {2} ^ {\\mathrm {T}} \\bar {x}) , & \\dots , & \\bar {a} _ {m} (a _ {m} ^ {\\mathrm {T}} \\bar {x}) \\end{array} \\right] ^ {\\mathrm {T}}}},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.342,
                0.579,
                0.741,
                0.624
            ],
            "angle": 0,
            "content": "\\[\n\\Psi (\\mathbf {x}) \\stackrel {{\\mathrm {d e f}}} {{=}} \\overline {{J (\\mathbf {x})}} ^ {\\mathrm {T}} J (\\mathbf {x}) = \\sum_ {j = 1} ^ {m} \\left[ \\begin{array}{c c} | \\bar {a} _ {j} ^ {\\mathrm {T}} x | ^ {2} a _ {j} \\bar {a} _ {j} ^ {\\mathrm {T}} & (\\bar {a} _ {j} ^ {\\mathrm {T}} x) ^ {2} a _ {j} a _ {j} ^ {\\mathrm {T}} \\\\ (\\overline {{\\bar {a} _ {j} ^ {\\mathrm {T}} x}}) ^ {2} \\bar {a} _ {j} \\bar {a} _ {j} ^ {\\mathrm {T}} & | \\bar {a} _ {j} ^ {\\mathrm {T}} x | ^ {2} \\bar {a} _ {j} a _ {j} ^ {\\mathrm {T}} \\end{array} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.635,
                0.607,
                0.652
            ],
            "angle": 0,
            "content": "因此，在第 \\(k\\) 步，高斯-牛顿法求解方程"
        },
        {
            "type": "equation",
            "bbox": [
                0.463,
                0.668,
                0.619,
                0.687
            ],
            "angle": 0,
            "content": "\\[\n\\Psi (\\mathbf {x} ^ {\\mathbf {k}}) d ^ {k} = - \\nabla f (\\mathbf {x} ^ {k})\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.703,
                0.589,
                0.72
            ],
            "angle": 0,
            "content": "以得到方向 \\(d^{k}\\); LM 方法则求解正则化方程"
        },
        {
            "type": "equation",
            "bbox": [
                0.436,
                0.736,
                0.825,
                0.756
            ],
            "angle": 0,
            "content": "\\[\n\\left(\\Psi \\left(\\mathbf {x} ^ {k}\\right) + \\lambda_ {k}\\right) d ^ {k} = - \\nabla f \\left(\\mathbf {x} ^ {k}\\right), \\tag {6.7.17}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.772,
                0.579,
                0.789
            ],
            "angle": 0,
            "content": "其中 \\(\\lambda_{k}\\) 是与 \\(f(\\mathbf{x}^{k})\\) 相关的参数．这里选取"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.801,
                0.716,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\lambda_ {k} = \\left\\{ \\begin{array}{l l} 7 0 0 0 0 n \\sqrt {n f (x ^ {k})}, & f (x ^ {k}) \\geqslant \\frac {1}{9 0 0 n} \\| x ^ {k} \\| _ {2} ^ {2}, \\\\ \\sqrt {f (x ^ {k})}, & \\text {其 他}. \\end{array} \\right.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "6.8 总结"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "291"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.156,
                0.739,
                0.203
            ],
            "angle": 0,
            "content": "当 \\(f(x^{k}) \\geqslant \\frac{1}{900 n} \\| x^{k}\\|_{2}^{2}\\) 时, 参数 \\(\\lambda_{k} = 70000 n \\sqrt{n f\\left(x^{k}\\right)}\\) 能够保证算法有全局 Q-线性收敛速度. 同时, 我们利用共轭梯度法求解线性方程(6.7.17), 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.288,
                0.215,
                0.622,
                0.237
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| \\left(\\Psi (\\mathbf {x} ^ {k}) + \\lambda_ {k}\\right) d ^ {k} + \\nabla f (\\mathbf {x} ^ {k}) \\right\\| \\leqslant \\eta_ {k} \\| \\nabla f (\\mathbf {x} ^ {k}) \\|,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.251,
                0.407,
                0.268
            ],
            "angle": 0,
            "content": "其中 \\(\\eta_{k} > 0\\) 是人为设置的参数"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.272,
                0.538,
                0.289
            ],
            "angle": 0,
            "content": "考虑编码衍射模型，其中信号采集的格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.217,
                0.299,
                0.69,
                0.345
            ],
            "angle": 0,
            "content": "\\[\nb _ {j} = \\left| \\sum_ {t = 0} ^ {n - 1} x _ {t} \\bar {d} _ {l} (t) e ^ {- i 2 \\pi k t / n} \\right| ^ {2}, \\quad j = (l, k), 0 \\leqslant k \\leqslant n - 1, 1 \\leqslant l \\leqslant L.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.356,
                0.741,
                0.435
            ],
            "angle": 0,
            "content": "上式表明，对给定的 \\(l\\) ，我们采集在波形（waveform） \\(d_{l}\\) 下信号 \\(\\{x_{t}\\}\\) 的衍射图的模长．通过改变 \\(l\\) 和相应的波形 \\(d_{l}\\) ，可以生成一系列编码衍射图．这里假设 \\(d_{l},l = 0,1,\\dots ,L\\) 是独立同分布的随机向量来模拟实际场景．具体地，令 \\(d_{l}(t) = c_{1}c_{2}\\) ，其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.253,
                0.448,
                0.655,
                0.549
            ],
            "angle": 0,
            "content": "\\[\nc _ {1} = \\left\\{ \\begin{array}{l l} + 1, & \\text {依 概 率} 0. 2 5, \\\\ - 1, & \\text {依 概 率} 0. 2 5, \\\\ + i, & \\text {依 概 率} 0. 2 5, \\\\ - i, & \\text {依 概 率} 0. 2 5, \\end{array} \\right. c _ {2} = \\left\\{ \\begin{array}{l l} \\frac {\\sqrt {2}}{2}, & \\text {依 概 率} 0. 8, \\\\ \\sqrt {3}, & \\text {依 概 率} 0. 2. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.56,
                0.744,
                0.735
            ],
            "angle": 0,
            "content": "我们分别测试不精确求解正则化方程(6.7.17） \\((\\eta_{k} = 0.1)\\) 的LM方法（ILM1）以及更精确 \\((\\eta_{k} = \\min \\{0.1,\\| \\nabla f(\\mathbf{x}^{k})\\| \\})\\) 的LM方法（ILM2)，求解Wirtinger梯度下降方法（WF）以及其加速版本Nesterov加速算法(Nes).真实信号 \\(x\\) 取为两张自然图片，分别为博雅塔和华表的图片，如图6.19所示．这里图片可以看成 \\(m\\times n\\) 矩阵，其中行、列指标表示像素点所在位置，取值表示像素点的灰度值．选取 \\(L = 20\\) ，并收集相应的衍射图模长．图6.20给出了不同算法的收敛情况，其中横坐标为CPU时间，纵坐标为当前迭代点 \\(\\mathbf{x}^k\\) 与真实信号 \\(x\\) 的相对误差，即 \\(\\min_{\\phi \\in [0,2\\pi ]}\\frac{1}{\\|x\\|}\\| \\mathbf{x}^k -xe^{i\\phi}\\|\\)"
        },
        {
            "type": "title",
            "bbox": [
                0.399,
                0.756,
                0.51,
                0.778
            ],
            "angle": 0,
            "content": "6.8 总结"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.795,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "本章简单介绍了若干种经典的无约束优化算法，这些算法的起源都很早，但仍然能够流传至今并经受住时间的考验，正是因为它们在实际问题中取得了很好的效果。虽然随着技术的进步，更高效、优美的算法不断被发展"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "292"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "image",
            "bbox": [
                0.26,
                0.204,
                0.533,
                0.439
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.304,
                0.449,
                0.488,
                0.461
            ],
            "angle": 0,
            "content": "(a) 博雅塔，图片像素为 \\(601 \\times 541\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.552,
                0.204,
                0.825,
                0.439
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.602,
                0.449,
                0.774,
                0.461
            ],
            "angle": 0,
            "content": "(b) 华表，图片像素为 \\(601 \\times 541\\)"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.471,
                0.481,
                0.614,
                0.497
            ],
            "angle": 0,
            "content": "图6.19 测试图片"
        },
        {
            "type": "image",
            "bbox": [
                0.264,
                0.597,
                0.51,
                0.746
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.357,
                0.756,
                0.416,
                0.768
            ],
            "angle": 0,
            "content": "(a) 博雅塔"
        },
        {
            "type": "image",
            "bbox": [
                0.517,
                0.597,
                0.765,
                0.746
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.617,
                0.756,
                0.666,
                0.768
            ],
            "angle": 0,
            "content": "(b) 华表"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.402,
                0.788,
                0.682,
                0.805
            ],
            "angle": 0,
            "content": "图6.20 相对误差和计算时间对比图"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "6.8 总结"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "293"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.195
            ],
            "angle": 0,
            "content": "出来，但是本章所列出的算法形式及其背后所蕴含的思想仍然有很强的指导意义."
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.203,
                0.744,
                0.365
            ],
            "angle": 0,
            "content": "常用的精确线搜索算法有黄金分割法（0.618方法）和斐波那契（Fibonacci）方法，读者可参考[171]与[3].有关多项式插值类型的线搜索可以参考[59]，满足Wolfe准则的非精确线搜索算法可参考[69].基于Wolfe准则的线搜索算法有标准的子程序，直接调用即可，例如MINPACK-2[11].除在直线上进行搜索外，我们还有曲线搜索方法，这类方法大多用于牛顿类算法当中，以便产生负曲率方向．其中基于二阶Armijo搜索准则的方法有Goldfarb方法[83]和McCormick方法[131]；基于二阶Wolfe准则的算法有More-Sorensen方法[136]."
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.372,
                0.744,
                0.534
            ],
            "angle": 0,
            "content": "在梯度类算法中我们略去了非线性共轭梯度法的介绍。早期的共轭梯度法由Hestenes和Stiefel在1952年提出[101]，主要用于求解正定线性方程组。它的基本思想是将求解线性方程组转化为一个正定二次函数的最小值问题。之后Fletcher和Reeves将其移植到了一般的非线性函数上，并给出了FR格式[68]。由于线性共轭梯度法有很多等价格式，移植到非线性情况会对应不同的格式，例如PR格式[153]和DY格式[50]。非线性共轭梯度法往往比普通梯度法有更好的表现，一些有关收敛性的结果可参考[51,79,94,212]。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.542,
                0.744,
                0.768
            ],
            "angle": 0,
            "content": "牛顿法是利用海瑟矩阵构造下降方向的算法，它有很好的局部收敛性。但经典牛顿法不够稳定，在实际应用中我们通常使用修正牛顿法求解。比较古老的修正策略由Goldstein在1967年提出[84]，当海瑟矩阵不正定时，选取负梯度作为下降方向。这一做法完全舍弃了函数的二阶信息，不推荐使用。而最简单的对海瑟矩阵的修正是Goldfeld提出的加常数倍单位矩阵的做法，即使用 \\( B^{k} + \\tau_{k}I \\) 作为海瑟矩阵，这个策略本质上和信赖域有一定联系。修正的Cholesky分解法是Gill和Murray提出的[80]，除此以外还有基于不定矩阵分解[32]的Fletcher-Freeman算法[70]，这些算法都能有效提高牛顿法的稳定性。有关牛顿一共轭梯度法读者可以参考[89,139]，使用有限差分格式近似牛顿矩阵的方法可参考[175]，这些算法都能有效减少牛顿法的计算量。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.744,
                0.854
            ],
            "angle": 0,
            "content": "在拟牛顿算法中比较重要的是BFGS方法和L-BFGS方法．L-BFGS方法是可以利用在大规模无约束优化问题上的算法，更细致的讨论可以参见[37,123,139,144].L-BFGS算法的代码实现可参考L-BFGS-B[134,215]以及其在GPU上的一种实现[67].对于有些问题，目标函数的海瑟矩阵是稀"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "294"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "疏的，此时如果直接用拟牛顿方法则会破坏稀疏性，所以我们必须根据原问题海瑟矩阵的稀疏结构来设计稀疏的拟牛顿更新，这方面的内容可参考[159, 182]，本书中不做讨论."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.219,
                0.826,
                0.298
            ],
            "angle": 0,
            "content": "在信赖域算法中，除本书介绍的两种算法以外，还有Powell提出的Dogleg方法[158]，双折Dogleg方法[58]以及Byrd等人提出的二维子空间法[38].有关截断共轭梯度法的更多性质可参考[207].信赖域算法近几十年的进展总结可参阅[208]."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.302,
                0.827,
                0.442
            ],
            "angle": 0,
            "content": "高斯－牛顿法是较常用的求解非线性最小二乘问题的算法。这个算法充分利用了最小二乘问题的结构，是二阶算法很好的近似。我们指出，在求解其他问题时也可利用高斯－牛顿的思想来构造高效的算法。例如，若目标函数的海瑟矩阵可自然地写成两部分的和：第一部分容易计算且占主要地位，第二部分难以计算，则可以先精确计算第一部分的值，然后寻找第二部分的近似（或完全舍弃）。这种做法的效果通常要比直接整体近似海瑟矩阵要好，比较具体的应用可参考[106]。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.447,
                0.826,
                0.505
            ],
            "angle": 0,
            "content": "本章中部分内容编写参考了[145]，包括线搜索方法、（拟）牛顿类算法、信赖域算法、非线性最小二乘问题算法。信赖域算法中的截断共轭梯度法参考了[175]，（次）梯度算法参考了Lieven Vandenberghe教授的课件。"
        },
        {
            "type": "title",
            "bbox": [
                0.507,
                0.535,
                0.58,
                0.556
            ],
            "angle": 0,
            "content": "习题6"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.573,
                0.825,
                0.653
            ],
            "angle": 0,
            "content": "6.1 设 \\(f(x)\\) 是连续可微函数, \\(d^{k}\\) 是一个下降方向, 且 \\(f(x)\\) 在射线 \\(\\left\\{x^{k} + \\alpha d^{k} \\mid \\alpha > 0\\right\\}\\) 上有下界. 求证: 当 \\(0 < c_{1} < c_{2} < 1\\) 时, 总是存在满足Wolfe准则(6.1.4a)(6.1.4b)的点. 并举一个反例说明当 \\(0 < c_{2} < c_{1} < 1\\) 时, 满足Wolfe准则的点可能不存在."
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.661,
                0.825,
                0.704
            ],
            "angle": 0,
            "content": "6.2 \\(f\\) 为正定二次函数 \\(f(x) = \\frac{1}{2} x^{\\mathrm{T}}Ax + b^{\\mathrm{T}}x\\), \\(d^{k}\\) 为下降方向, \\(x^{k}\\) 为当前迭代点. 试求出精确线搜索步长"
        },
        {
            "type": "equation",
            "bbox": [
                0.465,
                0.716,
                0.659,
                0.743
            ],
            "angle": 0,
            "content": "\\[\n\\alpha_{k} = \\operatorname *{arg  min}_{\\alpha >0}f(x^{k} + \\alpha d^{k}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.752,
                0.726,
                0.769
            ],
            "angle": 0,
            "content": "并由此推出最速下降法的步长满足(6.2.2)式（见定理6.2）"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.782,
                0.489,
                0.797
            ],
            "angle": 0,
            "content": "6.3 利用定理6.5证明推论6.2"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.811,
                0.424,
                0.827
            ],
            "angle": 0,
            "content": "6.4 考虑非光滑函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.825,
                0.655,
                0.855
            ],
            "angle": 0,
            "content": "\\[\nf(x) = \\max_{1\\leqslant i\\leqslant K}x_{i} + \\frac{1}{2}\\| x\\|^{2},\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.223,
                0.132
            ],
            "angle": 0,
            "content": "习题6"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "295"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.157,
                0.553,
                0.174
            ],
            "angle": 0,
            "content": "其中 \\(x\\in \\mathbb{R}^n\\) ， \\(K\\in [1,n]\\) 为一个给定的正整数"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.188,
                0.589,
                0.205
            ],
            "angle": 0,
            "content": "(a) 求出 \\( f(x) \\) 的最小值点 \\( x^{*} \\) 和对应的函数值 \\( f^{*} \\);"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.211,
                0.737,
                0.263
            ],
            "angle": 0,
            "content": "(b) 证明 \\( f(x) \\) 在区域 \\(\\{x \\mid \\|x\\| \\leqslant R \\stackrel{\\mathrm{def}}{=} 1 / \\sqrt{K}\\}\\) 上是 \\( G \\)-利普希茨连续的，其中 \\( G = 1 + \\frac{1}{\\sqrt{K}} \\)；"
        },
        {
            "type": "text",
            "bbox": [
                0.218,
                0.267,
                0.737,
                0.324
            ],
            "angle": 0,
            "content": "(c) 设初值 \\(x^{0} = 0\\), 考虑使用次梯度算法(6.3.1)对 \\(\\min f(x)\\) 进行求解,步长 \\(\\alpha_{k}\\) 可任意选取, 证明: 存在一种次梯度的取法, 在 \\(k (k < K)\\) 次迭代后,"
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.324,
                0.577,
                0.359
            ],
            "angle": 0,
            "content": "\\[\n\\hat {f} ^ {k} - f ^ {*} \\geqslant \\frac {G R}{2 (1 + \\sqrt {K})},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.245,
                0.366,
                0.737,
                0.418
            ],
            "angle": 0,
            "content": "其中 \\(\\hat{f}^k\\) 的定义和定理6.5相同．并根据此例子推出次梯度算法的收敛速度 \\(\\mathcal{O}\\left(\\frac{GR}{\\sqrt{K}}\\right)\\) 是不能改进的."
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.425,
                0.446,
                0.442
            ],
            "angle": 0,
            "content": "6.5 考虑非平方 \\(\\ell_2\\) 正则项优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.337,
                0.45,
                0.61,
                0.482
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad f (x) = \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2} + \\mu \\| x \\| _ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.491,
                0.591,
                0.509
            ],
            "angle": 0,
            "content": "其中 \\(A \\in \\mathbb{R}^{m \\times n}\\)，注意这个问题并不是岭回归问题。"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.522,
                0.737,
                0.56
            ],
            "angle": 0,
            "content": "(a) 若 \\(A\\) 为列正交矩阵，即 \\(A^{\\mathrm{T}}A = I\\) ，利用不可微函数的一阶最优性条件求出该优化问题的显式解；"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.568,
                0.737,
                0.647
            ],
            "angle": 0,
            "content": "(b) 对一般的 \\(A\\) 我们可以使用迭代算法来求解这个问题, 试设计出不引入次梯度的一种梯度类算法求解该优化问题. 提示: \\(f(x)\\) 仅在一点处不可导, 若这个点不是最小值点, 则次梯度算法和梯度法等价."
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.522,
                0.737,
                0.647
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.661,
                0.737,
                0.7
            ],
            "angle": 0,
            "content": "6.6 设函数 \\( f(x) = \\| x\\|^{\\beta} \\)，其中 \\( \\beta > 0 \\) 为给定的常数。考虑使用经典牛顿法(6.4.2)对 \\( f(x) \\) 进行极小化，初值 \\( x^0 \\neq 0 \\)。证明："
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.713,
                0.659,
                0.73
            ],
            "angle": 0,
            "content": "(a) 若 \\(\\beta > 1\\) 且 \\(\\beta \\neq 2\\), 则 \\(x^{k}\\) 收敛到 0 的速度为 Q-线性的;"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.738,
                0.473,
                0.755
            ],
            "angle": 0,
            "content": "(b) 若 \\(0 < \\beta < 1\\) ，则牛顿法发散；"
        },
        {
            "type": "text",
            "bbox": [
                0.218,
                0.765,
                0.533,
                0.781
            ],
            "angle": 0,
            "content": "(c) 试解释定理 6.6 在 (a) 中不成立的原因"
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.713,
                0.659,
                0.781
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.794,
                0.737,
                0.853
            ],
            "angle": 0,
            "content": "6.7 设矩阵 \\(A\\) 为 \\(n\\) 阶对称矩阵，\\(d^{k}\\) 为给定的非零向量．若对任意满足 \\(\\| d\\| = \\| d^k\\|\\) 的 \\(d\\in \\mathbb{R}^n\\) ，均有 \\((d - d^{k})^{\\mathrm{T}}A(d - d^{k})\\geqslant 0\\) ，证明：\\(A\\) 是半正定矩阵."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "296"
        },
        {
            "type": "header",
            "bbox": [
                0.632,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第六章 无约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.156,
                0.826,
                0.214
            ],
            "angle": 0,
            "content": "6.8 设 \\( f(x) \\) 为正定二次函数，且假定在迭代过程中 \\((s^k - H^k y^k)^{\\mathrm{T}}y^k > 0\\) 对任意的 \\( k \\) 均满足，其中 \\( H^k \\) 由SR1公式(6.5.10)产生的拟牛顿矩阵．证明："
        },
        {
            "type": "equation",
            "bbox": [
                0.448,
                0.218,
                0.677,
                0.236
            ],
            "angle": 0,
            "content": "\\[\nH ^ {k} y ^ {j} = s ^ {j}, \\quad j = 0, 1, \\dots , k - 1,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.297,
                0.246,
                0.825,
                0.305
            ],
            "angle": 0,
            "content": "其中 \\(k\\) 是任意给定的整数。这个结论说明对于正定二次函数，SR1公式产生的拟牛顿矩阵在当前点处满足割线方程，且历史迭代产生的 \\((s^j, y^j)\\) 也满足割线方程。"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.318,
                0.825,
                0.335
            ],
            "angle": 0,
            "content": "6.9 仿照 BFGS 公式的推导过程，试利用待定系数法推导 DFP 公式(6.5.15)."
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.349,
                0.664,
                0.366
            ],
            "angle": 0,
            "content": "6.10 考虑共轭梯度法中的Hestenes-Stiefel (HS)格式"
        },
        {
            "type": "equation",
            "bbox": [
                0.42,
                0.374,
                0.705,
                0.412
            ],
            "angle": 0,
            "content": "\\[\nd ^ {k + 1} = - \\nabla f (x ^ {k + 1}) + \\frac {\\nabla f (x ^ {k + 1}) ^ {\\mathrm {T}} y ^ {k}}{(y ^ {k}) ^ {\\mathrm {T}} d ^ {k}} d ^ {k},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.297,
                0.421,
                0.825,
                0.522
            ],
            "angle": 0,
            "content": "其中 \\(y^{k}\\) 的定义如(6.5.13)式．假设在迭代过程中 \\(d^{k}\\) 均为下降方向且精确搜索条件 \\(\\nabla f(x^{k + 1})^{\\mathrm{T}}d^{k} = 0\\) 满足，试说明HS格式可看成是某一种特殊的拟牛顿方法．提示：将HS格式改写为拟牛顿迭代格式，并根据此格式构造另一个拟牛顿矩阵使其满足割线方程(6.5.4)，注意拟牛顿矩阵需要满足对称性和正定性."
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.535,
                0.432,
                0.552
            ],
            "angle": 0,
            "content": "6.11 证明等式(6.5.20)."
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.566,
                0.58,
                0.582
            ],
            "angle": 0,
            "content": "6.12 设 \\(m(d)\\) 为具有如下形式的二次函数："
        },
        {
            "type": "equation",
            "bbox": [
                0.478,
                0.593,
                0.646,
                0.623
            ],
            "angle": 0,
            "content": "\\[\nm (d) = g ^ {\\mathrm {T}} d + \\frac {1}{2} d ^ {\\mathrm {T}} B d,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.633,
                0.571,
                0.649
            ],
            "angle": 0,
            "content": "其中 \\(B\\) 为对称矩阵，证明以下结论："
        },
        {
            "type": "text",
            "bbox": [
                0.304,
                0.663,
                0.825,
                0.701
            ],
            "angle": 0,
            "content": "(a) \\(m(d)\\) 存在全局极小值当且仅当 \\(B\\) 半正定且 \\(g\\) 在 \\(B\\) 的值空间中；若 \\(B\\) 半正定，则满足 \\(Bd = -g\\) 的 \\(d\\) 均为 \\(m(d)\\) 的全局极小值点；"
        },
        {
            "type": "text",
            "bbox": [
                0.304,
                0.709,
                0.68,
                0.726
            ],
            "angle": 0,
            "content": "(b) \\(m(d)\\) 的全局极小值唯一当且仅当 \\(B\\) 严格正定."
        },
        {
            "type": "list",
            "bbox": [
                0.304,
                0.663,
                0.825,
                0.726
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.739,
                0.825,
                0.777
            ],
            "angle": 0,
            "content": "6.13 (小样本问题) 设 \\(J(x) \\in \\mathbb{R}^{m \\times n}\\) 为最小二乘问题(6.7.1)中 \\(r(x)\\) 在点 \\(x\\) 处的雅可比矩阵，其中 \\(m \\ll n\\)。设 \\(J(x)\\) 行满秩，证明："
        },
        {
            "type": "equation",
            "bbox": [
                0.448,
                0.79,
                0.674,
                0.81
            ],
            "angle": 0,
            "content": "\\[\n\\hat {d} = - J (x) ^ {\\mathrm {T}} (J (x) J (x) ^ {\\mathrm {T}}) ^ {- 1} r (x)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.825,
                0.687,
                0.842
            ],
            "angle": 0,
            "content": "给出了高斯-牛顿方程(6.7.3)的一个 \\(\\ell_2\\) 范数最小解"
        }
    ],
    [
        {
            "type": "title",
            "bbox": [
                0.272,
                0.252,
                0.636,
                0.285
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.335,
                0.383,
                0.352
            ],
            "angle": 0,
            "content": "本章考虑约束优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.361,
                0.737,
                0.387
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathcal {X} _ {1}} f (x), \\tag {7.0.1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.413,
                0.382,
                0.504,
                0.397
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & x \\in \\mathcal {X}, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.408,
                0.744,
                0.551
            ],
            "angle": 0,
            "content": "这里 \\(\\mathcal{X} \\subset \\mathbb{R}^n\\) 为问题的可行域。与无约束问题不同，约束优化问题中自变量 \\(x\\) 不能任意取值，这导致许多无约束优化算法不能使用。例如梯度法中沿着负梯度方向下降所得的点未必是可行点，要寻找的最优解处目标函数的梯度也不是零向量。这使得约束优化问题比无约束优化问题要复杂许多。本章将介绍一些罚函数法，它们将约束作为惩罚项加到目标函数中，从而转化为我们熟悉的无约束优化问题求解。此外我们还针对线性规划这一特殊的约束优化问题介绍内点法，它的思想可以被应用到很多一般问题的求解。"
        },
        {
            "type": "title",
            "bbox": [
                0.374,
                0.579,
                0.534,
                0.6
            ],
            "angle": 0,
            "content": "7.1 罚函数法"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.62,
                0.452,
                0.638
            ],
            "angle": 0,
            "content": "7.1.1 等式约束的二次罚函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.651,
                0.741,
                0.709
            ],
            "angle": 0,
            "content": "上一章介绍了各种各样的求解无约束优化问题的方法。那么，我们能否通过将问题(7.0.1)变形为无约束优化问题来求解呢？为此考虑一种简单的情况，假设问题约束中仅含等式约束，即考虑问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.719,
                0.737,
                0.749
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} f (x), \\tag {7.1.1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.372,
                0.749,
                0.546,
                0.765
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & c _ {i} (x) = 0, \\quad i \\in \\mathcal {E}, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "其中变量 \\(x \\in \\mathbb{R}^n\\)，\\(\\mathcal{E}\\) 为等式约束的指标集，\\(c_i(x)\\) 为连续函数。在某些特殊场合下，可以通过直接求解（非线性）方程组 \\(c_i(x) = 0\\) 消去部分变量，将其转化为无约束问题。但对一般的函数 \\(c_i(x)\\) 来说，变量消去这一操作是不可实现的，我们必须采用其他方法来处理这种问题。"
        },
        {
            "type": "page_number",
            "bbox": [
                0.44,
                0.869,
                0.471,
                0.882
            ],
            "angle": 0,
            "content": "297"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "298"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.256
            ],
            "angle": 0,
            "content": "罚函数法的思想是将约束优化问题 (7.1.1) 转化为无约束优化问题来进行求解。为了保证解的逼近质量，无约束优化问题的目标函数为原约束优化问题的目标函数加上与约束函数有关的惩罚项。对于可行域外的点，惩罚项为正，即对该点进行惩罚；对于可行域内的点，惩罚项为0，即不做任何惩罚。因此，惩罚项会促使无约束优化问题的解落在可行域内。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.26,
                0.825,
                0.299
            ],
            "angle": 0,
            "content": "对于等式约束问题，惩罚项的选取方式有很多，结构最简单的是二次函数。这里给出二次罚函数的定义。"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.311,
                0.825,
                0.348
            ],
            "angle": 0,
            "content": "定义7.1（等式约束的二次罚函数）对等式约束最优化问题(7.1.1)，定义二次罚函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.427,
                0.356,
                0.825,
                0.391
            ],
            "angle": 0,
            "content": "\\[\nP _ {E} (x, \\sigma) = f (x) + \\frac {1}{2} \\sigma \\sum_ {i \\in \\mathcal {E}} c _ {i} ^ {2} (x), \\tag {7.1.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.402,
                0.657,
                0.419
            ],
            "angle": 0,
            "content": "其中等式右端第二项称为惩罚项，\\(\\sigma > 0\\) 称为罚因子。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.432,
                0.825,
                0.531
            ],
            "angle": 0,
            "content": "由于这种罚函数对不满足约束的点进行惩罚，在迭代过程中点列一般处于可行域之外，因此它也被称为外点罚函数。二次罚函数的特点如下：对于非可行点而言，当 \\(\\sigma\\) 变大时，惩罚项在罚函数中的权重加大，对罚函数求极小，相当于迫使其极小点向可行域靠近；在可行域中，\\(P_{E}(x, \\sigma)\\) 的全局极小点与约束最优化问题 (7.1.1) 的最优解相同。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.536,
                0.662,
                0.552
            ],
            "angle": 0,
            "content": "为了直观理解罚函数的作用，我们给出一个例子。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.566,
                0.457,
                0.581
            ],
            "angle": 0,
            "content": "例7.1 考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.473,
                0.593,
                0.612,
                0.633
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  x + \\sqrt {3} y, \\\\ \\begin{array}{l l} \\text {s . t .} & x ^ {2} + y ^ {2} = 1. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.645,
                0.705,
                0.69
            ],
            "angle": 0,
            "content": "容易求出该问题最优解为 \\(\\left(-\\frac{1}{2}, -\\frac{\\sqrt{3}}{2}\\right)^{\\mathrm{T}}\\) 考虑二次罚函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.387,
                0.699,
                0.696,
                0.73
            ],
            "angle": 0,
            "content": "\\[\nP _ {E} (x, y, \\sigma) = x + \\sqrt {3} y + \\frac {\\sigma}{2} (x ^ {2} + y ^ {2} - 1) ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.737,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "并在图7.1中绘制出 \\(\\sigma = 1\\) 和 \\(\\sigma = 10\\) 对应的罚函数的等高线。可以看出，随着 \\(\\sigma\\) 增大，二次罚函数 \\(P_{E}(x,y,\\sigma)\\) 的最小值和原问题最小值越来越接近，但最优点附近的等高线越来越趋于扁平，这导致求解无约束优化问题的难度变大。此外，当 \\(\\sigma = 10\\) 时函数出现了一个极大值，罚函数图形在 \\(\\left(-\\frac{1}{2}, -\\frac{\\sqrt{3}}{2}\\right)^{\\mathrm{T}}\\) 附近出现了一个鞍点。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.283,
                0.133
            ],
            "angle": 0,
            "content": "7.1 罚函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "299"
        },
        {
            "type": "image",
            "bbox": [
                0.193,
                0.171,
                0.443,
                0.346
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.286,
                0.367,
                0.34,
                0.38
            ],
            "angle": 0,
            "content": "(a) \\(\\sigma = 1\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.476,
                0.169,
                0.726,
                0.346
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.567,
                0.367,
                0.627,
                0.38
            ],
            "angle": 0,
            "content": "(b) \\(\\sigma = 10\\)"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.282,
                0.399,
                0.625,
                0.417
            ],
            "angle": 0,
            "content": "图7.1 二次罚函数取不同 \\(\\sigma\\) 时等高线的变化"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.45,
                0.739,
                0.509
            ],
            "angle": 0,
            "content": "从以上例子知道，给定罚因子 \\(\\sigma\\) ，我们可通过求解 \\(P_{E}(x,\\sigma)\\) 的最小值点作为原问题的近似解．但实际情况并不总是这样，下面这个例子表明，当 \\(\\sigma\\) 选取过小时罚函数可能无下界."
        },
        {
            "type": "title",
            "bbox": [
                0.205,
                0.533,
                0.37,
                0.549
            ],
            "angle": 0,
            "content": "例7.2 考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.387,
                0.568,
                0.52,
                0.588
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{r l} \\min  & - x ^ {2} + 2 y ^ {2}, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.592,
                0.484,
                0.606
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{r l} \\mathrm {s . t .} & x = 1. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.629,
                0.603,
                0.647
            ],
            "angle": 0,
            "content": "通过消去变量容易得知最优解就是 \\((1,0)^{\\mathrm{T}}\\) 。但考虑罚函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.315,
                0.663,
                0.593,
                0.693
            ],
            "angle": 0,
            "content": "\\[\nP _ {E} (x, y, \\sigma) = - x ^ {2} + 2 y ^ {2} + \\frac {\\sigma}{2} (x - 1) ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.71,
                0.447,
                0.726
            ],
            "angle": 0,
            "content": "对任意的 \\(\\sigma \\leqslant 2\\) ，该罚函数是无界的."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.751,
                0.738,
                0.809
            ],
            "angle": 0,
            "content": "出现以上现象的原因是当罚因子过小时，不可行点处的函数下降抵消了罚函数对约束违反的惩罚。实际上所有外点罚函数法均存在这个问题，因此 \\(\\sigma\\) 的初值选取不应该太小。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.816,
                0.738,
                0.854
            ],
            "angle": 0,
            "content": "我们先在算法7.1中给出等式约束的二次罚函数法，之后对每一步进行具体解释。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "300"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.259,
                0.159,
                0.431,
                0.175
            ],
            "angle": 0,
            "content": "算法7.1二次罚函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.18,
                0.636,
                0.198
            ],
            "angle": 0,
            "content": "1. 给定 \\(\\sigma_{1} > 0, x^{0}, k \\gets 1\\). 罚因子增长系数 \\(\\rho > 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.201,
                0.487,
                0.217
            ],
            "angle": 0,
            "content": "2. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.22,
                0.654,
                0.247
            ],
            "angle": 0,
            "content": "3. 以 \\(x^{k}\\) 为初始点，求解 \\(x^{k + 1} = \\underset {x}{\\arg \\min}P_{E}(x,\\sigma_{k})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.248,
                0.429,
                0.264
            ],
            "angle": 0,
            "content": "4. 选取 \\(\\sigma_{k + 1} = \\rho \\sigma_k\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.268,
                0.386,
                0.282
            ],
            "angle": 0,
            "content": "5. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.288,
                0.371,
                0.303
            ],
            "angle": 0,
            "content": "6. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.18,
                0.654,
                0.303
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.332,
                0.827,
                0.432
            ],
            "angle": 0,
            "content": "算法7.1的执行过程比较直观：即先选取一系列指数增长的罚因子 \\(\\sigma_{k}\\)，然后针对每个罚因子求解二次罚函数 \\(P_{E}(x,\\sigma_{k})\\) 的最小值点（或局部极小值点）。这种逐步增加罚因子的做法在实际中被广泛使用，例如在LASSO问题求解中这一策略被称为连续化（Continuation）。算法第三行中argmin的含义是如下情况之一："
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.447,
                0.591,
                0.464
            ],
            "angle": 0,
            "content": "(1) \\(x^{k + 1}\\) 是罚函数 \\(P_{E}(x,\\sigma_{k})\\) 的全局极小解；"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.481,
                0.591,
                0.499
            ],
            "angle": 0,
            "content": "(2) \\(x^{k + 1}\\) 是罚函数 \\(P_{E}(x,\\sigma_{k})\\) 的局部极小解；"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.515,
                0.825,
                0.554
            ],
            "angle": 0,
            "content": "(3) \\(x^{k + 1}\\) 不是罚函数 \\(P_{E}(x,\\sigma_{k})\\) 的严格的极小解，但近似满足一阶最优性条件 \\(\\nabla_{x}P_{E}(x^{k + 1},\\sigma_{k})\\approx 0.\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.447,
                0.825,
                0.554
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.57,
                0.827,
                0.753
            ],
            "angle": 0,
            "content": "根据前面的叙述，在算法7.1中需要注意如下三点：第一，对参数 \\(\\sigma_{k}\\) 的选取需要非常小心，若 \\(\\sigma_{k}\\) 增长太快，则子问题不易求解（具体分析见下一小节末尾对算法数值困难的讨论）．若增长太慢，则算法需要的外迭代数（算法中的while循环）会增多．一个比较合理的取法是根据当前 \\(P_{E}(x,\\sigma_{k})\\) 的求解难度来确定 \\(\\sigma_{k}\\) 的增幅，若当前子问题收敛很快，可以在下一步选取较大的\\(\\sigma_{k + 1}\\) ，否则就不宜过分增大 \\(\\sigma_{k}\\) ．第二，在前面的例子中我们提到了 \\(P_{E}(x,\\sigma)\\) 在 \\(\\sigma\\) 较小时可能无界，此时迭代就会发散．当求解子问题时，一旦检测到迭代点发散就应该立即终止迭代并增大罚因子．第三，子问题求解的精度必须足够精确，为保证收敛，子问题求解误差需要趋于零."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.783,
                0.421,
                0.801
            ],
            "angle": 0,
            "content": "7.1.2 收敛性分析"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "本小节讨论等式约束的二次罚函数法的收敛性. 为了讨论方便, 我们假设对每个 \\(\\sigma_{k}\\), \\(P_{E}(x, \\sigma_{k})\\) 的最小值点都是存在的. 注意这个假设对某些优化问"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "7.1 罚函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "301"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.737,
                0.195
            ],
            "angle": 0,
            "content": "题是不对的，其本质原因是因为二次罚函数的惩罚力度不够，因此我们不会使用二次罚函数法去求解不满足此假设的优化问题。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.207,
                0.742,
                0.275
            ],
            "angle": 0,
            "content": "定理7.1(二次罚函数法的收敛性1）设 \\(x^{k + 1}\\) 是 \\(P_{E}(x,\\sigma_{k})\\) 的全局极小解，\\(\\sigma_{k}\\) 单调上升趋于无穷，则 \\(\\{x^k\\}\\) 的每个极限点 \\(x^{*}\\) 都是原问题的全局极小解证明．设 \\(\\bar{x}\\) 是原问题(7.1.1)的全局极小解，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.301,
                0.29,
                0.607,
                0.308
            ],
            "angle": 0,
            "content": "\\[\nf (\\bar {x}) \\leqslant f (x), \\quad \\forall x   \\text {满 足}   c _ {i} (x) = 0,   i \\in \\mathcal {E}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.322,
                0.739,
                0.358
            ],
            "angle": 0,
            "content": "由定理条件，\\(x^{k+1}\\) 是 \\(P_E(x, \\sigma_k)\\) 的全局极小值，我们有 \\(P_E(x^{k+1}, \\sigma_k) \\leqslant P_E(\\bar{x}, \\sigma_k)\\)，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.23,
                0.359,
                0.738,
                0.392
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k + 1}\\right) + \\frac {\\sigma_ {k}}{2} \\sum_ {i \\in \\mathcal {E}} c _ {i} ^ {2} \\left(x ^ {k + 1}\\right) \\leqslant f (\\bar {x}) + \\frac {\\sigma_ {k}}{2} \\sum_ {i \\in \\mathcal {E}} c _ {i} ^ {2} (\\bar {x}) = f (\\bar {x}), \\tag {7.1.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.398,
                0.245,
                0.414
            ],
            "angle": 0,
            "content": "整理可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.325,
                0.411,
                0.737,
                0.446
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i \\in \\mathcal {E}} c _ {i} ^ {2} \\left(x ^ {k + 1}\\right) \\leqslant \\frac {2}{\\sigma_ {k}} \\left(f (\\bar {x}) - f \\left(x ^ {k + 1}\\right)\\right). \\tag {7.1.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.453,
                0.738,
                0.492
            ],
            "angle": 0,
            "content": "设 \\(x^{*}\\) 是 \\(\\{x^k\\}\\) 的一个极限点，为了方便，不妨设 \\(x^{k}\\to x^{*}\\) ：在(7.1.4)式中令\\(k\\rightarrow \\infty\\) ，根据 \\(c_{i}(x)\\) 和 \\(f(x)\\) 的连续性以及 \\(\\sigma_k\\to +\\infty\\) 可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.399,
                0.504,
                0.507,
                0.534
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i \\in \\mathcal {E}} c _ {i} ^ {2} (x ^ {*}) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.545,
                0.738,
                0.605
            ],
            "angle": 0,
            "content": "这说明 \\(x^{*}\\) 是原问题的一个可行解．由(7.1.3)式可得 \\(f(x^{k + 1})\\leqslant f(\\bar{x})\\) ，两边取极限得 \\(f(x^{*})\\leqslant f(\\bar{x})\\) ．由 \\(\\bar{x}\\) 的最优性可知 \\(f(x^{*}) = f(\\bar{x})\\) ，即 \\(x^{*}\\) 也是全局极小解. □"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.618,
                0.741,
                0.699
            ],
            "angle": 0,
            "content": "以上定理表明，若可以找到子问题的全局极小解，则它们的极限点为原问题的最小值点。但实际应用当中，求 \\( P_{E}(x, \\sigma_{k}) \\) 的全局极小解是难以做到的，我们只能将子问题求解到一定精度。因此定理7.1的应用场合十分有限。下面给出另一个定理，它从最优性条件给出了迭代点列的收敛性。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.71,
                0.763,
                0.79
            ],
            "angle": 0,
            "content": "定理7.2（二次罚函数法的收敛性2）设 \\(f(x)\\) 与 \\(c_{i}(x), i \\in \\mathcal{E}\\) 连续可微，正数序列 \\(\\varepsilon_{k} \\to 0, \\sigma_{k} \\to +\\infty\\) ，在算法7.1中，子问题的解 \\(x^{k+1}\\) 满足 \\(\\|\\nabla_{x} P_{E}(x^{k+1}, \\sigma_{k})\\| \\leqslant \\varepsilon_{k}\\)，而对 \\(\\{x^{k}\\}\\) 的任何极限点 \\(x^{*}\\)，都有 \\(\\{\\nabla c_{i}(x^{*}), i \\in \\mathcal{E}\\}\\) 线性无关，则 \\(x^{*}\\) 是等式约束最优化问题(7.1.1)的KKT点，且"
        },
        {
            "type": "equation",
            "bbox": [
                0.326,
                0.803,
                0.737,
                0.828
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\left(- \\sigma_ {k} c _ {i} \\left(x ^ {k + 1}\\right)\\right) = \\lambda_ {i} ^ {*}, \\quad \\forall i \\in \\mathcal {E}, \\tag {7.1.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.53,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(\\lambda_i^*\\) 是约束 \\(c_{i}(x^{*}) = 0\\) 对应的拉格朗日乘子."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "302"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.536,
                0.174
            ],
            "angle": 0,
            "content": "证明. 容易求出 \\(P_{E}(x, \\sigma_{k})\\) 的梯度为"
        },
        {
            "type": "equation",
            "bbox": [
                0.393,
                0.186,
                0.824,
                0.214
            ],
            "angle": 0,
            "content": "\\[\n\\nabla P _ {E} (x, \\sigma_ {k}) = \\nabla f (x) + \\sum_ {i \\in \\mathcal {E}} \\sigma_ {k} c _ {i} (x) \\nabla c _ {i} (x). \\tag {7.1.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.222,
                0.594,
                0.24
            ],
            "angle": 0,
            "content": "根据子问题求解的终止准则，对 \\(x^{k + 1}\\) 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.386,
                0.247,
                0.824,
                0.289
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| \\nabla f \\left(x ^ {k + 1}\\right) + \\sum_ {i \\in \\mathcal {E}} \\sigma_ {k} c _ {i} \\left(x ^ {k + 1}\\right) \\nabla c _ {i} \\left(x ^ {k + 1}\\right) \\right\\| \\leqslant \\varepsilon_ {k}. \\tag {7.1.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.295,
                0.711,
                0.312
            ],
            "angle": 0,
            "content": "利用三角不等式 \\(\\|a\\| - \\|b\\| \\leqslant \\|a + b\\|\\) 可以把(7.1.7)式变形为"
        },
        {
            "type": "equation",
            "bbox": [
                0.366,
                0.32,
                0.824,
                0.362
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| \\sum_ {i \\in \\mathcal {E}} c _ {i} \\left(x ^ {k + 1}\\right) \\nabla c _ {i} \\left(x ^ {k + 1}\\right) \\right\\| \\leqslant \\frac {1}{\\sigma_ {k}} \\left(\\varepsilon_ {k} + \\| \\nabla f \\left(x ^ {k + 1}\\right) \\|\\right). \\tag {7.1.8}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.369,
                0.824,
                0.407
            ],
            "angle": 0,
            "content": "为了方便，不妨设 \\(\\{x^k\\}\\) 收敛于 \\(x^{*}\\)，在(7.1.8)式中不等号两边同时令 \\(k\\to \\infty\\) 根据 \\(f(x),c_{i}(x)\\) 的连续性，"
        },
        {
            "type": "equation",
            "bbox": [
                0.463,
                0.418,
                0.621,
                0.446
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i \\in \\mathcal {E}} c _ {i} (x ^ {*}) \\nabla c _ {i} (x ^ {*}) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.454,
                0.824,
                0.491
            ],
            "angle": 0,
            "content": "又由于 \\(\\nabla c_{i}(x^{*})\\) 线性无关，此时必有 \\(c_{i}(x^{*}) = 0, \\forall i \\in \\mathcal{E}\\)。这表明 \\(x^{*}\\) 实际上是一个可行点。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.495,
                0.824,
                0.534
            ],
            "angle": 0,
            "content": "以下我们说明点 \\(x^{*}\\) 满足KKT条件中的梯度条件，为此需要构造拉格朗日乘子 \\(\\lambda^{*} = (\\lambda_{1}^{*},\\lambda_{2}^{*},\\dots ,\\lambda_{|\\mathcal{E}|}^{*})^{\\mathrm{T}}\\) ，其中 \\(|\\mathcal{E}|\\) 表示 \\(\\mathcal{E}\\) 中元素的个数．记"
        },
        {
            "type": "equation",
            "bbox": [
                0.466,
                0.546,
                0.823,
                0.562
            ],
            "angle": 0,
            "content": "\\[\n\\nabla c (x) = \\left[ \\nabla c _ {i} (x) \\right] _ {i \\in \\varepsilon}, \\tag {7.1.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.573,
                0.824,
                0.61
            ],
            "angle": 0,
            "content": "并定义 \\(\\lambda_i^k = -\\sigma_k c_i(x^{k + 1})\\) ， \\(\\lambda^k = (\\lambda_1^*,\\lambda_2^*,\\dots ,\\lambda_{|\\mathcal{E}|}^*)^{\\mathrm{T}}\\) ，则梯度式(7.1.6)可以改写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.389,
                0.614,
                0.695,
                0.633
            ],
            "angle": 0,
            "content": "\\[\n\\nabla c (x ^ {k + 1}) \\lambda^ {k} = \\nabla f (x ^ {k + 1}) - \\nabla P _ {E} (x ^ {k + 1}, \\sigma_ {k}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.64,
                0.824,
                0.679
            ],
            "angle": 0,
            "content": "由条件知 \\(\\nabla c(x^{*})\\) 是列满秩矩阵，而 \\(x^{k}\\rightarrow x^{*}\\) ，因此当 \\(k\\) 充分大时， \\(\\nabla c(x^{k + 1})\\) 应该是列满秩矩阵，进而可以利用 \\(\\nabla c(x^{k + 1})\\) 的广义逆来表示 \\(\\lambda^k\\) ："
        },
        {
            "type": "equation",
            "bbox": [
                0.288,
                0.689,
                0.797,
                0.709
            ],
            "angle": 0,
            "content": "\\[\n\\lambda^ {k} = \\left(\\nabla c \\left(x ^ {k + 1}\\right) ^ {\\mathrm {T}} \\nabla c \\left(x ^ {k + 1}\\right)\\right) ^ {- 1} \\nabla c \\left(x ^ {k + 1}\\right) ^ {\\mathrm {T}} \\left(\\nabla f \\left(x ^ {k + 1}\\right) - \\nabla_ {x} P _ {E} \\left(x ^ {k + 1}, \\sigma_ {k}\\right)\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.72,
                0.734,
                0.737
            ],
            "angle": 0,
            "content": "等式两侧关于 \\(k\\) 取极限, 并注意到 \\(\\nabla_{x} P_{E}\\left(x^{k+1}, \\sigma_{k}\\right) \\rightarrow 0\\), 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.746,
                0.737,
                0.772
            ],
            "angle": 0,
            "content": "\\[\n\\lambda^ {*} \\stackrel {\\text {d e f}} {=} \\lim  _ {k \\rightarrow \\infty} \\lambda^ {k} = (\\nabla c (x ^ {*}) ^ {\\mathrm {T}} \\nabla c (x ^ {*})) ^ {- 1} \\nabla c (x ^ {*}) ^ {\\mathrm {T}} \\nabla f (x ^ {*}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.779,
                0.573,
                0.795
            ],
            "angle": 0,
            "content": "最后在梯度表达式(7.1.6)中令 \\(k \\to \\infty\\) 可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.449,
                0.807,
                0.636,
                0.825
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x ^ {*}) - \\nabla c (x ^ {*}) \\lambda^ {*} = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.824,
                0.853
            ],
            "angle": 0,
            "content": "这说明KKT条件中的梯度条件成立， \\(\\lambda^{*}\\) 就是点 \\(x^{*}\\) 对应的拉格朗日乘子."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "7.1 罚函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "303"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.157,
                0.744,
                0.299
            ],
            "angle": 0,
            "content": "在定理7.2的证明过程中，还可以得到一个推论：不管 \\(\\{\\nabla c_i(x^*)\\}\\) 是否线性无关，通过算法7.1给出解 \\(x^{k}\\) 的聚点总是 \\(\\phi (x) = \\| c(x)\\| ^2\\) 的一个稳定点.这说明即便没有找到可行解，我们也找到了使得约束 \\(c(x) = 0\\) 违反度相对较小的一个解．此外，定理7.2虽然不要求每一个子问题精确求解，但要获得原问题的解，子问题解的精度需要越来越高．它并没有给出一个非渐进的误差估计，即没有说明当给定原问题解的目标精度时，子问题的求解精度 \\(\\varepsilon_{k}\\) 应该如何选取."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.302,
                0.741,
                0.382
            ],
            "angle": 0,
            "content": "作为等式约束二次罚函数法的总结，最后简要分析一下算法7.1的数值困难。我们知道，要想得到原问题的解，罚因子 \\(\\sigma_{k}\\) 必须趋于正无穷。以下从矩阵条件数的角度说明，当 \\(\\sigma_{k}\\) 趋于正无穷时，子问题求解难度会显著变大。考虑罚函数 \\(P_{E}(x,\\sigma)\\) 的海瑟矩阵："
        },
        {
            "type": "equation",
            "bbox": [
                0.2,
                0.394,
                0.741,
                0.425
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {x x} ^ {2} P _ {E} (x, \\sigma) = \\nabla^ {2} f (x) + \\sum_ {i \\in \\mathcal {E}} \\sigma c _ {i} (x) \\nabla^ {2} c _ {i} (x) + \\sigma \\nabla c (x) \\nabla c (x) ^ {\\mathrm {T}}, \\tag {7.1.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.434,
                0.744,
                0.493
            ],
            "angle": 0,
            "content": "其中 \\(\\nabla c(x)\\) 如(7.1.9)式定义．我们现在考虑当 \\(x\\) 接近最优点时，海瑟矩阵的变化情况．由定理7.2，在 \\(x\\approx x^{*}\\) 时，应该有 \\(-\\sigma c_{i}(x)\\approx \\lambda_{i}^{*}\\) .根据这一近似，我们可以使用拉格朗日函数 \\(L(x,\\lambda^{*})\\) 来近似(7.1.10)式等号右边的前两项："
        },
        {
            "type": "equation",
            "bbox": [
                0.284,
                0.506,
                0.739,
                0.526
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {x x} ^ {2} P _ {E} (x, \\sigma) \\approx \\nabla_ {x x} ^ {2} L (x, \\lambda^ {*}) + \\sigma \\nabla c (x) \\nabla c (x) ^ {\\mathrm {T}}, \\tag {7.1.11}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.538,
                0.741,
                0.66
            ],
            "angle": 0,
            "content": "其中 \\(\\nabla c(x)\\nabla c(x)^{\\mathrm{T}}\\) 是一个半正定矩阵且奇异，它有 \\((n - |\\mathcal{E}|)\\) 个特征值都是0．注意，(7.1.11)式右边包含两个矩阵：一个定值矩阵和一个最大特征值趋于正无穷的奇异矩阵．从直观上来说，海瑟矩阵 \\(\\nabla_{xx}^2 P_E(x,\\sigma)\\) 的条件数将会越来越大，这意味着子问题的等高线越来越密集，使用梯度类算法求解将会变得非常困难．若使用牛顿法，则求解牛顿方程本身就是一个非常困难的问题．因此在实际应用中，我们不可能令罚因子趋于正无穷."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.685,
                0.492,
                0.703
            ],
            "angle": 0,
            "content": "7.1.3 一般约束问题的二次罚函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.716,
                0.741,
                0.754
            ],
            "angle": 0,
            "content": "上一小节仅仅考虑了等式约束优化问题，那么对于不等式约束的问题应该如何设计二次罚函数呢？不等式约束优化问题有如下形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.368,
                0.766,
                0.737,
                0.79
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {(x)} f (x), \\tag {7.1.12}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.379,
                0.788,
                0.538,
                0.803
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & c _ {i} (x) \\leqslant 0, i \\in \\mathcal {I}. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "显然，它和等式约束优化问题最大的不同就是允许 \\(c_{i}(x) < 0\\) 发生，而若采用原来的方式定义罚函数为 \\(\\| c(x)\\|^2\\) ，它也会惩罚 \\(c_{i}(x) < 0\\) 的可行点，这显"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "304"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "然不是我们需要的。针对问题(7.1.12)，我们必须对原有二次罚函数进行改造来得到新的二次罚函数，它应该具有如下特点：仅仅惩罚 \\( c_{i}(x) > 0 \\) 的那些点，而对可行点不作惩罚。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.229,
                0.825,
                0.266
            ],
            "angle": 0,
            "content": "定义7.2（不等式约束的二次罚函数）对不等式约束最优化问题(7.1.12)，定义二次罚函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.274,
                0.825,
                0.311
            ],
            "angle": 0,
            "content": "\\[\nP _ {I} (x, \\sigma) = f (x) + \\frac {1}{2} \\sigma \\sum_ {i \\in \\mathcal {I}} \\tilde {c} _ {i} ^ {2} (x), \\tag {7.1.13}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.321,
                0.628,
                0.338
            ],
            "angle": 0,
            "content": "其中等式右端第二项称为惩罚项，\\(\\tilde{c}_i(x)\\) 的定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.456,
                0.353,
                0.825,
                0.371
            ],
            "angle": 0,
            "content": "\\[\n\\tilde {c} _ {i} (x) = \\max  \\left\\{c _ {i} (x), 0 \\right\\}, \\tag {7.1.14}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.386,
                0.436,
                0.402
            ],
            "angle": 0,
            "content": "常数 \\(\\sigma > 0\\) 称为罚因子."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.416,
                0.825,
                0.516
            ],
            "angle": 0,
            "content": "注意到函数 \\(h(t) = (\\min \\{t,0\\})^2\\) 关于 \\(t\\) 是可导的，因此 \\(P_{I}(x,\\sigma)\\) 的梯度也存在，可以使用梯度类算法来求解子问题．然而一般来讲 \\(P_{I}(x,\\sigma)\\) 不是二阶可导的，因此不能直接利用二阶算法（如牛顿法）求解子问题，这也是不等式约束问题二次罚函数的不足之处．求解不等式约束问题的罚函数法的结构和算法7.1完全相同，这里略去相关说明."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.52,
                0.812,
                0.536
            ],
            "angle": 0,
            "content": "一般的约束优化问题可能既含等式约束又含不等式约束，它的形式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.456,
                0.55,
                0.548,
                0.568
            ],
            "angle": 0,
            "content": "\\[\n\\min  f (x),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.466,
                0.571,
                0.825,
                0.588
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & c _ {i} (x) = 0, i \\in \\mathcal {E}, \\end{array} \\tag {7.1.15}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.504,
                0.592,
                0.626,
                0.609
            ],
            "angle": 0,
            "content": "\\[\nc _ {i} (x) \\leqslant 0, i \\in \\mathcal {I}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.622,
                0.825,
                0.658
            ],
            "angle": 0,
            "content": "针对这个问题，我们只需要将两种约束的罚函数相加就能得到一般约束优化问题的二次罚函数。"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.673,
                0.824,
                0.709
            ],
            "angle": 0,
            "content": "定义7.3（一般约束的二次罚函数）对一般约束最优化问题(7.1.15)，定义二次罚函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.718,
                0.825,
                0.754
            ],
            "angle": 0,
            "content": "\\[\nP (x, \\sigma) = f (x) + \\frac {1}{2} \\sigma \\left[ \\sum_ {i \\in \\mathcal {E}} c _ {i} ^ {2} (x) + \\sum_ {i \\in \\mathcal {I}} \\tilde {c} _ {i} ^ {2} (x) \\right], \\tag {7.1.16}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.765,
                0.825,
                0.802
            ],
            "angle": 0,
            "content": "其中等式右端第二项称为惩罚项，\\(\\tilde{c}_i(x)\\) 的定义如(7.1.14)式，常数 \\(\\sigma > 0\\) 称为罚因子。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "同样地，我们可以使用合适的办法来求解罚函数子问题，在这里不再叙述具体算法。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "7.1 罚函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "305"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.156,
                0.314,
                0.174
            ],
            "angle": 0,
            "content": "7.1.4 应用举例"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.202,
                0.737,
                0.24
            ],
            "angle": 0,
            "content": "许多优化问题建模都可以看成是应用了罚函数的思想，因此罚函数法也可自然应用到这类问题的求解中."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.31,
                0.338,
                0.326
            ],
            "angle": 0,
            "content": "1. LASSO问题求解"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.355,
                0.344,
                0.371
            ],
            "angle": 0,
            "content": "考虑 LASSO 问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.345,
                0.395,
                0.562,
                0.427
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\frac {1}{2} \\| A x - b \\| ^ {2} + \\mu \\| x \\| _ {1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.452,
                0.737,
                0.489
            ],
            "angle": 0,
            "content": "其中 \\(\\mu > 0\\) 是正则化参数。我们知道求解LASSO问题的最终目标是为了解决如下基追踪(BP)问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.514,
                0.509,
                0.55
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\quad \\| x \\| _ {1}, \\\\ \\begin{array}{l l} \\text {s . t .} & A x = b, \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.583,
                0.737,
                0.621
            ],
            "angle": 0,
            "content": "在这里 \\(A x = b\\) 是一个欠定方程组. 注意到 BP 问题是一个等式约束的非光滑优化问题, 我们使用二次罚函数作用于等式约束 \\(A x = b\\), 可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.349,
                0.646,
                0.559,
                0.676
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\| x \\| _ {1} + \\frac {\\sigma}{2} \\| A x - b \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.701,
                0.74,
                0.854
            ],
            "angle": 0,
            "content": "令 \\(\\mu = \\frac{1}{\\sigma}\\)，则容易看出使用 \\(\\frac{1}{\\mu}\\) 作为二次罚因子时，BP问题的罚函数子问题就等价于LASSO问题。这一观察至少说明了以下两点：第一，LASSO问题的解和BP问题的解本身不等价，当 \\(\\mu\\) 趋于0时，LASSO问题的解收敛到BP问题的解；第二，当 \\(\\mu\\) 比较小时，根据之前的讨论，此时BP问题罚函数比较病态，若直接求解则收敛速度会很慢。根据罚函数的思想，罚因子应该逐渐增加到无穷，这等价于在LASSO问题中先取一个较大的 \\(\\mu\\)，之后再不断缩小 \\(\\mu\\) 直至达到我们所要求解的值。具体算法在算法7.2中给出。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "306"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "code_caption",
            "bbox": [
                0.26,
                0.159,
                0.542,
                0.175
            ],
            "angle": 0,
            "content": "算法7.2 LASSO问题求解的罚函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.179,
                0.78,
                0.199
            ],
            "angle": 0,
            "content": "1. 给定初值 \\(x^0\\) ，最终参数 \\(\\mu\\) ，初始参数 \\(\\mu_0\\) ，因子 \\(\\gamma \\in (0,1)\\) ，\\(k \\gets 0\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.201,
                0.416,
                0.219
            ],
            "angle": 0,
            "content": "2. while \\(\\mu_k \\geqslant \\mu\\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.218,
                0.797,
                0.251
            ],
            "angle": 0,
            "content": "3. 以 \\(x^k\\) 为初值，求解问题 \\(x^{k + 1} = \\underset {x}{\\arg \\min}\\left\\{\\frac{1}{2}\\| Ax - b\\| ^2 +\\mu_k\\| x\\| _1\\right\\} .\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.251,
                0.415,
                0.266
            ],
            "angle": 0,
            "content": "4. if \\(\\mu_{k} = \\mu\\) then"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.269,
                0.486,
                0.286
            ],
            "angle": 0,
            "content": "5. 停止迭代，输出 \\(x^{k + 1}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.292,
                0.341,
                0.306
            ],
            "angle": 0,
            "content": "6. else"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.311,
                0.57,
                0.33
            ],
            "angle": 0,
            "content": "7. 更新罚因子 \\(\\mu_{k + 1} = \\max \\{\\mu ,\\gamma \\mu_k\\}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.333,
                0.402,
                0.347
            ],
            "angle": 0,
            "content": "8. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.353,
                0.357,
                0.367
            ],
            "angle": 0,
            "content": "9. end if"
        },
        {
            "type": "text",
            "bbox": [
                0.265,
                0.374,
                0.371,
                0.388
            ],
            "angle": 0,
            "content": "10. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.265,
                0.179,
                0.797,
                0.388
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.418,
                0.831,
                0.455
            ],
            "angle": 0,
            "content": "求解 LASSO 子问题可以使用之前介绍过的次梯度法, 也可以使用第八章将提到的多种非光滑函数的优化方法求解. 图7.2展示了分别使用罚函数法"
        },
        {
            "type": "image",
            "bbox": [
                0.264,
                0.473,
                0.523,
                0.63
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.338,
                0.639,
                0.446,
                0.652
            ],
            "angle": 0,
            "content": "(a) 函数值变化趋势"
        },
        {
            "type": "image",
            "bbox": [
                0.564,
                0.474,
                0.822,
                0.63
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.626,
                0.639,
                0.759,
                0.652
            ],
            "angle": 0,
            "content": "(b) 约束违反度变化趋势"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.347,
                0.672,
                0.735,
                0.687
            ],
            "angle": 0,
            "content": "图7.2 使用次梯度法和罚函数法求解LASSO问题"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.712,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "和次梯度法求解 LASSO 问题的结果, 其中使用与第 6.2 节中同样的 \\(A\\) 和 \\(b\\), 并取 LASSO 问题的正则化参数为 \\(\\mu = 10^{-3}\\). 在罚函数法中, 令 \\(\\mu\\) 从 10 开始, 因子 \\(\\gamma = 0.1\\), 次梯度法选取固定步长 \\(\\alpha = 0.0002\\). 从图 7.2 中我们可明显看到罚函数法的效果比直接使用次梯度法要好, 在迭代初期, 由于 \\(\\mu\\) 较大, 这意味着约束 \\(Ax = b\\) 可以不满足, 从而算法可将优化的重点放到 \\(\\|x\\|_1\\) 上; 随着迭代进行, \\(\\mu\\) 单调减小, 此时算法将更注重于可行性 (\\(\\|Ax - b\\|\\) 的大小). 直接取 \\(\\mu = 10^{-3}\\) 效果不佳, 这是因为惩罚项 \\(\\|Ax - b\\|^2\\) 的权重太大,"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.283,
                0.133
            ],
            "angle": 0,
            "content": "7.1 罚函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "307"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.739,
                0.236
            ],
            "angle": 0,
            "content": "次梯度法会尽量使得迭代点满足 \\(A x = b\\), 而忽视了 \\(\\|x\\|_{1}\\) 项的作用, 图7.2也可说明这一问题. 在每个 LASSO 子问题中我们使用了次梯度法求解, 若使用第八章的近似点梯度法求解子问题, 那么算法7.2等价于求解 \\(\\ell_{1}\\) 极小化问题的 FPC (fixed-point continuation) 算法 [95]."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.262,
                0.31,
                0.278
            ],
            "angle": 0,
            "content": "2. 矩阵补全问题"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.292,
                0.737,
                0.33
            ],
            "angle": 0,
            "content": "第一章介绍了低秩矩阵恢复问题（又称矩阵补全问题），并引入了该问题的两种形式，即问题(1.3.2)和问题(1.3.3). 若考虑问题(1.3.2)，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.344,
                0.562,
                0.384
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\quad \\| X \\| _ {*}, \\\\ \\begin{array}{r l} \\mathbf {s . t .} & X _ {i j} = M _ {i j}, (i, j) \\in \\Omega , \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.396,
                0.536,
                0.412
            ],
            "angle": 0,
            "content": "我们对其中的等式约束引入二次罚函数可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.424,
                0.585,
                0.46
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad \\| X \\| _ {*} + \\frac {\\sigma}{2} \\sum_ {(i, j) \\in \\Omega} \\left(X _ {i j} - M _ {i j}\\right) ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.473,
                0.57,
                0.506
            ],
            "angle": 0,
            "content": "当罚因子 \\(\\sigma = \\frac{1}{\\mu}\\) 时，罚函数恰好对应问题(1.3.3)，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.318,
                0.518,
                0.737,
                0.556
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad \\mu \\| X \\| _ {*} + \\frac {1}{2} \\sum_ {(i, j) \\in \\Omega} \\left(X _ {i j} - M _ {i j}\\right) ^ {2}. \\tag {7.1.17}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.568,
                0.698,
                0.585
            ],
            "angle": 0,
            "content": "因此我们可以使用罚函数法的策略求解矩阵补全问题，具体见算法7.3。"
        },
        {
            "type": "title",
            "bbox": [
                0.172,
                0.602,
                0.463,
                0.618
            ],
            "angle": 0,
            "content": "算法7.3矩阵补全问题求解的罚函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.623,
                0.696,
                0.642
            ],
            "angle": 0,
            "content": "1. 给定初值 \\(X^0\\) ，最终参数 \\(\\mu\\) ，初始参数 \\(\\mu_0\\) ，因子 \\(\\gamma \\in (0,1)\\) ， \\(k\\gets 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.645,
                0.329,
                0.661
            ],
            "angle": 0,
            "content": "2. while \\(\\mu_{k}\\geqslant \\mu\\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.664,
                0.694,
                0.683
            ],
            "angle": 0,
            "content": "3. 以 \\(X^{k}\\) 为初值, \\(\\mu = \\mu_{k}\\) 为正则化参数求解问题(7.1.17), 得 \\(X^{k + 1}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.687,
                0.329,
                0.703
            ],
            "angle": 0,
            "content": "4. if \\(\\mu_{k} = \\mu\\) then"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.706,
                0.403,
                0.723
            ],
            "angle": 0,
            "content": "5. 停止迭代，输出 \\(X^{k + 1}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.729,
                0.254,
                0.742
            ],
            "angle": 0,
            "content": "6. else"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.748,
                0.483,
                0.766
            ],
            "angle": 0,
            "content": "7. 更新罚因子 \\(\\mu_{k + 1} = \\max \\{\\mu ,\\gamma \\mu_k\\}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.77,
                0.316,
                0.784
            ],
            "angle": 0,
            "content": "8. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.79,
                0.27,
                0.804
            ],
            "angle": 0,
            "content": "9. end if"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.811,
                0.298,
                0.825
            ],
            "angle": 0,
            "content": "10. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.177,
                0.832,
                0.284,
                0.845
            ],
            "angle": 0,
            "content": "11. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.177,
                0.623,
                0.696,
                0.845
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "308"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.827,
                0.278
            ],
            "angle": 0,
            "content": "由于我们还没有学习如何处理带核范数的优化问题，这里先跳过子问题求解的叙述。实际上求解子问题(7.1.17)可使用之后讲到的近似点梯度法和加速近似点梯度法（包括FISTA算法[16, 181])。如果罚函数法使用（不精确）近似点梯度法求解每个子问题，则算法7.3实际上等价于FPC(fixed-pointcontinuation)或FPCA(fixed-pointcontinuation withapproximateSVD)算法[130]。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.31,
                0.499,
                0.327
            ],
            "angle": 0,
            "content": "7.1.5 其他类型的罚函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.343,
                0.696,
                0.359
            ],
            "angle": 0,
            "content": "作为本节的扩展，我们再介绍一些其他类型的罚函数。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.39,
                0.396,
                0.406
            ],
            "angle": 0,
            "content": "1. 内点罚函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.422,
                0.825,
                0.522
            ],
            "angle": 0,
            "content": "前面介绍的二次罚函数均属于外点罚函数，即在求解过程中允许自变量 \\(x\\) 位于原问题可行域之外，当罚因子趋于无穷时，子问题最优解序列从可行域外部逼近最优解。自然地，如果我们想要使得子问题最优解序列从可行域内部逼近最优解，则需要构造内点罚函数。顾名思义，内点罚函数在迭代时始终要求自变量 \\(x\\) 不能违反约束，因此它主要用于不等式约束优化问题。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.526,
                0.825,
                0.584
            ],
            "angle": 0,
            "content": "考虑含不等式约束的优化问题(7.1.12)，为了使得迭代点始终在可行域内，当迭代点趋于可行域边界时，我们需要罚函数趋于正无穷，常用的罚函数是对数罚函数。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.602,
                0.825,
                0.639
            ],
            "angle": 0,
            "content": "定义7.4（对数罚函数）对不等式约束最优化问题(7.1.12)，定义对数罚函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.413,
                0.656,
                0.825,
                0.684
            ],
            "angle": 0,
            "content": "\\[\nP _ {I} (x, \\sigma) = f (x) - \\sigma \\sum_ {i \\in \\mathcal {I}} \\ln (- c _ {i} (x)), \\tag {7.1.18}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.699,
                0.657,
                0.715
            ],
            "angle": 0,
            "content": "其中等式右端第二项称为惩罚项，\\(\\sigma > 0\\) 称为罚因子。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.733,
                0.827,
                0.853
            ],
            "angle": 0,
            "content": "容易看到，\\(P_{I}(x,\\sigma)\\) 的定义域为 \\(\\{x\\mid c_i(x) < 0\\}\\)，因此在迭代过程中自变量 \\(x\\) 严格位于可行域内部。当 \\(x\\) 趋于可行域边界时，由于对数罚函数的特点，\\(P_{I}(x,\\sigma)\\) 会趋于正无穷，这说明对数罚函数的极小值严格位于可行域内部。然而，对原问题(7.1.12)，它的最优解通常位于可行域边界，即 \\(c_{i}(x)\\leqslant 0\\) 中至少有一个取到等号，此时我们需要调整罚因子 \\(\\sigma\\) 使其趋于0，这会减弱对数罚函数在边界附近的惩罚效果。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.283,
                0.133
            ],
            "angle": 0,
            "content": "7.1 罚函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "309"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.157,
                0.368,
                0.173
            ],
            "angle": 0,
            "content": "例7.3 考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.334,
                0.185,
                0.571,
                0.203
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad x ^ {2} + 2 x y + y ^ {2} + 2 x - 2 y,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.208,
                0.482,
                0.224
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & x \\geqslant 0, y \\geqslant 0, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.236,
                0.63,
                0.252
            ],
            "angle": 0,
            "content": "容易求出该问题最优解为 \\(x = 0, y = 1\\)。我们考虑对数罚函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.252,
                0.266,
                0.655,
                0.285
            ],
            "angle": 0,
            "content": "\\[\nP _ {I} (x, y, \\sigma) = x ^ {2} + 2 x y + y ^ {2} + 2 x - 2 y - \\sigma (\\ln x + \\ln y).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.299,
                0.738,
                0.315
            ],
            "angle": 0,
            "content": "并在图7.3中绘制出 \\(\\sigma = 1\\) 和 \\(\\sigma = 0.4\\) 对应的等高线。可以看出，随着 \\(\\sigma\\) 减小，"
        },
        {
            "type": "image",
            "bbox": [
                0.191,
                0.344,
                0.435,
                0.527
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.287,
                0.549,
                0.339,
                0.561
            ],
            "angle": 0,
            "content": "(a) \\(\\sigma = 1\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.476,
                0.344,
                0.717,
                0.527
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.565,
                0.549,
                0.628,
                0.561
            ],
            "angle": 0,
            "content": "(b) \\(\\sigma = 0.4\\)"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.29,
                0.58,
                0.616,
                0.597
            ],
            "angle": 0,
            "content": "图7.3 对数罚函数取不同 \\(\\sigma\\) 时等高线变化"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.619,
                0.737,
                0.657
            ],
            "angle": 0,
            "content": "对数罚函数 \\(P_{I}(x,y,\\sigma)\\) 的最小值点和原问题最小值点越来越接近，但当 \\(x\\) 和 \\(y\\) 趋于可行域边界时，对数罚函数趋于正无穷."
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.669,
                0.536,
                0.686
            ],
            "angle": 0,
            "content": "算法7.4给出了基于对数罚函数的优化方法"
        },
        {
            "type": "title",
            "bbox": [
                0.172,
                0.701,
                0.342,
                0.717
            ],
            "angle": 0,
            "content": "算法7.4对数罚函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.723,
                0.651,
                0.741
            ],
            "angle": 0,
            "content": "1. 给定 \\(\\sigma_0 > 0\\) ，可行解 \\(x^0\\) ， \\(k\\gets 0\\) .罚因子缩小系数 \\(\\rho \\in (0,1)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.744,
                0.399,
                0.76
            ],
            "angle": 0,
            "content": "2. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.763,
                0.563,
                0.78
            ],
            "angle": 0,
            "content": "3. 以 \\(x^{k}\\) 为初始点，求解 \\(x^{k + 1} = \\arg \\min P_{I}(x,\\sigma_{k})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.789,
                0.341,
                0.806
            ],
            "angle": 0,
            "content": "4. 选取 \\(\\sigma_{k + 1} = \\rho \\sigma_k\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.811,
                0.297,
                0.825
            ],
            "angle": 0,
            "content": "5. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.831,
                0.284,
                0.845
            ],
            "angle": 0,
            "content": "6. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.723,
                0.651,
                0.845
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "310"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.825,
                0.195
            ],
            "angle": 0,
            "content": "和二次罚函数法不同，算法7.4要求初始点 \\(x^0\\) 是一个可行解，这是根据对数罚函数法本身的要求．一个常用的对数罚函数收敛准则可以是"
        },
        {
            "type": "equation",
            "bbox": [
                0.452,
                0.21,
                0.632,
                0.241
            ],
            "angle": 0,
            "content": "\\[\n\\sigma_ {k} \\sum_ {i \\in \\mathcal {I}} \\ln (- c _ {i} (x ^ {k + 1})) \\leqslant \\varepsilon ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.253,
                0.825,
                0.291
            ],
            "angle": 0,
            "content": "其中 \\(\\varepsilon > 0\\) 为给定的精度。实际上，可以证明（见习题7.4）算法7.4产生的迭代点列满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.435,
                0.295,
                0.646,
                0.325
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\sigma_ {k} \\sum_ {i \\in \\mathcal {I}} \\ln (- c _ {i} (x ^ {k + 1})) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.334,
                0.825,
                0.413
            ],
            "angle": 0,
            "content": "同样地，内点罚函数法也会有类似外点罚函数法的数值困难，即当 \\(\\sigma\\) 趋于0时，子问题 \\(P_{I}(x,\\sigma)\\) 的海瑟矩阵条件数会趋于无穷，因此对子问题的求解将会越来越困难。这个现象其实也可从图7.3中发现，读者可仿照二次罚函数的情形对对数罚函数进行类似的分析。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.441,
                0.396,
                0.457
            ],
            "angle": 0,
            "content": "2. 精确罚函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.473,
                0.826,
                0.593
            ],
            "angle": 0,
            "content": "我们已经介绍了二次罚函数和对数罚函数，它们的一个共同特点就是在求解的时候必须令罚因子趋于正无穷或零，这会带来一定的数值困难。而对于有些罚函数，在问题求解时不需要令罚因子趋于正无穷（或零），这种罚函数称为精确罚函数。换句话说，若罚因子选取适当，对罚函数进行极小化得到的解恰好就是原问题的精确解。这个性质在设计算法时非常有用，使用精确罚函数的算法通常会有比较好的性质。"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.598,
                0.531,
                0.614
            ],
            "angle": 0,
            "content": "常用的精确罚函数是 \\(\\ell_1\\) 罚函数"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.63,
                0.814,
                0.647
            ],
            "angle": 0,
            "content": "定义7.5 \\((\\ell_1\\) 罚函数）对一般约束最优化问题(7.1.15)，定义 \\(\\ell_1\\) 罚函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.386,
                0.66,
                0.825,
                0.693
            ],
            "angle": 0,
            "content": "\\[\nP (x, \\sigma) = f (x) + \\sigma \\left[ \\sum_ {i \\in \\mathcal {E}} | c _ {i} (x) | + \\sum_ {i \\in \\mathcal {I}} \\tilde {c} _ {i} (x) \\right] \\tag {7.1.19}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.706,
                0.825,
                0.743
            ],
            "angle": 0,
            "content": "其中等式右端第二项称为惩罚项，\\(\\tilde{c}_i(x)\\) 的定义如(7.1.14)式，常数 \\(\\sigma > 0\\) 称为罚因子。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.759,
                0.825,
                0.817
            ],
            "angle": 0,
            "content": "在这里和二次罚函数不同，我们用绝对值代替平方来构造惩罚项，实际上是对约束的 \\(\\ell_1\\) 范数进行惩罚。注意，\\(\\ell_1\\) 罚函数不是可微函数，求解此罚函数导出的子问题依赖第八章的内容。"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.822,
                0.719,
                0.839
            ],
            "angle": 0,
            "content": "下面的定理揭示了 \\(\\ell_1\\) 罚函数的精确性，证明可参考[96]."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.368,
                0.133
            ],
            "angle": 0,
            "content": "7.2 增广拉格朗日函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "311"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.157,
                0.739,
                0.219
            ],
            "angle": 0,
            "content": "定理7.3 设 \\(x^{*}\\) 是一般约束优化问题(7.1.15)的一个严格局部极小解，且满足KKT条件(5.5.8)，其对应的拉格朗日乘子为 \\(\\lambda_{i}^{*}, i \\in \\mathcal{E} \\cup \\mathcal{I}\\)，则当罚因子 \\(\\sigma > \\sigma^{*}\\) 时，\\(x^{*}\\) 也为 \\(P(x, \\sigma)\\) 的一个局部极小解，其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.356,
                0.229,
                0.551,
                0.255
            ],
            "angle": 0,
            "content": "\\[\n\\sigma^ {*} = \\| \\lambda^ {*} \\| _ {\\infty} \\stackrel {{\\mathrm {d e f}}} {{=}} \\max  _ {i} | \\lambda_ {i} ^ {*} |.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.27,
                0.741,
                0.329
            ],
            "angle": 0,
            "content": "定理7.3说明了对于精确罚函数，当罚因子充分大（不需要是正无穷）时，原问题的极小值点就是 \\(\\ell_{1}\\) 罚函数的极小值点，这和定理7.1的结果是有区别的。"
        },
        {
            "type": "title",
            "bbox": [
                0.312,
                0.364,
                0.596,
                0.386
            ],
            "angle": 0,
            "content": "7.2 增广拉格朗日函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.405,
                0.53,
                0.422
            ],
            "angle": 0,
            "content": "在二次罚函数法中，根据定理7.2，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.433,
                0.737,
                0.467
            ],
            "angle": 0,
            "content": "\\[\nc _ {i} \\left(x ^ {k + 1}\\right) \\approx - \\frac {\\lambda_ {i} ^ {*}}{\\sigma_ {k}}, \\quad \\forall i \\in \\mathcal {E}. \\tag {7.2.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.478,
                0.739,
                0.557
            ],
            "angle": 0,
            "content": "因此，为了保证可行性，罚因子必须趋于正无穷。此时，子问题因条件数爆炸而难以求解。那么，是否可以通过对二次罚函数进行某种修正，使得对有限的罚因子，得到的逼近最优解也是可行的？增广拉格朗日函数法就是这样的一个方法。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.588,
                0.588,
                0.607
            ],
            "angle": 0,
            "content": "7.2.1 等式约束优化问题的增广拉格朗日函数法"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.621,
                0.414,
                0.638
            ],
            "angle": 0,
            "content": "1. 增广拉格朗日函数法的构造"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.653,
                0.738,
                0.712
            ],
            "angle": 0,
            "content": "增广拉格朗日函数法的每一步构造一个增广拉格朗日函数，而该函数的构造依赖于拉格朗日函数和约束的二次罚函数。具体地，对于等式约束优化问题(7.1.1)，增广拉格朗日函数定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.292,
                0.722,
                0.737,
                0.759
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\sigma} (x, \\lambda) = f (x) + \\sum_ {i \\in \\mathcal {E}} \\lambda_ {i} c _ {i} (x) + \\frac {1}{2} \\sigma \\sum_ {i \\in \\mathcal {E}} c _ {i} ^ {2} (x), \\tag {7.2.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.772,
                0.738,
                0.811
            ],
            "angle": 0,
            "content": "即在拉格朗日函数的基础上，添加约束的二次罚函数。在第 \\(k\\) 步迭代，给定罚因子 \\(\\sigma_{k}\\) 和乘子 \\(\\lambda^{k}\\)，增广拉格朗日函数 \\(L_{\\sigma_k}(x,\\lambda^k)\\) 的最小值点 \\(x^{k + 1}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.188,
                0.825,
                0.737,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {x} L _ {\\sigma_ {k}} \\left(x ^ {k + 1}, \\lambda^ {k}\\right) = \\nabla f \\left(x ^ {k + 1}\\right) + \\sum_ {i \\in \\mathcal {E}} \\left(\\lambda_ {i} ^ {k} + \\sigma_ {k} c _ {i} \\left(x ^ {k + 1}\\right)\\right) \\nabla c _ {i} \\left(x ^ {k + 1}\\right) = 0. \\tag {7.2.3}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "312"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.725,
                0.174
            ],
            "angle": 0,
            "content": "对于优化问题(7.1.1)，其最优解 \\(x^{*}\\) 以及相应的乘子 \\(\\lambda^{*}\\) 需满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.214,
                0.826,
                0.245
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f \\left(x ^ {*}\\right) + \\sum_ {i \\in \\mathcal {E}} \\lambda_ {i} ^ {*} \\nabla c _ {i} \\left(x ^ {*}\\right) = 0. \\tag {7.2.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.281,
                0.825,
                0.319
            ],
            "angle": 0,
            "content": "为使增广拉格朗日函数法产生的迭代点列收敛到 \\(x^{*}\\) ，需要保证等式(7.2.3)(7.2.4)在最优解处的一致性．因此，对于充分大的 \\(k\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.426,
                0.36,
                0.657,
                0.38
            ],
            "angle": 0,
            "content": "\\[\n\\lambda_ {i} ^ {*} \\approx \\lambda_ {i} ^ {k} + \\sigma_ {k} c _ {i} (x ^ {k + 1}), \\quad \\forall i \\in \\mathcal {E}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.42,
                0.349,
                0.436
            ],
            "angle": 0,
            "content": "上式等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.452,
                0.471,
                0.826,
                0.505
            ],
            "angle": 0,
            "content": "\\[\nc _ {i} \\left(x ^ {k + 1}\\right) \\approx \\frac {1}{\\sigma_ {k}} \\left(\\lambda_ {i} ^ {*} - \\lambda_ {i} ^ {k}\\right), \\tag {7.2.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.548,
                0.828,
                0.647
            ],
            "angle": 0,
            "content": "所以，当 \\(\\lambda_i^k\\) 足够接近 \\(\\lambda_i^*\\) 时，点 \\(x^{k + 1}\\) 处的约束违反度将会远小于 \\(\\frac{1}{\\sigma_k}\\)。注意，在(7.2.1)式中约束违反度是正比于 \\(\\frac{1}{\\sigma_k}\\) 的。即增广拉格朗日函数法可以通过有效地更新乘子来降低约束违反度。(7.2.5)式表明，乘子的一个有效的更新格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.418,
                0.688,
                0.664,
                0.709
            ],
            "angle": 0,
            "content": "\\[\n\\lambda_ {i} ^ {k + 1} = \\lambda_ {i} ^ {k} + \\sigma_ {k} c _ {i} (x ^ {k + 1}), \\quad \\forall i \\in \\mathcal {E}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.756,
                0.825,
                0.796
            ],
            "angle": 0,
            "content": "那么，我们得到问题(7.1.1)的增广拉格朗日函数法，见算法7.5，其中 \\(c(x) = [c_i(x)]_{i\\in \\varepsilon}\\) 并沿用上一节中的定义"
        },
        {
            "type": "equation",
            "bbox": [
                0.461,
                0.836,
                0.622,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\nabla c (x) = [ \\nabla c _ {i} (x) ] _ {i \\in \\mathcal {E}}.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.368,
                0.133
            ],
            "angle": 0,
            "content": "7.2 增广拉格朗日函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "313"
        },
        {
            "type": "title",
            "bbox": [
                0.172,
                0.159,
                0.394,
                0.175
            ],
            "angle": 0,
            "content": "算法7.5增广拉格朗日函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.179,
                0.737,
                0.219
            ],
            "angle": 0,
            "content": "1. 选取初始点 \\(x^0\\)，乘子 \\(\\lambda^0\\)，罚因子 \\(\\sigma_0 > 0\\)，罚因子更新常数 \\(\\rho > 0\\)，约束违反度常数 \\(\\varepsilon > 0\\) 和精度要求 \\(\\eta_k > 0\\)。并令 \\(k = 0\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.222,
                0.354,
                0.238
            ],
            "angle": 0,
            "content": "2. for \\( k = 0,1,2,\\dots \\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.242,
                0.384,
                0.259
            ],
            "angle": 0,
            "content": "3. 以 \\(x^k\\) 为初始点，求解"
        },
        {
            "type": "list",
            "bbox": [
                0.18,
                0.179,
                0.737,
                0.259
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.412,
                0.262,
                0.541,
                0.285
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} L _ {\\sigma_ {k}} (x, \\lambda^ {k}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.292,
                0.359,
                0.308
            ],
            "angle": 0,
            "content": "得到满足精度条件"
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.31,
                0.553,
                0.33
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| \\nabla_ {x} L _ {\\sigma_ {k}} \\left(x, \\lambda^ {k}\\right) \\right\\| \\leqslant \\eta_ {k}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.339,
                0.295,
                0.357
            ],
            "angle": 0,
            "content": "的解 \\(x^{k + 1}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.361,
                0.378,
                0.378
            ],
            "angle": 0,
            "content": "4. if \\(\\| c(x^{k + 1})\\| \\leqslant \\varepsilon\\) then"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.381,
                0.472,
                0.398
            ],
            "angle": 0,
            "content": "5. 返回近似解 \\(x^{k + 1},\\lambda^k\\) ，终止迭代"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.404,
                0.269,
                0.417
            ],
            "angle": 0,
            "content": "6. end if"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.422,
                0.472,
                0.44
            ],
            "angle": 0,
            "content": "7. 更新乘子: \\(\\lambda^{k+1} = \\lambda^k + \\sigma_k c(x^{k+1})\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.444,
                0.404,
                0.461
            ],
            "angle": 0,
            "content": "8. 更新罚因子： \\(\\sigma_{k + 1} = \\rho \\sigma_k\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.466,
                0.262,
                0.479
            ],
            "angle": 0,
            "content": "9. end for"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.361,
                0.472,
                0.479
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.507,
                0.737,
                0.545
            ],
            "angle": 0,
            "content": "下面以一个例子来说明增广朗日函数法相较于二次罚函数法在控制约束违反度上的优越性."
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.56,
                0.369,
                0.575
            ],
            "angle": 0,
            "content": "例7.4 考虑优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.589,
                0.737,
                0.617
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\left[ - \\sqrt {3}, 1 \\right]} x + \\sqrt {3} y, \\tag {7.2.6}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.611,
                0.522,
                0.628
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & x ^ {2} + y ^ {2} = 1. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.643,
                0.737,
                0.703
            ],
            "angle": 0,
            "content": "容易求出该问题最优解为 \\(x^{*} = \\left(-\\frac{1}{2}, - \\frac{\\sqrt{3}}{2}\\right)^{\\mathrm{T}}\\) ，相应的拉格朗日乘子 \\(\\lambda^{*} = 1\\) 我们考虑增广拉格朗日函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.235,
                0.714,
                0.673,
                0.743
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\sigma} (x, y, \\lambda) = x + \\sqrt {3} y + \\lambda \\left(x ^ {2} + y ^ {2} - 1\\right) + \\frac {\\sigma}{2} \\left(x ^ {2} + y ^ {2} - 1\\right) ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.753,
                0.737,
                0.853
            ],
            "angle": 0,
            "content": "并在图 7.4 中绘制出 \\(L_{2}(x, y, 0.9)\\) 的等高线，图中标“*”的点为原问题的最优解 \\(x^{*}\\)，标“○”的点为罚函数或增广拉格朗日函数的最优解。对于二次罚函数，其最优解约为 \\((-0.5957, -1.0319)\\)，与最优解 \\(x^{*}\\) 的欧几里得距离约为 0.1915，约束违反度约为 0.4197。对于增广拉格朗日函数，其最优解约为 \\((-0.5100, -0.8833)\\)，与最优解 \\(x^{*}\\) 的欧几里得距离约为 0.02，约束违反度约"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "314"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.825,
                0.219
            ],
            "angle": 0,
            "content": "为 0.0403 . 可以看出, 增广拉格朗日函数的最优解更接近真实解, 与最优解的距离以及约束违反度都约为二次罚函数法的 \\(\\frac{1}{10}\\). 需要注意的是, 这依赖于乘子的选取."
        },
        {
            "type": "image",
            "bbox": [
                0.28,
                0.256,
                0.523,
                0.432
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.355,
                0.452,
                0.44,
                0.466
            ],
            "angle": 0,
            "content": "(a) 二次罚函数"
        },
        {
            "type": "image",
            "bbox": [
                0.561,
                0.257,
                0.811,
                0.431
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.623,
                0.452,
                0.746,
                0.466
            ],
            "angle": 0,
            "content": "(b) 增广拉格朗日函数"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.278,
                0.485,
                0.807,
                0.503
            ],
            "angle": 0,
            "content": "图7.4 二次罚函数和增广拉格朗日函数取罚因子 \\(\\sigma = 2\\) 时等高线变化"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.527,
                0.827,
                0.65
            ],
            "angle": 0,
            "content": "随着罚因子 \\(\\sigma_{k}\\) 的变大，\\(L_{\\sigma_k}(x,\\lambda^k)\\) 关于 \\(x\\) 的海瑟矩阵的条件数也会越来越大，从而使得迭代点 \\(x^{k + 1}\\) 的求解难度提高．但是，当 \\(\\sigma_{k}\\) 和 \\(\\sigma_{k + 1}\\) 比较接近时，\\(x^{k}\\) 可以作为求解 \\(x^{k + 1}\\) 时的一个初始点，从而加快收敛．因此，罚因子 \\(\\sigma_{k}\\) 增长得不能太快．但如果 \\(\\sigma_{k}\\) 增长得太慢，迭代点列 \\(\\{x^k\\}\\) 收敛到原问题解的速度会下降．在实际中，我们需要注意参数 \\(\\rho\\) 的选取，一个经验的取法是 \\(\\rho \\in [2,10]\\)."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.68,
                0.347,
                0.696
            ],
            "angle": 0,
            "content": "2. 收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.712,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "增广拉格朗日函数作为罚函数的一种，一个关于它的自然的问题是其极小值点和原问题(7.1.1)的极小值点有什么关系．假设 \\(x^{*},\\lambda^{*}\\) 分别为等式约束问题(7.1.1)的局部极小解和相应的乘子，并且二阶充分条件成立，可以证明，在已知 \\(\\lambda^{*}\\) 的情况下，对于有限大的 \\(\\sigma\\) ， \\(x^{*}\\) 也为增广拉格朗日函数\\(L_{\\sigma}(x,\\lambda^{*})\\) 的严格局部极小解．当 \\(\\lambda^{*}\\) 未知时，对于足够接近 \\(\\lambda^{*}\\) 的 \\(\\lambda\\) 以及足够大的 \\(\\sigma\\) ，增广拉格朗日函数 \\(L_{\\sigma}(x,\\lambda)\\) 的局部极小解会与 \\(x^{*}\\) 足够接近．这也说明了增广拉格朗日函数在一定条件下是精确罚函数"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.368,
                0.133
            ],
            "angle": 0,
            "content": "7.2 增广拉格朗日函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "315"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.739,
                0.257
            ],
            "angle": 0,
            "content": "定理7.4 设 \\(x^{*}, \\lambda^{*}\\) 分别为问题(7.1.1)的局部极小解和相应的乘子，并且在点 \\(x^{*}\\) 处LICQ和二阶充分条件成立．那么，存在一个有限大的常数 \\(\\bar{\\sigma}\\)，使得对任意的 \\(\\sigma \\geqslant \\bar{\\sigma}\\)，\\(x^{*}\\) 都是 \\(L_{\\sigma}(x, \\lambda^{*})\\) 的严格局部极小解．反之，如果 \\(x^{*}\\) 为 \\(L_{\\sigma}(x, \\lambda^{*})\\) 的局部极小解且满足 \\(c_{i}(x^{*}) = 0, i \\in \\mathcal{E}\\)，那么 \\(x^{*}\\) 为问题(7.1.1)的局部极小解."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.271,
                0.714,
                0.288
            ],
            "angle": 0,
            "content": "证明. 因为 \\(x^{*}\\) 为问题 (7.1.1) 的局部极小解且二阶充分条件成立, 所以"
        },
        {
            "type": "equation",
            "bbox": [
                0.235,
                0.299,
                0.737,
                0.399
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\nabla_ {x} L (x ^ {*}, \\lambda^ {*}) = \\nabla f (x ^ {*}) + \\sum_ {i \\in \\mathcal {E}} \\lambda_ {i} ^ {*} \\nabla c _ {i} (x ^ {*}) = 0, \\\\ u ^ {\\mathrm {T}} \\nabla_ {x x} ^ {2} L \\left(x ^ {*}, \\lambda^ {*}\\right) u = u ^ {\\mathrm {T}} \\left(\\nabla^ {2} f \\left(x ^ {*}\\right) + \\sum_ {i \\in \\mathcal {E}} \\lambda_ {i} ^ {*} \\nabla^ {2} c _ {i} \\left(x ^ {*}\\right)\\right) u \\tag {7.2.7} \\\\ > 0, \\quad \\forall u \\text {满 足} \\nabla c (x ^ {*}) ^ {\\mathrm {T}} u = 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.411,
                0.738,
                0.448
            ],
            "angle": 0,
            "content": "根据上面的条件，我们来证 \\(x^{*}\\) 对于 \\(L_{\\sigma}(x,\\lambda^{*})\\) 的最优性.因为 \\(c_{i}(x^{*}) = 0,i\\in \\mathcal{E}\\) 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.27,
                0.451,
                0.64,
                0.492
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\nabla_ {x} L _ {\\sigma} \\left(x ^ {*}, \\lambda^ {*}\\right) = \\nabla_ {x} L \\left(x ^ {*}, \\lambda^ {*}\\right) = 0, \\\\ \\nabla_ {x x} ^ {2} L _ {\\sigma} \\left(x ^ {*}, \\lambda^ {*}\\right) = \\nabla_ {x x} ^ {2} L \\left(x ^ {*}, \\lambda^ {*}\\right) + \\sigma \\nabla c \\left(x ^ {*}\\right) \\nabla c \\left(x ^ {*}\\right) ^ {\\mathrm {T}}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.501,
                0.378,
                0.517
            ],
            "angle": 0,
            "content": "对于充分大的 \\(\\sigma\\) ，可以证明"
        },
        {
            "type": "equation",
            "bbox": [
                0.382,
                0.532,
                0.525,
                0.552
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {x x} ^ {2} L _ {\\sigma} \\left(x ^ {*}, \\lambda^ {*}\\right) \\succ 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.567,
                0.738,
                0.604
            ],
            "angle": 0,
            "content": "事实上，如果对于任意的 \\(\\sigma = k, k = 1,2,\\dots\\) ，都存在 \\(u_{k}\\) 满足 \\(\\| u_k\\| = 1\\) ，且使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.224,
                0.619,
                0.685,
                0.64
            ],
            "angle": 0,
            "content": "\\[\nu _ {k} ^ {\\mathrm {T}} \\nabla_ {x x} ^ {2} L _ {\\sigma} (x ^ {*}, \\lambda^ {*}) u _ {k} = u _ {k} ^ {\\mathrm {T}} \\nabla_ {x x} ^ {2} L (x ^ {*}, \\lambda^ {*}) u _ {k} + k \\| \\nabla c (x ^ {*}) ^ {\\mathrm {T}} u _ {k} \\| ^ {2} \\leqslant 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.654,
                0.194,
                0.67
            ],
            "angle": 0,
            "content": "则"
        },
        {
            "type": "equation",
            "bbox": [
                0.256,
                0.667,
                0.653,
                0.699
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| \\nabla c \\left(x ^ {*}\\right) ^ {\\mathrm {T}} u _ {k} \\right\\| ^ {2} \\leqslant - \\frac {1}{k} u _ {k} ^ {\\mathrm {T}} \\nabla_ {x x} ^ {2} L \\left(x ^ {*}, \\lambda^ {*}\\right) u _ {k} \\rightarrow 0, \\quad k \\rightarrow \\infty .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.703,
                0.564,
                0.721
            ],
            "angle": 0,
            "content": "因为 \\(\\{u_k\\}\\) 为有界序列，必存在聚点，设为 \\(u\\)。那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.309,
                0.735,
                0.599,
                0.755
            ],
            "angle": 0,
            "content": "\\[\n\\nabla c (x ^ {*}) ^ {\\mathrm {T}} u = 0, \\quad u ^ {\\mathrm {T}} \\nabla_ {x x} ^ {2} L (x ^ {*}, \\lambda^ {*}) u \\leqslant 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.77,
                0.61,
                0.787
            ],
            "angle": 0,
            "content": "这与(7.2.7)式矛盾．故存在有限大的 \\(\\bar{\\sigma}\\) ，使得当 \\(\\sigma \\geqslant \\bar{\\sigma}\\) 时，"
        },
        {
            "type": "equation",
            "bbox": [
                0.382,
                0.802,
                0.526,
                0.821
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {x x} ^ {2} L _ {\\sigma} (x ^ {*}, \\lambda^ {*}) \\succ 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.471,
                0.853
            ],
            "angle": 0,
            "content": "因而 \\(x^{*}\\) 是 \\(L_{\\sigma}(x,\\lambda^{*})\\) 的严格局部极小解"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "316"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.824,
                0.195
            ],
            "angle": 0,
            "content": "反之，如果 \\(x^{*}\\) 满足 \\(c_{i}(x^{*}) = 0\\) 且为 \\(L_{\\sigma}(x,\\lambda^{*})\\) 的局部极小解，那么对于任意与 \\(x^{*}\\) 充分接近的可行点 \\(x\\) ，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.21,
                0.686,
                0.23
            ],
            "angle": 0,
            "content": "\\[\nf (x ^ {*}) = L _ {\\sigma} (x ^ {*}, \\lambda^ {*}) \\leqslant L _ {\\sigma} (x, \\lambda^ {*}) = f (x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.244,
                0.58,
                0.262
            ],
            "angle": 0,
            "content": "因此，\\(x^{*}\\) 为问题(7.1.1)的一个局部极小解"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.246,
                0.826,
                0.259
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.276,
                0.825,
                0.335
            ],
            "angle": 0,
            "content": "对于算法7.5，通过进一步假设乘子点列的有界性以及收敛点处的约束品性，我们可以证明算法迭代产生的点列 \\(\\{x^k\\}\\) 会有子列收敛到问题(7.1.1)的一阶稳定点。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.348,
                0.825,
                0.407
            ],
            "angle": 0,
            "content": "定理7.5（增广拉格朗日函数法的收敛性）假设乘子列 \\(\\{\\lambda^k\\}\\) 是有界的，罚因子 \\(\\sigma_k \\to +\\infty, k \\to \\infty\\) ，算法7.5中精度 \\(\\eta_k \\to 0\\) ，迭代点列 \\(\\{x^k\\}\\) 的一个子序列 \\(x^{k_j+1}\\) 收敛到 \\(x^*\\)，并且在点 \\(x^*\\) 处LICQ成立．那么存在 \\(\\lambda^*\\)，满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.405,
                0.42,
                0.681,
                0.462
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\lambda^ {k _ {j} + 1} \\rightarrow \\lambda^ {*}, \\quad j \\rightarrow \\infty , \\\\ \\nabla f (x ^ {*}) + \\nabla c (x ^ {*}) \\lambda^ {*} = 0, \\quad c (x ^ {*}) = 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.477,
                0.579,
                0.495
            ],
            "angle": 0,
            "content": "证明. 对于增广拉格朗日函数 \\(L_{\\sigma_k}(x,\\lambda^k)\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.308,
                0.509,
                0.777,
                0.55
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\nabla_ {x} L _ {\\sigma_ {k}} \\left(x ^ {k + 1}, \\lambda^ {k}\\right) = \\nabla f \\left(x ^ {k + 1}\\right) + \\nabla c \\left(x ^ {k + 1}\\right) \\left(\\lambda^ {k} + \\sigma_ {k} c \\left(x ^ {k + 1}\\right)\\right) \\\\ = \\nabla f (x ^ {k + 1}) + \\nabla c (x ^ {k + 1}) \\lambda^ {k + 1} = \\nabla_ {x} L (x ^ {k + 1}, \\lambda^ {k + 1}). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.566,
                0.825,
                0.604
            ],
            "angle": 0,
            "content": "那么，对于任意使得 \\(\\mathrm{rank}(\\nabla c(x^{k_j + 1})) = m = |\\mathcal{E}|\\) 的 \\(k_{j}\\) （根据假设和点 \\(x^{*}\\) 处LICQ成立，当 \\(x^{k_j + 1}\\) 充分接近 \\(x^{*}\\) 时此式成立），"
        },
        {
            "type": "equation",
            "bbox": [
                0.258,
                0.617,
                0.824,
                0.638
            ],
            "angle": 0,
            "content": "\\[\n\\lambda^ {k _ {j} + 1} = \\left(\\nabla c (x ^ {k _ {j} + 1}) ^ {\\mathrm {T}} \\nabla c (x ^ {k _ {j} + 1})\\right) ^ {- 1} \\nabla c (x ^ {k _ {j} + 1}) ^ {\\mathrm {T}} \\left(\\nabla_ {x} L _ {\\sigma_ {k}} \\left(x ^ {k _ {j} + 1}, \\lambda^ {k _ {j}}\\right) - \\nabla f \\left(x ^ {k _ {j} + 1}\\right)\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.652,
                0.585,
                0.671
            ],
            "angle": 0,
            "content": "因为 \\(\\| \\nabla_{x}L_{\\sigma_{k}}(x^{k_{j} + 1},\\lambda^{k_{j}})\\| \\leqslant \\eta_{k_{j}}\\to 0\\) ，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.339,
                0.683,
                0.742,
                0.706
            ],
            "angle": 0,
            "content": "\\[\n\\lambda^ {k _ {j} + 1} \\rightarrow \\lambda^ {*} \\stackrel {\\text {d e f}} {=} - \\left(\\nabla c (x ^ {*}) ^ {\\mathrm {T}} \\nabla c (x ^ {*})\\right) ^ {- 1} \\nabla c (x ^ {*}) ^ {\\mathrm {T}} \\nabla f (x ^ {*})\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.721,
                0.298,
                0.736
            ],
            "angle": 0,
            "content": "以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.477,
                0.741,
                0.605,
                0.759
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {x} L \\left(x ^ {*}, \\lambda^ {*}\\right) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.769,
                0.663,
                0.788
            ],
            "angle": 0,
            "content": "而 \\(\\{\\lambda^k\\}\\) 是有界的，并且 \\(\\lambda^{k_j} + \\sigma_{k_j}c(x^{k_j + 1})\\to \\lambda^*\\)，所以"
        },
        {
            "type": "text",
            "bbox": [
                0.455,
                0.795,
                0.63,
                0.814
            ],
            "angle": 0,
            "content": "\\(\\{\\sigma_{k_j}c(x^{k_j + 1})\\}\\) 是有界的."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.823,
                0.389,
                0.839
            ],
            "angle": 0,
            "content": "又 \\(\\sigma_{k}\\rightarrow +\\infty\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.502,
                0.836,
                0.58,
                0.853
            ],
            "angle": 0,
            "content": "\\[\nc (x ^ {*}) = 0.\n\\]"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.838,
                0.825,
                0.851
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.368,
                0.133
            ],
            "angle": 0,
            "content": "7.2 增广拉格朗日函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "317"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.214
            ],
            "angle": 0,
            "content": "定理7.5依赖于乘子列 \\(\\{\\lambda_k\\}\\) 的有界性, \\(\\{x^k\\}\\) 的子序列收敛性, 以及收敛点处的 LICQ. 这里, 我们不加证明地给出更一般性的收敛结果, 证明过程可以参考 [21] 命题 2.7."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.233,
                0.739,
                0.292
            ],
            "angle": 0,
            "content": "定理7.6 (增广拉格朗日函数法的收敛性——更弱的假设) 假设 \\(x^{*}, \\lambda^{*}\\) 分别是问题(7.1.1)的严格局部极小解和相应的拉格朗日乘子，那么，存在足够大的常数 \\(\\bar{\\sigma} > 0\\) 和足够小的常数 \\(\\delta > 0\\) ，如果对某个 \\(k\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.352,
                0.303,
                0.559,
                0.336
            ],
            "angle": 0,
            "content": "\\[\n\\frac {1}{\\sigma_ {k}} \\left\\| \\lambda^ {k} - \\lambda^ {*} \\right\\| <   \\delta , \\quad \\sigma_ {k} \\geqslant \\bar {\\sigma},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.348,
                0.194,
                0.364
            ],
            "angle": 0,
            "content": "则"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.37,
                0.53,
                0.389
            ],
            "angle": 0,
            "content": "\\[\n\\lambda^ {k} \\to \\lambda^ {*}, \\quad x ^ {k} \\to x ^ {*}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.402,
                0.737,
                0.47
            ],
            "angle": 0,
            "content": "同时，如果 \\(\\limsup_{k\\to \\infty}\\sigma_k < + \\infty\\) 且 \\(\\lambda^k\\neq \\lambda^*,\\forall k\\) ，则 \\(\\{\\lambda^k\\}\\) 收敛的速度是Q-线性的；如果 \\(\\limsup_{k\\to \\infty}\\sigma_k = +\\infty\\) 且 \\(\\lambda^k\\neq \\lambda^*,\\forall k\\) ，则 \\(\\{\\lambda^k\\}\\) 收敛的速度是Q-超线性的."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.488,
                0.741,
                0.555
            ],
            "angle": 0,
            "content": "定理7.6不需要假设 \\(\\{\\sigma_k\\}\\) 趋于正无穷（尽管由 \\(\\lim_{k\\to \\infty}\\sup_{k\\to \\infty}\\sigma_k = +\\infty\\) 可以推出Q-超线性收敛）以及 \\(\\{x^k\\}\\) 的子序列收敛性．相应地，这里需要找到合适的 \\(\\lambda^k\\) 和 \\(\\sigma_{k}\\)"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.585,
                0.588,
                0.604
            ],
            "angle": 0,
            "content": "7.2.2 一般约束优化问题的增广拉格朗日函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.619,
                0.383,
                0.635
            ],
            "angle": 0,
            "content": "对于一般约束优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.368,
                0.651,
                0.462,
                0.669
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\min  & f (x), \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.675,
                0.737,
                0.693
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & c _ {i} (x) = 0, i \\in \\mathcal {E}, \\end{array} \\tag {7.2.8}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.416,
                0.7,
                0.54,
                0.718
            ],
            "angle": 0,
            "content": "\\[\nc _ {i} (x) \\leqslant 0, i \\in \\mathcal {I},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.733,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "也可以定义其增广拉格朗日函数以及设计相应的增广拉格朗日函数法. 在拉格朗日函数的定义中, 往往倾向于将简单的约束 (比如非负约束、盒子约束等) 保留, 对复杂的约束引入乘子. 这里, 对于带不等式约束的优化问题, 我们先通过引入松弛变量将不等式约束转化为等式约束和简单的非负约束, 再对保留非负约束形式的拉格朗日函数添加等式约束的二次罚函数来构造增广拉格朗日函数."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "318"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.157,
                0.432,
                0.174
            ],
            "angle": 0,
            "content": "1. 增广拉格朗日函数"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.187,
                0.75,
                0.204
            ],
            "angle": 0,
            "content": "对于问题 (7.2.8)，通过引入松弛变量可以得到如下等价形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.216,
                0.531,
                0.238
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, s} f (x),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.452,
                0.247,
                0.824,
                0.271
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & c _ {i} (x) = 0, i \\in \\mathcal {E}, \\end{array} \\tag {7.2.9}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.49,
                0.271,
                0.642,
                0.287
            ],
            "angle": 0,
            "content": "\\[\nc _ {i} (x) + s _ {i} = 0, i \\in \\mathcal {I},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.49,
                0.296,
                0.584,
                0.31
            ],
            "angle": 0,
            "content": "\\[\ns _ {i} \\geqslant 0, i \\in \\mathcal {I}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.323,
                0.556,
                0.339
            ],
            "angle": 0,
            "content": "保留非负约束，可以构造拉格朗日函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.304,
                0.351,
                0.779,
                0.381
            ],
            "angle": 0,
            "content": "\\[\nL (x, s, \\lambda , \\mu) = f (x) + \\sum_ {i \\in \\mathcal {E}} \\lambda_ {i} c _ {i} (x) + \\sum_ {i \\in \\mathcal {I}} \\mu_ {i} \\left(c _ {i} (x) + s _ {i}\\right), s _ {i} \\geqslant 0, i \\in \\mathcal {I}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.39,
                0.654,
                0.407
            ],
            "angle": 0,
            "content": "记问题(7.2.9)中等式约束的二次罚函数为 \\(p(x,s)\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.407,
                0.418,
                0.676,
                0.448
            ],
            "angle": 0,
            "content": "\\[\np (x, s) = \\sum_ {i \\in \\mathcal {E}} c _ {i} ^ {2} (x) + \\sum_ {i \\in \\mathcal {I}} \\left(c _ {i} (x) + s _ {i}\\right) ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.457,
                0.512,
                0.473
            ],
            "angle": 0,
            "content": "我们构造增广拉格朗日函数如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.485,
                0.737,
                0.516
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\sigma} (x, s, \\lambda , \\mu) = f (x) + \\sum_ {i \\in \\mathcal {E}} \\lambda_ {i} c _ {i} (x) + \\sum_ {i \\in \\mathcal {I}} \\mu_ {i} \\left(c _ {i} (x) + s _ {i}\\right) +\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.455,
                0.518,
                0.631,
                0.547
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\sigma}{2} p (x, s), \\quad s _ {i} \\geqslant 0, i \\in \\mathcal {I},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.554,
                0.389,
                0.57
            ],
            "angle": 0,
            "content": "其中 \\(\\sigma\\) 为罚因子"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.595,
                0.448,
                0.611
            ],
            "angle": 0,
            "content": "2. 增广拉格朗日函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.624,
                0.827,
                0.641
            ],
            "angle": 0,
            "content": "在第 \\(k\\) 步迭代中，给定乘子 \\(\\lambda^k,\\mu^k\\) 和罚因子 \\(\\sigma_{k}\\) ，我们需要求解如下问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.411,
                0.654,
                0.825,
                0.678
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, s} \\quad L _ {\\sigma_ {k}} \\left(x, s, \\lambda^ {k}, \\mu^ {k}\\right), \\quad \\text {s . t .} \\quad s \\geqslant 0 \\tag {7.2.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.688,
                0.825,
                0.747
            ],
            "angle": 0,
            "content": "以得到 \\(x^{k + 1}, s^{k + 1}\\). 求解问题(7.2.10)的一个有效的方法是投影梯度法（将在第八章的近似点梯度法中介绍）. 另外一种方法是消去 \\(s\\), 求解只关于 \\(x\\) 的优化问题. 具体地, 固定 \\(x\\), 关于 \\(s\\) 的子问题可以表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.376,
                0.755,
                0.706,
                0.789
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {s \\geqslant 0} \\sum_ {i \\in \\mathcal {I}} \\mu_ {i} (c _ {i} (x) + s _ {i}) + \\frac {\\sigma_ {k}}{2} \\sum_ {i \\in \\mathcal {I}} (c _ {i} (x) + s _ {i}) ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.797,
                0.824,
                0.814
            ],
            "angle": 0,
            "content": "根据凸优化问题的最优性理论，\\(s\\) 为以上问题的一个全局最优解，当且仅当"
        },
        {
            "type": "equation",
            "bbox": [
                0.405,
                0.821,
                0.825,
                0.857
            ],
            "angle": 0,
            "content": "\\[\ns _ {i} = \\max  \\left\\{- \\frac {\\mu_ {i}}{\\sigma_ {k}} - c _ {i} (x), 0 \\right\\}, \\quad i \\in \\mathcal {I}. \\tag {7.2.11}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.368,
                0.133
            ],
            "angle": 0,
            "content": "7.2 增广拉格朗日函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "319"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.397,
                0.175
            ],
            "angle": 0,
            "content": "将 \\(s_i\\) 的表达式代入 \\(L_{\\sigma_k}\\) 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.23,
                0.182,
                0.737,
                0.261
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} L _ {\\sigma_ {k}} \\left(x, \\lambda^ {k}, \\mu^ {k}\\right) = f (x) + \\sum_ {i \\in \\mathcal {E}} \\lambda_ {i} c _ {i} (x) + \\frac {\\sigma_ {k}}{2} \\sum_ {i \\in \\mathcal {E}} c _ {i} ^ {2} (x) + \\\\ \\frac {\\sigma_ {k}}{2} \\sum_ {i \\in \\mathcal {I}} \\left(\\max  \\left\\{\\frac {\\mu_ {i}}{\\sigma_ {k}} + c _ {i} (x), 0 \\right\\} ^ {2} - \\frac {\\mu_ {i} ^ {2}}{\\sigma_ {k} ^ {2}}\\right), \\tag {7.2.12} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.268,
                0.739,
                0.304
            ],
            "angle": 0,
            "content": "其为关于 \\(x\\) 的连续可微函数（如果 \\(f(x), c_{i}(x), i \\in \\mathcal{I} \\cup \\mathcal{E}\\) 连续可微）。因此，问题 (7.2.10)等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.378,
                0.308,
                0.531,
                0.331
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} L _ {\\sigma_ {k}} (x, \\lambda^ {k}, \\mu^ {k}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.335,
                0.737,
                0.372
            ],
            "angle": 0,
            "content": "并可以利用梯度法进行求解。这样做的一个好处是，我们消去了变量 \\(s\\)，从而在低维空间 \\(\\mathbb{R}^n\\) 中（问题 (7.2.10)的决策空间为 \\(\\mathbb{R}^{n + |\\mathcal{I}|}\\)）求解极小点。"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.376,
                0.7,
                0.394
            ],
            "angle": 0,
            "content": "对于问题(7.2.9)，其最优解 \\(x^{*},s^{*}\\) 和乘子 \\(\\lambda^*,\\mu^*\\) 需满足KKT条件："
        },
        {
            "type": "equation",
            "bbox": [
                0.29,
                0.402,
                0.627,
                0.434
            ],
            "angle": 0,
            "content": "\\[\n0 = \\nabla f (x ^ {*}) + \\sum_ {i \\in \\mathcal {E}} \\lambda_ {i} ^ {*} \\nabla c _ {i} (x ^ {*}) + \\sum_ {i \\in \\mathcal {I}} \\mu_ {i} ^ {*} \\nabla c _ {i} (x ^ {*}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.284,
                0.438,
                0.389,
                0.457
            ],
            "angle": 0,
            "content": "\\[\n\\mu_ {i} ^ {*} \\geqslant 0, i \\in \\mathcal {I},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.463,
                0.388,
                0.482
            ],
            "angle": 0,
            "content": "\\[\ns _ {i} ^ {*} \\geqslant 0, i \\in \\mathcal {I}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.491,
                0.446,
                0.509
            ],
            "angle": 0,
            "content": "问题(7.2.10)的最优解 \\(x^{k + 1},s^{k + 1}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.282,
                0.517,
                0.644,
                0.548
            ],
            "angle": 0,
            "content": "\\[\n0 = \\nabla f (x ^ {k + 1}) + \\sum_ {i \\in \\mathcal {E}} \\left(\\lambda_ {i} ^ {k} + \\sigma_ {k} c _ {i} (x ^ {k + 1})\\right) \\nabla c _ {i} (x ^ {k + 1}) +\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.311,
                0.552,
                0.609,
                0.583
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i \\in \\mathcal {I}} \\left(\\mu_ {i} ^ {k} + \\sigma_ {k} \\left(c _ {i} \\left(x ^ {k + 1}\\right) + s _ {i} ^ {k + 1}\\right)\\right) \\nabla c _ {i} \\left(x ^ {k + 1}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.263,
                0.586,
                0.573,
                0.622
            ],
            "angle": 0,
            "content": "\\[\ns _ {i} ^ {k + 1} = \\max  \\left\\{- \\frac {\\mu_ {i} ^ {k}}{\\sigma_ {k}} - c _ {i} (x ^ {k + 1}), 0 \\right\\}, \\quad i \\in \\mathcal {I}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.629,
                0.68,
                0.645
            ],
            "angle": 0,
            "content": "对比问题(7.2.9)和问题(7.2.10)的KKT条件，易知乘子的更新格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.302,
                0.654,
                0.539,
                0.674
            ],
            "angle": 0,
            "content": "\\[\n\\lambda_ {i} ^ {k + 1} = \\lambda_ {i} ^ {k} + \\sigma_ {k} c _ {i} (x ^ {k + 1}), \\quad i \\in \\mathcal {E},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.305,
                0.678,
                0.605,
                0.699
            ],
            "angle": 0,
            "content": "\\[\n\\mu_ {i} ^ {k + 1} = \\max  \\left\\{\\mu_ {i} ^ {k} + \\sigma_ {k} c _ {i} \\left(x ^ {k + 1}\\right), 0 \\right\\}, \\quad i \\in \\mathcal {I}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.708,
                0.434,
                0.724
            ],
            "angle": 0,
            "content": "对于等式约束，约束违反度定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.271,
                0.732,
                0.636,
                0.775
            ],
            "angle": 0,
            "content": "\\[\nv _ {k} (x ^ {k + 1}) = \\sqrt {\\sum_ {i \\in \\mathcal {E}} c _ {i} ^ {2} (x ^ {k + 1}) + \\sum_ {i \\in \\mathcal {I}} \\left(c _ {i} (x ^ {k + 1}) + s _ {i} ^ {k + 1}\\right) ^ {2}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.781,
                0.457,
                0.798
            ],
            "angle": 0,
            "content": "根据 (7.2.11) 式消去 \\(s\\), 约束违反度为"
        },
        {
            "type": "equation",
            "bbox": [
                0.253,
                0.806,
                0.654,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nv _ {k} \\left(x ^ {k + 1}\\right) = \\sqrt {\\sum_ {i \\in \\mathcal {E}} c _ {i} ^ {2} \\left(x ^ {k + 1}\\right) + \\sum_ {i \\in \\mathcal {I}} \\max  \\left\\{c _ {i} \\left(x ^ {k + 1}\\right) , - \\frac {\\mu_ {i} ^ {k}}{\\sigma_ {k}} \\right\\} ^ {2}}.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "320"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.157,
                0.827,
                0.277
            ],
            "angle": 0,
            "content": "综上，我们给出约束优化问题 (7.2.10) 的增广拉格朗日函数法，见算法 7.6. 该算法和算法 7.5 结构相似，但它给出了算法参数的一种具体更新方式。每次计算出子问题的近似解 \\( x^{k+1} \\) 后，算法需要判断约束违反度 \\( v_{k}(x^{k+1}) \\) 是否满足精度要求。若满足，则进行乘子的更新，并提高子问题求解精度，此时罚因子不变；若不满足，则不进行乘子的更新，并适当增大罚因子以便得到约束违反度更小的解。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.321,
                0.616,
                0.339
            ],
            "angle": 0,
            "content": "7.2.3 凸优化问题的增广拉格朗日函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.358,
                0.424,
                0.374
            ],
            "angle": 0,
            "content": "考虑凸优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.395,
                0.824,
                0.441
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x \\in \\mathbb {R} ^ {n}} f (x), \\tag {7.2.13} \\\\ \\begin{array}{l l} \\text {s . t .} & c _ {i} (x) \\leqslant 0, i = 1, 2, \\dots , m, \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.46,
                0.825,
                0.519
            ],
            "angle": 0,
            "content": "其中 \\(f:\\mathbb{R}^n\\to \\mathbb{R},c_i:\\mathbb{R}^n\\to \\mathbb{R},i = 1,2,\\dots ,m\\) 为闭凸函数．为了叙述的方便，这里考虑不等式形式的凸优化问题．定义可行域为 \\(\\mathcal{X} = \\{x\\mid c_i(x)\\leqslant 0,i = 1,2,\\dots ,m\\}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.525,
                0.825,
                0.562
            ],
            "angle": 0,
            "content": "对于问题 (7.2.13)，根据上一小节介绍的不等式约束的增广拉格朗日函数表达式 (7.2.12)（这里 \\(\\mathcal{E} = \\emptyset\\)），其增广拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.334,
                0.579,
                0.749,
                0.622
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\sigma} (x, \\lambda) = f (x) + \\frac {\\sigma}{2} \\sum_ {i = 1} ^ {m} \\left(\\max  \\left\\{\\frac {\\lambda_ {i}}{\\sigma} + c _ {i} (x), 0 \\right\\} ^ {2} - \\frac {\\lambda_ {i} ^ {2}}{\\sigma^ {2}}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.638,
                0.531,
                0.655
            ],
            "angle": 0,
            "content": "其中 \\(\\lambda\\) 和 \\(\\sigma\\) 分别为乘子以及罚因子"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.662,
                0.825,
                0.699
            ],
            "angle": 0,
            "content": "给定一列单调递增的乘子 \\(\\sigma_{k} \\uparrow \\sigma_{\\infty}\\), 以及初始乘子 \\(\\lambda^{0}\\), 问题 (7.2.13) 的增广拉格朗日函数法为"
        },
        {
            "type": "equation",
            "bbox": [
                0.295,
                0.715,
                0.825,
                0.774
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{l} x ^ {k + 1} \\approx \\underset {x \\in \\mathbb {R} ^ {n}} {\\arg \\min } L _ {\\sigma_ {k}} (x, \\lambda^ {k}), \\\\ \\lambda^ {k + 1} = \\lambda^ {k} + \\sigma_ {k} \\nabla_ {\\lambda} L _ {\\sigma_ {k}} \\left(x ^ {k + 1}, \\lambda^ {k}\\right) = \\max  \\left\\{0, \\lambda^ {k} + \\sigma_ {k} c \\left(x ^ {k + 1}\\right) \\right\\}. \\end{array} \\right. \\tag {7.2.14}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "为了方便叙述，以下定义 \\(\\phi_{k}(x) = L_{\\sigma_{k}}(x,\\lambda^{k})\\) ．由于 \\(\\phi_k(x)\\) 的最小值点的显式表达式通常是未知的，我们往往调用迭代算法求其一个近似解．为了保证收敛性，我们要求该近似解至少满足如下非精确条件之一（参考[163,"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.368,
                0.133
            ],
            "angle": 0,
            "content": "7.2 增广拉格朗日函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.735,
                0.131
            ],
            "angle": 0,
            "content": "321"
        },
        {
            "type": "title",
            "bbox": [
                0.171,
                0.194,
                0.5,
                0.21
            ],
            "angle": 0,
            "content": "算法7.6问题(7.2.10)的增广拉格朗日函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.215,
                0.737,
                0.28
            ],
            "angle": 0,
            "content": "1. 选取初始点 \\(x^0\\)，乘子 \\(\\lambda^0, \\mu^0\\)，罚因子 \\(\\sigma_0 > 0\\)，约束违反度常数 \\(\\varepsilon > 0\\)，精度常数 \\(\\eta > 0\\)，以及常数 \\(0 < \\alpha \\leqslant \\beta \\leqslant 1\\) 和 \\(\\rho > 1\\)。令 \\(\\eta_0 = \\frac{1}{\\sigma_0}, \\varepsilon_0 = \\frac{1}{\\sigma_0^\\alpha}\\) 以及 \\(k = 0\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.284,
                0.353,
                0.3
            ],
            "angle": 0,
            "content": "2. for \\( k = 0,1,2,\\dots \\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.305,
                0.383,
                0.321
            ],
            "angle": 0,
            "content": "3. 以 \\(x^k\\) 为初始点，求解"
        },
        {
            "type": "list",
            "bbox": [
                0.18,
                0.215,
                0.737,
                0.321
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.401,
                0.336,
                0.551,
                0.361
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} L _ {\\sigma_ {k}} (x, \\lambda^ {k}, \\mu^ {k}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.371,
                0.357,
                0.387
            ],
            "angle": 0,
            "content": "得到满足精度条件"
        },
        {
            "type": "equation",
            "bbox": [
                0.387,
                0.402,
                0.567,
                0.422
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| L _ {\\sigma_ {k}} \\left(x ^ {k + 1}, \\lambda^ {k}, \\mu^ {k}\\right) \\right\\| _ {2} \\leqslant \\eta_ {k}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.436,
                0.295,
                0.453
            ],
            "angle": 0,
            "content": "的解 \\(x^{k + 1}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.457,
                0.375,
                0.475
            ],
            "angle": 0,
            "content": "4. if \\(v_{k}(x^{k + 1})\\leqslant \\varepsilon_{k}\\) then"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.477,
                0.603,
                0.496
            ],
            "angle": 0,
            "content": "5. if \\(v_{k}(x^{k + 1})\\leqslant \\varepsilon\\) 且 \\(\\| \\nabla_xL_{\\sigma_k}(x^{k + 1},\\lambda^k,\\mu^k)\\| _2\\leqslant \\eta\\) then"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.499,
                0.513,
                0.517
            ],
            "angle": 0,
            "content": "6. 得到逼近解 \\(x^{k + 1},\\lambda^k,\\mu^k\\) ，终止迭代"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.521,
                0.286,
                0.535
            ],
            "angle": 0,
            "content": "7. end if"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.541,
                0.313,
                0.557
            ],
            "angle": 0,
            "content": "8. 更新乘子："
        },
        {
            "type": "list",
            "bbox": [
                0.183,
                0.457,
                0.603,
                0.557
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.334,
                0.57,
                0.57,
                0.59
            ],
            "angle": 0,
            "content": "\\[\n\\lambda_ {i} ^ {k + 1} = \\lambda_ {i} ^ {k} + \\sigma_ {k} c _ {i} (x ^ {k + 1}), \\quad i \\in \\mathcal {E},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.594,
                0.635,
                0.614
            ],
            "angle": 0,
            "content": "\\[\n\\mu_ {i} ^ {k + 1} = \\max  \\left\\{\\mu_ {i} ^ {k} + \\sigma_ {k} c _ {i} \\left(x ^ {k + 1}\\right), 0 \\right\\}, \\quad i \\in \\mathcal {I}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.637,
                0.414,
                0.653
            ],
            "angle": 0,
            "content": "9. 罚因子不变： \\(\\sigma_{k + 1} = \\sigma_k\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.178,
                0.654,
                0.699,
                0.687
            ],
            "angle": 0,
            "content": "10. 减小子问题求解误差和约束违反度： \\(\\eta_{k + 1} = \\frac{\\eta_k}{\\sigma_{k + 1}},\\varepsilon_{k + 1} = \\frac{\\varepsilon_k}{\\sigma_{k + 1}^\\beta}.\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.178,
                0.687,
                0.253,
                0.7
            ],
            "angle": 0,
            "content": "11. else"
        },
        {
            "type": "text",
            "bbox": [
                0.178,
                0.705,
                0.4,
                0.721
            ],
            "angle": 0,
            "content": "12. 乘子不变: \\(\\lambda^{k+1} = \\lambda^k\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.178,
                0.727,
                0.421,
                0.743
            ],
            "angle": 0,
            "content": "13. 更新罚因子： \\(\\sigma_{k + 1} = \\rho \\sigma_k\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.178,
                0.743,
                0.698,
                0.776
            ],
            "angle": 0,
            "content": "14. 调整子问题求解误差和约束违反度: \\(\\eta_{k+1} = \\frac{1}{\\sigma_{k+1}}, \\varepsilon_{k+1} = \\frac{1}{\\sigma_{k+1}^{\\alpha}}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.178,
                0.776,
                0.268,
                0.789
            ],
            "angle": 0,
            "content": "15. end if"
        },
        {
            "type": "text",
            "bbox": [
                0.178,
                0.796,
                0.262,
                0.809
            ],
            "angle": 0,
            "content": "16. end for"
        },
        {
            "type": "list",
            "bbox": [
                0.178,
                0.637,
                0.699,
                0.809
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "322"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.347,
                0.174
            ],
            "angle": 0,
            "content": "204, 214]):"
        },
        {
            "type": "equation",
            "bbox": [
                0.301,
                0.185,
                0.825,
                0.221
            ],
            "angle": 0,
            "content": "\\[\n\\phi_ {k} \\left(x ^ {k + 1}\\right) - \\inf  \\phi_ {k} \\leqslant \\frac {\\varepsilon_ {k} ^ {2}}{2 \\sigma_ {k}}, \\quad \\varepsilon_ {k} \\geqslant 0, \\sum_ {k = 1} ^ {\\infty} \\varepsilon_ {k} <   + \\infty , \\tag {7.2.15}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.301,
                0.225,
                0.825,
                0.261
            ],
            "angle": 0,
            "content": "\\[\n\\phi_ {k} \\left(x ^ {k + 1}\\right) - \\inf  \\phi_ {k} \\leqslant \\frac {\\delta_ {k} ^ {2}}{2 \\sigma_ {k}} \\| \\lambda^ {k + 1} - \\lambda^ {k} \\| _ {2} ^ {2}, \\quad \\delta_ {k} \\geqslant 0, \\sum_ {k = 1} ^ {\\infty} \\delta_ {k} <   + \\infty , \\tag {7.2.16}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.294,
                0.265,
                0.825,
                0.297
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {d i s t} \\left(0, \\partial \\phi_ {k} \\left(x ^ {k + 1}\\right)\\right) \\leqslant \\frac {\\delta_ {k} ^ {\\prime}}{\\sigma_ {k}} \\| \\lambda^ {k + 1} - \\lambda^ {k} \\| _ {2}, \\quad 0 \\leqslant \\delta_ {k} ^ {\\prime} \\rightarrow 0, \\tag {7.2.17}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.309,
                0.825,
                0.348
            ],
            "angle": 0,
            "content": "其中 \\(\\varepsilon_{k},\\delta_{k},\\delta_{k}^{\\prime}\\) 是人为设定的参数， \\(\\mathrm{dist}(0,\\partial \\phi_k(x^{k + 1}))\\) 表示0到集合 \\(\\partial \\phi_k(x^{k + 1})\\) 的欧几里得距离．根据 \\(\\lambda^{k + 1}\\) 的更新格式(7.2.14)，容易得知"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.362,
                0.72,
                0.404
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| \\lambda^ {k + 1} - \\lambda^ {k} \\right\\| _ {2} = \\left\\| \\max  \\left\\{0, \\lambda^ {k} + \\sigma_ {k} c \\left(x ^ {k + 1}\\right) \\right\\} - \\lambda^ {k} \\right\\| _ {2} \\\\ = \\left\\| \\max  \\left\\{- \\lambda^ {k}, \\sigma_ {k} c \\left(x ^ {k + 1}\\right) \\right\\} \\right\\| _ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.418,
                0.83,
                0.476
            ],
            "angle": 0,
            "content": "由于 \\(\\inf \\phi_{k}\\) 是未知的，直接验证上述不精确条件中的(7.2.15)式和(7.2.16)式是数值上不可行的．但是，如果 \\(\\phi_{k}\\) 是 \\(\\alpha\\) -强凸函数（在某些应用中可以计算出 \\(\\alpha\\) 或得到其估计值)，那么（见习题2.15）"
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.487,
                0.825,
                0.518
            ],
            "angle": 0,
            "content": "\\[\n\\phi_ {k} (x) - \\inf  \\phi_ {k} \\leqslant \\frac {1}{2 \\alpha} \\operatorname {d i s t} ^ {2} (0, \\partial \\phi_ {k} (x)). \\tag {7.2.18}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.528,
                0.741,
                0.545
            ],
            "angle": 0,
            "content": "根据 (7.2.18)式，可以进一步构造如下数值可验证的不精确条件："
        },
        {
            "type": "equation",
            "bbox": [
                0.311,
                0.558,
                0.679,
                0.592
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {d i s t} \\left(0, \\partial \\phi_ {k} \\left(x ^ {k + 1}\\right)\\right) \\leqslant \\sqrt {\\frac {\\alpha}{\\sigma_ {k}}} \\varepsilon_ {k}, \\quad \\varepsilon_ {k} \\geqslant 0, \\sum_ {k = 1} ^ {\\infty} \\varepsilon_ {k} <   + \\infty ,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.311,
                0.598,
                0.775,
                0.633
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {d i s t} \\left(0, \\partial \\phi_ {k} \\left(x ^ {k + 1}\\right)\\right) \\leqslant \\sqrt {\\frac {\\alpha}{\\sigma_ {k}}} \\delta_ {k} \\| \\lambda^ {k + 1} - \\lambda^ {k} \\| _ {2}, \\quad \\delta_ {k} \\geqslant 0, \\sum_ {k = 1} ^ {\\infty} \\delta_ {k} <   + \\infty ,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.311,
                0.637,
                0.682,
                0.669
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {d i s t} \\left(0, \\partial \\phi_ {k} \\left(x ^ {k + 1}\\right)\\right) \\leqslant \\frac {\\delta_ {k} ^ {\\prime}}{\\sigma_ {k}} \\| \\lambda^ {k + 1} - \\lambda^ {k} \\| _ {2}, \\quad 0 \\leqslant \\delta_ {k} ^ {\\prime} \\rightarrow 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.681,
                0.825,
                0.718
            ],
            "angle": 0,
            "content": "这里，我们给出不精确条件(7.2.15)下的增广拉格朗日函数法(7.2.14)的收敛性．证明细节可以参考[163]定理4."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.732,
                0.825,
                0.811
            ],
            "angle": 0,
            "content": "定理7.7（凸问题的增广拉格朗日函数法的收敛性）假设 \\(\\{x^k\\}, \\{\\lambda^k\\}\\) 为问题(7.2.13)的增广拉格朗日函数法(7.2.14)生成的序列，\\(x^{k+1}\\) 满足不精确条件(7.2.15). 如果问题(7.2.13)的Slater约束品性成立，那么序列 \\(\\{\\lambda^k\\}\\) 是有界且收敛的，记极限为 \\(\\lambda^\\infty\\) ，则 \\(\\lambda^\\infty\\) 为对偶问题的一个最优解."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "如果存在一个 \\(\\gamma\\) ，使得下水平集 \\(\\{x\\in \\mathcal{X}|f(x)\\leqslant \\gamma \\}\\) 是非空有界的，那么序列 \\(\\{x^k\\}\\) 也是有界的，并且其所有的聚点都是问题(7.2.13)的最优解."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.368,
                0.133
            ],
            "angle": 0,
            "content": "7.2 增广拉格朗日函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "323"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.214
            ],
            "angle": 0,
            "content": "注7.1 这里的乘子 \\(\\lambda^k\\) 与文章[163]中的互为相反数，其原因是在构造拉格朗日函数的时候，我们引入的乘子为 \\(-\\lambda (\\geqslant 0)\\)，而文章[163]引入的乘子为 \\(\\lambda (\\geqslant 0)\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.227,
                0.736,
                0.263
            ],
            "angle": 0,
            "content": "注7.2 和定理7.7类似，同样有基于不精确条件(7.2.16)和(7.2.17)的收敛性结果．见[163]定理5."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.289,
                0.53,
                0.308
            ],
            "angle": 0,
            "content": "7.2.4 基追踪问题的增广拉格朗日函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.32,
                0.737,
                0.399
            ],
            "angle": 0,
            "content": "这一小节将以基追踪（BP）问题为例讨论增广拉格朗日函数法及其收敛性。我们将看到针对一些凸问题，增广拉格朗日函数法会有比较特殊的性质，比如固定罚因子也能保证算法的收敛性甚至有限终止性等。本小节的内容主要参考了[206, 214]。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.403,
                0.63,
                0.42
            ],
            "angle": 0,
            "content": "设 \\(A \\in \\mathbb{R}^{m \\times n}(m \\leqslant n), b \\in \\mathbb{R}^m, x \\in \\mathbb{R}^n\\), BP 问题 (4.1.8) 为"
        },
        {
            "type": "equation",
            "bbox": [
                0.348,
                0.433,
                0.737,
                0.456
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\| x \\| _ {1}, \\quad \\text {s . t .} \\quad A x = b. \\tag {7.2.19}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.463,
                0.647,
                0.48
            ],
            "angle": 0,
            "content": "引入拉格朗日乘子 \\(y \\in \\mathbb{R}^{m}\\), BP 问题 (7.2.19) 的拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.343,
                0.492,
                0.566,
                0.511
            ],
            "angle": 0,
            "content": "\\[\nL (x, y) = \\| x \\| _ {1} + y ^ {\\mathrm {T}} (A x - b),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.524,
                0.28,
                0.539
            ],
            "angle": 0,
            "content": "那么对偶函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.287,
                0.548,
                0.619,
                0.599
            ],
            "angle": 0,
            "content": "\\[\ng (y) = \\inf  _ {x} L (x, y) = \\left\\{ \\begin{array}{l l} {- b ^ {\\mathrm {T}} y,} & {\\| A ^ {\\mathrm {T}} y \\| _ {\\infty} \\leqslant 1,} \\\\ {- \\infty ,} & {\\text {其 他}.} \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.607,
                0.406,
                0.624
            ],
            "angle": 0,
            "content": "因此，我们得到如下对偶问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.335,
                0.636,
                0.737,
                0.661
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {y \\in \\mathbb {R} ^ {m}} b ^ {\\mathrm {T}} y, \\quad \\text {s . t .} \\quad \\| A ^ {\\mathrm {T}} y \\| _ {\\infty} \\leqslant 1. \\tag {7.2.20}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.671,
                0.498,
                0.687
            ],
            "angle": 0,
            "content": "通过引入变量 \\(s\\)，上述问题可以等价地写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.283,
                0.699,
                0.737,
                0.725
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {y \\in \\mathbb {R} ^ {m}, s \\in \\mathbb {R} ^ {n}} b ^ {\\mathrm {T}} y, \\quad \\text {s . t .} \\quad A ^ {\\mathrm {T}} y - s = 0, \\| s \\| _ {\\infty} \\leqslant 1. \\tag {7.2.21}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.733,
                0.643,
                0.75
            ],
            "angle": 0,
            "content": "下面讨论如何对原始问题和对偶问题应用增广拉格朗日函数法."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.774,
                0.447,
                0.79
            ],
            "angle": 0,
            "content": "1. 原始问题的增广拉格朗日函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.804,
                0.664,
                0.82
            ],
            "angle": 0,
            "content": "引入罚因子 \\(\\sigma\\) 和乘子 \\(\\lambda\\), 问题 (7.2.19)的增广拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.828,
                0.737,
                0.857
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\sigma} (x, \\lambda) = \\| x \\| _ {1} + \\lambda^ {\\mathrm {T}} (A x - b) + \\frac {\\sigma}{2} \\| A x - b \\| _ {2} ^ {2}. \\tag {7.2.22}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "324"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.256
            ],
            "angle": 0,
            "content": "在增广拉格朗日函数法的一般理论中，需要罚因子 \\(\\sigma\\) 足够大来保证迭代收敛（控制约束违反度）。对于BP问题(7.2.19)，后面可以证明，对于固定的非负罚因子也能够保证收敛性（尽管在实际中动态调整罚因子可能会使得算法更快收敛）。现在考虑固定罚因子 \\(\\sigma\\) 情形的增广拉格朗日函数法。在第 \\(k\\) 步迭代，更新格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.268,
                0.824,
                0.345
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{l} x ^ {k + 1} = \\underset {x \\in \\mathbb {R} ^ {n}} {\\arg \\min } L _ {\\sigma} \\left(x, \\lambda^ {k}\\right) = \\underset {x \\in \\mathbb {R} ^ {n}} {\\arg \\min } \\left\\{\\| x \\| _ {1} + \\frac {\\sigma}{2} \\| A x - b + \\frac {\\lambda^ {k}}{\\sigma} \\| _ {2} ^ {2} \\right\\}, \\\\ \\lambda^ {k + 1} = \\lambda^ {k} + \\sigma \\left(A x ^ {k + 1} - b\\right). \\end{array} \\right. \\tag {7.2.23}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.347,
                0.824,
                0.386
            ],
            "angle": 0,
            "content": "设迭代的初始点为 \\(x^{0} = \\lambda^{0} = 0\\). 考虑迭代格式(7.2.23)中的第一步, 假设 \\(x^{k+1}\\) 为 \\(L_{\\sigma}(x, \\lambda^{k})\\) 的一个全局极小解, 那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.389,
                0.398,
                0.691,
                0.435
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial \\| x ^ {k + 1} \\| _ {1} + \\sigma A ^ {\\mathrm {T}} \\left(A x ^ {k + 1} - b + \\frac {\\lambda^ {k}}{\\sigma}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.447,
                0.304,
                0.463
            ],
            "angle": 0,
            "content": "因此,"
        },
        {
            "type": "equation",
            "bbox": [
                0.462,
                0.469,
                0.825,
                0.488
            ],
            "angle": 0,
            "content": "\\[\n- A ^ {\\mathrm {T}} \\lambda^ {k + 1} \\in \\partial \\| x ^ {k + 1} \\| _ {1}. \\tag {7.2.24}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.501,
                0.825,
                0.54
            ],
            "angle": 0,
            "content": "满足上式的 \\(x^{k + 1}\\) 往往是不能显式得到的, 需要采用迭代算法来进行求解, 比如上一章介绍的次梯度法, 以及第八章将介绍的近似点梯度法, 等等."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.544,
                0.826,
                0.664
            ],
            "angle": 0,
            "content": "这里沿用第6.2节中的 \\(A\\) 和 \\(b\\) 的生成方式，且选取不同的稀疏度 \\(r = 0.1\\) 和 \\(r = 0.2\\)。我们固定罚因子 \\(\\sigma\\)，并采用近似点梯度法作为求解器，不精确地求解关于 \\(x\\) 的子问题以得到 \\(x^{k+1}\\)。具体地，设置求解精度 \\(\\eta_k = 10^{-k}\\)，并且使用 BB 步长作为线搜索初始步长。图7.5展示了算法产生的迭代点与最优点的距离变化以及约束违反度的走势。从图7.5中可以看到：对于 BP 问题，固定的 \\(\\sigma\\) 也可以保证增广拉格朗日函数法收敛。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.669,
                0.825,
                0.707
            ],
            "angle": 0,
            "content": "我们将证明对于固定的二次罚项系数 \\(\\sigma = 1\\) ，迭代格式(7.2.23)具有有限终止性．根据(7.2.24)式，先证明迭代格式(7.2.23)的一些基本性质."
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.723,
                0.824,
                0.761
            ],
            "angle": 0,
            "content": "引理7.1 设迭代序列 \\(\\{x^k\\}, \\{\\lambda^k\\}\\) 是算法(7.2.23)从初始点 \\(x^0 = \\lambda^0 = 0\\) 产生的序列，则它们满足"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.776,
                0.724,
                0.796
            ],
            "angle": 0,
            "content": "(1) \\(\\| Ax^k - b\\|_2\\) 是单调下降的：\\(\\| Ax^{k+1} - b\\|_2 \\leqslant \\| Ax^k - b\\|_2\\)；"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.808,
                0.67,
                0.839
            ],
            "angle": 0,
            "content": "(2) 若存在 \\(\\tilde{x}\\) 满足 \\(A\\tilde{x} = b\\)，则 \\(\\frac{\\sigma}{2}\\|Ax^k - b\\|_2^2 \\leqslant \\frac{1}{k}\\|\\tilde{x}\\|_1\\)；"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.776,
                0.724,
                0.839
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.368,
                0.133
            ],
            "angle": 0,
            "content": "7.2 增广拉格朗日函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "325"
        },
        {
            "type": "image",
            "bbox": [
                0.175,
                0.159,
                0.45,
                0.328
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.27,
                0.336,
                0.356,
                0.349
            ],
            "angle": 0,
            "content": "(a) 约束违反度"
        },
        {
            "type": "image",
            "bbox": [
                0.458,
                0.159,
                0.737,
                0.327
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.541,
                0.336,
                0.652,
                0.35
            ],
            "angle": 0,
            "content": "(b) 与最优点的距离"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.294,
                0.368,
                0.614,
                0.385
            ],
            "angle": 0,
            "content": "图7.5 增广拉格朗日函数法求解BP问题"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.414,
                0.456,
                0.43
            ],
            "angle": 0,
            "content": "证明. 由迭代格式(7.2.23)的第一步,"
        },
        {
            "type": "equation",
            "bbox": [
                0.276,
                0.441,
                0.635,
                0.501
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| x ^ {k + 1} \\right\\| _ {1} + \\left(\\lambda^ {k}\\right) ^ {\\mathrm {T}} \\left(A x ^ {k + 1} - b\\right) + \\frac {\\sigma}{2} \\left\\| A x ^ {k + 1} - b \\right\\| _ {2} ^ {2} \\\\ \\leqslant \\| x ^ {k} \\| _ {1} + \\left(\\lambda^ {k}\\right) ^ {\\mathrm {T}} \\left(A x ^ {k} - b\\right) + \\frac {\\sigma}{2} \\| A x ^ {k} - b \\| _ {2} ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.512,
                0.462,
                0.529
            ],
            "angle": 0,
            "content": "由于 \\(\\| x\\| _1\\) 的凸性和(7.2.24)式，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.306,
                0.545,
                0.601,
                0.565
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| x ^ {k + 1} \\right\\| _ {1} \\geqslant \\left\\| x ^ {k} \\right\\| _ {1} + \\left\\langle - A ^ {\\mathrm {T}} \\lambda^ {k}, x ^ {k + 1} - x ^ {k} \\right\\rangle .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.582,
                0.285,
                0.598
            ],
            "angle": 0,
            "content": "结合上面两式，"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.604,
                0.56,
                0.622
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| A x ^ {k + 1} - b \\right\\| _ {2} \\leqslant \\left\\| A x ^ {k} - b \\right\\| _ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.635,
                0.564,
                0.652
            ],
            "angle": 0,
            "content": "下面证明第二个结论．由迭代格式(7.2.23)的第二步，"
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.668,
                0.582,
                0.687
            ],
            "angle": 0,
            "content": "\\[\nA ^ {T} \\left(\\lambda^ {k + 1} - \\lambda^ {k}\\right) = \\sigma A ^ {T} \\left(A x ^ {k + 1} - b\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.7,
                0.737,
                0.742
            ],
            "angle": 0,
            "content": "由 \\(\\frac{\\sigma}{2}\\|Ax - b\\|_2^2\\) 和 \\(\\|x\\|_1\\) 的凸性（分别应用于点 \\(x^{k+1}\\) 和 \\(x^k\\) 处）以及 (7.2.24) 式，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.238,
                0.754,
                0.672,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\frac {\\sigma}{2} \\| A x ^ {k + 1} - b \\| _ {2} ^ {2} - \\frac {\\sigma}{2} \\| A x - b \\| _ {2} ^ {2} \\\\ \\leqslant \\left\\langle A ^ {\\mathrm {T}} \\left(\\lambda^ {k + 1} - \\lambda^ {k}\\right), x ^ {k + 1} - x \\right\\rangle \\\\ = \\left\\langle A ^ {\\mathrm {T}} \\lambda^ {k + 1}, x ^ {k + 1} - x \\right\\rangle - \\left\\langle A ^ {\\mathrm {T}} \\lambda^ {k}, x ^ {k} - x \\right\\rangle - \\left\\langle A ^ {\\mathrm {T}} \\lambda^ {k}, x ^ {k + 1} - x ^ {k} \\right\\rangle \\\\ \\leqslant \\left\\langle A ^ {\\mathrm {T}} \\lambda^ {k + 1}, x ^ {k + 1} - x \\right\\rangle - \\left\\langle A ^ {\\mathrm {T}} \\lambda^ {k}, x ^ {k} - x \\right\\rangle + \\| x ^ {k + 1} \\| _ {1} - \\| x ^ {k} \\| _ {1}. \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "326"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.156,
                0.56,
                0.174
            ],
            "angle": 0,
            "content": "由 \\(\\| Ax^k - b \\|_2\\) 的单调性和 \\(\\| x \\|_1\\) 的凸性，"
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.18,
                0.731,
                0.299
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} k \\left(\\frac {\\sigma}{2} \\| A x ^ {k} - b \\| _ {2} ^ {2} - \\frac {\\sigma}{2} \\| A x - b \\| _ {2} ^ {2}\\right) \\\\ \\leqslant \\sum_ {j = 1} ^ {k} \\left(\\frac {\\sigma}{2} \\| A x ^ {j} - b \\| _ {2} ^ {2} - \\frac {\\sigma}{2} \\| A x - b \\| _ {2} ^ {2}\\right) \\\\ \\leqslant \\left\\langle A ^ {\\mathrm {T}} \\lambda^ {k}, x ^ {k} - x \\right\\rangle + \\| x ^ {k} \\| _ {1} - \\left\\langle A ^ {\\mathrm {T}} \\lambda^ {0}, x ^ {0} - x \\right\\rangle - \\| x ^ {0} \\| _ {1} \\\\ \\leqslant \\| x \\| _ {1}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.308,
                0.393,
                0.324
            ],
            "angle": 0,
            "content": "取 \\(x = \\tilde{x}\\) ，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.426,
                0.33,
                0.824,
                0.361
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\sigma}{2} \\| A x ^ {k} - b \\| _ {2} ^ {2} \\leqslant \\frac {1}{k} \\| \\tilde {x} \\| _ {1}. \\quad \\square \\tag {7.2.25}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.366,
                0.824,
                0.402
            ],
            "angle": 0,
            "content": "下面的引理表明，增广拉格朗日函数法(7.2.23)得到的点列中的点如果是可行的，则为原始问题(7.2.19)的一个最优解。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.413,
                0.824,
                0.45
            ],
            "angle": 0,
            "content": "引理7.2 假设问题(7.2.19)的可行域非空，\\(x^k\\) 是由迭代格式(7.2.23)得到的满足 \\(Ax^k = b\\) 的迭代点，则 \\(x^k\\) 是BP问题(7.2.19)的一个解。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.46,
                0.644,
                0.477
            ],
            "angle": 0,
            "content": "证明. 对任意 \\(x\\)，由 \\(\\| x \\|_1\\) 的凸性和(7.2.24)式，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.425,
                0.487,
                0.823,
                0.553
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| x ^ {k} \\right\\| _ {1} \\leqslant \\left\\| x \\right\\| _ {1} - \\left\\langle x - x ^ {k}, - A ^ {\\mathrm {T}} \\lambda^ {k} \\right\\rangle \\\\ = \\| x \\| _ {1} + \\left\\langle A x - A x ^ {k}, \\lambda^ {k} \\right\\rangle \\tag {7.2.26} \\\\ = \\| x \\| _ {1} + \\langle A x - b, \\lambda^ {k} \\rangle , \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.562,
                0.824,
                0.6
            ],
            "angle": 0,
            "content": "因此，对任意的满足 \\(Ax = b\\) 的 \\(x\\) ，都有 \\(\\| x^k\\| _1\\leqslant \\| x\\| _1\\) ，于是 \\(x^{k}\\) 是问题(7.2.19)的最优解. □"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.613,
                0.824,
                0.671
            ],
            "angle": 0,
            "content": "根据上面的引理，还可以证明增广拉格朗日函数法(7.2.23)会在有限步迭代内收敛到问题(7.2.19)的最优解，即存在正整数 \\(K\\) ，当 \\(k > K\\) 时， \\(x^{k}\\) 为问题(7.2.19)的解"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.681,
                0.824,
                0.74
            ],
            "angle": 0,
            "content": "定理7.8 假设问题(7.2.19)的可行域非空，迭代序列 \\(\\{x^k\\}, \\{\\lambda^k\\}\\) 是由迭代格式 (7.2.23) 从初始点 \\(x^0 = \\lambda^0 = 0\\) 产生的，则存在正整数 \\(K\\) 使得任意的 \\(x^k, k \\geqslant K\\) 是问题(7.2.19)的解。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.748,
                0.694,
                0.767
            ],
            "angle": 0,
            "content": "证明. 对指标集 \\(\\{1,2,\\dots ,n\\}\\) 的任一划分 \\((I_{+}^{j},I_{-}^{j},E^{j})\\) ，令"
        },
        {
            "type": "equation",
            "bbox": [
                0.289,
                0.775,
                0.796,
                0.795
            ],
            "angle": 0,
            "content": "\\[\nU ^ {j} \\stackrel {{\\text {d e f}}} {{=}} U \\left(I _ {+} ^ {j}, I _ {-} ^ {j}, E ^ {j}\\right) = \\left\\{x \\mid x _ {i} \\geqslant 0, i \\in I _ {+} ^ {j}; x _ {i} \\leqslant 0, i \\in I _ {-} ^ {j}; x _ {i} = 0, i \\in E ^ {j} \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.289,
                0.801,
                0.824,
                0.853
            ],
            "angle": 0,
            "content": "\\[\nH ^ {j} \\stackrel {\\text {d e f}} {=} \\min  _ {x \\in \\mathbb {R} ^ {n}} \\left\\{\\frac {1}{2} \\| A x - b \\| _ {2} ^ {2} \\mid x \\in U ^ {j} \\right\\}. \\tag {7.2.27}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.368,
                0.133
            ],
            "angle": 0,
            "content": "7.2 增广拉格朗日函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "327"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.711,
                0.175
            ],
            "angle": 0,
            "content": "对于迭代点 \\(\\lambda^k\\) ，我们可以定义指标集 \\(\\{1,2,\\dots ,n\\}\\) 的划分 \\((I_{+}^{j_k},I_{-}^{j_k},E^{j_k})\\) 为"
        },
        {
            "type": "equation",
            "bbox": [
                0.17,
                0.188,
                0.737,
                0.21
            ],
            "angle": 0,
            "content": "\\[\nI _ {+} ^ {j _ {k}} = \\left\\{i: \\left(A ^ {\\mathrm {T}} \\lambda^ {k}\\right) _ {i} = - 1 \\right\\}, I _ {-} ^ {j _ {k}} = \\left\\{i: \\left(A ^ {\\mathrm {T}} \\lambda^ {k}\\right) _ {i} = 1 \\right\\}, E ^ {j _ {k}} = \\left\\{i: \\left(A ^ {\\mathrm {T}} \\lambda^ {k}\\right) _ {i} \\in (- 1, 1) \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.222,
                0.518,
                0.24
            ],
            "angle": 0,
            "content": "由 \\(U^{j}\\) 的定义和 \\(-A^{\\mathrm{T}}\\lambda^{k}\\in \\partial ||x^{k}||_{1}\\) 知， \\(x^{k}\\in U^{j_{k}}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.244,
                0.739,
                0.327
            ],
            "angle": 0,
            "content": "因为问题(7.2.19)的可行域非空，故存在 \\(\\tilde{x}\\) 满足 \\(\\| A\\tilde{x} - b \\| = 0\\)。由引理7.1的(2)，对任意满足 \\(H^j > 0\\) 的 \\(j\\)，存在一个充分大的 \\(K_j\\) 使得 \\(x^k \\notin U^j, \\forall k \\geqslant K_j\\)。于是取 \\(K = \\max_{j} \\{K_j \\mid H^j > 0\\}\\)，有 \\(H^{j_k} = 0, \\forall k \\geqslant K\\)。结合 \\(\\| x \\|_1\\) 的凸性和(7.2.24)式，对 \\(k \\geqslant K\\) 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.314,
                0.34,
                0.597,
                0.361
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| x ^ {k} \\right\\| _ {1} + \\left(\\lambda^ {k}\\right) ^ {\\mathrm {T}} A x ^ {k} \\leqslant \\left\\| x \\right\\| _ {1} + \\left(\\lambda^ {k}\\right) ^ {\\mathrm {T}} A x.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.374,
                0.738,
                0.413
            ],
            "angle": 0,
            "content": "容易验证等号成立当且仅当 \\(x \\in U^{j_k}\\) （注意 \\(x^k \\in U^{j_k}\\)）．由于 \\(H^{j_k} = 0\\)，取 \\(\\tilde{x} \\in U^{j_k}\\) 且 \\(\\|A\\tilde{x} - b\\| = 0\\)，根据 \\(x^k\\) 的最优性有"
        },
        {
            "type": "equation",
            "bbox": [
                0.201,
                0.422,
                0.707,
                0.453
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\sigma}{2} \\| A x ^ {k} - b \\| ^ {2} \\leqslant \\| \\tilde {x} \\| _ {1} - \\| x ^ {k} \\| _ {1} + (\\lambda^ {k}) ^ {\\mathrm {T}} A (\\tilde {x} - x ^ {k}) + \\frac {\\sigma}{2} \\| A \\tilde {x} - b \\| ^ {2} \\leqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.462,
                0.592,
                0.48
            ],
            "angle": 0,
            "content": "由引理7.2可知， \\(x^{k},\\forall k\\geqslant K\\) 都是问题(7.2.19)的最优解"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.493,
                0.739,
                0.593
            ],
            "angle": 0,
            "content": "定理7.8假设了关于 \\(x^{k + 1}\\) 的子问题可以精确求解．对于一般的矩阵 \\(A\\) （例如非对角的情形)，该子问题的精确解是难以求得的．在实际中，我们采用迭代算法来进行求解，具体算法会在第八章中介绍．即使利用迭代算法求得近似解 \\(x^{k + 1}\\) ，增广拉格朗日函数法也有非常好的数值表现，因此在实际中非常受欢迎."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.597,
                0.738,
                0.677
            ],
            "angle": 0,
            "content": "我们知道BP问题是线性规划问题的特例，因此，对于线性规划问题，是否也可以设计相应的增广拉格朗日函数法？答案是肯定的．对于线性规划问题，我们可以类似地证明有限终止性[120]．对于一般凸优化问题的增广拉格朗日函数法，读者可以参考经典文献[163]."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.7,
                0.406,
                0.718
            ],
            "angle": 0,
            "content": "2. 与Bregman算法的等价性"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.729,
                0.738,
                0.768
            ],
            "angle": 0,
            "content": "在文章[206]中，作者提出了求解BP问题(7.2.19)的Bregman迭代算法．对于凸函数 \\(h(x) = \\| x\\| _1\\) ，定义其Bregman距离："
        },
        {
            "type": "equation",
            "bbox": [
                0.317,
                0.782,
                0.591,
                0.802
            ],
            "angle": 0,
            "content": "\\[\nD _ {h} ^ {g} (x, y) = h (x) - h (y) - \\langle g, x - y \\rangle ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.738,
                0.855
            ],
            "angle": 0,
            "content": "其中 \\(g \\in \\partial h(y)\\) 为函数 \\(h\\) 在点 \\(y\\) 处的一个次梯度．对于一般的凸函数 \\(h\\) ，容易证明 \\(D_h^g (x,y) \\neq D_h^g (y,x)\\)，所以 \\(D_h^g (x,y)\\) 不一定是距离函数．但是，我们"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "328"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.156,
                0.825,
                0.195
            ],
            "angle": 0,
            "content": "可以证明：对于任意的 \\(x, y\\)，都有 \\(D_h^g(x, y) \\geqslant 0\\)；对连接 \\(x, y\\) 的线段上的任一点 \\(z\\)，都有 \\(D_h^g(x, y) \\geqslant D_h^g(z, y)\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.198,
                0.771,
                0.216
            ],
            "angle": 0,
            "content": "有了Bregman距离之后，问题(7.2.19)的Bregman迭代算法为："
        },
        {
            "type": "equation",
            "bbox": [
                0.331,
                0.22,
                0.826,
                0.281
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{l} x ^ {k + 1} = \\underset {x \\in \\mathbb {R} ^ {n}} {\\arg \\min } \\left\\{D _ {h} ^ {g ^ {k}} \\left(x, x ^ {k}\\right) + \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2} \\right\\}, \\\\ g ^ {k + 1} = g ^ {k} - A ^ {\\mathrm {T}} \\left(A x ^ {k + 1} - b\\right). \\end{array} \\right. \\tag {7.2.28}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.287,
                0.825,
                0.326
            ],
            "angle": 0,
            "content": "在上面的格式中，我们需要说明 \\(g^{k + 1}\\in \\partial h(x^{k + 1})\\)。事实上，根据点 \\(x^{k + 1}\\) 的最优性条件，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.405,
                0.334,
                0.677,
                0.354
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial h (x ^ {k + 1}) - g ^ {k} + A ^ {\\mathrm {T}} (A x ^ {k + 1} - b),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.364,
                0.304,
                0.38
            ],
            "angle": 0,
            "content": "因此,"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.383,
                0.688,
                0.403
            ],
            "angle": 0,
            "content": "\\[\ng ^ {k + 1} = g ^ {k} - A ^ {\\mathrm {T}} \\left(A x ^ {k + 1} - b\\right) \\in \\partial h \\left(x ^ {k + 1}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.409,
                0.825,
                0.467
            ],
            "angle": 0,
            "content": "对比增广拉格朗日函数法 (7.2.23)，令罚因子 \\(\\sigma = 1\\)，并记算法的初始点为 \\(x^0, \\lambda^0\\)，我们不难看出，如果算法(7.2.28)的初始点设置为 \\(x^0, -A^{\\mathrm{T}}\\lambda^0\\)，那么在第 \\(k\\) 步迭代有如下对应关系："
        },
        {
            "type": "equation",
            "bbox": [
                0.49,
                0.476,
                0.591,
                0.496
            ],
            "angle": 0,
            "content": "\\[\ng ^ {k} = - A ^ {\\mathrm {T}} \\lambda^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.506,
                0.825,
                0.543
            ],
            "angle": 0,
            "content": "也就是说，在合理选取初始点的情况下，两个算法得到的迭代点列是完全一致的."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.567,
                0.534,
                0.584
            ],
            "angle": 0,
            "content": "3. 对偶问题的增广拉格朗日函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.597,
                0.462,
                0.613
            ],
            "angle": 0,
            "content": "考虑对偶问题(7.2.21)："
        },
        {
            "type": "equation",
            "bbox": [
                0.362,
                0.623,
                0.72,
                0.649
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {y \\in \\mathbb {R} ^ {m}, s \\in \\mathbb {R} ^ {n}} b ^ {\\mathrm {T}} y, \\quad \\text {s . t .} \\quad A ^ {\\mathrm {T}} y - s = 0, \\quad \\| s \\| _ {\\infty} \\leqslant 1.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.655,
                0.676,
                0.672
            ],
            "angle": 0,
            "content": "引入拉格朗日乘子 \\(\\lambda\\) 和罚因子 \\(\\sigma\\) ，增广拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.315,
                0.677,
                0.767,
                0.705
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\sigma} (y, s, \\lambda) = b ^ {T} y + \\lambda^ {T} \\left(A ^ {T} y - s\\right) + \\frac {\\sigma}{2} \\| A ^ {T} y - s \\| _ {2} ^ {2}, \\quad \\| s \\| _ {\\infty} \\leqslant 1.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.71,
                0.58,
                0.727
            ],
            "angle": 0,
            "content": "那么，增广拉格朗日函数法的迭代格式为："
        },
        {
            "type": "equation",
            "bbox": [
                0.34,
                0.732,
                0.745,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{l} (y ^ {k + 1}, s ^ {k + 1}) = \\underset {y, \\| s \\| _ {\\infty} \\leqslant 1} {\\arg \\min } L _ {\\sigma_ {k}} (y, s, \\lambda^ {k}) \\\\ \\hskip 2 8. 4 5 2 7 5 6 p t = \\underset {y, \\| s \\| _ {\\infty} \\leqslant 1} {\\arg \\min } \\left\\{b ^ {\\mathrm {T}} y + \\frac {\\sigma_ {k}}{2} \\| A ^ {\\mathrm {T}} y - s + \\frac {\\lambda}{\\sigma_ {k}} \\| _ {2} ^ {2} \\right\\}, \\\\ \\hskip 2 8. 4 5 2 7 5 6 p t \\lambda^ {k + 1} = \\lambda^ {k} + \\sigma_ {k} (A ^ {\\mathrm {T}} y ^ {k + 1} - s ^ {k + 1}), \\\\ \\sigma_ {k + 1} = \\min  \\{\\rho \\sigma_ {k}, \\bar {\\sigma} \\}, \\end{array} \\right.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.368,
                0.133
            ],
            "angle": 0,
            "content": "7.2 增广拉格朗日函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "329"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.736,
                0.195
            ],
            "angle": 0,
            "content": "其中 \\(\\rho > 1\\) 和 \\(\\bar{\\sigma} < +\\infty\\) 为算法参数。由于 \\((y^{k+1}, s^{k+1})\\) 的显式表达式是未知的，我们需要利用迭代算法来进行求解。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.198,
                0.737,
                0.256
            ],
            "angle": 0,
            "content": "除了利用投影梯度法求解关于 \\((y,s)\\) 的联合最小化问题外，还可以利用最优性条件将 \\(s\\) 用 \\(y\\) 来表示，转而求解只关于 \\(y\\) 的最小化问题。具体地，关于 \\(s\\) 的极小化问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.292,
                0.262,
                0.614,
                0.294
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {s} \\quad \\frac {\\sigma}{2} \\| A ^ {T} y - s + \\frac {\\lambda}{\\sigma} \\| _ {2} ^ {2}, \\quad \\text {s . t .} \\quad \\| s \\| _ {\\infty} \\leqslant 1.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.3,
                0.347,
                0.315
            ],
            "angle": 0,
            "content": "通过简单地推导，可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.358,
                0.322,
                0.737,
                0.357
            ],
            "angle": 0,
            "content": "\\[\ns = \\mathcal {P} _ {\\| s \\| _ {\\infty} \\leqslant 1} \\left(A ^ {\\mathrm {T}} y + \\frac {\\lambda}{\\sigma}\\right), \\tag {7.2.29}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.364,
                0.557,
                0.382
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{P}_{\\| s\\|_{\\infty}\\leqslant 1}\\) 为集合 \\(\\{s:\\| s\\|_{\\infty}\\leqslant 1\\}\\) 的投影算子，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.393,
                0.584,
                0.412
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {P} _ {\\| s \\| _ {\\infty} \\leqslant 1} (x) = \\max  \\{\\min  \\{x, 1 \\}, - 1 \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.422,
                0.586,
                0.438
            ],
            "angle": 0,
            "content": "将 \\(s\\) 的表达式代入增广拉格朗日函数中，我们得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.289,
                0.445,
                0.619,
                0.483
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\sigma} (y, \\lambda) = b ^ {\\mathrm {T}} y + \\frac {\\sigma}{2} \\left\\| \\psi \\left(A ^ {\\mathrm {T}} y + \\frac {\\lambda}{\\sigma}\\right) \\right\\| _ {2} ^ {2} - \\frac {\\lambda^ {2}}{2 \\sigma},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.489,
                0.654,
                0.507
            ],
            "angle": 0,
            "content": "其中 \\(\\psi (x) = \\mathrm{sign}(x)\\max \\{|x| - 1,0\\}\\) ， \\(\\operatorname {sign}(x)\\) 表示 \\(x\\) 的符号，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.358,
                0.514,
                0.548,
                0.589
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {s i g n} (x) = \\left\\{ \\begin{array}{l l} 1, & x > 0, \\\\ 0, & x = 0, \\\\ - 1, & x <   0. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.597,
                0.736,
                0.634
            ],
            "angle": 0,
            "content": "注意，为了记号简洁，我们仍然使用 \\(L_{\\sigma}\\) 来表示增广拉格朗日函数，但变量个数有所变化."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.639,
                0.455,
                0.655
            ],
            "angle": 0,
            "content": "消去 \\(s\\) 的增广拉格朗日函数法为："
        },
        {
            "type": "equation",
            "bbox": [
                0.246,
                0.66,
                0.737,
                0.768
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{l} y ^ {k + 1} = \\underset {y} {\\arg \\min } \\left\\{b ^ {\\mathrm {T}} y + \\frac {\\sigma}{2} \\left\\| \\psi \\left(A ^ {\\mathrm {T}} y + \\frac {\\lambda}{\\sigma}\\right) \\right\\| _ {2} ^ {2} \\right\\}, \\\\ \\lambda^ {k + 1} = \\sigma_ {k} \\psi \\left(A ^ {\\mathrm {T}} y ^ {k + 1} + \\frac {\\lambda^ {k}}{\\sigma_ {k}}\\right), \\\\ \\sigma_ {k + 1} = \\min  \\left\\{\\rho \\sigma_ {k}, \\bar {\\sigma} \\right\\}. \\end{array} \\right. \\tag {7.2.30}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.775,
                0.737,
                0.814
            ],
            "angle": 0,
            "content": "在迭代格式(7.2.30)的第一步中，我们不能得到关于 \\(y^{k + 1}\\) 的显式表达式．但是由于 \\(L_{\\sigma_k}(y,\\lambda^k)\\) 关于 \\(y\\) 是连续可微的，且其梯度为"
        },
        {
            "type": "equation",
            "bbox": [
                0.306,
                0.821,
                0.6,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {y} L _ {\\sigma_ {k}} \\left(y, \\lambda^ {k}\\right) = b + \\sigma_ {k} A \\psi \\left(A ^ {T} y + \\frac {\\lambda^ {k}}{\\sigma_ {k}}\\right).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "330"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.825,
                0.194
            ],
            "angle": 0,
            "content": "可以利用梯度法对其进行求解。除此之外，还可以采用半光滑牛顿法，相关内容可以参考 [119, 214]。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.198,
                0.825,
                0.236
            ],
            "angle": 0,
            "content": "记 \\(\\phi_{k}(y) = L_{\\sigma_{k}}(y,\\lambda^{k})\\) 为了保证收敛性，根据一般凸优化问题的增广拉格朗日函数法的收敛条件(7.2.15)，我们要求 \\(y^{k + 1}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.245,
                0.825,
                0.282
            ],
            "angle": 0,
            "content": "\\[\n\\phi_ {k} \\left(y ^ {k + 1}\\right) - \\inf  \\phi_ {k} \\leqslant \\frac {\\varepsilon_ {k} ^ {2}}{2 \\sigma_ {k}}, \\quad \\varepsilon_ {k} \\geqslant 0, \\sum_ {k = 1} ^ {\\infty} \\varepsilon_ {k} + \\infty , \\tag {7.2.31}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.293,
                0.461,
                0.309
            ],
            "angle": 0,
            "content": "其中 \\(\\varepsilon_{k}\\) 是人为设定的参数"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.313,
                0.551,
                0.33
            ],
            "angle": 0,
            "content": "根据定理7.7，有如下收敛性定理"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.341,
                0.825,
                0.421
            ],
            "angle": 0,
            "content": "定理7.9 假设 \\(\\{y^k\\}, \\{\\lambda^k\\}\\) 是由迭代格式(7.2.30)产生的序列，并且 \\(y^{k+1}\\) 的求解精度满足(7.2.31)式，而矩阵 \\(A\\) 是行满秩的。那么，序列 \\(\\{y^k\\}\\) 是有界的，且其任一聚点均为问题(7.2.21)的最优解。同时，序列 \\(\\{\\lambda^k\\}\\) 有界且收敛，其极限为原始问题(7.2.19)的某个最优解。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.434,
                0.741,
                0.45
            ],
            "angle": 0,
            "content": "注7.3 定理7.9假设 \\(A\\) 是行满秩的，因此，我们知道可行域"
        },
        {
            "type": "equation",
            "bbox": [
                0.457,
                0.464,
                0.623,
                0.483
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {X} = \\left\\{y \\mid \\| A ^ {\\mathrm {T}} y \\| _ {\\infty} \\leqslant 1 \\right\\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.497,
                0.824,
                0.534
            ],
            "angle": 0,
            "content": "是有界的。由于 \\(0 \\in \\mathcal{X}\\)，故 \\(\\{x \\in \\mathcal{X} \\mid f(x) \\leqslant 0\\}\\) 是非空有界的。根据约束的线性性，易知问题 (7.2.20) 的 Slater 约束品性成立。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.542,
                0.825,
                0.587
            ],
            "angle": 0,
            "content": "这里注意，\\(\\phi_k\\) 只是凸的，并不是强凸的。我们可以通过添加 \\(\\frac{1}{2\\sigma_k} \\| y - y^k \\|_2^2\\) 并求解"
        },
        {
            "type": "equation",
            "bbox": [
                0.387,
                0.586,
                0.693,
                0.622
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} \\approx \\underset {y} {\\arg \\min } \\left\\{\\phi_ {k} (y) + \\frac {1}{2 \\sigma_ {k}} \\| y - y ^ {k} \\| _ {2} ^ {2} \\right\\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.628,
                0.825,
                0.697
            ],
            "angle": 0,
            "content": "使得 \\(y^{k + 1}\\) 满足不精确条件 (7.2.31). 此时, 函数 \\(\\phi_k(y) + \\frac{1}{2\\sigma_k} \\| y - y^k \\|_2^2\\) 是 \\(\\frac{1}{\\sigma_k}\\) 强凸的. 修改后的迭代点列的收敛性基本与原始问题增广拉格朗日函数法的一致, 证明细节可以参考 [120, 214]."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.722,
                0.636,
                0.74
            ],
            "angle": 0,
            "content": "7.2.5 半定规划问题的增广拉格朗日函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.753,
                0.5,
                0.77
            ],
            "angle": 0,
            "content": "考虑半定规划问题(5.4.18)："
        },
        {
            "type": "equation",
            "bbox": [
                0.417,
                0.783,
                0.518,
                0.805
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in \\mathcal {S} ^ {n}} \\langle C, X \\rangle ,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.426,
                0.81,
                0.824,
                0.829
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\text {s . t .} \\quad \\langle A _ {i}, X \\rangle = b _ {i}, i = 1, 2, \\dots , m, \\end{array} \\tag {7.2.32}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.465,
                0.838,
                0.515,
                0.852
            ],
            "angle": 0,
            "content": "\\[\nX \\succeq 0,\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.368,
                0.133
            ],
            "angle": 0,
            "content": "7.2 增广拉格朗日函数法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "331"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.343,
                0.174
            ],
            "angle": 0,
            "content": "和其对偶问题(5.4.20)："
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.174,
                0.484,
                0.2
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {y \\in \\mathbb {R} ^ {m}} - b ^ {\\mathrm {T}} y,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.389,
                0.2,
                0.737,
                0.24
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad \\sum_ {i = 1} ^ {m} y _ {i} A _ {i} \\preceq C. \\tag {7.2.33}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.244,
                0.737,
                0.302
            ],
            "angle": 0,
            "content": "我们可以利用增广拉格朗日函数法求解. 对于原始问题(7.2.32)，引入乘子 \\(\\lambda \\in \\mathbb{R}^m\\)，罚因子 \\(\\sigma\\)，并记 \\(\\mathcal{A}(X) = (\\langle A_1, X \\rangle, \\langle A_2, X \\rangle, \\dots, \\langle A_m, X \\rangle)^{\\mathrm{T}}\\)，则增广拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.219,
                0.309,
                0.686,
                0.338
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\sigma} (X, \\lambda) = \\langle C, X \\rangle - \\lambda^ {T} (\\mathcal {A} (X) - b) + \\frac {\\sigma}{2} \\| \\mathcal {A} (X) - b \\| _ {2} ^ {2}, \\quad X \\succeq 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.344,
                0.398,
                0.36
            ],
            "angle": 0,
            "content": "那么，增广拉格朗日函数法为"
        },
        {
            "type": "equation",
            "bbox": [
                0.326,
                0.368,
                0.737,
                0.452
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{l} X ^ {k + 1} \\approx \\underset {X \\in S _ {+} ^ {n}} {\\arg \\min } L _ {\\sigma_ {k}} \\left(X, \\lambda^ {k}\\right), \\\\ \\lambda^ {k + 1} = \\lambda^ {k} - \\sigma_ {k} (\\mathcal {A} \\left(X ^ {k + 1}\\right) - b), \\\\ \\sigma_ {k + 1} = \\min  \\left\\{\\rho \\sigma_ {k}, \\bar {\\sigma} \\right\\}. \\end{array} \\right. \\tag {7.2.34}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.461,
                0.716,
                0.478
            ],
            "angle": 0,
            "content": "这里，当迭代收敛时，\\(X^k\\) 和 \\(\\lambda^k\\) 分别收敛到问题(7.2.32)和(7.2.33)的解"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.482,
                0.737,
                0.54
            ],
            "angle": 0,
            "content": "同样地，我们也可以采用增广拉格朗日函数法求解对偶问题(7.2.33). 具体地，引入松弛变量 \\(S\\succeq 0\\) ，乘子 \\(\\Lambda \\in S^n\\) 以及罚因子 \\(\\sigma\\) ，增广拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.192,
                0.546,
                0.713,
                0.589
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\sigma} (y, S, \\Lambda) = - b ^ {T} y + \\left\\langle \\Lambda , \\sum_ {i = 1} ^ {m} y _ {i} A _ {i} + S - C \\right\\rangle + \\frac {\\sigma}{2} \\| \\sum_ {i = 1} ^ {m} y _ {i} A _ {i} + S - C \\| _ {F} ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.595,
                0.519,
                0.612
            ],
            "angle": 0,
            "content": "在第 \\(k\\) 步，增广拉格朗日函数法的更新公式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.62,
                0.56,
                0.65
            ],
            "angle": 0,
            "content": "\\[\n(y ^ {k + 1}, S ^ {k + 1}) \\approx \\underset {y \\in \\mathbb {R} ^ {m}} {\\arg \\min } L _ {\\sigma_ {k}} (y, S, \\Lambda^ {k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.654,
                0.61,
                0.69
            ],
            "angle": 0,
            "content": "\\[\n\\Lambda^ {k + 1} = \\Lambda^ {k} + \\sum_ {i = 1} ^ {m} y _ {i} ^ {k + 1} A _ {i} + S ^ {k + 1} - C,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.35,
                0.697,
                0.501,
                0.715
            ],
            "angle": 0,
            "content": "\\[\n\\sigma_ {k + 1} = \\min  \\left\\{\\rho \\sigma_ {k}, \\bar {\\sigma} \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.724,
                0.671,
                0.74
            ],
            "angle": 0,
            "content": "我们可以利用最优性条件消去 \\(S\\). 具体地, 关于 \\(S\\) 的极小化问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.747,
                0.582,
                0.783
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {S \\in \\mathcal {S} _ {+} ^ {n}} \\quad \\frac {\\sigma}{2} \\| \\sum_ {i = 1} ^ {m} y _ {i} A _ {i} + S - C + \\frac {\\Lambda}{\\sigma} \\| _ {F} ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.791,
                0.363,
                0.807
            ],
            "angle": 0,
            "content": "通过简单地推导，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.813,
                0.737,
                0.857
            ],
            "angle": 0,
            "content": "\\[\nS = \\mathcal {P} _ {\\mathcal {S} _ {+} ^ {n}} \\left(C - \\sum_ {i = 1} ^ {m} y _ {i} A _ {i} - \\frac {\\Lambda}{\\sigma}\\right), \\tag {7.2.35}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "332"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.558,
                0.174
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{P}_{S_n^*}\\) 为到半定锥集 \\(S_{+}^{n}\\) 的投影算子"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.178,
                0.627,
                0.195
            ],
            "angle": 0,
            "content": "将(7.2.35)式代入增广拉格朗日函数，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.301,
                0.202,
                0.78,
                0.254
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\sigma} (y, \\Lambda) = - b ^ {\\mathrm {T}} y + \\frac {\\sigma}{2} \\left(\\left\\| \\mathcal {P} _ {\\mathcal {S} _ {+} ^ {n}} \\left(\\sum_ {i = 1} ^ {m} y _ {i} A _ {i} - C + \\frac {\\Lambda}{\\sigma}\\right) \\right\\| _ {F} ^ {2} - \\frac {\\| \\Lambda \\| _ {F} ^ {2}}{\\sigma^ {2}}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.261,
                0.487,
                0.277
            ],
            "angle": 0,
            "content": "那么，增广拉格朗日函数法为"
        },
        {
            "type": "equation",
            "bbox": [
                0.4,
                0.287,
                0.597,
                0.318
            ],
            "angle": 0,
            "content": "\\[\ny^{k + 1}\\approx \\operatorname *{argmin}_{y\\in \\mathbb{R}^{m}}L_{\\sigma_{k}}(y,\\Lambda^{k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.321,
                0.825,
                0.365
            ],
            "angle": 0,
            "content": "\\[\n\\Lambda^ {k + 1} = \\sigma \\mathcal {P} _ {\\mathcal {S} _ {+} ^ {n}} \\left(\\sum_ {i = 1} ^ {m} y _ {i} ^ {k + 1} A _ {i} - C + \\frac {\\Lambda^ {k}}{\\sigma_ {k}}\\right), \\tag {7.2.36}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.369,
                0.553,
                0.389
            ],
            "angle": 0,
            "content": "\\[\n\\sigma_ {k + 1} = \\min  \\left\\{\\rho \\sigma_ {k}, \\bar {\\sigma} \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.398,
                0.825,
                0.478
            ],
            "angle": 0,
            "content": "可以验证 \\(L_{\\sigma_k}(y, \\Lambda^k)\\) 关于 \\(y\\) 是连续可微的，并利用梯度法求得 \\(y^{k+1}\\)。另外，还可以验证 \\(L_{\\sigma_k}(y, \\Lambda^k)\\) 关于 \\(y\\) 是强半光滑的，因而可以调用半光滑牛顿法来进行更快速地求解。这部分相关内容可以参考 [214]。当迭代收敛时，\\(y^k\\) 和 \\(\\Lambda^k\\) 分别收敛到问题 (7.2.33) 和问题 (7.2.32) 的解。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.481,
                0.826,
                0.602
            ],
            "angle": 0,
            "content": "对比(7.2.34)式和(7.2.36)式的第一步, 我们可以发现: (7.2.34)式中的 \\(X^{k+1}\\) 是在半正定锥 \\(\\mathcal{S}_{+}^{n}\\) 中进行求解, 这是一个约束优化问题, 但是(7.2.36)式的 \\(y^{k+1}\\) 是在向量空间 \\(\\mathbb{R}^{m}\\) 中进行求解, 并且其对应于一个可微的无约束优化问题. 因此, 在实际中, 如果问题(7.2.32)中的约束个数 \\(m\\) 较少时, 我们一般先考虑其对偶问题（半定规划问题对偶的对偶为其本身), 即问题(7.2.33), 然后再用增广拉格朗日函数法进行求解."
        },
        {
            "type": "title",
            "bbox": [
                0.425,
                0.632,
                0.658,
                0.654
            ],
            "angle": 0,
            "content": "7.3 线性规划内点法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.671,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "线性规划是非常经典的约束优化问题，它的目标函数和约束都是线性函数。因为其形式简单，在现实中有非常多的应用，线性规划一直受到人们的格外关注。求解线性规划问题的算法非常之多，最经典的要数Dantzig在1947年提出的单纯形法[52]。我们知道，由于线性规划问题具有特殊结构，它的解必然是在可行域的顶点（或某一边界处）取到，而单纯形法则是通过某种方式不断列出可行域的顶点然后一步一步寻找问题的最优解。由于线性规划可行域的顶点数可能多达 \\(\\mathcal{O}(2^n)\\) 个（\\(n\\) 为自变量维数），因此单纯形法最坏情况下的复杂度是指数量级。实际上我们也可以构造出特殊的例子，使得单纯形法遍历可行域中的每一个顶点。这一现象表明对于某些大型问题和"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "7.3 线性规划内点法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "333"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.157,
                0.739,
                0.194
            ],
            "angle": 0,
            "content": "病态问题，单纯形法的效果可能很差，我们必须寻找其他办法来求解线性规划问题."
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.199,
                0.744,
                0.343
            ],
            "angle": 0,
            "content": "在大约30年后，内点法应运而生，其中比较实用的算法是Karmarkar在1984年提出的线性规划算法[111]．内点法是在可行域内部寻找一条路径最终抵达其边界，这和单纯形法有着截然不同的思想．由于迭代点处于可行域内部，因此求解每个子问题的计算代价都远高于仅仅在可行域边界移动的单纯形法．然而内点法的一步迭代对问题解的改善是显著的，正因为如此，可以证明内点法实际上是一个多项式时间算法．在本节中我们将介绍线性规划内点法的基本思想以及一些实现过程，但略去技术细节方面的讨论."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.369,
                0.373,
                0.387
            ],
            "angle": 0,
            "content": "7.3.1 原始-对偶算法"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.402,
                0.52,
                0.419
            ],
            "angle": 0,
            "content": "首先写出线性规划的原始问题和对偶问题"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.432,
                0.393,
                0.451
            ],
            "angle": 0,
            "content": "(P) min \\(c^{\\mathrm{T}}x\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.451,
                0.432,
                0.565,
                0.451
            ],
            "angle": 0,
            "content": "(D) max \\(b^{\\mathrm{T}}y\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.327,
                0.457,
                0.42,
                0.474
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\text {s . t .} A x = b, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.501,
                0.457,
                0.629,
                0.476
            ],
            "angle": 0,
            "content": "\\[\n\\mathrm {s . t .} A ^ {\\mathrm {T}} y + s = c,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.357,
                0.483,
                0.406,
                0.499
            ],
            "angle": 0,
            "content": "\\[\nx \\geqslant 0,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.531,
                0.483,
                0.577,
                0.498
            ],
            "angle": 0,
            "content": "\\[\ns \\geqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.688,
                0.483,
                0.737,
                0.5
            ],
            "angle": 0,
            "content": "(7.3.1)"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.517,
                0.393,
                0.534
            ],
            "angle": 0,
            "content": "写出问题(7.3.1)的KKT条件："
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.552,
                0.737,
                0.568
            ],
            "angle": 0,
            "content": "\\[\nA x = b, \\tag {7.3.2a}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.351,
                0.575,
                0.737,
                0.593
            ],
            "angle": 0,
            "content": "\\[\nA ^ {\\mathrm {T}} y + s = c, \\tag {7.3.2b}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.382,
                0.601,
                0.737,
                0.617
            ],
            "angle": 0,
            "content": "\\[\nx _ {i} s _ {i} = 0, i = 1, 2, \\dots , n, \\tag {7.3.2c}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.355,
                0.625,
                0.737,
                0.641
            ],
            "angle": 0,
            "content": "\\[\nx \\geqslant 0, s \\geqslant 0. \\tag {7.3.2d}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.659,
                0.739,
                0.76
            ],
            "angle": 0,
            "content": "原始 - 对偶算法作为一种内点法, 它实际上是利用条件 (7.3.2) 不断在可行域的相对内部产生迭代点的过程。具体来说, 原始 - 对偶算法构造的解满足条件 (7.3.2a)(7.3.2b) 以及 (7.3.2d), 而只能近似地满足条件 (7.3.2c)。当条件 (7.3.2d) 满足且条件 (7.3.2c) 对任意的 \\(i\\) 不满足时, 我们有 \\(x_{i} s_{i} > 0, \\forall i\\), 这意味着点 \\((x, s)\\) 为可行域的相对内点, 也是内点法得名的原因。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "注7.4实际上单纯形法的构造也可理解为利用了KKT条件(7.3.2)．不过它舍弃了条件 \\(x\\geqslant 0,s\\geqslant 0\\) ，并保证其他三个条件在迭代过程中成立．而条件(7.3.2a)-(7.3.2c)处理起来并不复杂，因此单纯形法迭代一步非常迅速，其终止准则恰好可以检查迭代点是否满足条件(7.3.2d)."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "334"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.235
            ],
            "angle": 0,
            "content": "由上面的分析可知, 条件(7.3.2c)在内点法中不能严格满足, 而我们想要算法最终收敛到线性规划问题的解, 因此希望 \\(x_{i} s_{i} \\rightarrow 0, \\forall i\\). 这个条件就可以作为内点法的终止条件. 实际上, 我们可以对内点 \\(x > 0, s > 0\\) 定义互补条件(7.3.2c)违反度的度量"
        },
        {
            "type": "equation",
            "bbox": [
                0.464,
                0.24,
                0.825,
                0.277
            ],
            "angle": 0,
            "content": "\\[\n\\mu = \\frac {1}{n} \\sum_ {i = 1} ^ {n} x _ {i} s _ {i} = \\frac {x ^ {\\mathrm {T}} s}{n}, \\tag {7.3.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.282,
                0.775,
                0.3
            ],
            "angle": 0,
            "content": "也称为对偶间隙. 当 \\(\\mu\\) 趋于0时，\\((x,s)\\) 将越来越接近可行域的边界"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.304,
                0.826,
                0.34
            ],
            "angle": 0,
            "content": "综上所述，线性规划原始－对偶算法的目标是给定当前可行点 \\((x,y,s)\\)，寻找下一个点"
        },
        {
            "type": "equation",
            "bbox": [
                0.42,
                0.345,
                0.663,
                0.363
            ],
            "angle": 0,
            "content": "\\[\n(\\tilde {x}, \\tilde {y}, \\tilde {s}) = (x, y, s) + (\\Delta x, \\Delta y, \\Delta z)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.37,
                0.407,
                0.385
            ],
            "angle": 0,
            "content": "使得如下条件成立："
        },
        {
            "type": "equation",
            "bbox": [
                0.42,
                0.39,
                0.825,
                0.455
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{c c} A ^ {\\mathrm {T}} \\tilde {y} + \\tilde {s} = c, & \\tilde {s} > 0, \\\\ A \\tilde {x} = b, & \\tilde {x} > 0, \\\\ \\tilde {x} _ {i} \\tilde {s} _ {i} = \\sigma \\mu , & i = 1, 2, \\dots , n. \\end{array} \\right. \\tag {7.3.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.461,
                0.825,
                0.54
            ],
            "angle": 0,
            "content": "其中 \\(0 < \\sigma < 1\\) 是取定的常数。条件(7.3.4)也被称为是扰动KKT条件，最后一个条件可进一步使用分量乘积简化为 \\(\\tilde{x} \\odot \\tilde{s} = \\sigma \\mu \\mathbf{1}\\) 。可以用如下方式来理解最后一个条件：假设 \\(\\mu\\) 是当前点 \\((x, y, s)\\) 处的对偶间隙，我们希望迭代下一步时这个度量将会缩小一个比例 \\(\\sigma\\) 。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.544,
                0.755,
                0.56
            ],
            "angle": 0,
            "content": "我们通过如下方法近似求解(7.3.4)：首先展开方程组可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.419,
                0.567,
                0.665,
                0.635
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{c} A (x + \\Delta x) = b, \\\\ A ^ {\\mathrm {T}} (y + \\Delta y) + (s + \\Delta s) = c, \\\\ (s + \\Delta s) \\odot (x + \\Delta x) = \\sigma \\mu \\mathbf {1}, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.642,
                0.616,
                0.658
            ],
            "angle": 0,
            "content": "去除高阶非线性项 \\(\\Delta x\\odot \\Delta s\\) 后得到线性方程组："
        },
        {
            "type": "equation",
            "bbox": [
                0.391,
                0.664,
                0.693,
                0.739
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{c} A \\Delta x = r _ {p} \\stackrel {{\\mathrm {d e f}}} {{=}} b - A x, \\\\ A ^ {\\mathrm {T}} \\Delta y + \\Delta s = r _ {d} \\stackrel {{\\mathrm {d e f}}} {{=}} c - s - A ^ {\\mathrm {T}} y, \\\\ x \\odot \\Delta s + s \\odot \\Delta x = r _ {c} \\stackrel {{\\mathrm {d e f}}} {{=}} \\sigma \\mu \\mathbf {1} - x \\odot s, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.746,
                0.825,
                0.785
            ],
            "angle": 0,
            "content": "其中 \\(r = (r_{p}, r_{d}, r_{c})^{\\mathrm{T}}\\) 刻画了 KKT 条件(7.3.2)的残量。记 \\(L_{x} = \\operatorname{Diag}(x)\\)，\\(L_{s} = \\operatorname{Diag}(s)\\)，我们将方程组化为矩阵形式"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.788,
                0.825,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\left[ \\begin{array}{c c c} A & 0 & 0 \\\\ 0 & A ^ {\\mathrm {T}} & I \\\\ L _ {s} & 0 & L _ {x} \\end{array} \\right] \\left[ \\begin{array}{l} \\Delta x \\\\ \\Delta y \\\\ \\Delta s \\end{array} \\right] = \\left[ \\begin{array}{l} r _ {p} \\\\ r _ {d} \\\\ r _ {c} \\end{array} \\right]. \\tag {7.3.5}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "7.3 线性规划内点法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "335"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.565,
                0.174
            ],
            "angle": 0,
            "content": "利用矩阵分块消元，可以直接求解方程(7.3.5)，得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.184,
                0.737,
                0.252
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{l} \\Delta y = \\left(A L _ {s} ^ {- 1} L _ {x} A ^ {\\mathrm {T}}\\right) ^ {- 1} \\left(r _ {p} + A L _ {s} ^ {- 1} \\left(L _ {x} r _ {d} - r _ {c}\\right)\\right), \\\\ \\Delta s = r _ {d} - A ^ {\\mathrm {T}} \\Delta y, \\\\ \\Delta x = - L _ {s} ^ {- 1} \\left(L _ {x} \\Delta s - r _ {c}\\right), \\end{array} \\right. \\tag {7.3.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.263,
                0.633,
                0.281
            ],
            "angle": 0,
            "content": "其中 \\(A L_{s}^{-1}L_{x}A^{\\mathrm{T}}\\) 是对称矩阵，当 \\(A\\) 满秩时，\\(A L_{s}^{-1}L_{x}A^{\\mathrm{T}}\\) 正定."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.285,
                0.737,
                0.364
            ],
            "angle": 0,
            "content": "一般来说，即使初始点 \\((x, y, s)\\) 是可行的，求解线性方程组(7.3.5)产生的更新 \\((\\tilde{x}, \\tilde{y}, \\tilde{s})\\) 也不一定是可行解。由于前两个方程(7.3.2a)(7.3.2b)是线性的，在迭代过程中可以一直满足。但 \\(x > 0, s > 0\\) 这个约束不能保证一直成立。为此我们考虑采用线搜索中的回溯法来确定一个合适的更新"
        },
        {
            "type": "equation",
            "bbox": [
                0.271,
                0.376,
                0.737,
                0.396
            ],
            "angle": 0,
            "content": "\\[\n\\left(x ^ {k + 1}, y ^ {k + 1}, s ^ {k + 1}\\right) = \\left(x ^ {k}, y ^ {k}, s ^ {k}\\right) + \\alpha_ {k} \\left(\\Delta x ^ {k}, \\Delta y ^ {k}, \\Delta s ^ {k}\\right), \\tag {7.3.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.408,
                0.736,
                0.446
            ],
            "angle": 0,
            "content": "其中 \\(\\alpha_{k} = \\alpha_{0}\\rho^{k_{0}}\\) ，并选取最小的整数 \\(k_{0}\\) 使得 \\(x^{k + 1} > 0,s^{k + 1} > 0\\) ，这里 \\(0 < \\rho < 1,\\alpha_0\\) 是给定常数."
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.45,
                0.645,
                0.466
            ],
            "angle": 0,
            "content": "适用于求解线性规划的原始 - 对偶算法可总结为如下过程:"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.478,
                0.501,
                0.496
            ],
            "angle": 0,
            "content": "(1) 给定初始可行点 \\((x^{0},y^{0},s^{0})\\) ，令 \\(k\\gets 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.51,
                0.451,
                0.526
            ],
            "angle": 0,
            "content": "(2) 构造方程(7.3.5)，获得解(7.3.6)；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.539,
                0.492,
                0.555
            ],
            "angle": 0,
            "content": "(3) 使用线搜索(7.3.7)求得下一步可行解;"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.568,
                0.622,
                0.585
            ],
            "angle": 0,
            "content": "(4) 若满足停机条件，终止；否则令 \\(k \\gets k + 1\\) ，转步 (2)."
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.478,
                0.622,
                0.585
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.597,
                0.737,
                0.697
            ],
            "angle": 0,
            "content": "从算法的迭代过程来看，原始 - 对偶算法有点类似于带约束的线搜索类算法，即先确定下降方向 \\((\\Delta x, \\Delta y, \\Delta s)\\)，再选取合适的步长使得下一步迭代点仍然是可行域的严格内点。该算法的主要计算量来自方程(7.3.5)的求解。步长 \\(\\alpha_{k}\\) 的选取也是内点法的一个关键因素，我们在下一个小节将介绍更好的选择步长的方法。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.722,
                0.353,
                0.74
            ],
            "angle": 0,
            "content": "7.3.2 路径追踪算法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.753,
                0.737,
                0.853
            ],
            "angle": 0,
            "content": "我们用动态的观点再次考察扰动的 KKT 条件(7.3.4). 随着迭代进行, 这个条件中的 \\(\\mu\\) 将趋于 0 . 根据隐函数定理, 给定 \\(\\mu\\) 时条件(7.3.4)决定的解是存在唯一的. 原始 - 对偶算法的过程就是在不断寻找满足条件(7.3.4)的点的近似, 对任意的 \\(\\mu\\), 满足条件(7.3.4)的点是非常重要的: 为此我们引入下面的定义."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "336"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.291,
                0.157,
                0.795,
                0.174
            ],
            "angle": 0,
            "content": "定义7.6（中心路径）给定参数 \\(\\tau > 0\\)，点 \\((x_{\\tau}, y_{\\tau}, s_{\\tau})\\) 满足如下方程："
        },
        {
            "type": "equation",
            "bbox": [
                0.472,
                0.19,
                0.531,
                0.206
            ],
            "angle": 0,
            "content": "\\[\nA x = b,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.437,
                0.21,
                0.824,
                0.236
            ],
            "angle": 0,
            "content": "\\[\nA ^ {\\mathrm {T}} y + s = c, \\tag {7.3.8}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.232,
                0.649,
                0.248
            ],
            "angle": 0,
            "content": "\\[\nx _ {i} s _ {i} = \\tau , \\quad i = 1, 2, \\dots , n,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.439,
                0.253,
                0.532,
                0.267
            ],
            "angle": 0,
            "content": "\\[\nx > 0, s > 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.283,
                0.383,
                0.299
            ],
            "angle": 0,
            "content": "则称单参数曲线"
        },
        {
            "type": "equation",
            "bbox": [
                0.448,
                0.304,
                0.825,
                0.323
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {C} = \\left\\{\\left(x _ {\\tau}, y _ {\\tau}, s _ {\\tau}\\right) \\mid \\tau > 0 \\right\\} \\tag {7.3.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.334,
                0.588,
                0.35
            ],
            "angle": 0,
            "content": "为中心路径，称方程(7.3.8)为中心路径方程"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.365,
                0.825,
                0.401
            ],
            "angle": 0,
            "content": "实际上，从罚函数角度来说，可以证明方程(7.3.8)实际是罚函数形式优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.401,
                0.402,
                0.682,
                0.437
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad c ^ {\\mathrm {T}} x - \\tau \\sum_ {i = 1} ^ {n} \\ln x _ {i}, \\quad \\text {s . t .} \\quad A x = b\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.445,
                0.371,
                0.461
            ],
            "angle": 0,
            "content": "的最优性条件."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.466,
                0.827,
                0.669
            ],
            "angle": 0,
            "content": "在这里注意，由上一小节给出的原始－对偶算法产生的迭代点序列虽然是可行解，但是它们一般不在中心路径上．原因有两点，一是因为求解方程(7.3.5)时忽略了高阶项 \\(\\Delta x\\odot \\Delta s\\) ，在这一步引入了误差；二是因为采用了线搜索来选取 \\(\\alpha_{k}\\) ，这只能保证下一步的点落在可行域 \\(x > 0,s > 0\\) 内，而中心路径方程要求 \\(x\\odot s\\) 的每个分量都有相同的值 \\(\\tau\\) ，在实际迭代中这个条件一般不会满足．实际上，中心路径这一名字也是由此而来．我们知道 \\(x_{i}s_{i} = 0\\) 意味着点 \\((x,s)\\) 已经接近可行域的边缘，如果继续进行迭代，则迭代点将会紧贴定义域边缘进行更新，这有违于内点法的思想．比较理想的情况就是\\(x_{i}s_{i}\\) 能以较一致的速度下降到0，而不是各个分量下降参差不齐，以上就是我们考虑中心路径的原因."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.672,
                0.827,
                0.752
            ],
            "angle": 0,
            "content": "我们希望在原始-对偶算法中，迭代点列 \\((x^{k},y^{k},s^{k})\\) 应该在中心路径 \\(\\mathcal{C}\\) 附近移动，跟随曲线 \\(\\mathcal{C}\\) 直至到达最优值点，这就是下面要介绍的路径追踪算法．将点列 \\((x^{k},y^{k},s^{k})\\) 限制在中心路径 \\(\\mathcal{C}\\) 附近的方式就是选取合适的线搜索算法．考虑线性规划问题的严格可行域"
        },
        {
            "type": "equation",
            "bbox": [
                0.352,
                0.768,
                0.731,
                0.786
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {F} ^ {\\circ} = \\left\\{\\left(x, y, s\\right) \\mid A x = b, A ^ {\\mathrm {T}} y + s = c, x > 0, s > 0 \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.803,
                0.436,
                0.819
            ],
            "angle": 0,
            "content": "并定义中心路径邻域为"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.835,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {N} _ {- \\infty} (\\gamma) = \\left\\{\\left(x, y, s\\right) \\in \\mathcal {F} ^ {\\circ} \\mid x _ {i} s _ {i} \\geqslant \\gamma \\mu , \\forall i \\right\\}, \\tag {7.3.10}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "7.3 线性规划内点法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "337"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.156,
                0.744,
                0.236
            ],
            "angle": 0,
            "content": "如图7.6, (7.3.10)式是其中一种较常用的中心路径邻域, 当某点处于这个邻域中时, \\(x \\odot s\\) 的每个分量至少为 \\(\\gamma \\mu\\), 其中 \\(\\gamma\\) 通常取一个较小的正数, 如 \\(10^{-3}\\), 且一般不大于迭代算法(7.3.4)中的 \\(\\sigma\\). 当 \\(\\gamma\\) 趋于 0 时, 邻域 \\(\\mathcal{N}_{-\\infty}(\\gamma)\\) 将会和可行域越来越接近."
        },
        {
            "type": "image",
            "bbox": [
                0.22,
                0.313,
                0.691,
                0.485
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.306,
                0.51,
                0.604,
                0.527
            ],
            "angle": 0,
            "content": "图7.6 中心路径以及中心路径邻域 \\(\\mathcal{N}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.561,
                0.741,
                0.662
            ],
            "angle": 0,
            "content": "有了中心路径邻域的概念，我们就可以写出带路径追踪的原始-对偶算法了（也简称为路径追踪算法，见算法7.7）。该算法的关键在于如何选取最大的 \\(\\alpha_{k}\\) 。实际上，由方程(7.3.5)的性质，只需要保证在下一步迭代时 \\((x,y,s)\\) 满足 \\(x_{i}s_{i}\\geqslant \\gamma \\mu\\) 即可。这是关于 \\(\\alpha\\) 的 \\(n\\) 个二次不等式，求解比较容易。再结合条件 \\(x > 0,s > 0\\) 就能很容易地确定 \\(\\alpha_{k}\\) ，细节留给读者完成。"
        },
        {
            "type": "title",
            "bbox": [
                0.172,
                0.684,
                0.345,
                0.701
            ],
            "angle": 0,
            "content": "算法7.7路径追踪算法"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.705,
                0.616,
                0.724
            ],
            "angle": 0,
            "content": "1. 选取初值 \\((x^{0},y^{0},s^{0})\\in \\mathcal{F}^{\\circ}\\) ，参数 \\(0 < \\gamma < \\sigma < 1\\) ， \\(k\\gets 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.727,
                0.4,
                0.743
            ],
            "angle": 0,
            "content": "2. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.748,
                0.503,
                0.765
            ],
            "angle": 0,
            "content": "3. 求解方程(7.3.5)得到更新 \\((\\Delta x, \\Delta y, \\Delta s)\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.768,
                0.714,
                0.786
            ],
            "angle": 0,
            "content": "4. 选取最大的 \\(\\alpha \\in (0,1]\\) 使得下一步迭代点落在 \\(\\mathcal{N}_{-\\infty}(\\gamma)\\) 内，记为 \\(\\alpha_{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.788,
                0.6,
                0.807
            ],
            "angle": 0,
            "content": "5. 更新 \\((x^{k + 1},y^{k + 1},s^{k + 1}) = (x^k,y^k,s^k) + \\alpha_k(\\Delta x,\\Delta y,\\Delta s)\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.811,
                0.297,
                0.825
            ],
            "angle": 0,
            "content": "6. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.832,
                0.284,
                0.846
            ],
            "angle": 0,
            "content": "7. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.181,
                0.705,
                0.714,
                0.846
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "338"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.157,
                0.76,
                0.174
            ],
            "angle": 0,
            "content": "接下来直接给出算法7.7的一些性质，详细的证明可参考[145]."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.185,
                0.561,
                0.202
            ],
            "angle": 0,
            "content": "引理7.3 设 \\((x,y,s) \\in \\mathcal{N}_{-\\infty}(\\gamma)\\)，记"
        },
        {
            "type": "equation",
            "bbox": [
                0.378,
                0.214,
                0.705,
                0.232
            ],
            "angle": 0,
            "content": "\\[\n(x (\\alpha), y (\\alpha), s (\\alpha)) = (x, y, s) + \\alpha (\\Delta x, \\Delta y, \\Delta s).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.238,
                0.548,
                0.274
            ],
            "angle": 0,
            "content": "则对任意的 \\(\\alpha \\in \\left[0,2^{3 / 2}\\gamma \\frac{1 - \\gamma}{1 + \\gamma}\\frac{\\sigma}{n}\\right]\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.435,
                0.279,
                0.65,
                0.298
            ],
            "angle": 0,
            "content": "\\[\n(x (\\alpha), y (\\alpha), s (\\alpha)) \\in \\mathcal {N} _ {- \\infty} (\\gamma).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.31,
                0.599,
                0.326
            ],
            "angle": 0,
            "content": "引理7.3说明了在算法7.7中至少可以选取"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.333,
                0.615,
                0.366
            ],
            "angle": 0,
            "content": "\\[\n\\alpha_ {k} = 2 ^ {3 / 2} \\frac {\\sigma}{n} \\gamma \\frac {1 - \\gamma}{1 + \\gamma},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.373,
                0.532,
                0.389
            ],
            "angle": 0,
            "content": "虽然这样选取的 \\(\\alpha_{k}\\) 不一定是最大的"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.394,
                0.562,
                0.41
            ],
            "angle": 0,
            "content": "基于上面的结果，有如下的收敛性："
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.421,
                0.825,
                0.482
            ],
            "angle": 0,
            "content": "定理7.10（原始－对偶算法的收敛性）给定参数 \\(0 < \\gamma < \\sigma < 1\\)，设 \\(\\mu_{k} = \\frac{(x^{k})^{\\mathrm{T}}s^{k}}{n}\\) 为算法7.7产生的对偶间隙，且初值 \\((x^{0},y^{0},s^{0})\\in \\mathcal{N}_{-\\infty}(\\gamma)\\)，则存在与维数 \\(n\\) 无关的常数 \\(c\\)，使得对任意 \\(k\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.49,
                0.614,
                0.518
            ],
            "angle": 0,
            "content": "\\[\n\\mu_ {k + 1} \\leqslant \\left(1 - \\frac {c}{n}\\right) \\mu_ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.526,
                0.821,
                0.573
            ],
            "angle": 0,
            "content": "更进一步地，对任意给定的精度 \\(\\varepsilon \\in (0,1)\\)，存在迭代步数 \\(K = \\mathcal{O}\\left(n\\ln \\frac{1}{\\varepsilon}\\right)\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.579,
                0.615,
                0.596
            ],
            "angle": 0,
            "content": "\\[\n\\mu_ {k} \\leqslant \\varepsilon \\mu_ {0}, \\quad \\forall k \\geqslant K.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.606,
                0.827,
                0.747
            ],
            "angle": 0,
            "content": "定理7.10表明对偶间隙是呈指数式趋于 0 的，且当维数 \\( n \\) 越大时收敛于 0 的速度也就越慢。这个结果揭示了内点法确实可以做到在多项式时间内产生给定精度的解，从这方面来看它比单纯形法更加快速，在实际应用中也是如此。虽然在最初的设计中，内点法的效率远不如单纯形法，但随着人们的不断完善，内点法已经成为主流的线性规划求解算法之一，并且在很多问题上要优于单纯形法。或许在将来的某一天，人们会继续加深对线性规划这一经典问题的理解，从而设计出更好的算法来取代内点法。"
        },
        {
            "type": "title",
            "bbox": [
                0.486,
                0.777,
                0.597,
                0.798
            ],
            "angle": 0,
            "content": "7.4 总结"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "本章介绍了一般约束优化问题的罚函数法、增广拉格朗日函数法以及线性规划问题的内点法。相较于罚函数法，增广拉格朗日函数法具有更好的"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.224,
                0.133
            ],
            "angle": 0,
            "content": "习题7"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "339"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.157,
                0.739,
                0.236
            ],
            "angle": 0,
            "content": "理论性质（尤其是对凸优化问题）．关于凸优化问题的增广拉格朗日函数法，读者可以进一步参考[163]．我们知道增广拉格朗日函数法的困难之一是决策变量的更新．除了前面介绍的利用半光滑性质，还可以利用交替方向乘子法，其给出了一种有效的更新方式，我们会在第八章中详细介绍该方法."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.241,
                0.744,
                0.362
            ],
            "angle": 0,
            "content": "目前，单纯形法和内点法以及它们的变形可以有效解决很多线性规划问题。内点法也是解决中小规模半定规划问题的主要算法。对于大规模半定规划问题，由于每一次迭代时形成和求解线性系统的计算代价比较昂贵，内点法的计算效率受到很大的限制。近年来，人们发展了一些基于增广拉格朗日函数的算法，能部分解决一些内点法不太适用的问题。对于一般的约束优化问题，也可以设计相应有效的内点法。相关的内容读者可以参考[145]第19章。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.365,
                0.741,
                0.424
            ],
            "angle": 0,
            "content": "本章的罚函数法、一般优化问题的增广拉格朗日函数法、线性规划内点法相关内容的编写参考了[145]，凸优化问题、基追踪问题的增广拉格朗日函数法的编写参考了[163,204,206,214]."
        },
        {
            "type": "title",
            "bbox": [
                0.419,
                0.456,
                0.492,
                0.477
            ],
            "angle": 0,
            "content": "习题7"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.495,
                0.741,
                0.534
            ],
            "angle": 0,
            "content": "7.1 构造一个等式约束优化问题，使得它存在一个局部极小值，但对于任意的 \\(\\sigma > 0\\) ，它的二次罚函数是无界的。"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.548,
                0.39,
                0.565
            ],
            "angle": 0,
            "content": "7.2 考虑等式约束优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.364,
                0.581,
                0.501,
                0.596
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad - x _ {1} x _ {2} x _ {3},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.6,
                0.585,
                0.617
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad x _ {1} + 2 x _ {2} + 3 x _ {3} = 6 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.209,
                0.631,
                0.741,
                0.713
            ],
            "angle": 0,
            "content": "使用二次罚函数求解该问题，当固定罚因子 \\(\\sigma_{k}\\) 时，写出二次罚函数的最优解 \\(x^{k + 1}\\) 。当 \\(\\sigma_{k} \\to +\\infty\\) 时，写出该优化问题的解并求出约束的拉格朗日乘子。此外，当罚因子 \\(\\sigma\\) 满足什么条件时，二次罚函数的海瑟矩阵 \\(\\nabla_{xx}^2 P_E(x,\\sigma)\\) 是正定的？"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.726,
                0.39,
                0.743
            ],
            "angle": 0,
            "content": "7.3 考虑等式约束优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.339,
                0.759,
                0.61,
                0.777
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad f (x), \\quad \\text {s . t .} \\quad c _ {i} (x) = 0, i \\in \\mathcal {E},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.794,
                0.39,
                0.81
            ],
            "angle": 0,
            "content": "定义一般形式的罚函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.353,
                0.825,
                0.597,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nP _ {E} (x, \\sigma) = f (x) + \\sigma \\sum_ {i \\in \\mathcal {E}} \\varphi (c _ {i} (x)),\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "340"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.157,
                0.795,
                0.174
            ],
            "angle": 0,
            "content": "其中 \\(\\varphi(t)\\) 是充分光滑的函数，且 \\(t = 0\\) 是其 \\(s\\) 阶零点（\\(s \\geqslant 2\\)），即"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.189,
                0.747,
                0.209
            ],
            "angle": 0,
            "content": "\\[\n\\varphi (0) = \\varphi^ {\\prime} (0) = \\dots = \\varphi^ {(s - 1)} (0) = 0, \\quad \\varphi^ {(s)} (0) \\neq 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.224,
                0.825,
                0.262
            ],
            "angle": 0,
            "content": "设 \\(x^{k}, \\sigma_{k}\\) 的选取方式和算法 7.1 的相同，且 \\(\\{x^{k}\\}\\) 存在极限 \\(x^{*}\\)，在点 \\(x^{*}\\) 处 LICQ（见定义 5.9）成立。"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.277,
                0.827,
                0.316
            ],
            "angle": 0,
            "content": "(a) 证明: \\(\\sigma_{k}(c_{i}(x^{k}))^{s - 1}, \\forall i \\in \\mathcal{E}\\) 极限存在, 其极限 \\(\\lambda_{i}^{*}\\) 为约束 \\(c_{i}(x^{*}) = 0\\) 对应的拉格朗日乘子;"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.324,
                0.662,
                0.342
            ],
            "angle": 0,
            "content": "(b) 求 \\(P_{E}(x,\\sigma)\\) 关于 \\(x\\) 的海瑟矩阵 \\(\\nabla_{xx}^{2}P_{E}(x,\\sigma)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.35,
                0.825,
                0.39
            ],
            "angle": 0,
            "content": "(c) 设在 (a) 中 \\(\\lambda_i^* \\neq 0, \\forall i \\in \\mathcal{E}\\)，证明：当 \\(\\sigma_k \\to +\\infty\\) 时，\\(\\nabla_{xx}^2 P_E(x^k, \\sigma_k)\\) 有 \\(m\\) 个特征值的模长与 \\(\\sigma_k^{1/(s-1)}\\) 同阶，其中 \\(m = |\\mathcal{E}|\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.305,
                0.277,
                0.827,
                0.39
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.403,
                0.827,
                0.481
            ],
            "angle": 0,
            "content": "7.4 考虑不等式约束优化问题 (7.1.12)，其中 \\(f\\) 在可行域 \\(\\mathcal{X}\\) 上有下界，现使用对数罚函数法进行求解（算法 7.4）．假设在算法 7.4 的每一步子问题能求出罚函数的全局极小值点 \\(x^{k+1}\\)，证明：算法 7.4 在有限次迭代后终止，或者"
        },
        {
            "type": "equation",
            "bbox": [
                0.456,
                0.485,
                0.667,
                0.515
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} c _ {k} \\sum_ {i \\in \\mathcal {I}} \\ln (- c _ {i} (x ^ {k + 1})) = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.522,
                0.34,
                0.538
            ],
            "angle": 0,
            "content": "并且"
        },
        {
            "type": "equation",
            "bbox": [
                0.472,
                0.543,
                0.652,
                0.567
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} f (x ^ {k}) = \\inf  _ {x \\in \\operatorname {i n t}. x} f (x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.58,
                0.827,
                0.616
            ],
            "angle": 0,
            "content": "7.5 考虑一般约束优化问题 (7.1.15)，现在针对等式约束使用二次罚函数，对不等式约束使用对数罚函数："
        },
        {
            "type": "equation",
            "bbox": [
                0.385,
                0.627,
                0.741,
                0.662
            ],
            "angle": 0,
            "content": "\\[\nP (x, \\sigma) = f (x) + \\frac {\\sigma}{2} \\sum_ {i \\in \\mathcal {E}} c _ {i} ^ {2} (x) - \\frac {1}{\\sigma} \\sum_ {i \\in \\mathcal {I}} \\ln (- c _ {i} (x)),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.674,
                0.784,
                0.692
            ],
            "angle": 0,
            "content": "其中 \\(\\mathbf{dom}P = \\{x\\mid c_i(x) < 0,i\\in \\mathcal{I}\\}\\) .令罚因子 \\(\\sigma_k\\to +\\infty\\) ，定义"
        },
        {
            "type": "equation",
            "bbox": [
                0.472,
                0.706,
                0.652,
                0.734
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\underset {x} {\\arg \\min } P (x, \\sigma_ {k}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.745,
                0.828,
                0.784
            ],
            "angle": 0,
            "content": "假定涉及的所有函数都是连续的，\\(\\{x \\mid c_i(x) \\leqslant 0, i \\in \\mathcal{I}\\}\\) 是有界闭集，\\(x^*\\) 为问题 (7.1.15) 的解。试证明如下结论："
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.797,
                0.517,
                0.821
            ],
            "angle": 0,
            "content": "(a) \\(\\lim_{k\\to \\infty}P(x^{k + 1},\\sigma_k) = f(x^*)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.307,
                0.826,
                0.503,
                0.856
            ],
            "angle": 0,
            "content": "(b) \\(\\lim_{k\\to \\infty}\\sigma_k\\sum_{i\\in \\mathcal{E}}c_i^2 (x^{k + 1}) = 0;\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.306,
                0.797,
                0.517,
                0.856
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.223,
                0.131
            ],
            "angle": 0,
            "content": "习题7"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "341"
        },
        {
            "type": "text",
            "bbox": [
                0.22,
                0.155,
                0.462,
                0.19
            ],
            "angle": 0,
            "content": "(c) \\(\\lim_{k\\to \\infty}\\frac{1}{\\sigma_k}\\sum_{i\\in \\mathcal{I}}\\ln (-c_i(x^{k + 1})) = 0.\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.197,
                0.737,
                0.255
            ],
            "angle": 0,
            "content": "7.6 (Morrison 方法) 考虑等式约束优化问题 (7.1.1)，设其最优解为 \\(x^{*}\\)。令 \\(M\\) 是最优函数值 \\(f(x^{*})\\) 的一个下界估计（即 \\(M \\leqslant f(x^{*})\\)），构造辅助函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.257,
                0.605,
                0.287
            ],
            "angle": 0,
            "content": "\\[\nv (M, x) = [ f (x) - M ] ^ {2} + \\sum_ {i \\in \\mathcal {E}} c _ {i} ^ {2} (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.291,
                0.454,
                0.307
            ],
            "angle": 0,
            "content": "Morrison 方法的迭代步骤如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.376,
                0.314,
                0.575,
                0.372
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} x ^ {k} = \\underset {x} {\\arg \\min } v (M _ {k}, x), \\\\ M _ {k + 1} = M _ {k} + \\sqrt {v (M _ {k} , x ^ {k})}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.377,
                0.343,
                0.393
            ],
            "angle": 0,
            "content": "试回答以下问题："
        },
        {
            "type": "text",
            "bbox": [
                0.218,
                0.404,
                0.414,
                0.422
            ],
            "angle": 0,
            "content": "(a) 证明: \\( f(x^{k}) \\leqslant f(x^{*}) \\);"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.428,
                0.536,
                0.446
            ],
            "angle": 0,
            "content": "(b) 若 \\(M_{k} \\leqslant f(x^{*})\\) ，证明：\\(M_{k+1} \\leqslant f(x^{*})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.219,
                0.452,
                0.427,
                0.475
            ],
            "angle": 0,
            "content": "(c) 证明: \\(\\lim_{k\\to \\infty}M_k = f(x^*)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.475,
                0.735,
                0.512
            ],
            "angle": 0,
            "content": "(d) 求 \\(v(M, x)\\) 关于 \\(x\\) 的海瑟矩阵，并说明Morrison方法和算法7.1的联系."
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.404,
                0.735,
                0.512
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.181,
                0.525,
                0.406,
                0.541
            ],
            "angle": 0,
            "content": "7.7 考虑不等式约束优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.34,
                0.552,
                0.608,
                0.571
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad f (x), \\quad \\text {s . t .} \\quad c _ {i} (x) \\leqslant 0, i \\in \\mathcal {I}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.218,
                0.575,
                0.737,
                0.638
            ],
            "angle": 0,
            "content": "(a) 定义函数 \\( F(x) = \\sup_{\\lambda_i \\geqslant 0} \\left\\{ f(x) + \\sum_{i \\in \\mathcal{I}} \\lambda_i c_i(x) \\right\\} \\)，证明：原问题等价于无约束优化问题 \\(\\min_x F(x)\\)；"
        },
        {
            "type": "text",
            "bbox": [
                0.218,
                0.64,
                0.322,
                0.655
            ],
            "angle": 0,
            "content": "(b) 定义函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.275,
                0.66,
                0.71,
                0.705
            ],
            "angle": 0,
            "content": "\\[\n\\hat {F} (x, \\lambda^ {k}, \\sigma_ {k}) = \\sup  _ {\\lambda_ {i} \\geqslant 0} \\left\\{f (x) + \\sum_ {i \\in \\mathcal {I}} \\lambda_ {i} c _ {i} (x) - \\frac {\\sigma_ {k}}{2} \\sum_ {i \\in \\mathcal {I}} (\\lambda_ {i} - \\lambda_ {i} ^ {k}) ^ {2} \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.246,
                0.709,
                0.468,
                0.727
            ],
            "angle": 0,
            "content": "求 \\(\\hat{F} (x,\\lambda^k,\\sigma_k)\\) 的显式表达式；"
        },
        {
            "type": "text",
            "bbox": [
                0.218,
                0.734,
                0.397,
                0.75
            ],
            "angle": 0,
            "content": "(c) 考虑如下优化算法:"
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.757,
                0.51,
                0.784
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k} = \\underset {x} {\\arg \\min } \\hat {F} (x, \\lambda^ {k}, \\sigma_ {k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.308,
                0.787,
                0.677,
                0.831
            ],
            "angle": 0,
            "content": "\\[\n\\lambda^ {k + 1} = \\underset {\\lambda \\geqslant 0} {\\arg \\max } \\left\\{\\sum_ {i \\in \\mathcal {I}} \\lambda_ {i} c _ {i} \\left(x ^ {k}\\right) - \\frac {\\sigma_ {k}}{2} \\sum_ {i \\in \\mathcal {I}} \\left(\\lambda_ {i} - \\lambda_ {i} ^ {k}\\right) ^ {2} \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.31,
                0.836,
                0.463,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\sigma_ {k + 1} = \\min \\{\\rho \\sigma_ {k}, \\bar {\\sigma} \\},\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "342"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第七章 约束优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.335,
                0.157,
                0.597,
                0.174
            ],
            "angle": 0,
            "content": "试说明其与算法7.5的区别和联系"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.188,
                0.439,
                0.204
            ],
            "angle": 0,
            "content": "7.8 对于 LASSO 问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.453,
                0.213,
                0.671,
                0.245
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2} + \\mu \\| x \\| _ {1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.255,
                0.668,
                0.271
            ],
            "angle": 0,
            "content": "写出该问题及其对偶问题的增广拉格朗日函数法"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.286,
                0.443,
                0.302
            ],
            "angle": 0,
            "content": "7.9 考虑线性规划问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.436,
                0.317,
                0.686,
                0.341
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} c ^ {\\mathrm {T}} x, \\quad \\text {s . t .} \\quad A x = b,   x \\geqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.352,
                0.707,
                0.369
            ],
            "angle": 0,
            "content": "(a) 写出该问题及其对偶问题的增广拉格朗日函数法；"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.378,
                0.465,
                0.394
            ],
            "angle": 0,
            "content": "(b) 分析有限终止性."
        },
        {
            "type": "list",
            "bbox": [
                0.305,
                0.352,
                0.707,
                0.394
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.408,
                0.755,
                0.425
            ],
            "angle": 0,
            "content": "7.10 证明：方程(7.3.5)的系数矩阵非奇异当且仅当 \\(A\\) 是行满秩的。"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.439,
                0.767,
                0.456
            ],
            "angle": 0,
            "content": "7.11 给出求解方程(7.3.5)（即内点法线性系统子问题）的详细过程"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.469,
                0.825,
                0.506
            ],
            "angle": 0,
            "content": "7.12 对线性规划问题(7.3.1)中的原始问题(P)，构造带等式约束的内点罚函数子问题"
        },
        {
            "type": "list",
            "bbox": [
                0.26,
                0.408,
                0.825,
                0.506
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.478,
                0.506,
                0.643,
                0.559
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\quad c ^ {T} x - \\tau \\sum_ {i = 1} ^ {n} \\ln x _ {i}, \\\\ \\begin{array}{l l} \\text {s . t .} & A x = b, \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.297,
                0.568,
                0.834,
                0.626
            ],
            "angle": 0,
            "content": "其中 \\(\\tau > 0\\) 为罚因子。试说明求解该问题等价于求解中心路径方程(7.3.8)，并且进一步说明当 \\(\\tau \\to 0\\) 时，该问题的解收敛于满足KKT方程(7.3.2)的点。"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.641,
                0.735,
                0.659
            ],
            "angle": 0,
            "content": "7.13 详细说明在算法7.7中如何选取最大的 \\(\\alpha\\) 使得 \\(\\alpha \\in \\mathcal{N}_{-\\infty}(\\gamma)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.672,
                0.757,
                0.688
            ],
            "angle": 0,
            "content": "7.14 考虑部分变量为自由变量（即无非负约束）的线性规划问题："
        },
        {
            "type": "list",
            "bbox": [
                0.26,
                0.641,
                0.757,
                0.688
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.478,
                0.701,
                0.645,
                0.765
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x, y} \\quad c ^ {\\mathrm {T}} x + d ^ {\\mathrm {T}} y, \\\\ \\begin{array}{l l} \\text {s . t .} & A _ {1} x + A _ {2} y = b, \\end{array} \\\\ x \\geqslant 0, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.297,
                0.78,
                0.825,
                0.817
            ],
            "angle": 0,
            "content": "在这里注意变量 \\(y\\) 没有非负约束。试推导求解此问题的原始 - 对偶算法，给出类似于(7.3.5)式的方程组并给出其解的显式表达式。"
        }
    ],
    [
        {
            "type": "title",
            "bbox": [
                0.272,
                0.252,
                0.636,
                0.285
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.335,
                0.457,
                0.352
            ],
            "angle": 0,
            "content": "本章主要考虑如下复合优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.363,
                0.737,
                0.39
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\psi (x) \\stackrel {\\text {d e f}} {=} f (x) + h (x), \\tag {8.0.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.399,
                0.739,
                0.581
            ],
            "angle": 0,
            "content": "其中 \\(f(x)\\) 为可微函数（可能非凸），\\(h(x)\\) 可能为不可微函数。问题 (8.0.1) 出现在很多应用领域中，例如压缩感知、图像处理、机器学习等，如何高效求解该问题是近年来的热门课题。第六章曾利用光滑化的思想处理不可微项 \\(h(x)\\)，但这种做法没有充分利用 \\(h(x)\\) 的性质，在实际应用中有一定的局限性。而本章将介绍若干适用于求解问题 (8.0.1) 的方法并给出一些理论性质。我们首先引入针对问题 (8.0.1) 直接进行求解的近似点梯度法和 Nesterov 加速算法，之后介绍求解特殊结构复合优化问题的近似点算法、分块坐标下降法、对偶算法以及交替方向乘子法，最后介绍处理 \\(\\nabla f(x)\\) 难以精确计算情形的随机优化算法。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.585,
                0.739,
                0.707
            ],
            "angle": 0,
            "content": "需要注意的是，许多实际问题并不直接具有本章介绍的算法所能处理的形式，我们需要利用拆分、引入辅助变量等技巧将其进行等价变形，最终化为合适的优化问题。具体可参考第8.4节和第8.6节的内容。此外，本章涉及的定理证明需要较多第二章中的内容，为了方便，我们默认所有次梯度计算规则的前提成立（加法、线性变量替换等，见第2.7.4节）。这些前提在绝大多数应用中都会满足。"
        },
        {
            "type": "title",
            "bbox": [
                0.349,
                0.736,
                0.558,
                0.758
            ],
            "angle": 0,
            "content": "8.1 近似点梯度法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "在机器学习、图像处理领域中, 许多模型包含两部分: 一部分是误差项,一般为光滑函数; 另外一部分是正则项, 可能为非光滑函数, 用来保证求解问题的特殊结构. 例如最常见的 LASSO 问题就是用 \\(\\ell_{1}\\) 范数构造正则项保证求解的参数是稀疏的, 从而起到筛选变量的作用. 由于有非光滑部分的存"
        },
        {
            "type": "page_number",
            "bbox": [
                0.44,
                0.87,
                0.469,
                0.882
            ],
            "angle": 0,
            "content": "343"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "344"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.156,
                0.827,
                0.381
            ],
            "angle": 0,
            "content": "在，此类问题属于非光滑的优化问题，我们可以考虑使用次梯度算法进行求解。然而次梯度算法并不能充分利用光滑部分的信息，也很难在迭代中保证非光滑项对应的解的结构信息，这使得次梯度算法在求解这类问题时往往收敛较慢。本节将介绍求解这类问题非常有效的一种算法——近似点梯度算法。它能克服次梯度算法的缺点，充分利用光滑部分的信息，并在迭代过程中显式地保证解的结构，从而能够达到和求解光滑问题的梯度算法相近的收敛速度。在后面的内容中，我们首先引入邻近算子，它是近似点梯度算法中处理非光滑部分的关键；接着介绍近似点梯度算法的迭代格式，并给出一些实际的例子；最后给出这个算法的一些收敛性证明，并将看到它确实有和光滑梯度算法相似的收敛速度。为了讨论简便，我们主要介绍凸函数的情形。最后一小节简单介绍非凸函数的邻近算子，供感兴趣的读者阅读。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.405,
                0.4,
                0.423
            ],
            "angle": 0,
            "content": "8.1.1 邻近算子"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.436,
                0.825,
                0.515
            ],
            "angle": 0,
            "content": "邻近算子是处理非光滑问题的一个非常有效的工具，也与许多算法的设计密切相关，比如我们即将介绍的近似点梯度法和近似点算法等。当然该算子并不局限于非光滑函数，也可以用来处理光滑函数。本小节将介绍邻近算子的相关内容，为引入近似点梯度算法做准备。"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.52,
                0.491,
                0.536
            ],
            "angle": 0,
            "content": "首先给出邻近算子的定义"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.546,
                0.759,
                0.562
            ],
            "angle": 0,
            "content": "定义8.1（邻近算子）对于一个凸函数 \\(h\\) ，定义它的邻近算子为"
        },
        {
            "type": "equation",
            "bbox": [
                0.382,
                0.567,
                0.825,
                0.602
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {p r o x} _ {h} (x) = \\underset {u \\in \\mathbf {d o m} h} {\\arg \\min } \\left\\{h (u) + \\frac {1}{2} \\| u - x \\| ^ {2} \\right\\}. \\tag {8.1.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.608,
                0.826,
                0.708
            ],
            "angle": 0,
            "content": "可以看到，邻近算子的目的是求解一个距 \\(x\\) 不算太远的点，并使函数值 \\(h(x)\\) 也相对较小。一个很自然的问题是，上面给出的邻近算子的定义是不是有意义的，即定义中的优化问题的解是不是存在唯一的。若答案是肯定的，我们就可使用邻近算子去构建迭代格式。下面的定理将给出定义中优化问题解的存在唯一性。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.718,
                0.825,
                0.755
            ],
            "angle": 0,
            "content": "定理8.1（邻近算子是良定义的）如果 \\(h\\) 是适当的闭凸函数，则对任意的 \\(x \\in \\mathbb{R}^n\\)，\\(\\operatorname{prox}_h(x)\\) 的值存在且唯一。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.764,
                0.826,
                0.822
            ],
            "angle": 0,
            "content": "证明．为了简化证明，我们假设 \\(h\\) 至少在定义域内的一点处存在次梯度，保证次梯度存在的一个充分条件是 \\(\\mathbf{dom}h\\) 内点集非空．对于比较复杂的情况读者可参考[14]命题12.15．定义辅助函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.44,
                0.826,
                0.644,
                0.857
            ],
            "angle": 0,
            "content": "\\[\nm (u) = h (u) + \\frac {1}{2} \\| u - x \\| ^ {2},\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.1 近似点梯度法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "345"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.738,
                0.195
            ],
            "angle": 0,
            "content": "下面利用Weierstrass定理（定理5.1）来说明 \\(m(u)\\) 最小值点的存在性．因为 \\(h(u)\\) 是凸函数，且至少在一点处存在次梯度，所以 \\(h(u)\\) 有全局下界："
        },
        {
            "type": "equation",
            "bbox": [
                0.357,
                0.211,
                0.55,
                0.23
            ],
            "angle": 0,
            "content": "\\[\nh (u) \\geqslant h (v) + \\theta^ {\\mathrm {T}} (u - v),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.246,
                0.465,
                0.263
            ],
            "angle": 0,
            "content": "这里 \\(v \\in \\mathbf{dom}h\\) ， \\(\\theta \\in \\partial h(v)\\) .进而得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.305,
                0.274,
                0.602,
                0.339
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} m (u) = h (u) + \\frac {1}{2} \\| u - x \\| ^ {2} \\\\ \\geqslant h (v) + \\theta^ {\\mathrm {T}} (u - v) + \\frac {1}{2} \\| u - x \\| ^ {2}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.349,
                0.738,
                0.388
            ],
            "angle": 0,
            "content": "这表明 \\(m(u)\\) 具有二次下界. 容易验证 \\(m(u)\\) 为适当闭函数且具有强制性（当 \\(\\| u\\| \\to +\\infty\\) 时，\\(m(u)\\to +\\infty\\)），根据定理5.1可知 \\(m(u)\\) 存在最小值."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.391,
                0.738,
                0.43
            ],
            "angle": 0,
            "content": "接下来证明唯一性. 注意到 \\(m(u)\\) 是强凸函数, 根据命题 2.3 的结果可直接得出 \\(m(u)\\) 的最小值唯一. 综上 \\(\\operatorname{prox}_h(x)\\) 是良定义的."
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.446,
                0.561,
                0.463
            ],
            "angle": 0,
            "content": "另外，根据最优性条件可以得到如下等价结论："
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.478,
                0.701,
                0.495
            ],
            "angle": 0,
            "content": "定理8.2（邻近算子与次梯度的关系）如果 \\(h\\) 是适当的闭凸函数，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.512,
                0.584,
                0.532
            ],
            "angle": 0,
            "content": "\\[\nu = \\operatorname {p r o x} _ {h} (x) \\iff x - u \\in \\partial h (u).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.546,
                0.738,
                0.584
            ],
            "angle": 0,
            "content": "证明．若 \\(u = \\mathrm{prox}_h(x)\\) ，则由最优性条件得 \\(0\\in \\partial h(u) + (u - x)\\) ，因此有\\(x - u\\in \\partial h(u)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.588,
                0.581,
                0.605
            ],
            "angle": 0,
            "content": "反之，若 \\(x - u\\in \\partial h(u)\\) 则由次梯度的定义可得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.277,
                0.621,
                0.63,
                0.641
            ],
            "angle": 0,
            "content": "\\[\nh (v) \\geqslant h (u) + (x - u) ^ {\\mathrm {T}} (v - u), \\quad \\forall v \\in \\mathbf {d o m} h.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.651,
                0.395,
                0.683
            ],
            "angle": 0,
            "content": "两边同时加 \\(\\frac{1}{2}\\| v - x\\|^2\\)，即有"
        },
        {
            "type": "equation",
            "bbox": [
                0.237,
                0.693,
                0.669,
                0.758
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} h (v) + \\frac {1}{2} \\| v - x \\| ^ {2} \\geqslant h (u) + (x - u) ^ {\\mathrm {T}} (v - u) + \\frac {1}{2} \\| v - x \\| ^ {2} \\\\ \\geqslant h (u) + \\frac {1}{2} \\| u - x \\| ^ {2}, \\quad \\forall v \\in \\mathbf {d o m} h. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.768,
                0.385,
                0.786
            ],
            "angle": 0,
            "content": "因此我们得到 \\(u = \\mathrm{prox}_h(x)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.802,
                0.557,
                0.819
            ],
            "angle": 0,
            "content": "用 \\(th\\) 代替 \\(h\\), 上面的等价结论形式上可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.318,
                0.836,
                0.589,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nu = \\operatorname {p r o x} _ {t h} (x) \\iff u \\in x - t \\partial h (u).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "346"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.156,
                0.825,
                0.235
            ],
            "angle": 0,
            "content": "邻近算子的计算可以看成是次梯度算法的隐式格式（后向迭代），这实际是近似点算法的迭代格式（见第8.3节）。对于非光滑情形，由于次梯度不唯一，显式格式的迭代并不唯一，而隐式格式却能得到唯一解。此外在步长的选择上面，隐式格式也要优于显式格式。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.24,
                0.825,
                0.297
            ],
            "angle": 0,
            "content": "下面给出一些常见的例子. 计算邻近算子的过程实际上是在求解一个优化问题, 我们给出 \\(\\ell_{1}\\) 范数和 \\(\\ell_{2}\\) 范数对应的计算过程, 其他例子读者可以自行验证."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.309,
                0.796,
                0.326
            ],
            "angle": 0,
            "content": "例8.1(邻近算子的例子) 在下面所有例子中, 常数 \\(t > 0\\) 为正实数."
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.338,
                0.364,
                0.353
            ],
            "angle": 0,
            "content": "(1) \\(\\ell_1\\) 范数："
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.367,
                0.756,
                0.386
            ],
            "angle": 0,
            "content": "\\[\nh (x) = \\| x \\| _ {1}, \\quad \\operatorname {p r o x} _ {t h} (x) = \\operatorname {s i g n} (x) \\max  \\left\\{| x | - t, 0 \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.402,
                0.668,
                0.42
            ],
            "angle": 0,
            "content": "证明. 邻近算子 \\(u = \\operatorname{prox}_{th}(x)\\) 的最优性条件为"
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.429,
                0.695,
                0.505
            ],
            "angle": 0,
            "content": "\\[\nx - u \\in t \\partial \\| u \\| _ {1} = \\left\\{ \\begin{array}{l l} \\{t \\}, & u > 0, \\\\ [ - t, t ], & u = 0, \\\\ \\{- t \\}, & u <   0, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.513,
                0.825,
                0.552
            ],
            "angle": 0,
            "content": "因此，当 \\(x > t\\) 时，\\(u = x - t\\)；当 \\(x < -t\\) 时，\\(u = x + t\\)；当 \\(x \\in [-t, t]\\) 时，\\(u = 0\\)，即有 \\(u = \\operatorname{sign}(x) \\max \\{|x| - t, 0\\}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.569,
                0.364,
                0.584
            ],
            "angle": 0,
            "content": "(2) \\(\\ell_2\\) 范数："
        },
        {
            "type": "equation",
            "bbox": [
                0.349,
                0.591,
                0.772,
                0.652
            ],
            "angle": 0,
            "content": "\\[\nh (x) = \\| x \\| _ {2}, \\quad \\mathrm {p r o x} _ {t h} (x) = \\left\\{ \\begin{array}{l l} {\\left(1 - \\frac {t}{\\| x \\| _ {2}}\\right) x,} & {\\| x \\| _ {2} \\geqslant t,} \\\\ {0,} & {\\text {其 他}.} \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.663,
                0.668,
                0.681
            ],
            "angle": 0,
            "content": "证明. 邻近算子 \\(u = \\operatorname{prox}_{th}(x)\\) 的最优性条件为"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.688,
                0.727,
                0.749
            ],
            "angle": 0,
            "content": "\\[\nx - u \\in t \\partial \\| u \\| _ {2} = \\left\\{ \\begin{array}{l l} \\left\\{\\frac {t u}{\\| u \\| _ {2}} \\right\\}, & u \\neq 0, \\\\ \\left\\{w: \\| w \\| _ {2} \\leqslant t \\right\\}, & u = 0, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.757,
                0.825,
                0.791
            ],
            "angle": 0,
            "content": "因此，当 \\(\\| x\\| _2 > t\\) 时， \\(u = x - \\frac{tx}{\\|x\\|_2}\\) 当 \\(\\| x\\| _2\\leqslant t\\) 时， \\(u = 0\\) □"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.801,
                0.531,
                0.818
            ],
            "angle": 0,
            "content": "(3) 二次函数 (其中 \\(A\\) 对称正定):"
        },
        {
            "type": "equation",
            "bbox": [
                0.336,
                0.826,
                0.785,
                0.857
            ],
            "angle": 0,
            "content": "\\[\nh (x) = \\frac {1}{2} x ^ {\\mathrm {T}} A x + b ^ {\\mathrm {T}} x + c, \\quad \\operatorname {p r o x} _ {t h} (x) = (I + t A) ^ {- 1} (x - t b).\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.317,
                0.133
            ],
            "angle": 0,
            "content": "8.1 近似点梯度法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "347"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.157,
                0.344,
                0.174
            ],
            "angle": 0,
            "content": "(4) 负自然对数的和："
        },
        {
            "type": "equation",
            "bbox": [
                0.232,
                0.184,
                0.715,
                0.224
            ],
            "angle": 0,
            "content": "\\[\nh (x) = - \\sum_ {i = 1} ^ {n} \\ln x _ {i}, \\quad \\operatorname {p r o x} _ {t h} (x) _ {i} = \\frac {x _ {i} + \\sqrt {x _ {i} ^ {2} + 4 t}}{2}, \\quad i = 1, 2, \\dots , n.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.239,
                0.739,
                0.298
            ],
            "angle": 0,
            "content": "除了直接利用定义计算，很多时候可以利用已知邻近算子的结果，计算其他邻近算子。下面我们给出一些常用的运算规则。运用这些简单的规则，可以求解更复杂的邻近算子。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.312,
                0.737,
                0.349
            ],
            "angle": 0,
            "content": "例8.2(邻近算子的运算规则）由邻近算子的定义和基本的计算推导，我们可以得出邻近算子满足如下运算规则："
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.363,
                0.499,
                0.38
            ],
            "angle": 0,
            "content": "(1) 变量的常数倍放缩以及平移 \\((\\lambda \\neq 0)\\):"
        },
        {
            "type": "equation",
            "bbox": [
                0.259,
                0.39,
                0.689,
                0.422
            ],
            "angle": 0,
            "content": "\\[\nh (x) = g (\\lambda x + a), \\quad \\operatorname {p r o x} _ {h} (x) = \\frac {1}{\\lambda} \\left(\\operatorname {p r o x} _ {\\lambda^ {2} g} (\\lambda x + a) - a)\\right);\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.437,
                0.515,
                0.453
            ],
            "angle": 0,
            "content": "(2) 函数（及变量）的常数倍放缩 \\((\\lambda > 0)\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.299,
                0.464,
                0.65,
                0.495
            ],
            "angle": 0,
            "content": "\\[\nh (x) = \\lambda g \\left(\\frac {x}{\\lambda}\\right), \\quad \\operatorname {p r o x} _ {h} (x) = \\lambda \\operatorname {p r o x} _ {\\lambda^ {- 1} g} \\left(\\frac {x}{\\lambda}\\right);\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.509,
                0.327,
                0.525
            ],
            "angle": 0,
            "content": "(3) 加上线性函数："
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.541,
                0.652,
                0.563
            ],
            "angle": 0,
            "content": "\\[\nh (x) = g (x) + a ^ {\\mathrm {T}} x, \\quad \\operatorname {p r o x} _ {h} (x) = \\operatorname {p r o x} _ {g} (x - a);\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.581,
                0.361,
                0.597
            ],
            "angle": 0,
            "content": "(4) 加上二次项 \\((u > 0)\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.237,
                0.609,
                0.712,
                0.638
            ],
            "angle": 0,
            "content": "\\[\nh (x) = g (x) + \\frac {u}{2} \\| x - a \\| _ {2} ^ {2}, \\quad \\operatorname {p r o x} _ {h} (x) = \\operatorname {p r o x} _ {\\theta g} (\\theta x + (1 - \\theta) a);\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.648,
                0.336,
                0.68
            ],
            "angle": 0,
            "content": "其中 \\(\\theta = \\frac{1}{1 + u}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.687,
                0.294,
                0.703
            ],
            "angle": 0,
            "content": "(5) 向量函数："
        },
        {
            "type": "equation",
            "bbox": [
                0.248,
                0.713,
                0.698,
                0.76
            ],
            "angle": 0,
            "content": "\\[\nh \\left(\\left[ \\begin{array}{c} x \\\\ y \\end{array} \\right]\\right) = \\varphi_ {1} (x) + \\varphi_ {2} (y), \\quad \\operatorname {p r o x} _ {h} \\left(\\left[ \\begin{array}{c} x \\\\ y \\end{array} \\right]\\right) = \\left[ \\begin{array}{c} \\operatorname {p r o x} _ {\\varphi_ {1}} (x) \\\\ \\operatorname {p r o x} _ {\\varphi_ {2}} (y) \\end{array} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.774,
                0.739,
                0.853
            ],
            "angle": 0,
            "content": "对于一般的复合函数，我们很难给出其邻近算子的显式解。不过当外层函数的邻近算子有显式解且内层函数是特殊的仿射函数的时候，求解复合函数的邻近算子就会容易得多。下面的例子给出了一般函数复合仿射变换的结果。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "348"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.225
            ],
            "angle": 0,
            "content": "例8.3 (仿射变换与邻近算子) 已知函数 \\( g(x) \\) 和矩阵 \\( A \\)，设 \\( h(x) = g(Ax + b) \\)。在通常情况下，我们不能使用 \\( g \\) 的邻近算子直接计算关于 \\( h \\) 的邻近算子。然而，如果有 \\( AA^{\\mathrm{T}} = \\frac{1}{\\alpha} I \\) （其中 \\( \\alpha \\) 为任意正常数），则"
        },
        {
            "type": "equation",
            "bbox": [
                0.331,
                0.23,
                0.75,
                0.252
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {p r o x} _ {h} (x) = \\left(I - \\alpha A ^ {\\mathrm {T}} A\\right) x + \\alpha A ^ {\\mathrm {T}} \\left(\\operatorname {p r o x} _ {\\alpha^ {- 1} g} (A x + b) - b\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.262,
                0.718,
                0.279
            ],
            "angle": 0,
            "content": "例如， \\(h(x_{1},x_{2},\\dots ,x_{m}) = g(x_{1} + x_{2} + \\dots +x_{m})\\) 的邻近算子为"
        },
        {
            "type": "equation",
            "bbox": [
                0.32,
                0.286,
                0.763,
                0.33
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {p r o x} _ {h} (x _ {1}, x _ {2}, \\dots , x _ {m}) _ {i} = x _ {i} - \\frac {1}{m} \\left(\\sum_ {j = 1} ^ {m} x _ {j} - \\operatorname {p r o x} _ {m g} \\left(\\sum_ {j = 1} ^ {m} x _ {j}\\right)\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.337,
                0.473,
                0.353
            ],
            "angle": 0,
            "content": "证明. 考虑如下优化问题:"
        },
        {
            "type": "equation",
            "bbox": [
                0.446,
                0.358,
                0.635,
                0.416
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {u, y} \\quad g (y) + \\frac {1}{2} \\| u - x \\| ^ {2}, \\\\ \\begin{array}{l l} \\text {s . t .} & A u + b = y, \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.428,
                0.825,
                0.464
            ],
            "angle": 0,
            "content": "则其解中的 \\(u = \\operatorname{prox}_h(x)\\)。固定 \\(y\\) 对于 \\(u\\) 求极小值，这是一个到仿射集的投影问题，其解为"
        },
        {
            "type": "equation",
            "bbox": [
                0.414,
                0.476,
                0.667,
                0.52
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} u = x + A ^ {\\mathrm {T}} \\left(A A ^ {\\mathrm {T}}\\right) ^ {- 1} (y - b - A x) \\\\ = (I - \\alpha A ^ {\\mathrm {T}} A) x + \\alpha A ^ {\\mathrm {T}} (y - b). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.532,
                0.538,
                0.549
            ],
            "angle": 0,
            "content": "将其代入优化问题，将目标函数化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.328,
                0.556,
                0.754,
                0.589
            ],
            "angle": 0,
            "content": "\\[\ng (y) + \\frac {\\alpha^ {2}}{2} \\| A ^ {T} (y - b - A x) \\| ^ {2} = g (y) + \\frac {\\alpha}{2} \\| y - b - A x \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.594,
                0.775,
                0.615
            ],
            "angle": 0,
            "content": "由此得到 \\( y = \\operatorname{prox}_{\\alpha^{-1}g}(Ax + b) \\)，再代入 \\( u \\) 的表达式中即可得到结果."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.624,
                0.825,
                0.661
            ],
            "angle": 0,
            "content": "另外一种比较常用的邻近算子是关于示性函数的邻近算子。集合 \\(C\\) 的示性函数定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.451,
                0.661,
                0.631,
                0.711
            ],
            "angle": 0,
            "content": "\\[\nI _ {C} (x) = \\left\\{ \\begin{array}{l l} 0, & x \\in C, \\\\ + \\infty , & \\text {其 他}, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.716,
                0.578,
                0.733
            ],
            "angle": 0,
            "content": "它可以用来把约束变成目标函数的一部分"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.744,
                0.825,
                0.78
            ],
            "angle": 0,
            "content": "例8.4（闭凸集上的投影）设 \\(C\\) 为 \\(\\mathbb{R}^n\\) 上的闭凸集，则示性函数 \\(I_{C}\\) 的邻近算子为点 \\(x\\) 到集合 \\(C\\) 的投影，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.787,
                0.705,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\operatorname {p r o x} _ {I _ {C}} (x) = \\underset {u} {\\arg \\min } \\left\\{I _ {C} (u) + \\frac {1}{2} \\| u - x \\| ^ {2} \\right\\} \\\\ = \\underset {u \\in C} {\\arg \\min } \\| u - x \\| ^ {2} = \\mathcal {P} _ {C} (x). \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.1 近似点梯度法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "349"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.429,
                0.174
            ],
            "angle": 0,
            "content": "此外，应用定理8.2可进一步得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.223,
                0.186,
                0.682,
                0.23
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} u = \\mathcal {P} _ {C} (x) \\Leftrightarrow x - u \\in \\partial I _ {C} (u) \\\\ \\Leftrightarrow (x - u) ^ {\\mathrm {T}} (z - u) \\leqslant I _ {C} (z) - I _ {C} (u) = 0, \\quad \\forall z \\in C. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.242,
                0.737,
                0.279
            ],
            "angle": 0,
            "content": "此结论有较强的几何意义：若点 \\(x\\) 位于 \\(C\\) 外部，则从投影点 \\(u\\) 指向 \\(x\\) 的向量与任意起点为 \\(u\\) 且指向 \\(C\\) 内部的向量的夹角为直角或钝角."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.305,
                0.353,
                0.323
            ],
            "angle": 0,
            "content": "8.1.2 近似点梯度法"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.336,
                0.737,
                0.373
            ],
            "angle": 0,
            "content": "下面将引入本节的重点——近似点梯度算法。我们将考虑如下复合优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.351,
                0.377,
                0.737,
                0.395
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad \\psi (x) = f (x) + h (x), \\tag {8.1.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.405,
                0.738,
                0.485
            ],
            "angle": 0,
            "content": "其中函数 \\(f\\) 为可微函数，其定义域 \\(\\mathbf{dom} f = \\mathbb{R}^n\\)，函数 \\(h\\) 为凸函数，可以是非光滑的，并且一般计算此项的邻近算子并不复杂。比如 LASSO 问题，两项分别为 \\(f(x) = \\frac{1}{2} \\|Ax - b\\|^2, h(x) = \\mu \\|x\\|_1\\)。一般的带凸集约束的优化问题也可以用 (8.1.2) 式表示，即对问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.501,
                0.501,
                0.524
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in C} \\quad \\phi (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.533,
                0.737,
                0.571
            ],
            "angle": 0,
            "content": "复合优化问题中的两项可以写作 \\( f(x) = \\phi (x), h(x) = I_C(x) \\)，其中 \\( I_{C}(x) \\) 为示性函数."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.575,
                0.737,
                0.633
            ],
            "angle": 0,
            "content": "近似点梯度法的思想非常简单：注意到 \\(\\psi(x)\\) 有两部分，对于光滑部分 \\(f\\) 做梯度下降，对于非光滑部分 \\(h\\) 使用邻近算子，则近似点梯度法的迭代公式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.336,
                0.635,
                0.737,
                0.656
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {t _ {k} h} \\left(x ^ {k} - t _ {k} \\nabla f \\left(x ^ {k}\\right)\\right), \\tag {8.1.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.665,
                0.738,
                0.724
            ],
            "angle": 0,
            "content": "其中 \\(t_{k} > 0\\) 为每次迭代的步长, 它可以是一个常数或者由线搜索得出. 近似点梯度法跟众多算法都有很强的联系, 在一些特定条件下, 近似点梯度法还可以转化为其他算法: 当 \\(h(x) = 0\\) 时, 迭代公式变为梯度下降法"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.737,
                0.538,
                0.757
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - t _ {k} \\nabla f (x ^ {k});\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.771,
                0.52,
                0.789
            ],
            "angle": 0,
            "content": "当 \\(h(x) = I_{\\mathrm{C}}(x)\\) 时，迭代公式变为投影梯度法"
        },
        {
            "type": "equation",
            "bbox": [
                0.351,
                0.802,
                0.556,
                0.822
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\mathcal {P} _ {C} \\left(x ^ {k} - t _ {k} \\nabla f \\left(x ^ {k}\\right)\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.429,
                0.853
            ],
            "angle": 0,
            "content": "近似点梯度法可以总结为算法8.1."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "350"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.159,
                0.431,
                0.175
            ],
            "angle": 0,
            "content": "算法8.1近似点梯度法"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.179,
                0.661,
                0.198
            ],
            "angle": 0,
            "content": "1. 输入：函数 \\( f(x), h(x) \\)，初始点 \\( x^0 \\)。初始化 \\( k = 0 \\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.201,
                0.487,
                0.217
            ],
            "angle": 0,
            "content": "2. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.22,
                0.535,
                0.241
            ],
            "angle": 0,
            "content": "3. \\(x^{k + 1} = \\mathrm{prox}_{t_kh}(x^k - t_k\\nabla f(x^k))\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.244,
                0.386,
                0.258
            ],
            "angle": 0,
            "content": "4. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.264,
                0.371,
                0.278
            ],
            "angle": 0,
            "content": "5. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.179,
                0.661,
                0.278
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.305,
                0.785,
                0.321
            ],
            "angle": 0,
            "content": "如何理解近似点梯度法？根据邻近算子的定义，把迭代公式展开："
        },
        {
            "type": "equation",
            "bbox": [
                0.287,
                0.331,
                0.796,
                0.404
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} x ^ {k + 1} = \\arg \\min  _ {u} \\left\\{h (u) + \\frac {1}{2 t _ {k}} \\| u - x ^ {k} + t _ {k} \\nabla f (x ^ {k}) \\| ^ {2} \\right\\} \\\\ = \\arg \\min  _ {u} \\left\\{h (u) + f \\left(x ^ {k}\\right) + \\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} \\left(u - x ^ {k}\\right) + \\frac {1}{2 t _ {k}} \\| u - x ^ {k} \\| ^ {2} \\right\\}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.415,
                0.825,
                0.473
            ],
            "angle": 0,
            "content": "可以发现，近似点梯度法实质上就是将问题的光滑部分线性展开再加上二次项并保留非光滑部分，然后求极小来作为每一步的估计。此外，根据定理8.2，近似点梯度算法可以形式上写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.371,
                0.487,
                0.71,
                0.508
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - t _ {k} \\nabla f (x ^ {k}) - t _ {k} g ^ {k}, \\quad g ^ {k} \\in \\partial h (x ^ {k + 1}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.522,
                0.825,
                0.558
            ],
            "angle": 0,
            "content": "其本质上是对光滑部分做显式的梯度下降，关于非光滑部分做隐式的梯度下降."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.564,
                0.825,
                0.61
            ],
            "angle": 0,
            "content": "算法8.1中步长 \\(t_k\\) 的选取较为关键。当 \\(f\\) 为梯度 \\(L\\)-利普希茨连续函数时，可取固定步长 \\(t_k = t \\leqslant \\frac{1}{L}\\)。当 \\(L\\) 未知时可使用线搜索准则"
        },
        {
            "type": "equation",
            "bbox": [
                0.301,
                0.62,
                0.825,
                0.653
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k + 1}\\right) \\leqslant f \\left(x ^ {k}\\right) + \\nabla f \\left(x ^ {k}\\right) ^ {\\mathrm {T}} \\left(x ^ {k + 1} - x ^ {k}\\right) + \\frac {1}{2 t _ {k}} \\| x ^ {k + 1} - x ^ {k} \\| ^ {2}. \\tag {8.1.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.663,
                0.825,
                0.763
            ],
            "angle": 0,
            "content": "我们将在第8.1.4小节中解释这样选取的原因．此外，还可利用BB步长作为 \\(t_k\\) 的初始估计并用非单调线搜索准则进行校正．由于 \\(\\psi (x)\\) 是不可微的函数，利用格式(6.2.7)或格式(6.2.8)进行计算时，应使用 \\(\\nabla f(x^{k})\\) 和 \\(\\nabla f(x^{k - 1})\\) （即光滑部分的梯度）计算与其对应的 \\(y^{k - 1}\\) ．类似地，仿照准则(6.1.6)可构造如下适用于近似点梯度法的非单调线搜索准则："
        },
        {
            "type": "equation",
            "bbox": [
                0.418,
                0.773,
                0.825,
                0.805
            ],
            "angle": 0,
            "content": "\\[\n\\psi \\left(x ^ {k + 1}\\right) \\leqslant C ^ {k} - \\frac {c _ {1}}{2 t _ {k}} \\| x ^ {k + 1} - x ^ {k} \\| ^ {2}, \\tag {8.1.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.815,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(c_{1} \\in (0, 1)\\) 为正常数，\\(C^{k}\\) 的定义同 (6.1.6) 式．注意，定义 \\(C^{k}\\) 时需要使用整体函数值 \\(\\psi(x^{k})\\)"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.1 近似点梯度法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "351"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.156,
                0.314,
                0.174
            ],
            "angle": 0,
            "content": "8.1.3 应用举例"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.188,
                0.339,
                0.205
            ],
            "angle": 0,
            "content": "1. LASSO问题求解"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.22,
                0.601,
                0.236
            ],
            "angle": 0,
            "content": "这里介绍如何使用近似点梯度法来求解 LASSO 问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.345,
                0.246,
                0.564,
                0.279
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad \\mu \\| x \\| _ {1} + \\frac {1}{2} \\| A x - b \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.289,
                0.499,
                0.321
            ],
            "angle": 0,
            "content": "令 \\(f(x) = \\frac{1}{2}\\| Ax - b\\|^2\\) ， \\(h(x) = \\mu \\| x\\| _1\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.331,
                0.334,
                0.504,
                0.354
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x) = A ^ {\\mathrm {T}} (A x - b),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.307,
                0.36,
                0.602,
                0.381
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {p r o x} _ {t _ {k} h} (x) = \\operatorname {s i g n} (x) \\max  \\left\\{| x | - t _ {k} \\mu , 0 \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.395,
                0.659,
                0.412
            ],
            "angle": 0,
            "content": "求解 LASSO 问题的近似点梯度算法可以由下面的迭代格式给出："
        },
        {
            "type": "equation",
            "bbox": [
                0.337,
                0.426,
                0.531,
                0.446
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = x ^ {k} - t _ {k} A ^ {\\mathrm {T}} (A x ^ {k} - b),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.325,
                0.45,
                0.586,
                0.471
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {s i g n} \\left(y ^ {k}\\right) \\max  \\left\\{\\left| y ^ {k} \\right| - t _ {k} \\mu , 0 \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.484,
                0.738,
                0.522
            ],
            "angle": 0,
            "content": "即第一步做梯度下降，第二步做收缩．特别地，第二步收缩算子保证了迭代过程中解的稀疏结构．这也解释了为什么近似点梯度算法效果好的原因."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.526,
                0.738,
                0.607
            ],
            "angle": 0,
            "content": "我们用同第6.2节中一样的 \\(A\\) 和 \\(b\\)，并取 \\(\\mu = 10^{-3}\\)。采用连续化的近似点梯度法来求解，分别取固定步长 \\(t = \\frac{1}{L}\\)，这里 \\(L = \\lambda_{\\max}(A^{\\mathrm{T}}A)\\)，和结合线搜索的BB步长。停机准则和参数 \\(\\mu\\) 的连续化设置和第6.2节中的光滑化梯度法一致，结果如图8.1。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.612,
                0.738,
                0.65
            ],
            "angle": 0,
            "content": "可以看到结合线搜索的BB步长能够显著提高算法的收敛速度，且比第6.2节中的光滑化梯度法收敛得更快。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.678,
                0.31,
                0.694
            ],
            "angle": 0,
            "content": "2. 低秩矩阵恢复"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.709,
                0.438,
                0.726
            ],
            "angle": 0,
            "content": "考虑低秩矩阵恢复模型(1.3.3)："
        },
        {
            "type": "equation",
            "bbox": [
                0.31,
                0.737,
                0.598,
                0.776
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X \\in \\mathbb {R} ^ {m \\times n}} \\quad \\mu \\| X \\| _ {*} + \\frac {1}{2} \\sum_ {(i, j) \\in \\Omega} (X _ {i j} - M _ {i j}) ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.789,
                0.707,
                0.806
            ],
            "angle": 0,
            "content": "其中 \\(M\\) 是想要恢复的低秩矩阵，但是只知道其在下标集 \\(\\Omega\\) 上的值。令"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.817,
                0.626,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nf (X) = \\frac {1}{2} \\sum_ {(i, j) \\in \\Omega} \\left(X _ {i j} - M _ {i j}\\right) ^ {2}, \\quad h (X) = \\mu \\| X \\| _ {*},\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "352"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "image",
            "bbox": [
                0.356,
                0.154,
                0.728,
                0.374
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.391,
                0.388,
                0.693,
                0.404
            ],
            "angle": 0,
            "content": "图8.1 近似点梯度法求解LASSO问题"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.43,
                0.414,
                0.445
            ],
            "angle": 0,
            "content": "定义矩阵 \\(P\\in \\mathbb{R}^{m\\times n}\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.46,
                0.445,
                0.623,
                0.496
            ],
            "angle": 0,
            "content": "\\[\nP _ {i j} = \\left\\{ \\begin{array}{l l} 1, & (i, j) \\in \\Omega , \\\\ 0, & \\text {其 他}, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.502,
                0.28,
                0.517
            ],
            "angle": 0,
            "content": "则"
        },
        {
            "type": "equation",
            "bbox": [
                0.417,
                0.515,
                0.625,
                0.547
            ],
            "angle": 0,
            "content": "\\[\nf (X) = \\frac {1}{2} \\| P \\odot (X - M) \\| _ {F} ^ {2},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.405,
                0.552,
                0.586,
                0.571
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (X) = P \\odot (X - M),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.575,
                0.707,
                0.595
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {p r o x} _ {t _ {k} h} (X) = U \\operatorname {D i a g} \\left(\\max  \\left\\{\\left| d \\right| - t _ {k} \\mu , 0 \\right\\}\\right) V ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.602,
                0.825,
                0.639
            ],
            "angle": 0,
            "content": "其中 \\(X = U\\mathrm{Diag}(d)V^{\\mathrm{T}}\\) 为矩阵 \\(X\\) 的约化的奇异值分解．近似点梯度法的迭代格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.449,
                0.653,
                0.654,
                0.672
            ],
            "angle": 0,
            "content": "\\[\nY ^ {k} = X ^ {k} - t _ {k} P \\odot (X ^ {k} - M),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.432,
                0.677,
                0.579,
                0.699
            ],
            "angle": 0,
            "content": "\\[\nX ^ {k + 1} = \\operatorname {p r o x} _ {t _ {k} h} (Y ^ {k}).\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.72,
                0.397,
                0.736
            ],
            "angle": 0,
            "content": "3. 小波模型求解"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.75,
                0.477,
                0.766
            ],
            "angle": 0,
            "content": "下面考虑小波分解模型："
        },
        {
            "type": "equation",
            "bbox": [
                0.407,
                0.774,
                0.825,
                0.806
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {u} \\| \\lambda \\odot (W u) \\| _ {1} + \\frac {1}{2} \\| A u - b \\| ^ {2}, \\tag {8.1.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.815,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(W \\in \\mathbb{R}^{m \\times n}\\) 是紧小波框架算子，即满足 \\(W^{\\mathrm{T}} W = I\\)，第二项为问题的损失函数。利用紧框架的性质，可以引入 \\(d = W u\\)，则 \\(u = W^{\\mathrm{T}} d\\)，可以使用近"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.317,
                0.133
            ],
            "angle": 0,
            "content": "8.1 近似点梯度法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "353"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.423,
                0.174
            ],
            "angle": 0,
            "content": "似点梯度法求解对应的合成模型："
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.186,
                0.737,
                0.217
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {d} \\quad \\| \\lambda \\odot d \\| _ {1} + \\frac {1}{2} \\| A W ^ {\\mathrm {T}} d - b \\| ^ {2}. \\tag {8.1.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.231,
                0.541,
                0.262
            ],
            "angle": 0,
            "content": "令 \\(f(d) = \\frac{1}{2}\\| AW^{\\mathrm{T}}d - b\\|^2\\) ， \\(h(d) = \\| \\lambda \\odot d\\| _1\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.279,
                0.543,
                0.297
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (d) = W A ^ {\\mathrm {T}} \\left(A W ^ {\\mathrm {T}} d - b\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.307,
                0.304,
                0.601,
                0.324
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {p r o x} _ {t _ {k} h} (d) = \\operatorname {s i g n} (d) \\max  \\left\\{\\left| d \\right| - t _ {k} \\lambda , 0 \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.342,
                0.51,
                0.358
            ],
            "angle": 0,
            "content": "近似点梯度算法可以由下面的迭代格式给出："
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.377,
                0.565,
                0.396
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = d ^ {k} - t _ {k} W A ^ {\\mathrm {T}} \\left(A W ^ {\\mathrm {T}} d ^ {k} - b\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.32,
                0.401,
                0.587,
                0.421
            ],
            "angle": 0,
            "content": "\\[\nd ^ {k + 1} = \\operatorname {s i g n} \\left(y ^ {k}\\right) \\max  \\left\\{\\left| y ^ {k} \\right| - t _ {k} \\lambda , 0 \\right\\}.\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.454,
                0.344,
                0.47
            ],
            "angle": 0,
            "content": "4. 平衡小波模型求解"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.487,
                0.737,
                0.524
            ],
            "angle": 0,
            "content": "平衡小波模型也是图像处理领域一个非常重要的模型，它可以写成如下形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.223,
                0.537,
                0.737,
                0.568
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {\\alpha} \\| \\lambda \\odot \\alpha \\| _ {1} + \\frac {\\kappa}{2} \\| (I - W W ^ {\\mathrm {T}}) \\alpha \\| ^ {2} + \\frac {1}{2} \\| A W ^ {\\mathrm {T}} \\alpha - b \\| ^ {2}. \\tag {8.1.8}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.583,
                0.738,
                0.62
            ],
            "angle": 0,
            "content": "这里并不要求 \\(W\\) 是紧框架，即不要求 \\(W^{\\mathrm{T}}W = I\\)。为了使用近似点梯度算法，令"
        },
        {
            "type": "equation",
            "bbox": [
                0.213,
                0.633,
                0.695,
                0.664
            ],
            "angle": 0,
            "content": "\\[\nf (\\alpha) = \\frac {\\kappa}{2} \\| (I - W W ^ {T}) \\alpha \\| ^ {2} + \\frac {1}{2} \\| A W ^ {T} \\alpha - b \\| ^ {2}, \\quad h (\\alpha) = \\| \\lambda \\odot \\alpha \\| _ {1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.677,
                0.194,
                0.693
            ],
            "angle": 0,
            "content": "则"
        },
        {
            "type": "equation",
            "bbox": [
                0.3,
                0.713,
                0.635,
                0.732
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (\\alpha) = \\kappa (I - W W ^ {\\mathrm {T}}) \\alpha + W A ^ {\\mathrm {T}} \\left(A W ^ {\\mathrm {T}} \\alpha - b\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.274,
                0.738,
                0.569,
                0.758
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {p r o x} _ {t _ {k} h} (\\alpha) = \\operatorname {s i g n} (\\alpha) \\max  \\left\\{\\left| \\alpha \\right| - t _ {k} \\lambda , 0 \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.775,
                0.51,
                0.792
            ],
            "angle": 0,
            "content": "近似点梯度算法可以由下面的迭代格式给出："
        },
        {
            "type": "equation",
            "bbox": [
                0.272,
                0.81,
                0.65,
                0.83
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = \\alpha^ {k} - t _ {k} \\left(\\kappa \\left(I - W W ^ {\\mathrm {T}}\\right) \\alpha^ {k} + W A ^ {\\mathrm {T}} \\left(A W ^ {\\mathrm {T}} \\alpha^ {k} - b\\right)\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.258,
                0.835,
                0.52,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\alpha^ {k + 1} = \\operatorname {s i g n} \\left(y ^ {k}\\right) \\max  \\left\\{\\left| y ^ {k} \\right| - t _ {k} \\lambda , 0 \\right\\}.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "354"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.42,
                0.174
            ],
            "angle": 0,
            "content": "8.1.4 收敛性分析"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.187,
                0.825,
                0.225
            ],
            "angle": 0,
            "content": "本小节介绍近似点梯度算法的收敛性。在提出近似点梯度算法时，我们仅仅要求 \\( f(x) \\) 为可微函数，但在收敛性分析时则要求 \\( f(x) \\) 也为凸函数。"
        },
        {
            "type": "title",
            "bbox": [
                0.292,
                0.235,
                0.357,
                0.25
            ],
            "angle": 0,
            "content": "假设8.1"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.261,
                0.823,
                0.299
            ],
            "angle": 0,
            "content": "(1) \\(f\\) 在其定义域 \\(\\mathbf{dom} f = \\mathbb{R}^n\\) 内为凸的；\\(\\nabla f\\) 在常数 \\(L\\) 意义下利普希茨连续，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.414,
                0.302,
                0.712,
                0.322
            ],
            "angle": 0,
            "content": "\\[\n\\| \\nabla f (x) - \\nabla f (y) \\| \\leqslant L \\| x - y \\|, \\quad \\forall x, y;\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.332,
                0.697,
                0.351
            ],
            "angle": 0,
            "content": "(2) \\(h\\) 是适当的闭凸函数（因此 \\(\\operatorname{prox}_{th}\\) 的定义是合理的）；"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.361,
                0.823,
                0.398
            ],
            "angle": 0,
            "content": "(3) 函数 \\(\\psi(x) = f(x) + h(x)\\) 的最小值 \\(\\psi^{*}\\) 是有限的，并且在点 \\(x^{*}\\) 处可以取到（并不要求唯一）."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.403,
                0.822,
                0.469
            ],
            "angle": 0,
            "content": "以上的条件可以保证近似点梯度法的收敛结果: 在定步长 \\(t_k \\in \\left(0, \\frac{1}{L}\\right]\\) 的情况下, 迭代点 \\(x^k\\) 处的函数值 \\(\\psi(x^k)\\) 以 \\(\\mathcal{O}\\left(\\frac{1}{k}\\right)\\) 的速率收敛到 \\(\\psi^*\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.47,
                0.679,
                0.485
            ],
            "angle": 0,
            "content": "在正式给出收敛定理之前，我们先引入一个新函数"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.496,
                0.823,
                0.531
            ],
            "angle": 0,
            "content": "定义8.2（梯度映射）设 \\(f(x)\\) 和 \\(h(x)\\) 满足假设8.1，\\(t > 0\\) 为正常数，定义梯度映射为"
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.53,
                0.824,
                0.56
            ],
            "angle": 0,
            "content": "\\[\nG _ {t} (x) = \\frac {1}{t} \\left(x - \\operatorname {p r o x} _ {t h} \\left(x - t \\nabla f (x)\\right)\\right). \\tag {8.1.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.564,
                0.824,
                0.601
            ],
            "angle": 0,
            "content": "通过计算可以发现 \\( G_{t}(x) \\) 为近似点梯度法每次迭代中的负的“搜索方向”，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.604,
                0.707,
                0.623
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {t h} \\left(x ^ {k} - t \\nabla f \\left(x ^ {k}\\right)\\right) = x ^ {k} - t G _ {t} \\left(x ^ {k}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.631,
                0.824,
                0.668
            ],
            "angle": 0,
            "content": "这里需要注意的是，\\(G_{t}(x)\\) 并不是 \\(\\psi = f + h\\) 的梯度或者次梯度，而由之前邻近算子与次梯度的关系可以得出"
        },
        {
            "type": "equation",
            "bbox": [
                0.415,
                0.68,
                0.824,
                0.698
            ],
            "angle": 0,
            "content": "\\[\nG _ {t} (x) - \\nabla f (x) \\in \\partial h (x - t G _ {t} (x)). \\tag {8.1.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.708,
                0.825,
                0.746
            ],
            "angle": 0,
            "content": "此外，\\(G_{t}(x)\\) 作为“搜索方向”，与算法的收敛性有很强的关系：\\(G_{t}(x) = 0\\) 当且仅当 \\(x\\) 为 \\(\\psi(x) = f(x) + h(x)\\) 的最小值点."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.75,
                0.679,
                0.767
            ],
            "angle": 0,
            "content": "有了上面的铺垫，我们介绍近似点梯度法的收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.772,
                0.825,
                0.82
            ],
            "angle": 0,
            "content": "定理8.3 在假设8.1下，取定步长为 \\(t_k = t \\in \\left(0, \\frac{1}{L}\\right]\\)，设 \\(\\{x^k\\}\\) 是由迭代格式(8.1.3)产生的序列，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.826,
                0.824,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\psi \\left(x ^ {k}\\right) - \\psi^ {*} \\leqslant \\frac {1}{2 k t} \\| x ^ {0} - x ^ {*} \\| ^ {2}. \\tag {8.1.11}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.1 近似点梯度法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "355"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.194
            ],
            "angle": 0,
            "content": "证明. 利用假设8.1中的利普希茨连续的性质，根据二次上界(2.2.3)可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.236,
                0.205,
                0.67,
                0.236
            ],
            "angle": 0,
            "content": "\\[\nf (y) \\leqslant f (x) + \\nabla f (x) ^ {\\mathrm {T}} (y - x) + \\frac {L}{2} \\| y - x \\| ^ {2}, \\quad \\forall x, y \\in \\mathbb {R} ^ {n}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.247,
                0.347,
                0.265
            ],
            "angle": 0,
            "content": "令 \\(y = x - tG_{t}(x)\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.247,
                0.277,
                0.66,
                0.31
            ],
            "angle": 0,
            "content": "\\[\nf (x - t G _ {t} (x)) \\leqslant f (x) - t \\nabla f (x) ^ {\\mathrm {T}} G _ {t} (x) + \\frac {t ^ {2} L}{2} \\| G _ {t} (x) \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.322,
                0.297,
                0.353
            ],
            "angle": 0,
            "content": "若 \\(0 < t \\leqslant \\frac{1}{L}\\), 则"
        },
        {
            "type": "equation",
            "bbox": [
                0.228,
                0.364,
                0.738,
                0.395
            ],
            "angle": 0,
            "content": "\\[\nf (x - t G _ {t} (x)) \\leqslant f (x) - t \\nabla f (x) ^ {\\mathrm {T}} G _ {t} (x) + \\frac {t}{2} \\| G _ {t} (x) \\| ^ {2}. \\tag {8.1.12}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.407,
                0.604,
                0.425
            ],
            "angle": 0,
            "content": "此外，由 \\(f(x),h(x)\\) 为凸函数，对任意 \\(z\\in \\mathbf{dom}\\psi\\) 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.231,
                0.439,
                0.676,
                0.459
            ],
            "angle": 0,
            "content": "\\[\nh (z) \\geqslant h (x - t G _ {t} (x)) + \\left(G _ {t} (x) - \\nabla f (x)\\right) ^ {\\mathrm {T}} (z - x + t G _ {t} (x)),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.233,
                0.464,
                0.463,
                0.483
            ],
            "angle": 0,
            "content": "\\[\nf (z) \\geqslant f (x) + \\nabla f (x) ^ {\\mathrm {T}} (z - x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.497,
                0.58,
                0.514
            ],
            "angle": 0,
            "content": "其中关于 \\(h(z)\\) 的不等式利用了关系式(8.1.10). 整理得"
        },
        {
            "type": "equation",
            "bbox": [
                0.204,
                0.53,
                0.738,
                0.55
            ],
            "angle": 0,
            "content": "\\[\nh \\left(x - t G _ {t} (x)\\right) \\leqslant h (z) - \\left(G _ {t} (x) - \\nabla f (x)\\right) ^ {\\mathrm {T}} (z - x + t G _ {t} (x)), \\tag {8.1.13}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.273,
                0.555,
                0.738,
                0.575
            ],
            "angle": 0,
            "content": "\\[\nf (x) \\leqslant f (z) - \\nabla f (x) ^ {\\mathrm {T}} (z - x). \\tag {8.1.14}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.592,
                0.56,
                0.609
            ],
            "angle": 0,
            "content": "将(8.1.12)—(8.1.14)式相加可得对任意 \\(z \\in \\mathbf{dom} \\psi\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.229,
                0.621,
                0.738,
                0.652
            ],
            "angle": 0,
            "content": "\\[\n\\psi (x - t G _ {t} (x)) \\leqslant \\psi (z) + G _ {t} (x) ^ {\\mathrm {T}} (x - z) - \\frac {t}{2} \\| G _ {t} (x) \\| ^ {2}. \\tag {8.1.15}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.663,
                0.371,
                0.68
            ],
            "angle": 0,
            "content": "因此，对于每一步的迭代，"
        },
        {
            "type": "equation",
            "bbox": [
                0.393,
                0.698,
                0.515,
                0.715
            ],
            "angle": 0,
            "content": "\\[\n\\tilde {x} = x - t G _ {t} (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.733,
                0.466,
                0.75
            ],
            "angle": 0,
            "content": "在全局不等式 (8.1.15) 中，取 \\(z = x^{*}\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.241,
                0.761,
                0.738,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\psi (\\tilde {x}) - \\psi^ {*} \\leqslant G _ {t} (x) ^ {\\mathrm {T}} (x - x ^ {*}) - \\frac {t}{2} \\| G _ {t} (x) \\| ^ {2} \\\\ = \\frac {1}{2 t} \\left(\\| x - x ^ {*} \\| ^ {2} - \\| x - x ^ {*} - t G _ {t} (x) \\| ^ {2}\\right) \\tag {8.1.16} \\\\ = \\frac {1}{2 t} \\left(\\| x - x ^ {*} \\| ^ {2} - \\| \\tilde {x} - x ^ {*} \\| ^ {2}\\right). \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "356"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.155,
                0.824,
                0.199
            ],
            "angle": 0,
            "content": "分别取 \\(x = x^{i - 1},\\tilde{x} = x^i,t = t_i = \\frac{1}{L},i = 1,2,\\dots ,k\\) ，代入不等式(8.1.16)并累加，"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.199,
                0.732,
                0.303
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\sum_ {i = 1} ^ {k} \\left(\\psi \\left(x ^ {i}\\right) - \\psi^ {*}\\right) \\leqslant \\frac {1}{2 t} \\sum_ {i = 1} ^ {k} \\left(\\| x ^ {i - 1} - x ^ {*} \\| ^ {2} - \\| x ^ {i} - x ^ {*} \\| ^ {2}\\right) \\\\ = \\frac {1}{2 t} \\left(\\| x ^ {0} - x ^ {*} \\| ^ {2} - \\| x ^ {k} - x ^ {*} \\| ^ {2}\\right) \\\\ \\leqslant \\frac {1}{2 t} \\| x ^ {0} - x ^ {*} \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.307,
                0.724,
                0.324
            ],
            "angle": 0,
            "content": "注意到在不等式 (8.1.15) 中，取 \\(z = x\\) 即可得到算法为下降法："
        },
        {
            "type": "equation",
            "bbox": [
                0.44,
                0.332,
                0.644,
                0.362
            ],
            "angle": 0,
            "content": "\\[\n\\psi (\\tilde {x}) \\leqslant \\psi (x) - \\frac {t}{2} \\| G _ {t} (x) \\| ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.372,
                0.452,
                0.389
            ],
            "angle": 0,
            "content": "即 \\(\\psi (x^i)\\) 为非增的，因此"
        },
        {
            "type": "equation",
            "bbox": [
                0.357,
                0.399,
                0.726,
                0.438
            ],
            "angle": 0,
            "content": "\\[\n\\psi \\left(x ^ {k}\\right) - \\psi^ {*} \\leqslant \\frac {1}{k} \\sum_ {i = 1} ^ {k} \\left(\\psi \\left(x ^ {i}\\right) - \\psi^ {*}\\right) \\leqslant \\frac {1}{2 k t} \\| x ^ {0} - x ^ {*} \\| ^ {2}.\n\\]"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.411,
                0.824,
                0.424
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.447,
                0.825,
                0.528
            ],
            "angle": 0,
            "content": "在定理8.3中，收敛性要求步长小于或等于 \\(\\nabla f\\) 对应的利普希茨常数 \\(L\\) 的倒数．但是在实际应用中，我们经常很难知道 \\(L\\) ，因此可以考虑线搜索的技巧．注意到之所以需要 \\(t\\leqslant \\frac{1}{L}\\) 的条件是因为不等式(8.1.12)，所以线搜索策略可以是从某个 \\(t = \\hat{t} >0\\) 开始进行回溯 \\((t\\gets \\beta t)\\) ，直到满足不等式"
        },
        {
            "type": "equation",
            "bbox": [
                0.316,
                0.538,
                0.825,
                0.568
            ],
            "angle": 0,
            "content": "\\[\nf (x - t G _ {t} (x)) \\leqslant f (x) - t \\nabla f (x) ^ {\\mathrm {T}} G _ {t} (x) + \\frac {t}{2} \\| G _ {t} (x) \\| ^ {2}. \\tag {8.1.17}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.576,
                0.824,
                0.613
            ],
            "angle": 0,
            "content": "注意，(8.1.17)式与前面给出的线搜索准则(8.1.4)是等价的，这也说明了准则(8.1.4)的合理性。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.618,
                0.605,
                0.634
            ],
            "angle": 0,
            "content": "类似于定理8.3，我们有如下收敛性定理："
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.647,
                0.824,
                0.685
            ],
            "angle": 0,
            "content": "定理8.4在假设8.1下，从某个 \\(t = \\hat{t} >0\\) 开始进行回溯（ \\(t\\gets \\beta t\\) ）直到满足不等式(8.1.17)，设 \\(\\{x^k\\}\\) 是由迭代格式(8.1.3)产生的序列，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.385,
                0.694,
                0.697,
                0.731
            ],
            "angle": 0,
            "content": "\\[\n\\psi (x ^ {k}) - \\psi^ {*} \\leqslant \\frac {1}{2 k \\min \\left\\{\\hat {t} , \\beta / L \\right\\}} \\| x ^ {0} - x ^ {*} \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.74,
                0.825,
                0.815
            ],
            "angle": 0,
            "content": "证明．由定理8.3的证明过程，当 \\(0 < t \\leqslant \\frac{1}{L}\\) 时，不等式(8.1.17)成立，因此由线搜索所得的步长 \\(t\\) 应该满足 \\(t \\geqslant t_{\\min} = \\min \\{\\hat{t}, \\frac{\\beta}{L}\\}\\). 利用和定理8.3同样的证明方法，我们有 \\(\\psi(x^i) < \\psi(x^{i-1})\\)，且"
        },
        {
            "type": "equation",
            "bbox": [
                0.361,
                0.824,
                0.721,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\psi \\left(x ^ {i}\\right) - \\psi^ {*} \\leqslant \\frac {1}{2 t _ {\\min }} \\left(\\| x ^ {i - 1} - x ^ {*} \\| ^ {2} - \\| x ^ {i} - x ^ {*} \\| ^ {2}\\right).\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.1 近似点梯度法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "357"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.737,
                0.174
            ],
            "angle": 0,
            "content": "从 \\(i = 1\\) 到 \\(i = k\\) 累加所有的不等式，并利用 \\(\\psi(x^i)\\) 是非增的，可得上界估计"
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.186,
                0.574,
                0.219
            ],
            "angle": 0,
            "content": "\\[\n\\psi (x ^ {k}) - \\psi^ {*} \\leqslant \\frac {1}{2 k t _ {\\operatorname* {m i n}}} \\| x ^ {0} - x ^ {*} \\| ^ {2}.\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.24,
                0.557,
                0.258
            ],
            "angle": 0,
            "content": "*8.1.5 非凸函数的邻近算子与近似点梯度法"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.272,
                0.737,
                0.309
            ],
            "angle": 0,
            "content": "作为本节的拓展，我们简单介绍对一般非凸函数如何定义邻近算子以及它的一些简单性质。同时将给出在非凸情况下近似点梯度法的结构。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.314,
                0.737,
                0.392
            ],
            "angle": 0,
            "content": "根据凸函数邻近算子的定义(8.1.1)，形式上邻近算子是计算一个函数的最小值点．我们引入凸性是为了保证最小值点存在唯一，而且方便计算．当 \\(h\\) 为非凸函数时，唯一性一般不能保证，但是至少可以保证最小值点存在因此对非凸函数定义邻近算子是可行的."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.397,
                0.523,
                0.413
            ],
            "angle": 0,
            "content": "本节对有下界的适当闭函数定义邻近算子"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.418,
                0.737,
                0.461
            ],
            "angle": 0,
            "content": "定义8.3（适当闭函数的邻近算子）设 \\(h\\) 是适当闭函数，且具有有限的下界，即满足 \\(\\inf_{x\\in \\mathbf{dom}h}h(x) > - \\infty\\) ，定义 \\(h\\) 的邻近算子为"
        },
        {
            "type": "equation",
            "bbox": [
                0.293,
                0.474,
                0.612,
                0.51
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {p r o x} _ {h} (x) = \\underset {u \\in \\mathbf {d o m} h} {\\arg \\min } \\left\\{h (u) + \\frac {1}{2} \\| u - x \\| ^ {2} \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.523,
                0.737,
                0.581
            ],
            "angle": 0,
            "content": "这里和凸函数情形不同的是，非凸函数 \\(h\\) 的邻近算子是一个集合函数，即 \\(\\operatorname{prox}_h(x)\\) 是一个集合。在凸函数的情形下，由于解的存在唯一性，\\(\\operatorname{prox}_h(x)\\) 只包含一个点。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.586,
                0.737,
                0.625
            ],
            "angle": 0,
            "content": "我们定义了非凸函数 \\(h\\) 的邻近算子 \\(\\mathrm{prox}_h\\)，一个自然的问题是，\\(\\mathrm{prox}_h\\) 是否是良定义的？对适当闭函数，可以证明 \\(\\mathrm{prox}_h\\) 是良定义的。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.639,
                0.737,
                0.678
            ],
            "angle": 0,
            "content": "命题8.1 设 \\(h\\) 是适当闭函数且 \\(\\inf_{x\\in \\mathbf{dom}h}h(x) > -\\infty\\)，则对任意的 \\(x\\in \\mathbf{dom}h\\)，\\(\\operatorname{prox}_h(x)\\) 是 \\(\\mathbb{R}^n\\) 上的非空紧集。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.689,
                0.737,
                0.749
            ],
            "angle": 0,
            "content": "证明. 定义 \\(g(u) = h(u) + \\frac{1}{2} \\| u - x\\|^2\\)，设 \\(\\inf_{x \\in \\mathbf{dom}h} h(x) = l\\)。取 \\(u_0 \\in \\mathbf{dom}h\\) 由于二次函数 \\(\\frac{1}{2}\\|u - x\\|^2\\) 无上界，因此存在 \\(R > 0\\)，使得当 \\(\\|u - x\\| > R\\) 时，"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.76,
                0.543,
                0.791
            ],
            "angle": 0,
            "content": "\\[\n\\frac {1}{2} \\| u - x \\| ^ {2} > g (u _ {0}) - l,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.802,
                0.48,
                0.819
            ],
            "angle": 0,
            "content": "即对任意满足 \\(\\| u - x\\| >R\\) 的 \\(u\\) ，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.399,
                0.836,
                0.506,
                0.854
            ],
            "angle": 0,
            "content": "\\[\ng (u) > g \\left(u _ {0}\\right).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "358"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.156,
                0.827,
                0.216
            ],
            "angle": 0,
            "content": "这说明下水平集 \\(\\{u \\mid g(u) \\leqslant g(u_0)\\}\\) 含于球 \\(\\|u - x\\| \\leqslant R\\) 内, 即 \\(g\\) 有一个非空有界下水平集. 显然 \\(g(u)\\) 是闭函数, 由 Weierstrass 定理 (定理5.1) 可知, \\(g(u)\\) 的最小值点集合 \\(\\operatorname{prox}_h(x)\\) 是非空紧集."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.228,
                0.825,
                0.266
            ],
            "angle": 0,
            "content": "以上命题说明，对于适当闭函数 \\(h\\) ，总是可以定义它的邻近算子 \\(\\mathrm{prox}_h\\) 尽管在实际使用的时候我们往往只选择 \\(\\mathrm{prox}_h\\) 中的一个元素."
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.27,
                0.807,
                0.287
            ],
            "angle": 0,
            "content": "下面简要介绍 \\(\\mathrm{prox}_h\\) 的性质. 我们知道, 对凸函数 \\(h\\), 有最优性条件"
        },
        {
            "type": "equation",
            "bbox": [
                0.422,
                0.3,
                0.659,
                0.318
            ],
            "angle": 0,
            "content": "\\[\nu = \\operatorname {p r o x} _ {h} (x) \\Leftrightarrow x - u \\in \\partial h (u).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.33,
                0.805,
                0.346
            ],
            "angle": 0,
            "content": "这实际上是定理8.2的结果。对于适当闭函数 \\(h\\)，是否也有类似的性质呢？"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.35,
                0.825,
                0.408
            ],
            "angle": 0,
            "content": "由于 \\(h\\) 不是凸函数，所以在一般情况下 \\(0 \\in \\partial h(x^{*})\\) 不能保证 \\(x^{*}\\) 是局部极小点。通常将所有满足 \\(0 \\in \\partial h(x)\\) 的点称为 \\(f(x)\\) 的临界点（或稳定点），这和讨论光滑函数的情形是一致的。根据一阶必要条件容易得出下面的结论："
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.42,
                0.722,
                0.436
            ],
            "angle": 0,
            "content": "推论8.1 设 \\(h\\) 是适当闭函数且有下界，\\(u \\in \\mathrm{prox}_h(x)\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.484,
                0.45,
                0.597,
                0.466
            ],
            "angle": 0,
            "content": "\\[\nx - u \\in \\partial h (u).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.475,
                0.805,
                0.505
            ],
            "angle": 0,
            "content": "证明. 容易计算出 \\(g(v) = h(v) + \\frac{1}{2} \\| v - x\\|^2\\) 的次微分（见定义5.3）为"
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.512,
                0.638,
                0.529
            ],
            "angle": 0,
            "content": "\\[\n\\partial g (v) = \\partial h (v) + \\{v - x \\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.542,
                0.754,
                0.558
            ],
            "angle": 0,
            "content": "其中“+”表示集合间的加法．根据 \\(u\\) 的定义以及定理5.7我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.427,
                0.572,
                0.655,
                0.589
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial g (u) = \\partial h (u) + \\{u - x \\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.602,
                0.367,
                0.617
            ],
            "angle": 0,
            "content": "而这又等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.485,
                0.623,
                0.597,
                0.639
            ],
            "angle": 0,
            "content": "\\[\nx - u \\in \\partial h (u).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.652,
                0.825,
                0.71
            ],
            "angle": 0,
            "content": "此推论说明在非凸情形下也能得到类似定理8.2的性质。这在分析非凸问题算法收敛性时有很大帮助。我们在以后介绍分块坐标下降法收敛性的时候会进一步说明这些性质是如何应用在算法分析中的。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.715,
                0.825,
                0.752
            ],
            "angle": 0,
            "content": "在本小节的最后，我们引入在非凸情形下的近似点梯度法。为此仍然考虑复合优化问题(8.1.2)："
        },
        {
            "type": "equation",
            "bbox": [
                0.438,
                0.766,
                0.644,
                0.783
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad \\psi (x) = f (x) + h (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.825,
                0.831
            ],
            "angle": 0,
            "content": "在这里 \\(f(x)\\) 是可微函数, \\(h(x)\\) 为适当闭函数 (不一定为凸), 则可以写出近似点梯度法为"
        },
        {
            "type": "equation",
            "bbox": [
                0.425,
                0.835,
                0.825,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} \\in \\operatorname {p r o x} _ {t _ {k} h} \\left(x ^ {k} - t _ {k} \\nabla f \\left(x ^ {k}\\right)\\right), \\tag {8.1.18}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.38,
                0.133
            ],
            "angle": 0,
            "content": "8.2 NESTEROV 加速算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "359"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.236
            ],
            "angle": 0,
            "content": "其中 \\(t_{k} > 0\\) 为步长, 可以取为固定值或由线搜索算法得出. 由于非凸函数的邻近算子没有唯一性, 在迭代时往往选取 \\(\\operatorname{prox}_{t_{k} h}\\) 中的一个元素. 此时的近似点梯度算法也具有收敛性, 其收敛性实际上是分块坐标下降法收敛性的特殊情形. 具体内容将在第 8.4 节中介绍."
        },
        {
            "type": "title",
            "bbox": [
                0.32,
                0.272,
                0.587,
                0.294
            ],
            "angle": 0,
            "content": "8.2 Nesterov 加速算法"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.313,
                0.744,
                0.592
            ],
            "angle": 0,
            "content": "上一节分析了近似点梯度算法的收敛速度：如果光滑部分的梯度是利普希茨连续的，则目标函数的收敛速度可以达到 \\(\\mathcal{O}\\left(\\frac{1}{k}\\right)\\)。一个自然的问题是如果仅用梯度信息，我们能不能取得更快的收敛速度。Nesterov 分别在 1983 年、1988 年和 2005 年提出了三种改进的一阶算法，收敛速度能达到 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\)。实际上，这三种算法都可以应用到近似点梯度算法上。在 Nesterov 加速算法刚提出的时候，由于牛顿算法有更快的收敛速度，Nesterov 加速算法在当时并没有引起太多的关注。但近年来，随着数据量的增大，牛顿型方法由于其过大的计算复杂度，不便于有效地应用到实际中，Nesterov 加速算法作为一种快速的一阶算法重新被挖掘出来并迅速流行起来。Beck 和 Teboulle 就在 2008 年给出了 Nesterov 在 1983 年提出的算法的近似点梯度法版本——FISTA。本节将对这些加速方法做一定的介绍和总结，主要讨论凸函数的加速算法，并给出相应的例子和收敛性证明。作为补充，我们也将简单介绍非凸问题上的加速算法。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.621,
                0.336,
                0.64
            ],
            "angle": 0,
            "content": "8.2.1 FISTA算法"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.655,
                0.39,
                0.672
            ],
            "angle": 0,
            "content": "考虑如下复合优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.35,
                0.689,
                0.737,
                0.713
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\psi (x) = f (x) + h (x), \\tag {8.2.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.725,
                0.739,
                0.832
            ],
            "angle": 0,
            "content": "其中 \\(f(x)\\) 是连续可微的凸函数且梯度是利普希茨连续的（利普希茨常数是 \\(L\\)），\\(h(x)\\) 是适当的闭凸函数。优化问题(8.2.1)由光滑部分 \\(f(x)\\) 和非光滑部分 \\(h(x)\\) 组成，可以使用近似点梯度法来求解这一问题，但是其收敛速度只有 \\(\\mathcal{O}\\left(\\frac{1}{k}\\right)\\)。很自然地，我们希望能够加速近似点梯度算法，这就是本小节要介绍的FISTA算法。"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.836,
                0.741,
                0.853
            ],
            "angle": 0,
            "content": "FISTA算法由两步组成：第一步沿着前两步的计算方向计算一个新点，"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "360"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.607,
                0.174
            ],
            "angle": 0,
            "content": "第二步在该新点处做一步近似点梯度迭代，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.182,
                0.672,
                0.214
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = x ^ {k - 1} + \\frac {k - 2}{k + 1} \\left(x ^ {k - 1} - x ^ {k - 2}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.41,
                0.217,
                0.65,
                0.238
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k} = \\operatorname {p r o x} _ {t _ {k} h} \\left(y ^ {k} - t _ {k} \\nabla f \\left(y ^ {k}\\right)\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.251,
                0.825,
                0.267
            ],
            "angle": 0,
            "content": "图8.2给出FISTA算法的迭代序列图。可以看到这一做法对每一步迭代的计"
        },
        {
            "type": "image",
            "bbox": [
                0.362,
                0.294,
                0.721,
                0.398
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.449,
                0.427,
                0.634,
                0.443
            ],
            "angle": 0,
            "content": "图8.2 FISTA一次迭代"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.465,
                0.825,
                0.53
            ],
            "angle": 0,
            "content": "算量几乎没有影响，而带来的效果是显著的。如果选取 \\( t_k \\) 为固定的步长并小于或等于 \\(\\frac{1}{L}\\)，其收敛速度达到了 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\)，我们将在收敛性分析中给出推导过程。完整的FISTA算法见算法8.2。"
        },
        {
            "type": "title",
            "bbox": [
                0.261,
                0.544,
                0.373,
                0.559
            ],
            "angle": 0,
            "content": "算法8.2FISTA"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.565,
                0.514,
                0.582
            ],
            "angle": 0,
            "content": "1. 输入： \\(x^{0} = x^{-1}\\in \\mathbb{R}^{n}\\) ， \\(k\\gets 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.587,
                0.487,
                0.602
            ],
            "angle": 0,
            "content": "2. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.602,
                0.581,
                0.632
            ],
            "angle": 0,
            "content": "3. 计算 \\( y^{k} = x^{k - 1} + \\frac{k - 2}{k + 1} (x^{k - 1} - x^{k - 2}) \\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.632,
                0.726,
                0.665
            ],
            "angle": 0,
            "content": "4. 选取 \\(t_k = t \\in \\left(0, \\frac{1}{L}\\right]\\), 计算 \\(x^k = \\operatorname{prox}_{t_k h}(y^k - t_k \\nabla f(y^k))\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.664,
                0.384,
                0.678
            ],
            "angle": 0,
            "content": "5. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.685,
                0.371,
                0.699
            ],
            "angle": 0,
            "content": "6. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.565,
                0.726,
                0.699
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.722,
                0.825,
                0.803
            ],
            "angle": 0,
            "content": "为了对算法做更好的推广, 可以给出FISTA算法的一个等价变形, 只是把原来算法中的第一步拆成两步迭代, 相应算法见算法8.3. 当 \\(\\gamma_{k} = \\frac{2}{k + 1}\\) 时, 并且取固定步长时, 两个算法是等价的. 但是当 \\(\\gamma_{k}\\) 采用别的取法时, 算法8.3将给出另一个版本的加速算法."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.807,
                0.825,
                0.856
            ],
            "angle": 0,
            "content": "对于该算法框架，我们需要确定如何选取步长 \\(t_k\\) 和 \\(\\gamma_k\\) ，这决定了算法的收敛速度．首先给出算法 8.3 以 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\) 的速度收敛的条件（具体的证明"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.38,
                0.133
            ],
            "angle": 0,
            "content": "8.2 NESTEROV 加速算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "361"
        },
        {
            "type": "title",
            "bbox": [
                0.172,
                0.158,
                0.411,
                0.175
            ],
            "angle": 0,
            "content": "算法8.3FISTA算法的等价变形"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.18,
                0.415,
                0.196
            ],
            "angle": 0,
            "content": "1. 输入： \\(v_{0} = x_{0}\\in \\mathbb{R}^{n}\\) ， \\(k\\gets 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.2,
                0.399,
                0.216
            ],
            "angle": 0,
            "content": "2. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.219,
                0.462,
                0.238
            ],
            "angle": 0,
            "content": "3. 计算 \\( y^{k} = (1 - \\gamma_{k})x^{k - 1} + \\gamma_{k}v^{k - 1} \\)"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.24,
                0.538,
                0.26
            ],
            "angle": 0,
            "content": "4. 选取 \\(t_k\\)，计算 \\(x^k = \\operatorname{prox}_{t_k h}(y^k - t_k \\nabla f(y^k))\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.26,
                0.456,
                0.292
            ],
            "angle": 0,
            "content": "5. 计算 \\( v^{k} = x^{k - 1} + \\frac{1}{\\gamma_{k}} (x^{k} - x^{k - 1}) \\)."
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.292,
                0.297,
                0.305
            ],
            "angle": 0,
            "content": "6. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.311,
                0.283,
                0.325
            ],
            "angle": 0,
            "content": "7. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.183,
                0.18,
                0.538,
                0.325
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.353,
                0.295,
                0.369
            ],
            "angle": 0,
            "content": "将在后面给出)："
        },
        {
            "type": "equation",
            "bbox": [
                0.261,
                0.378,
                0.737,
                0.411
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k}\\right) \\leqslant f \\left(y ^ {k}\\right) + \\left\\langle \\nabla f \\left(y ^ {k}\\right), x ^ {k} - y ^ {k} \\right\\rangle + \\frac {1}{2 t _ {k}} \\| x ^ {k} - y ^ {k} \\| _ {2} ^ {2}, \\tag {8.2.2}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.263,
                0.413,
                0.737,
                0.448
            ],
            "angle": 0,
            "content": "\\[\n\\gamma_ {1} = 1, \\quad \\frac {\\left(1 - \\gamma_ {i}\\right) t _ {i}}{\\gamma_ {i} ^ {2}} \\leqslant \\frac {t _ {i - 1}}{\\gamma_ {i - 1} ^ {2}}, \\quad i > 1, \\tag {8.2.3}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.264,
                0.451,
                0.737,
                0.487
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\gamma_ {k} ^ {2}}{t _ {k}} = \\mathcal {O} \\left(\\frac {1}{k ^ {2}}\\right). \\tag {8.2.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.496,
                0.737,
                0.54
            ],
            "angle": 0,
            "content": "可以看到当取 \\(t_k = \\frac{1}{L}\\)，\\(\\gamma_k = \\frac{2}{k + 1}\\) 时，以上条件满足。而且 \\(\\gamma_k\\) 的选取并不唯一，例如我们可以采取"
        },
        {
            "type": "equation",
            "bbox": [
                0.319,
                0.55,
                0.585,
                0.594
            ],
            "angle": 0,
            "content": "\\[\n\\gamma_ {1} = 1 \\quad \\frac {1}{\\gamma_ {k}} = \\frac {1}{2} \\left(1 + \\sqrt {1 + \\frac {4}{\\gamma_ {k - 1}}}\\right)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.603,
                0.645,
                0.638
            ],
            "angle": 0,
            "content": "来得到序列 \\(\\{\\gamma_k\\}\\), 这样导出的算法的收敛速度依然是 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.639,
                0.737,
                0.77
            ],
            "angle": 0,
            "content": "在算法 8.2 和算法 8.3 中都要求步长满足 \\(t_k \\leqslant \\frac{1}{L}\\), 此时条件 (8.2.2) 满足. 然而, 对绝大多数问题我们不知道函数 \\(\\nabla f\\) 的利普希茨常数. 为了在这种情况下条件 (8.2.2) 依然能满足, 需要使用线搜索来确定合适的 \\(t_k\\), 同时选取 \\(\\gamma_k\\)使得条件 (8.2.3) 和条件 (8.2.4) 同时满足, 从而使得算法达到 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\) 的收敛速度. 下面给出两个线搜索算法, 在执行它们时条件 (8.2.2)-(8.2.4) 同时得到满足, 进而可以得到相同收敛速度的结合线搜索的 FISTA 算法."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "第一种方法比较直观，类似于近似点梯度算法，它是在算法8.3的第4行中加入线搜索，并取 \\(\\gamma_{k} = \\frac{2}{k + 1}\\)，以回溯的方式找到满足条件(8.2.2)的 \\(t_k\\) 即可。每一次的起始步长取为前一步的步长 \\(t_{k - 1}\\)，通过不断指数式地减小步长 \\(t_k\\) 来使得条件(8.2.2)得到满足。注意，当 \\(t_k\\) 足够小时，条件(8.2.2)是一定"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "362"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.156,
                0.825,
                0.195
            ],
            "angle": 0,
            "content": "会得到满足的, 因此不会出现线搜索无法终止的情况。容易验证其他两个条件(8.2.3)(8.2.4) 在迭代过程中也得到满足。该算法的具体过程见算法 8.4。"
        },
        {
            "type": "title",
            "bbox": [
                0.261,
                0.222,
                0.426,
                0.238
            ],
            "angle": 0,
            "content": "算法8.4线搜索算法1"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.243,
                0.717,
                0.262
            ],
            "angle": 0,
            "content": "1. 输入： \\( t_k = t_{k-1} > 0 \\)，\\( \\rho < 1 \\)。参照点 \\( y^k \\) 及其梯度 \\( \\nabla f(y^k) \\)"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.264,
                0.541,
                0.283
            ],
            "angle": 0,
            "content": "2. 计算 \\(x^{k} = \\mathrm{prox}_{t_{k}h}(y^{k} - t_{k}\\nabla f(y^{k}))\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.285,
                0.56,
                0.303
            ],
            "angle": 0,
            "content": "3. while条件(8.2.2)对 \\(x^{k},y^{k}\\) 不满足do"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.308,
                0.373,
                0.323
            ],
            "angle": 0,
            "content": "4. \\(t_k\\gets \\rho t_k\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.326,
                0.593,
                0.346
            ],
            "angle": 0,
            "content": "5. 重新计算 \\(x^{k} = \\mathrm{prox}_{t_{k}h}(y^{k} - t_{k}\\nabla f(y^{k}))\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.349,
                0.371,
                0.362
            ],
            "angle": 0,
            "content": "6. end while"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.368,
                0.487,
                0.385
            ],
            "angle": 0,
            "content": "7. 输出：迭代点 \\(x^{k}\\)，步长 \\(t_k\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.272,
                0.243,
                0.717,
                0.385
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.427,
                0.825,
                0.507
            ],
            "angle": 0,
            "content": "在第一种方法中线搜索的初始步长 \\(t_{k}\\) 取为上一步的步长 \\(t_{k-1}\\), 并在迭代过程中不断减小, 这不利于算法较快收敛. 注意到算法 8.3 中参数 \\(\\gamma_{k}\\) 也是可调的, 这进一步增加了设计线搜索算法的灵活性. 第二种线搜索方法不仅改变步长 \\(t_{k}\\) 而且改变 \\(\\gamma_{k}\\), 所以 \\(y^{k}\\) 也随之改变. 该算法的具体过程见 8.5."
        },
        {
            "type": "title",
            "bbox": [
                0.261,
                0.535,
                0.427,
                0.551
            ],
            "angle": 0,
            "content": "算法8.5线搜索算法2"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.556,
                0.551,
                0.574
            ],
            "angle": 0,
            "content": "1. 输入： \\( t_k = \\hat{t} > 0 \\)， \\( t_{k-1} > 0 \\)， \\( \\rho < 1 \\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.579,
                0.328,
                0.594
            ],
            "angle": 0,
            "content": "2. loop"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.598,
                0.705,
                0.615
            ],
            "angle": 0,
            "content": "3. 取 \\(\\gamma_{k}\\) 为关于 \\(\\gamma\\) 的方程 \\(t_{k - 1}\\gamma^{2} = t_{k}\\gamma_{k - 1}^{2}(1 - \\gamma)\\) 的正根"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.618,
                0.664,
                0.635
            ],
            "angle": 0,
            "content": "4. 计算 \\( y^{k} = (1 - \\gamma_{k})x^{k - 1} + \\gamma_{k}v^{k - 1} \\) 和梯度 \\( \\nabla f(y^k) \\)."
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.639,
                0.558,
                0.657
            ],
            "angle": 0,
            "content": "5. 计算 \\(x^{k} = \\mathrm{prox}_{t_{k}h}(y^{k} - t_{k}\\nabla f(y^{k}))\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.66,
                0.56,
                0.676
            ],
            "angle": 0,
            "content": "6. if条件(8.2.2)对 \\(x^{k},y^{k}\\) 不满足then"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.683,
                0.39,
                0.697
            ],
            "angle": 0,
            "content": "7. \\(t_k\\gets \\rho t_k\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.703,
                0.341,
                0.715
            ],
            "angle": 0,
            "content": "8. else"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.723,
                0.399,
                0.737
            ],
            "angle": 0,
            "content": "9. 结束循环"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.744,
                0.357,
                0.757
            ],
            "angle": 0,
            "content": "10. end if"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.765,
                0.361,
                0.78
            ],
            "angle": 0,
            "content": "11. end loop"
        },
        {
            "type": "text",
            "bbox": [
                0.266,
                0.784,
                0.487,
                0.8
            ],
            "angle": 0,
            "content": "12. 输出：迭代点 \\(x^k\\)，步长 \\(t_k\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.261,
                0.556,
                0.705,
                0.8
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.836,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "由 \\(\\gamma_{k}\\) 的计算可知，其一定满足条件(8.2.3)且有 \\(0 < \\gamma_{k} \\leqslant 1\\) ，并且 \\(t_{k}\\) 的选"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.38,
                0.133
            ],
            "angle": 0,
            "content": "8.2 NESTEROV 加速算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "363"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.155,
                0.529,
                0.189
            ],
            "angle": 0,
            "content": "取必有一个下界 \\(t_{\\mathrm{min}}\\)。关于 \\(\\frac{\\gamma_k^2}{t_k}\\) 的估计，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.315,
                0.199,
                0.595,
                0.236
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\sqrt {t _ {k - 1}}}{\\gamma_ {k - 1}} = \\frac {\\sqrt {(1 - \\gamma_ {k}) t _ {k}}}{\\gamma_ {k}} \\leqslant \\frac {\\sqrt {t _ {k}}}{\\gamma_ {k}} - \\frac {\\sqrt {t _ {k}}}{2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.246,
                0.7,
                0.265
            ],
            "angle": 0,
            "content": "这里的不等号是由于 \\(\\sqrt{1 - x}\\) 在点 \\(x = 0\\) 处的凹性．反复利用上式可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.369,
                0.273,
                0.542,
                0.312
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\sqrt {t _ {k}}}{\\gamma_ {k}} \\geqslant \\sqrt {t _ {1}} + \\frac {1}{2} \\sum_ {i = 2} ^ {k} \\sqrt {t _ {i}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.322,
                0.211,
                0.339
            ],
            "angle": 0,
            "content": "因此"
        },
        {
            "type": "equation",
            "bbox": [
                0.235,
                0.347,
                0.737,
                0.387
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\gamma_ {k} ^ {2}}{t _ {k}} \\leqslant \\frac {1}{\\left(\\sqrt {t _ {1}} + \\frac {1}{2} \\sum_ {i = 2} ^ {k} \\sqrt {t _ {i}}\\right) ^ {2}} \\leqslant \\frac {4}{t _ {\\min } (k + 1) ^ {2}} = \\mathcal {O} \\left(\\frac {1}{k ^ {2}}\\right). \\tag {8.2.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.395,
                0.726,
                0.412
            ],
            "angle": 0,
            "content": "以上的分析说明了条件(8.2.3)和条件(8.2.4)在算法8.5的执行中也得到满足"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.416,
                0.737,
                0.495
            ],
            "angle": 0,
            "content": "算法8.5的执行过程比算法8.4的复杂。由于它同时改变了 \\( t_k \\) 和 \\( \\gamma_k \\)，迭代点 \\( x^k \\) 和参照点 \\( y^k \\) 在线搜索的过程中都发生了变化，点 \\( y^k \\) 处的梯度也需要重新计算。但此算法给我们带来的好处就是步长 \\( t_k \\) 不再单调下降，在迭代后期也可以取较大值，这会进一步加快收敛。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.499,
                0.739,
                0.62
            ],
            "angle": 0,
            "content": "总的来说，固定步长的FISTA算法对于步长的选取是较为保守的，为了保证收敛，有时不得不选取一个很小的步长，这使得固定步长的FISTA算法收敛较慢。如果采用线搜索，则在算法执行过程中会有很大机会选择符合条件的较大步长，因此线搜索可能加快算法的收敛，但代价就是每一步迭代的复杂度变高。在实际的FISTA算法中，需要权衡固定步长和线搜索算法的利弊，从而选择针对特定问题的高效算法。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.623,
                0.74,
                0.723
            ],
            "angle": 0,
            "content": "原始的FISTA算法不是一个下降算法，这里给出一个FISTA的下降算法变形，只需要对算法8.3的第4行进行修改。在计算邻近算子之后，我们并不立即选取此点作为新的迭代点，而是检查函数值在当前点处是否下降，只有当函数值下降时才更新迭代点。假设经过近似点映射之后的点为 \\(u\\) ，则对当前点 \\(x^k\\) 做如下更新："
        },
        {
            "type": "equation",
            "bbox": [
                0.336,
                0.732,
                0.737,
                0.783
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k} = \\left\\{ \\begin{array}{l l} u, & \\psi (u) \\leqslant \\psi \\left(x ^ {k - 1}\\right), \\\\ x ^ {k - 1}, & \\psi (u) > \\psi \\left(x ^ {k - 1}\\right). \\end{array} \\right. \\tag {8.2.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.794,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "完整的下降FISTA算法的结构如算法8.6所示，其中 \\(\\psi (x^{k})\\leqslant \\psi (x^{k - 1})\\) 恒成立．在实际计算过程中，由于步长或 \\(\\gamma_{k}\\) 会随着 \\(k\\) 变化，因此(8.2.6)式中的\\(\\psi (u) > \\psi (x^{k - 1})\\) 不会一直成立，即算法不会停留在某个 \\(x^{k - 1}\\) 而不进行更新"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "364"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.159,
                0.452,
                0.175
            ],
            "angle": 0,
            "content": "算法8.6下降FISTA算法"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.18,
                0.558,
                0.198
            ],
            "angle": 0,
            "content": "1. 输入： \\(v^0 = x^0 \\in \\mathbb{R}^n\\) ，初始化 \\(k \\gets 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.201,
                0.487,
                0.217
            ],
            "angle": 0,
            "content": "2. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.221,
                0.543,
                0.239
            ],
            "angle": 0,
            "content": "3. 计算 \\( y = (1 - \\gamma_k)x^{k-1} + \\gamma_k v^{k-1} \\)."
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.243,
                0.54,
                0.261
            ],
            "angle": 0,
            "content": "4. 计算 \\( u = \\mathrm{prox}_{t_k h}(y - t_k \\nabla f(y)) \\)."
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.264,
                0.517,
                0.279
            ],
            "angle": 0,
            "content": "5. 使用(8.2.6)式更新迭代点 \\(x^{k}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.28,
                0.543,
                0.31
            ],
            "angle": 0,
            "content": "6. 计算 \\( v^{k} = x^{k - 1} + \\frac{1}{\\gamma_{k}} (x^{k} - x^{k - 1}) \\)."
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.31,
                0.384,
                0.323
            ],
            "angle": 0,
            "content": "7. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.33,
                0.371,
                0.344
            ],
            "angle": 0,
            "content": "8. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.271,
                0.18,
                0.558,
                0.344
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.372,
                0.825,
                0.417
            ],
            "angle": 0,
            "content": "步长和 \\(\\gamma_{k}\\) 的选取只需使用固定步长 \\(t_k\\leqslant \\frac{1}{L},\\gamma_k = \\frac{2}{k + 1}\\) 或者使用前述的任意一种线搜索方法均可."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.448,
                0.44,
                0.466
            ],
            "angle": 0,
            "content": "8.2.2 其他加速算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.48,
                0.825,
                0.539
            ],
            "angle": 0,
            "content": "本小节将给出除FISTA算法外的另外两种加速算法，它们分别是Nesterov在1988年和2005年提出的算法的推广版本。此外，本小节还将简单提一下针对非凸复合优化问题的Nesterov加速算法。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.543,
                0.825,
                0.58
            ],
            "angle": 0,
            "content": "对于复合优化问题 (8.2.1)，我们给出第二类Nesterov加速算法，见算法8.7."
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.597,
                0.526,
                0.613
            ],
            "angle": 0,
            "content": "算法8.7 第二类Nesterov加速算法"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.618,
                0.538,
                0.635
            ],
            "angle": 0,
            "content": "1. 输入：令 \\(x^{0} = y^{0}\\)，初始化 \\(k \\gets 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.64,
                0.486,
                0.655
            ],
            "angle": 0,
            "content": "2. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.658,
                0.548,
                0.676
            ],
            "angle": 0,
            "content": "3. 计算 \\( z^{k} = (1 - \\gamma_{k})x^{k - 1} + \\gamma_{k}y^{k - 1} \\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.677,
                0.621,
                0.709
            ],
            "angle": 0,
            "content": "4. 计算 \\( y^{k} = \\mathrm{prox}_{(t_{k} / \\gamma_{k})h}\\left(y^{k - 1} - \\frac{t_{k}}{\\gamma_{k}}\\nabla f(z^{k})\\right) \\)."
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.71,
                0.534,
                0.727
            ],
            "angle": 0,
            "content": "5. 计算 \\(x^{k} = (1 - \\gamma_{k})x^{k - 1} + \\gamma_{k}y^{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.731,
                0.384,
                0.745
            ],
            "angle": 0,
            "content": "6. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.752,
                0.371,
                0.766
            ],
            "angle": 0,
            "content": "7. end while"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.772,
                0.355,
                0.788
            ],
            "angle": 0,
            "content": "8. 输出: \\(x^{k}\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.271,
                0.618,
                0.621,
                0.788
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "第二类Nesterov加速算法的一步迭代可参考图8.3.和经典FISTA算法的一个重要区别在于，第二类Nesterov加速算法中的三个序列 \\(\\{x^k\\}\\) ， \\(\\{y^k\\}\\)"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.38,
                0.133
            ],
            "angle": 0,
            "content": "8.2 NESTEROV 加速算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "365"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.739,
                0.205
            ],
            "angle": 0,
            "content": "和 \\(\\{z^{k}\\}\\) 都可以保证在定义域内. 而FISTA算法中的序列 \\(\\{y^{k}\\}\\) 不一定在定义域内. 在第二类Nesterov加速算法中, 我们同样可以取 \\(\\gamma_{k} = \\frac{2}{k + 1}, t_{k} = \\frac{1}{L}\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.3,
                0.23,
                0.607,
                0.251
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = \\operatorname {p r o x} _ {\\left(t _ {k} / \\gamma_ {k}\\right) h} \\left(y ^ {k - 1} - \\left(t _ {k} / \\gamma_ {k}\\right) \\nabla f \\left(z ^ {k}\\right)\\right)\n\\]"
        },
        {
            "type": "image",
            "bbox": [
                0.409,
                0.251,
                0.601,
                0.334
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.278,
                0.363,
                0.629,
                0.38
            ],
            "angle": 0,
            "content": "图8.3 第二类Nesterov加速算法的一步迭代"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.394,
                0.389,
                0.426
            ],
            "angle": 0,
            "content": "来获得 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\) 的收敛速度"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.427,
                0.737,
                0.444
            ],
            "angle": 0,
            "content": "针对问题(8.2.1)的第三类Nesterov加速算法框架见算法8.8. 该算法和"
        },
        {
            "type": "title",
            "bbox": [
                0.172,
                0.457,
                0.438,
                0.473
            ],
            "angle": 0,
            "content": "算法8.8第三类Nesterov加速算法"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.478,
                0.63,
                0.506
            ],
            "angle": 0,
            "content": "1. 输入：令 \\(x^0 \\in \\mathbf{dom} h\\)，\\(y^0 = \\underset{x \\in \\mathbf{dom} h}{\\arg \\min} \\| x \\|^2\\)。初始化 \\(k \\gets 1\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.506,
                0.399,
                0.521
            ],
            "angle": 0,
            "content": "2. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.525,
                0.461,
                0.543
            ],
            "angle": 0,
            "content": "3. 计算 \\(z^{k} = (1 - \\gamma_{k})x^{k - 1} + \\gamma_{k}y^{k - 1}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.542,
                0.56,
                0.583
            ],
            "angle": 0,
            "content": "4. 计算 \\( y^{k} = \\mathrm{prox}_{(t_{k}\\sum_{i = 1}^{k}1 / \\gamma_{i})h}\\left(-t_{k}\\sum_{i = 1}^{k}\\frac{1}{\\gamma_{i}}\\nabla f(z^{i})\\right) \\)."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.583,
                0.447,
                0.601
            ],
            "angle": 0,
            "content": "5. 计算 \\(x^{k} = (1 - \\gamma_{k})x^{k - 1} + \\gamma_{k}y^{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.605,
                0.297,
                0.619
            ],
            "angle": 0,
            "content": "6. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.626,
                0.283,
                0.64
            ],
            "angle": 0,
            "content": "7. end while"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.645,
                0.267,
                0.661
            ],
            "angle": 0,
            "content": "8. 输出: \\(x^{k}\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.181,
                0.478,
                0.63,
                0.661
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.684,
                0.739,
                0.754
            ],
            "angle": 0,
            "content": "第二类Nesterov加速算法8.7的区别仅仅在于 \\(y^{k}\\) 的更新，第三类Nesterov加速算法计算 \\(y^{k}\\) 时需要利用全部已有的 \\(\\{\\nabla f(z^i)\\} ,i = 1,2,\\dots ,k.\\) 同样地，该算法取 \\(\\gamma_{k} = \\frac{2}{k + 1},t_{k} = \\frac{1}{L}\\) 时，也有 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\) 的收敛速度."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.753,
                0.74,
                0.854
            ],
            "angle": 0,
            "content": "除了针对凸问题的加速算法，还有针对非凸复合优化问题的加速算法。仍然考虑问题(8.2.1)的形式，这里并不要求 \\(f\\) 是凸的，但是要求其是可微的且梯度是利普希茨连续的，\\(h\\) 与之前的要求相同。将针对凸函数的加速算法做一些修改，我们可以给出非凸复合优化问题的加速梯度法框架。在算法8.9中，\\(\\lambda_{k}\\) 和 \\(t_{k}\\) 分别为更新 \\(y^{k}\\) 和 \\(x^{k}\\) 的步长参数。从形式上看，算法8.9和"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "366"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.259,
                0.159,
                0.551,
                0.175
            ],
            "angle": 0,
            "content": "算法8.9复合优化问题的加速算法框架"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.18,
                0.826,
                0.217
            ],
            "angle": 0,
            "content": "1. 输入：令 \\(x^0 = y^0 \\in \\mathbb{R}^n\\)，取 \\(\\{\\gamma_k\\}\\) 使得 \\(\\gamma_1 = 1\\) 且当 \\(k \\geqslant 2\\) 时，\\(\\gamma_k \\in (0,1)\\). 初始化 \\(k \\gets 1\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.222,
                0.487,
                0.238
            ],
            "angle": 0,
            "content": "2. while 未达到收敛准则 do"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.18,
                0.826,
                0.238
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.241,
                0.512,
                0.26
            ],
            "angle": 0,
            "content": "3. \\(z^{k} = \\gamma_{k}y^{k - 1} + (1 - \\gamma_{k})x^{k - 1},\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.262,
                0.547,
                0.282
            ],
            "angle": 0,
            "content": "4. \\(y^{k} = \\mathrm{prox}_{\\lambda_{k}h}\\left(y^{k - 1} - \\lambda_{k}\\nabla f(z^{k})\\right),\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.283,
                0.522,
                0.303
            ],
            "angle": 0,
            "content": "5. \\(x^{k} = \\mathrm{prox}_{t_{k}h}\\left(z^{k} - t_{k}\\nabla f(z^{k})\\right)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.305,
                0.386,
                0.32
            ],
            "angle": 0,
            "content": "6. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.326,
                0.371,
                0.34
            ],
            "angle": 0,
            "content": "7. end while"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.345,
                0.356,
                0.362
            ],
            "angle": 0,
            "content": "8. 输出: \\(x^{k}\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.241,
                0.547,
                0.362
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.415,
                0.827,
                0.453
            ],
            "angle": 0,
            "content": "之前我们接触的任何一种算法都不相同，但可以证明当 \\(\\lambda_{k}\\) 和 \\(t_k\\) 取特定值时，它等价于之前介绍过的第二类Nesterov加速算法（见本章习题）."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.467,
                0.828,
                0.653
            ],
            "angle": 0,
            "content": "在非凸函数情形下，一阶算法一般只能保证收敛到一个稳定点，并不能保证收敛到最优解，因此无法用函数值与最优值的差来衡量优化算法解的精度。类似于非凸光滑函数利用梯度作为停止准则，对于非凸复合函数(8.2.1)，我们利用梯度映射（定义8.2）来判断算法是否收敛。注意到 \\(G_{t}(x) = 0\\) 是优化问题(8.2.1)的一阶必要条件，因此利用 \\(\\| G_{t_k}(x^k)\\|\\) 来刻画算法8.9的收敛速度。可以证明，当 \\(f\\) 为凸函数时，算法8.9的收敛速度与FISTA算法相同，两者都为 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\)；当 \\(f\\) 为非凸函数时，算法8.9也收敛，且收敛速度为 \\(\\mathcal{O}\\left(\\frac{1}{k}\\right)\\)，详见[77]。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.741,
                0.402,
                0.759
            ],
            "angle": 0,
            "content": "8.2.3 应用举例"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "之前我们用近似点梯度算法求解的模型，都可以用Nesterov加速算法来求解．为了简化篇幅，此处不再重复叙述对应的优化问题，而是直接给出相应的加速算法迭代格式."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.38,
                0.133
            ],
            "angle": 0,
            "content": "8.2 NESTEROV 加速算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "367"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.157,
                0.338,
                0.173
            ],
            "angle": 0,
            "content": "1. LASSO问题求解"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.19,
                0.663,
                0.207
            ],
            "angle": 0,
            "content": "求解 LASSO 问题的 FISTA 算法可以由下面的迭代格式给出："
        },
        {
            "type": "equation",
            "bbox": [
                0.326,
                0.219,
                0.567,
                0.252
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = x ^ {k - 1} + \\frac {k - 2}{k + 1} (x ^ {k - 1} - x ^ {k - 2}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.326,
                0.256,
                0.518,
                0.275
            ],
            "angle": 0,
            "content": "\\[\nw ^ {k} = y ^ {k} - t _ {k} A ^ {T} (A y ^ {k} - b),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.329,
                0.28,
                0.582,
                0.3
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k} = \\operatorname {s i g n} \\left(w ^ {k}\\right) \\max  \\left\\{\\left| w ^ {k} \\right| - t _ {k} \\mu , 0 \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.317,
                0.739,
                0.374
            ],
            "angle": 0,
            "content": "与近似点梯度算法相同，由于最后一步将 \\(w^{k}\\) 中绝对值小于 \\(t_{k} \\mu\\) 的分量置零，该算法能够保证迭代过程中解具有稀疏结构。我们也给出第二类 Nesterov 加速算法："
        },
        {
            "type": "equation",
            "bbox": [
                0.318,
                0.379,
                0.528,
                0.399
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k} = (1 - \\gamma_ {k}) x ^ {k - 1} + \\gamma_ {k} y ^ {k - 1},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.316,
                0.403,
                0.532,
                0.435
            ],
            "angle": 0,
            "content": "\\[\nw ^ {k} = y ^ {k - 1} - \\frac {t _ {k}}{\\gamma_ {k}} A ^ {T} (A z ^ {k} - b),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.318,
                0.438,
                0.592,
                0.473
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = \\operatorname {s i g n} \\left(w ^ {k}\\right) \\max  \\left\\{\\left| w ^ {k} \\right| - \\frac {t _ {k}}{\\gamma_ {k}} \\mu , 0 \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.319,
                0.476,
                0.512,
                0.496
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k} = (1 - \\gamma_ {k}) x ^ {k - 1} + \\gamma_ {k} y ^ {k},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.508,
                0.398,
                0.525
            ],
            "angle": 0,
            "content": "和第三类Nesterov加速算法："
        },
        {
            "type": "equation",
            "bbox": [
                0.3,
                0.54,
                0.509,
                0.56
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k} = (1 - \\gamma_ {k}) x ^ {k - 1} + \\gamma_ {k} y ^ {k - 1},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.298,
                0.565,
                0.509,
                0.603
            ],
            "angle": 0,
            "content": "\\[\nw ^ {k} = - t _ {k} \\sum_ {i = 1} ^ {k} \\frac {1}{\\gamma_ {i}} A ^ {T} (A z ^ {i} - b),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.3,
                0.606,
                0.61,
                0.649
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = \\operatorname {s i g n} \\left(w ^ {k}\\right) \\max  \\left\\{\\left| w ^ {k} \\right| - t _ {k} \\sum_ {i = 1} ^ {k} \\frac {1}{\\gamma_ {i}} \\mu , 0 \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.302,
                0.652,
                0.493,
                0.673
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k} = (1 - \\gamma_ {k}) x ^ {k - 1} + \\gamma_ {k} y ^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.69,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "我们用同第6.2节中一样的 \\(A\\) 和 \\(b\\), 并取 \\(\\mu = 10^{-3}\\), 分别利用连续化近似点梯度法、连续化FISTA加速算法、连续化第二类Nesterov算法来求解问题, 并分别取固定步长 \\(t = \\frac{1}{L}\\), 这里 \\(L = \\lambda_{\\max}(A^T A)\\), 和结合线搜索的BB步长. 停机准则与参数 \\(\\mu\\) 的连续化设置和第6.2节中的光滑化梯度法一致. 结果如图8.4. 可以看到: 就固定步长而言, FISTA算法相较于第二类Nesterov加速算法收敛得略快一些, 也可注意到FISTA算法是非单调算法. 同时, BB步长和线搜索技巧可以加速算法的收敛速度. 此外, 带线搜索的近似点梯度法可以比带线搜索的FISTA算法更快收敛."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "368"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "image",
            "bbox": [
                0.356,
                0.154,
                0.728,
                0.372
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.296,
                0.384,
                0.788,
                0.402
            ],
            "angle": 0,
            "content": "图8.4 使用近似点梯度法以及不同的加速算法求解LASSO问题"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.451,
                0.396,
                0.468
            ],
            "angle": 0,
            "content": "2. 小波模型求解"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.503,
                0.825,
                0.54
            ],
            "angle": 0,
            "content": "针对合成小波模型求解的FISTA算法和第二类Nesterov加速算法可以由下面的迭代格式给出："
        },
        {
            "type": "equation",
            "bbox": [
                0.418,
                0.574,
                0.652,
                0.604
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = d ^ {k - 1} + \\frac {k - 2}{k + 1} (d ^ {k - 1} - d ^ {k - 2}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.416,
                0.61,
                0.646,
                0.628
            ],
            "angle": 0,
            "content": "\\[\nw ^ {k} = y ^ {k} - t _ {k} W A ^ {T} \\left(A W ^ {T} y ^ {k} - b\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.419,
                0.635,
                0.669,
                0.653
            ],
            "angle": 0,
            "content": "\\[\nd ^ {k} = \\operatorname {s i g n} \\left(w ^ {k}\\right) \\max  \\left\\{\\left| w ^ {k} \\right| - t _ {k} \\lambda , 0 \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.689,
                0.28,
                0.703
            ],
            "angle": 0,
            "content": "和"
        },
        {
            "type": "equation",
            "bbox": [
                0.409,
                0.739,
                0.612,
                0.757
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k} = (1 - \\gamma_ {k}) d ^ {k - 1} + \\gamma_ {k} y ^ {k - 1},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.761,
                0.658,
                0.793
            ],
            "angle": 0,
            "content": "\\[\nw ^ {k} = y ^ {k - 1} - \\frac {t _ {k}}{\\gamma_ {k}} W A ^ {T} \\left(A W ^ {T} z ^ {k} - b\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.797,
                0.68,
                0.831
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = \\operatorname {s i g n} \\left(w ^ {k}\\right) \\max  \\left\\{\\left| w ^ {k} \\right| - \\frac {t _ {k}}{\\gamma_ {k}} \\lambda , 0 \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.836,
                0.596,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nd ^ {k} = (1 - \\gamma_ {k}) d ^ {k - 1} + \\gamma_ {k} y ^ {k}.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.17,
                0.117,
                0.38,
                0.133
            ],
            "angle": 0,
            "content": "8.2 NESTEROV 加速算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "369"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.157,
                0.344,
                0.174
            ],
            "angle": 0,
            "content": "3. 平衡小波模型求解"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.188,
                0.525,
                0.205
            ],
            "angle": 0,
            "content": "平衡小波模型求解的FISTA算法可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.269,
                0.217,
                0.504,
                0.248
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = \\alpha^ {k - 1} + \\frac {k - 2}{k + 1} (\\alpha^ {k - 1} - \\alpha^ {k - 2}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.268,
                0.253,
                0.643,
                0.272
            ],
            "angle": 0,
            "content": "\\[\nw ^ {k} = y ^ {k} - t _ {k} \\left(\\kappa \\left(I - W W ^ {T}\\right) y ^ {k} + W A ^ {T} \\left(A W ^ {T} y ^ {k} - b\\right)\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.27,
                0.278,
                0.521,
                0.296
            ],
            "angle": 0,
            "content": "\\[\n\\alpha^ {k} = \\operatorname {s i g n} \\left(w ^ {k}\\right) \\max  \\left\\{\\left| w ^ {k} \\right| - t _ {k} \\lambda , 0 \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.311,
                0.512,
                0.327
            ],
            "angle": 0,
            "content": "而相应的第二类Nesterov加速算法的格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.259,
                0.34,
                0.463,
                0.36
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k} = (1 - \\gamma_ {k}) \\alpha^ {k - 1} + \\gamma_ {k} y ^ {k - 1},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.256,
                0.365,
                0.654,
                0.396
            ],
            "angle": 0,
            "content": "\\[\nw ^ {k} = y ^ {k - 1} - \\frac {t _ {k}}{\\gamma_ {k}} (\\kappa (I - W W ^ {T}) z ^ {k} + W A ^ {T} (A W ^ {T} z ^ {k} - b)),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.258,
                0.399,
                0.53,
                0.434
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = \\operatorname {s i g n} (w ^ {k}) \\max  \\left\\{\\left| w ^ {k} \\right| - \\frac {t _ {k}}{\\gamma_ {k}} \\lambda , 0 \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.258,
                0.438,
                0.448,
                0.457
            ],
            "angle": 0,
            "content": "\\[\n\\alpha^ {k} = (1 - \\gamma_ {k}) \\alpha^ {k - 1} + \\gamma_ {k} y ^ {k}.\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.481,
                0.334,
                0.499
            ],
            "angle": 0,
            "content": "8.2.4 收敛性分析"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.513,
                0.737,
                0.592
            ],
            "angle": 0,
            "content": "本小节将给出Nesterov加速算法收敛速度的理论分析，我们将只针对凸优化问题分析FISTA算法和第二类Nesterov加速算法的收敛速度，而对于第三类Nesterov加速算法和非凸问题的加速算法，有兴趣的读者可以自行阅读相关文献."
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.597,
                0.614,
                0.613
            ],
            "angle": 0,
            "content": "下面的定理给出了固定步长的FISTA算法的收敛速度"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.629,
                0.737,
                0.674
            ],
            "angle": 0,
            "content": "定理8.5（固定步长FISTA算法收敛速度）在假设8.1的条件下，当用算法8.3求解凸复合优化问题(8.2.1)时，若取固定步长 \\(t_k = \\frac{1}{L}\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.317,
                0.686,
                0.737,
                0.72
            ],
            "angle": 0,
            "content": "\\[\n\\psi \\left(x ^ {k}\\right) - \\psi \\left(x ^ {*}\\right) \\leqslant \\frac {2 L}{(k + 1) ^ {2}} \\| x ^ {0} - x ^ {*} \\| ^ {2}. \\tag {8.2.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.732,
                0.558,
                0.752
            ],
            "angle": 0,
            "content": "证明. 首先根据 \\(x^{k} = \\operatorname{prox}_{t_{k}h}\\left(y^{k} - t_{k}\\nabla f\\left(y^{k}\\right)\\right)\\), 可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.332,
                0.766,
                0.577,
                0.786
            ],
            "angle": 0,
            "content": "\\[\n- x ^ {k} + y ^ {k} - t _ {k} \\nabla f (y ^ {k}) \\in t _ {k} \\partial h (x ^ {k}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.802,
                0.327,
                0.818
            ],
            "angle": 0,
            "content": "故对于任意的 \\(x\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.268,
                0.835,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nt _ {k} h (x) \\geqslant t _ {k} h \\left(x ^ {k}\\right) + \\left\\langle - x ^ {k} + y ^ {k} - t _ {k} \\nabla f \\left(y ^ {k}\\right), x - x ^ {k} \\right\\rangle . \\tag {8.2.8}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "370"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.154,
                0.717,
                0.186
            ],
            "angle": 0,
            "content": "另一方面由 \\(f\\) 的凸性、梯度利普希茨连续和 \\(t_k = \\frac{1}{L}\\) 可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.353,
                0.196,
                0.824,
                0.227
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k}\\right) \\leqslant f \\left(y ^ {k}\\right) + \\left\\langle \\nabla f \\left(y ^ {k}\\right), x ^ {k} - y ^ {k} \\right\\rangle + \\frac {1}{2 t _ {k}} \\| x ^ {k} - y ^ {k} \\| ^ {2}. \\tag {8.2.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.237,
                0.556,
                0.253
            ],
            "angle": 0,
            "content": "结合以上两个不等式，对于任意的 \\(x\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.266,
                0.823,
                0.423
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\psi (x ^ {k}) = f (x ^ {k}) + h (x ^ {k}) \\\\ \\leqslant h (x) + f \\left(y ^ {k}\\right) + \\langle \\nabla f \\left(y ^ {k}\\right), x - y ^ {k} \\rangle + \\frac {1}{t _ {k}} \\langle x ^ {k} - y ^ {k}, x - x ^ {k} \\rangle + \\\\ \\frac {1}{2 t _ {k}} \\left\\| x ^ {k} - y ^ {k} \\right\\| ^ {2} \\tag {8.2.10} \\\\ \\leqslant h (x) + f (x) + \\frac {1}{t _ {k}} \\langle x ^ {k} - y ^ {k}, x - x ^ {k} \\rangle + \\frac {1}{2 t _ {k}} \\| x ^ {k} - y ^ {k} \\| ^ {2} \\\\ = \\psi (x) + \\frac {1}{t _ {k}} \\langle x ^ {k} - y ^ {k}, x - x ^ {k} \\rangle + \\frac {1}{2 t _ {k}} \\| x ^ {k} - y ^ {k} \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.434,
                0.824,
                0.472
            ],
            "angle": 0,
            "content": "在(8.2.10)式中分别取 \\(x = x^{k - 1}\\) 和 \\(x = x^{*}\\), 并记 \\(\\psi (x^{*}) = \\psi^{*}\\), 再分别乘 \\(1 - \\gamma_{k}\\) 和 \\(\\gamma_{k}\\) 并相加得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.31,
                0.486,
                0.823,
                0.54
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\psi \\left(x ^ {k}\\right) - \\psi^ {*} - (1 - \\gamma_ {k}) \\left(\\psi \\left(x ^ {k - 1}\\right) - \\psi^ {*}\\right) \\\\ \\leqslant \\frac {1}{t _ {k}} \\left\\langle x ^ {k} - y ^ {k}, \\left(1 - \\gamma_ {k}\\right) x ^ {k - 1} + \\gamma_ {k} x ^ {*} - x ^ {k} \\right\\rangle + \\frac {1}{2 t _ {k}} \\| x ^ {k} - y ^ {k} \\| ^ {2}. \\tag {8.2.11} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.55,
                0.349,
                0.565
            ],
            "angle": 0,
            "content": "结合迭代式"
        },
        {
            "type": "equation",
            "bbox": [
                0.441,
                0.577,
                0.639,
                0.608
            ],
            "angle": 0,
            "content": "\\[\nv ^ {k} = x ^ {k - 1} + \\frac {1}{\\gamma_ {k}} (x ^ {k} - x ^ {k - 1}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.441,
                0.614,
                0.644,
                0.631
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k} = (1 - \\gamma_ {k}) x ^ {k - 1} + \\gamma_ {k} v ^ {k - 1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.644,
                0.437,
                0.659
            ],
            "angle": 0,
            "content": "不等式(8.2.11)可以化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.298,
                0.674,
                0.824,
                0.782
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\psi \\left(x ^ {k}\\right) - \\psi^ {*} - (1 - \\gamma_ {k}) \\left(\\psi \\left(x ^ {k - 1}\\right) - \\psi^ {*}\\right) \\\\ \\leqslant \\frac {1}{2 t _ {k}} \\left(\\| y ^ {k} - (1 - \\gamma_ {k}) x ^ {k - 1} - \\gamma_ {k} x ^ {*} \\| ^ {2} - \\| x ^ {k} - (1 - \\gamma_ {k}) x ^ {k - 1} - \\gamma_ {k} x ^ {*} \\| ^ {2}\\right) \\\\ = \\frac {\\gamma_ {k} ^ {2}}{2 t _ {k}} \\left(\\| v ^ {k - 1} - x ^ {*} \\| ^ {2} - \\| v ^ {k} - x ^ {*} \\| ^ {2}\\right). \\tag {8.2.12} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.795,
                0.496,
                0.811
            ],
            "angle": 0,
            "content": "注意到 \\(t_{k},\\gamma_{k}\\) 的取法满足不等式"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.822,
                0.824,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\frac {1 - \\gamma_ {k}}{\\gamma_ {k} ^ {2}} t _ {k} \\leqslant \\frac {1}{\\gamma_ {k - 1} ^ {2}} t _ {k - 1}, \\tag {8.2.13}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.38,
                0.133
            ],
            "angle": 0,
            "content": "8.2 NESTEROV 加速算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "371"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.485,
                0.174
            ],
            "angle": 0,
            "content": "可以得到一个有关相邻两步迭代的不等式"
        },
        {
            "type": "equation",
            "bbox": [
                0.186,
                0.179,
                0.737,
                0.229
            ],
            "angle": 0,
            "content": "\\[\n\\frac {t _ {k}}{\\gamma_ {k} ^ {2}} \\left(\\psi \\left(x ^ {k}\\right) - \\psi^ {*}\\right) + \\frac {1}{2} \\| v ^ {k} - x ^ {*} \\| ^ {2} \\leqslant \\frac {t _ {k - 1}}{\\gamma_ {k - 1} ^ {2}} \\left(\\psi \\left(x ^ {k - 1}\\right) - \\psi^ {*}\\right) + \\frac {1}{2} \\| v ^ {k - 1} - x ^ {*} \\| ^ {2}. \\tag {8.2.14}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.233,
                0.384,
                0.25
            ],
            "angle": 0,
            "content": "反复利用(8.2.14)式，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.182,
                0.256,
                0.737,
                0.293
            ],
            "angle": 0,
            "content": "\\[\n\\frac {t _ {k}}{\\gamma_ {k} ^ {2}} \\left(\\psi \\left(x ^ {k}\\right) - \\psi^ {*}\\right) + \\frac {1}{2} \\| v ^ {k} - x ^ {*} \\| ^ {2} \\leqslant \\frac {t _ {1}}{\\gamma_ {1} ^ {2}} \\left(\\psi \\left(x ^ {1}\\right) - \\psi^ {*}\\right) + \\frac {1}{2} \\| v ^ {1} - x ^ {*} \\| ^ {2}. \\tag {8.2.15}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.3,
                0.607,
                0.317
            ],
            "angle": 0,
            "content": "对 \\(k = 1\\), 注意到 \\(\\gamma_1 = 1, v^0 = x^0\\), 再次利用(8.2.12)式可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.218,
                0.325,
                0.736,
                0.396
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\frac {t _ {1}}{\\gamma_ {1} ^ {2}} \\left(\\psi \\left(x ^ {1}\\right) - \\psi^ {*}\\right) + \\frac {1}{2} \\| v ^ {1} - x ^ {*} \\| ^ {2} \\tag {8.2.16} \\\\ \\leqslant \\frac {(1 - \\gamma_ {1}) t _ {1}}{\\gamma_ {1} ^ {2}} (\\psi (x ^ {0}) - \\psi^ {*}) + \\frac {1}{2} \\| v ^ {0} - x ^ {*} \\| ^ {2} = \\frac {1}{2} \\| x ^ {0} - x ^ {*} \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.404,
                0.54,
                0.421
            ],
            "angle": 0,
            "content": "结合(8.2.15)式和(8.2.16)式可以得到(8.2.7)式"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.434,
                0.738,
                0.595
            ],
            "angle": 0,
            "content": "在定理8.5的证明中关键的一步在于建立(8.2.14)式，而建立这个递归关系并不需要 \\(t = \\frac{1}{L},\\gamma_k = \\frac{2}{k + 1}\\) 这一具体条件，我们只需要保证条件(8.2.2)和条件(8.2.3)成立即可．条件(8.2.2)主要依赖于 \\(f(x)\\) 的梯度利普希茨连续性；而(8.2.3)的成立依赖于 \\(\\gamma_{k}\\) 和 \\(t_k\\) 的选取．最后，条件(8.2.4)的成立保证了算法8.3的收敛速度达到 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\) ．也就是说，如果抽取条件(8.2.2)-(8.2.4)作为算法收敛的一般条件，则可以证明一大类FISTA算法的变形都具有 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\) 的收敛速度."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.607,
                0.737,
                0.666
            ],
            "angle": 0,
            "content": "推论8.2（一般FISTA算法的收敛速度）在假设8.1的条件下，当用算法8.3求解凸复合优化问题(8.2.1)时，若迭代点 \\(x^{k},y^{k}\\) ，步长 \\(t_k\\) 以及组合系数\\(\\gamma_{k}\\) 满足条件(8.2.2)-(8.2.4)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.375,
                0.671,
                0.737,
                0.703
            ],
            "angle": 0,
            "content": "\\[\n\\psi \\left(x ^ {k}\\right) - \\psi \\left(x ^ {*}\\right) \\leqslant \\frac {C}{k ^ {2}}, \\tag {8.2.17}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.712,
                0.737,
                0.763
            ],
            "angle": 0,
            "content": "其中 \\(C\\) 仅与函数 \\(f\\)，初始点 \\(x^0\\) 的选取有关。特别地，采用线搜索算法8.4和算法8.5的FISTA算法具有 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\) 的收敛速度。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.768,
                0.737,
                0.832
            ],
            "angle": 0,
            "content": "在这里我们指出，虽然已经抽象出了 \\(t_k, \\gamma_k\\) 满足的条件，但我们无法再找到其他的 \\(t_k, \\gamma_k\\) 来进一步改善FISTA算法的收敛速度，即 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\) 是FISTA算法所能达到的最高的收敛速度."
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.836,
                0.704,
                0.853
            ],
            "angle": 0,
            "content": "第二类Nesterov加速算法的收敛性分析可以使用相同的技术得到."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "372"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.154,
                0.826,
                0.201
            ],
            "angle": 0,
            "content": "定理8.6（第二类Nesterov加速算法收敛速度）取 \\(\\gamma_{k} = \\frac{2}{k + 1}\\) 和 \\(t_k = \\frac{1}{L}\\) 利用算法8.7求解问题(8.2.1)有如下收敛性结果："
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.206,
                0.826,
                0.241
            ],
            "angle": 0,
            "content": "\\[\n\\psi \\left(x ^ {k}\\right) - \\psi \\left(x ^ {*}\\right) \\leqslant \\frac {2 L}{(k + 1) ^ {2}} \\| x ^ {0} - x ^ {*} \\| ^ {2}. \\tag {8.2.18}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.246,
                0.752,
                0.283
            ],
            "angle": 0,
            "content": "证明．首先根据 \\(y^{k} = \\mathrm{prox}_{(t_{k} / \\gamma_{k})h}\\left(y^{k - 1} - \\left(\\frac{t_{k}}{\\gamma_{k}}\\right)\\nabla f(z^{k})\\right)\\) ，可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.404,
                0.287,
                0.68,
                0.308
            ],
            "angle": 0,
            "content": "\\[\n\\gamma_ {k} (y ^ {k - 1} - y ^ {k}) - t _ {k} \\nabla f (z ^ {k}) \\in t _ {k} \\partial h (y ^ {k}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.318,
                0.414,
                0.334
            ],
            "angle": 0,
            "content": "故对于任意的 \\(x\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.313,
                0.344,
                0.826,
                0.365
            ],
            "angle": 0,
            "content": "\\[\nt _ {k} h (x) \\geqslant t _ {k} h \\left(y ^ {k}\\right) + \\left\\langle \\gamma_ {k} \\left(y ^ {k - 1} - y ^ {k}\\right) - t _ {k} \\nabla f \\left(z ^ {k}\\right), x - y ^ {k} \\right\\rangle . \\tag {8.2.19}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.375,
                0.374,
                0.391
            ],
            "angle": 0,
            "content": "再由 \\(h\\) 的凸性，"
        },
        {
            "type": "equation",
            "bbox": [
                0.409,
                0.394,
                0.673,
                0.414
            ],
            "angle": 0,
            "content": "\\[\nh \\left(x ^ {k}\\right) \\leqslant \\left(1 - \\gamma_ {k}\\right) h \\left(x ^ {k - 1}\\right) + \\gamma_ {k} h \\left(y ^ {k}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.42,
                0.381,
                0.438
            ],
            "angle": 0,
            "content": "消去 \\(h(y^k)\\) 得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.295,
                0.446,
                0.825,
                0.505
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} h \\left(x ^ {k}\\right) \\leqslant \\left(1 - \\gamma_ {k}\\right) h \\left(x ^ {k - 1}\\right) \\\\ + \\gamma_ {k} \\left[ h (x) - \\left\\langle \\frac {\\gamma_ {k}}{t _ {k}} \\left(y ^ {k - 1} - y ^ {k}\\right) - \\nabla f \\left(z ^ {k}\\right), x - y ^ {k} \\right\\rangle \\right]. \\tag {8.2.20} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.511,
                0.677,
                0.528
            ],
            "angle": 0,
            "content": "利用 \\(f\\) 的凸性和梯度利普希茨连续的性质, 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.533,
                0.705,
                0.599
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (x ^ {k}) \\leqslant f (z ^ {k}) + \\langle \\nabla f (z ^ {k}), x ^ {k} - z ^ {k} \\rangle + \\frac {L}{2} \\| x ^ {k} - z ^ {k} \\| ^ {2} \\\\ = f (z ^ {k}) + \\langle \\nabla f (z ^ {k}), x ^ {k} - z ^ {k} \\rangle + \\frac {1}{2 t _ {k}} \\| x ^ {k} - z ^ {k} \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.605,
                0.747,
                0.624
            ],
            "angle": 0,
            "content": "用迭代步3减去迭代步1有 \\(x^{k} - z^{k} = \\gamma_{k}(y^{k} - y^{k - 1})\\) ，将此等式与"
        },
        {
            "type": "equation",
            "bbox": [
                0.447,
                0.633,
                0.637,
                0.653
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k} = (1 - \\gamma_ {k}) x ^ {k - 1} + \\gamma_ {k} y ^ {k}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.664,
                0.384,
                0.68
            ],
            "angle": 0,
            "content": "代入上式右端得"
        },
        {
            "type": "equation",
            "bbox": [
                0.278,
                0.685,
                0.825,
                0.734
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k}\\right) \\leqslant f \\left(z ^ {k}\\right) + \\left\\langle \\nabla f \\left(z ^ {k}\\right), \\left(1 - \\gamma_ {k}\\right) x ^ {k - 1} + \\gamma_ {k} y ^ {k} - z ^ {k} \\right\\rangle + \\frac {\\gamma_ {k} ^ {2}}{2 t _ {k}} \\| y ^ {k} - y ^ {k - 1} \\| ^ {2}. \\tag {8.2.22}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.737,
                0.315,
                0.754
            ],
            "angle": 0,
            "content": "注意到"
        },
        {
            "type": "equation",
            "bbox": [
                0.331,
                0.762,
                0.824,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (z ^ {k}) + \\langle \\nabla f (z ^ {k}), (1 - \\gamma_ {k}) x ^ {k - 1} + \\gamma_ {k} y ^ {k} - z ^ {k} \\rangle \\\\ = (1 - \\gamma_ {k}) [ f (z ^ {k}) + \\langle \\nabla f (z ^ {k}), x ^ {k - 1} - z ^ {k} \\rangle ] \\tag {8.2.23} \\\\ + \\gamma_ {k} \\left[ f \\left(z ^ {k}\\right) + \\left\\langle \\nabla f \\left(z ^ {k}\\right), y ^ {k} - z ^ {k} \\right\\rangle \\right] \\\\ \\leqslant \\left(1 - \\gamma_ {k}\\right) f \\left(x ^ {k - 1}\\right) + \\gamma_ {k} \\left[ f \\left(z ^ {k}\\right) + \\langle \\nabla f \\left(z ^ {k}\\right), y ^ {k} - z ^ {k} \\rangle \\right], \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "8.3 近似点算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "373"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.409,
                0.174
            ],
            "angle": 0,
            "content": "结合不等式(8.2.22) (8.2.23)得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.172,
                0.178,
                0.737,
                0.226
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {k}\\right) \\leqslant \\left(1 - \\gamma_ {k}\\right) f \\left(x ^ {k - 1}\\right) + \\gamma_ {k} \\left[ f \\left(z ^ {k}\\right) + \\left\\langle \\nabla f \\left(z ^ {k}\\right), y ^ {k} - z ^ {k} \\right\\rangle \\right] + \\frac {\\gamma_ {k} ^ {2}}{2 t _ {k}} \\| y ^ {k} - y ^ {k - 1} \\| ^ {2}. \\tag {8.2.24}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.231,
                0.457,
                0.247
            ],
            "angle": 0,
            "content": "将(8.2.20)式与(8.2.24)式相加，并结合"
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.256,
                0.576,
                0.277
            ],
            "angle": 0,
            "content": "\\[\nf (x) \\geqslant f \\left(z ^ {k}\\right) + \\langle \\nabla f \\left(z ^ {k}\\right), x - z ^ {k} \\rangle ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.286,
                0.269,
                0.302
            ],
            "angle": 0,
            "content": "再取 \\(x = x^{*}\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.189,
                0.309,
                0.737,
                0.405
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\psi (x ^ {k}) - (1 - \\gamma_ {k}) \\psi (x ^ {k - 1}) \\\\ \\leqslant \\gamma_ {k} \\left[ h \\left(x ^ {*}\\right) + f \\left(x ^ {*}\\right) - \\frac {\\gamma_ {k}}{t _ {k}} \\langle y ^ {k - 1} - y ^ {k}, x ^ {*} - y ^ {k} \\rangle \\right] + \\frac {\\gamma_ {k} ^ {2}}{2 t _ {k}} \\| y ^ {k} - y ^ {k - 1} \\| ^ {2} \\tag {8.2.25} \\\\ \\leqslant \\gamma_ {k} \\psi (x ^ {*}) + \\frac {\\gamma_ {k} ^ {2}}{2 t _ {k}} (\\| y ^ {k - 1} - x ^ {*} \\| _ {2} ^ {2} - \\| y ^ {k} - x ^ {*} \\| ^ {2}). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.41,
                0.738,
                0.447
            ],
            "angle": 0,
            "content": "这个不等式和(8.2.12)式的形式完全相同，因此后续过程可按照定理8.5进行推导，最终我们可以得到(8.2.18)式. □"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.46,
                0.739,
                0.517
            ],
            "angle": 0,
            "content": "同样地，注意到定理8.6推导的关键步骤仍为条件(8.2.2)-(8.2.4)．因此对采用线搜索步长的第二类Nesterov加速算法，我们仍然有相同的收敛结果."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.527,
                0.738,
                0.585
            ],
            "angle": 0,
            "content": "推论8.3（一般第二类Nesterov加速算法的收敛速度）当用算法8.7求解凸复合优化问题(8.2.1)时，若迭代点 \\(x^{k},y^{k}\\) ，步长 \\(t_k\\) 以及组合系数 \\(\\gamma_{k}\\) 满足条件(8.2.2)—(8.2.4)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.375,
                0.59,
                0.737,
                0.621
            ],
            "angle": 0,
            "content": "\\[\n\\psi \\left(x ^ {k}\\right) - \\psi \\left(x ^ {*}\\right) \\leqslant \\frac {C}{k ^ {2}}, \\tag {8.2.26}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.627,
                0.496,
                0.644
            ],
            "angle": 0,
            "content": "其中 \\(C\\) 仅和函数 \\(f\\) ，初始点 \\(x^0\\) 的选取有关."
        },
        {
            "type": "title",
            "bbox": [
                0.361,
                0.673,
                0.547,
                0.695
            ],
            "angle": 0,
            "content": "8.3 近似点算法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.712,
                0.74,
                0.853
            ],
            "angle": 0,
            "content": "前两节分别讨论了近似点梯度算法和Nesterov加速算法，它们能够处理部分不可微的目标函数．对于一般形式的目标函数，可以用近似点算法，该算法是近似点梯度算法的一种特殊情况．我们将其单独拿出来讨论，是因为近似点算法具有一些特殊的理论性质，例如其与增广拉格朗日函数法有某种等价关系．本节首先给出近似点算法的格式和其加速版本，然后叙述其与增广拉格朗日函数法的关系，最后讨论近似点算法的收敛性以及介绍Moreau-Yosida正则化以便进一步理解该算法."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "374"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.421,
                0.174
            ],
            "angle": 0,
            "content": "8.3.1 近似点算法"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.187,
                0.614,
                0.204
            ],
            "angle": 0,
            "content": "本小节将讨论如下一般形式的最优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.495,
                0.217,
                0.825,
                0.241
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad \\psi (x), \\tag {8.3.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.248,
                0.825,
                0.327
            ],
            "angle": 0,
            "content": "其中 \\(\\psi\\) 是一个适当的闭凸函数，这里并不要求 \\(\\psi\\) 是可微或连续的（例如 \\(\\psi\\) 的一部分可以是凸集的示性函数）。对于不可微的 \\(\\psi\\)，我们可以用次梯度算法，但是该方法往往收敛较慢，且收敛条件比较苛刻。我们也可以考虑如下隐式格式的次梯度算法："
        },
        {
            "type": "equation",
            "bbox": [
                0.452,
                0.339,
                0.825,
                0.358
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - t _ {k} \\partial \\psi \\left(x ^ {k + 1}\\right). \\tag {8.3.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.371,
                0.825,
                0.409
            ],
            "angle": 0,
            "content": "上面的格式只是形式上的。类似于之前的近似点梯度算法，可以用邻近算子表示隐式格式：近似点算法格式可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.393,
                0.42,
                0.825,
                0.477
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} x ^ {k + 1} = \\operatorname {p r o x} _ {t _ {k} \\psi} (x ^ {k}) \\\\ = \\arg \\min  _ {u} \\left\\{\\psi (u) + \\frac {1}{2 t _ {k}} \\| u - x ^ {k} \\| _ {2} ^ {2} \\right\\}, \\tag {8.3.3} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.485,
                0.825,
                0.605
            ],
            "angle": 0,
            "content": "其中 \\(t_k\\) 为第 \\(k\\) 步迭代时的步长，可为固定值或通过某种合适的线搜索策略得到。回顾近似点梯度法的迭代格式，会发现这个算法可以看做是近似点梯度法在 \\(f(x) = 0\\) 时的情况，但不同之处在于：在近似点梯度法中，非光滑项 \\(h(x)\\) 的邻近算子通常比较容易计算；而在近似点算法中，\\(\\psi(x)\\) 的邻近算子通常是难以求解的，绝大多数情况下需借助其他类型的迭代法进行（不精确）求解。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.618,
                0.825,
                0.675
            ],
            "angle": 0,
            "content": "注8.1 在近似点算法迭代格式(8.3.3)中，我们构造了一个看似比原问题(8.3.1)形式更复杂的子问题。这样的构造带来的好处是：子问题的目标函数是一个强凸函数，更加便于使用迭代法进行求解。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.688,
                0.825,
                0.725
            ],
            "angle": 0,
            "content": "与近似点梯度法类似，同样可以对近似点算法进行加速。与其对应的FISTA算法的迭代格式可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.732,
                0.724,
                0.768
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k} = \\operatorname {p r o x} _ {t _ {k} \\psi} \\left(x ^ {k - 1} + \\gamma_ {k} \\frac {1 - \\gamma_ {k - 1}}{\\gamma_ {k - 1}} \\left(x ^ {k - 1} - x ^ {k - 2}\\right)\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.775,
                0.617,
                0.792
            ],
            "angle": 0,
            "content": "第二类Nesterov加速算法的迭代格式可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.351,
                0.804,
                0.731,
                0.827
            ],
            "angle": 0,
            "content": "\\[\nv ^ {k} = \\operatorname {p r o x} _ {\\left(t _ {k} / \\gamma_ {k}\\right) \\psi} \\left(v ^ {k - 1}\\right), \\quad x ^ {k} = \\left(1 - \\gamma_ {k}\\right) x ^ {k - 1} + \\gamma_ {k} v ^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.58,
                0.853
            ],
            "angle": 0,
            "content": "关于算法参数的选择，我们提出两种策略："
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "8.3 近似点算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "375"
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.154,
                0.543,
                0.187
            ],
            "angle": 0,
            "content": "- 策略1：取固定步长 \\(t_k = t\\) 以及 \\(\\gamma_k = \\frac{2}{k + 1}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.192,
                0.737,
                0.228
            ],
            "angle": 0,
            "content": "- 策略2：对于可变步长 \\(t_k\\)，当 \\(k = 1\\) 时取 \\(\\gamma_1 = 1\\)；当 \\(k > 1\\) 时，\\(\\gamma_k\\) 由方程"
        },
        {
            "type": "list",
            "bbox": [
                0.193,
                0.154,
                0.737,
                0.228
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.228,
                0.544,
                0.263
            ],
            "angle": 0,
            "content": "\\[\n\\frac {(1 - \\gamma_ {k}) t _ {k}}{\\gamma_ {k} ^ {2}} = \\frac {t _ {k - 1}}{\\gamma_ {k - 1} ^ {2}}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.268,
                0.255,
                0.284
            ],
            "angle": 0,
            "content": "确定."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.309,
                0.49,
                0.328
            ],
            "angle": 0,
            "content": "8.3.2 与增广拉格朗日函数法的关系"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.341,
                0.737,
                0.399
            ],
            "angle": 0,
            "content": "这一小节将讨论近似点算法与增广拉格朗日函数法之间的关系。这是近似点算法一个非常重要的性质，了解该性质能增进对增广拉格朗日函数的理解。考虑具有如下形式的优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.373,
                0.413,
                0.737,
                0.436
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x) + h (A x), \\tag {8.3.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.444,
                0.588,
                0.46
            ],
            "angle": 0,
            "content": "其中 \\(f, h\\) 为适当的闭凸函数. 容易计算出其对偶问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.32,
                0.474,
                0.737,
                0.492
            ],
            "angle": 0,
            "content": "\\[\n\\max  \\quad \\psi (z) = - f ^ {*} (- A ^ {T} z) - h ^ {*} (z), \\tag {8.3.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.506,
                0.434,
                0.522
            ],
            "angle": 0,
            "content": "其中 \\(f^{*},h^{*}\\) 分别为 \\(f,h\\) 的共轭函数"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.527,
                0.737,
                0.562
            ],
            "angle": 0,
            "content": "问题 (8.3.4) 描述了很广泛的一类凸优化问题，我们给出三个常见的例子。"
        },
        {
            "type": "title",
            "bbox": [
                0.205,
                0.577,
                0.254,
                0.592
            ],
            "angle": 0,
            "content": "例8.5"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.605,
                0.737,
                0.641
            ],
            "angle": 0,
            "content": "(1) 当 \\(h\\) 是单点集 \\(\\{b\\}\\) 的示性函数时, 问题(8.3.4)等价于线性等式约束优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.417,
                0.654,
                0.51,
                0.677
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.684,
                0.529,
                0.698
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A x = b. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.716,
                0.66,
                0.732
            ],
            "angle": 0,
            "content": "(2) 当 \\(h\\) 是凸集 \\(C\\) 上的示性函数时, 问题(8.3.4)等价于约束问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.417,
                0.745,
                0.51,
                0.763
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\min  & f (x), \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.427,
                0.769,
                0.53,
                0.784
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A x \\in C. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.802,
                0.628,
                0.819
            ],
            "angle": 0,
            "content": "(3) 当 \\(h(y) = \\| y - b\\|\\) 时，问题(8.3.4)等价于正则优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.836,
                0.566,
                0.853
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad f (x) + \\| A x - b \\|.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "376"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.157,
                0.725,
                0.174
            ],
            "angle": 0,
            "content": "对于对偶问题 (8.3.5)，我们使用近似点算法进行更新，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.292,
                0.179,
                0.789,
                0.214
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = \\operatorname {p r o x} _ {t \\psi} (z ^ {k}) = \\underset {z} {\\arg \\min } \\left\\{f ^ {*} (- A ^ {T} z) + h ^ {*} (z) + \\frac {1}{2 t _ {k}} \\| z - z ^ {k} \\| _ {2} ^ {2} \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.22,
                0.774,
                0.236
            ],
            "angle": 0,
            "content": "而对于原始问题 (8.3.4)，我们引入中间变量 \\(y\\)，可以得到其等价形式"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.246,
                0.825,
                0.274
            ],
            "angle": 0,
            "content": "\\[\n\\min  f (x) + h (y), \\tag {8.3.6}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.479,
                0.274,
                0.581,
                0.288
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A x = y. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.296,
                0.825,
                0.334
            ],
            "angle": 0,
            "content": "对问题 (8.3.6) 可以用增广拉格朗日函数法进行求解，其迭代格式分为最小化增广拉格朗日函数和对偶更新两步："
        },
        {
            "type": "equation",
            "bbox": [
                0.314,
                0.338,
                0.769,
                0.375
            ],
            "angle": 0,
            "content": "\\[\n\\left(x ^ {k + 1}, y ^ {k + 1}\\right) = \\underset {x, y} {\\operatorname {a r g m i n}} \\left\\{f (x) + h (y) + \\frac {t _ {k}}{2} \\| A x - y + \\frac {1}{t _ {k}} z ^ {k} \\| _ {2} ^ {2} \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.368,
                0.38,
                0.586,
                0.4
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = z ^ {k} + t _ {k} \\left(A x ^ {k + 1} - y ^ {k + 1}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.407,
                0.825,
                0.485
            ],
            "angle": 0,
            "content": "接下来介绍本节最重要的结论：对对偶问题 (8.3.5) 用近似点算法，实际上等价于对原始问题 (8.3.6) 用增广拉格朗日函数法。由于问题 (8.3.4) 和问题 (8.3.5) 的自变量不同，我们必须理清二者变量的对应关系。下面有关共轭函数的结论是证明两个算法等价关系的基础。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.496,
                0.824,
                0.534
            ],
            "angle": 0,
            "content": "命题8.2设 \\(f(x)\\) 是适当的闭凸函数， \\(f^{*}(y)\\) 是其共轭函数，则对任意的 \\(y\\in \\mathbf{dom}f^{*}\\) 和 \\(x\\in \\mathbf{dom}f\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.447,
                0.545,
                0.635,
                0.563
            ],
            "angle": 0,
            "content": "\\[\ny \\in \\partial f (x) \\Leftrightarrow x \\in \\partial f ^ {*} (y).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.573,
                0.825,
                0.611
            ],
            "angle": 0,
            "content": "证明．由于 \\(f\\) 是适当闭函数，根据定理2.15有 \\(f^{**} = f\\) ，根据 \\(f^{*}(y)\\) 的定义， \\(y\\in \\partial f(x)\\) 即表明 \\(x\\) 满足最优性条件，因此"
        },
        {
            "type": "equation",
            "bbox": [
                0.465,
                0.62,
                0.619,
                0.64
            ],
            "angle": 0,
            "content": "\\[\nx ^ {T} y - f (x) = f ^ {*} (y).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.65,
                0.367,
                0.665
            ],
            "angle": 0,
            "content": "由自共轭性得"
        },
        {
            "type": "equation",
            "bbox": [
                0.429,
                0.669,
                0.654,
                0.689
            ],
            "angle": 0,
            "content": "\\[\nf ^ {* *} (x) = f (x) = x ^ {\\mathrm {T}} y - f ^ {*} (y),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.695,
                0.825,
                0.734
            ],
            "angle": 0,
            "content": "这说明 \\(y\\) 是最优值点, 即 \\(y\\) 满足最优性条件 \\(x \\in \\partial f^{*}(y)\\). 另一个方向的结论可类似地得到."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.746,
                0.768,
                0.763
            ],
            "angle": 0,
            "content": "为了方便，我们将增广拉格朗日函数法一步迭代写为如下形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.309,
                0.768,
                0.773,
                0.805
            ],
            "angle": 0,
            "content": "\\[\n(\\hat {x}, \\hat {y}) = \\underset {x, y} {\\arg \\min } \\left\\{f (x) + h (y) + z ^ {\\mathrm {T}} (A x - y) + \\frac {t}{2} \\| A x - y \\| _ {2} ^ {2} \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.34,
                0.811,
                0.484,
                0.829
            ],
            "angle": 0,
            "content": "\\[\nu = z + t (A \\hat {x} - \\hat {y}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.836,
                0.714,
                0.853
            ],
            "angle": 0,
            "content": "下面证明上述迭代中 \\(u\\) 的更新实际等价于近似点算法的更新"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "8.3 近似点算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "377"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.275,
                0.173
            ],
            "angle": 0,
            "content": "证明. 问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.273,
                0.183,
                0.637,
                0.215
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, y} \\quad f (x) + h (y) + z ^ {\\mathrm {T}} (A x - y) + \\frac {t}{2} \\| A x - y \\| _ {2} ^ {2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.226,
                0.247,
                0.243
            ],
            "angle": 0,
            "content": "可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.253,
                0.563,
                0.285
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, y, w} \\quad f (x) + h (y) + \\frac {t}{2} \\| w \\| _ {2} ^ {2},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.355,
                0.287,
                0.523,
                0.316
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\text {s . t .} \\quad A x - y + \\frac {z}{t} = w. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.325,
                0.578,
                0.355
            ],
            "angle": 0,
            "content": "对约束 \\(Ax - y + \\frac{z}{t} = w\\) 引入乘子 \\(u\\) ，由最优性条件有"
        },
        {
            "type": "equation",
            "bbox": [
                0.237,
                0.364,
                0.672,
                0.393
            ],
            "angle": 0,
            "content": "\\[\nA \\hat {x} - \\hat {y} + \\frac {z}{t} = w, \\quad - A ^ {\\mathrm {T}} u \\in \\partial f (\\hat {x}), \\quad u \\in \\partial h (\\hat {y}), \\quad t w = u,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.403,
                0.737,
                0.439
            ],
            "angle": 0,
            "content": "消去 \\(w\\) 得 \\(u = z + t(A\\hat{x} - \\hat{y})\\). 下面只需要讨论 \\(\\hat{x}\\) 和 \\(\\hat{y}\\) 满足的条件. 根据命题 8.2 可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.339,
                0.443,
                0.57,
                0.462
            ],
            "angle": 0,
            "content": "\\[\n\\hat {x} \\in \\partial f ^ {*} (- A ^ {\\mathrm {T}} u), \\quad \\hat {y} \\in \\partial h ^ {*} (u),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.473,
                0.384,
                0.489
            ],
            "angle": 0,
            "content": "代入 \\(u\\) 的表达式最终可得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.498,
                0.611,
                0.53
            ],
            "angle": 0,
            "content": "\\[\n0 \\in - A \\partial f ^ {*} (- A ^ {T} u) + \\partial h ^ {*} (u) + \\frac {1}{t} (u - z).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.54,
                0.446,
                0.559
            ],
            "angle": 0,
            "content": "这正是 \\(u = \\mathrm{prox}_{t\\psi}(z)\\) 的最优性条件."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.56,
                0.738,
                0.599
            ],
            "angle": 0,
            "content": "另一方面，若有 \\(u = \\mathrm{prox}_{t\\psi}(z)\\) ，则选取 \\(\\hat{x}\\in \\partial f^{*}(-A^{\\mathrm{T}}u)\\) 及 \\(\\hat{y}\\in \\partial h^{*}(u)\\) 即可恢复出增广拉格朗日函数法中的变量. □"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.612,
                0.739,
                0.713
            ],
            "angle": 0,
            "content": "本节说明，针对某些形式的问题，对原始问题应用增广拉格朗日函数法等价于对对偶问题应用近似点算法。实际上，此结论对不少优化问题都是成立的。由于增广拉格朗日函数法是一类比较有效的处理约束优化问题的算法，根据等价性，如果子问题能够高效求解，近似点算法也应该具有不错的表现。这一点将在应用举例中具体说明。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.738,
                0.314,
                0.756
            ],
            "angle": 0,
            "content": "8.3.3 应用举例"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.769,
                0.338,
                0.786
            ],
            "angle": 0,
            "content": "1. LASSO问题求解"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.799,
                0.344,
                0.816
            ],
            "angle": 0,
            "content": "考虑 LASSO 问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.825,
                0.737,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\quad \\mu \\| x \\| _ {1} + \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2}. \\tag {8.3.7}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "378"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.659,
                0.174
            ],
            "angle": 0,
            "content": "引入变量 \\( y = Ax - b \\)，问题(8.3.7)可以等价地转化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.375,
                0.183,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, y} \\quad \\psi (x, y) \\stackrel {\\text {d e f}} {=} \\mu \\| x \\| _ {1} + \\frac {1}{2} \\| y \\| _ {2} ^ {2} + I _ {\\mathcal {D}} (x, y), \\tag {8.3.8}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.226,
                0.698,
                0.243
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{D} = \\{(x,y) \\mid Ax - y = b\\}\\)，\\(I_{\\mathcal{D}}\\) 为集合 \\(\\mathcal{D}\\) 的示性函数。"
        },
        {
            "type": "text",
            "bbox": [
                0.291,
                0.246,
                0.789,
                0.264
            ],
            "angle": 0,
            "content": "对于问题(8.3.8)，我们采用近似点算法进行求解，其第 \\(k\\) 步迭代为"
        },
        {
            "type": "equation",
            "bbox": [
                0.271,
                0.272,
                0.825,
                0.309
            ],
            "angle": 0,
            "content": "\\[\n\\left(x ^ {k + 1}, y ^ {k + 1}\\right) \\approx \\underset {x, y} {\\arg \\min } \\quad \\left\\{\\psi (x, y) + \\frac {1}{2 t _ {k}} \\left(\\| x - x ^ {k} \\| _ {2} ^ {2} + \\| y - y ^ {k} \\| _ {2} ^ {2}\\right) \\right\\}, \\tag {8.3.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.32,
                0.825,
                0.357
            ],
            "angle": 0,
            "content": "其中 \\(t_k\\) 为步长。由于问题 (8.3.9) 没有显式解，我们需要采用迭代算法来进行求解，比如罚函数法、增广拉格朗日函数法等。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.361,
                0.825,
                0.399
            ],
            "angle": 0,
            "content": "除了直接求解问题(8.3.9)，另一种比较实用的方式是通过对偶问题的解来构造 \\((x^{k + 1},y^{k + 1})\\) ，引入拉格朗日乘子 \\(z\\) ，问题(8.3.9)的对偶函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.306,
                0.408,
                0.776,
                0.551
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\Phi_ {k} (z) = \\inf  _ {x} \\left\\{\\mu \\| x \\| _ {1} + z ^ {\\mathrm {T}} A x + \\frac {1}{2 t _ {k}} \\| x - x ^ {k} \\| _ {2} ^ {2} \\right\\} \\\\ + \\inf  _ {y} \\left\\{\\frac {1}{2} \\| y \\| _ {2} ^ {2} - z ^ {\\mathrm {T}} y + \\frac {1}{2 t _ {k}} \\| y - y ^ {k} \\| _ {2} ^ {2} \\right\\} - b ^ {\\mathrm {T}} z \\\\ = \\mu \\Gamma_ {\\mu t _ {k}} \\left(x ^ {k} - t _ {k} A ^ {\\mathrm {T}} z\\right) - \\frac {1}{2 t _ {k}} \\left(\\| x ^ {k} - t _ {k} A ^ {\\mathrm {T}} z \\| _ {2} ^ {2} - \\| x ^ {k} \\| _ {2} ^ {2}\\right) \\\\ - \\frac {t _ {k}}{2 (t _ {k} + 1)} \\| z \\| _ {2} ^ {2} - \\frac {1}{t _ {k} + 1} z ^ {\\mathrm {T}} y ^ {k} + \\frac {1}{2 (t _ {k} + 1)} \\| y ^ {k} \\| _ {2} ^ {2} - b ^ {\\mathrm {T}} z. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.56,
                0.304,
                0.576
            ],
            "angle": 0,
            "content": "这里，"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.574,
                0.685,
                0.609
            ],
            "angle": 0,
            "content": "\\[\n\\Gamma_ {\\mu t _ {k}} (u) = \\inf  _ {x} \\left\\{\\| x \\| _ {1} + \\frac {1}{2 \\mu t _ {k}} \\| x - u \\| _ {2} ^ {2} \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.614,
                0.825,
                0.649
            ],
            "angle": 0,
            "content": "注意到 \\(\\Gamma_{\\mu t_k}(u)\\) 的表达式可以利用 \\(\\ell_1\\) 范数的邻近算子得到，记函数 \\(q_{\\mu t_k}:\\mathbb{R}\\to\\) \\(\\mathbb{R}\\) 为"
        },
        {
            "type": "equation",
            "bbox": [
                0.415,
                0.65,
                0.667,
                0.711
            ],
            "angle": 0,
            "content": "\\[\nq _ {\\mu t _ {k}} (v) = \\left\\{ \\begin{array}{l l} \\frac {v ^ {2}}{2 \\mu t _ {k}}, & | v | \\leqslant \\mu t _ {k}, \\\\ | v | - \\frac {\\mu t _ {k}}{2}, & | v | > \\mu t _ {k}, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.716,
                0.281,
                0.731
            ],
            "angle": 0,
            "content": "则"
        },
        {
            "type": "equation",
            "bbox": [
                0.463,
                0.73,
                0.62,
                0.766
            ],
            "angle": 0,
            "content": "\\[\n\\Gamma_ {\\mu t _ {k}} (u) = \\sum_ {i = 1} ^ {n} q _ {\\mu t _ {k}} (u _ {i}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.772,
                0.825,
                0.809
            ],
            "angle": 0,
            "content": "其为极小点 \\(x = \\mathrm{prox}_{\\mu t_k\\| \\cdot \\|_1}(u)\\) 处的目标函数值．易知 \\(\\Gamma_{\\mu t_k}(u)\\) 是关于 \\(u\\) 的连续可微函数且梯度为"
        },
        {
            "type": "equation",
            "bbox": [
                0.423,
                0.819,
                0.66,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {u} \\Gamma_ {\\mu t _ {k}} (u) = \\frac {u - \\operatorname* {p r o x} _ {\\mu t _ {k} \\| \\cdot \\| _ {1}} (u)}{\\mu t _ {k}}.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "8.3 近似点算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "379"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.157,
                0.444,
                0.174
            ],
            "angle": 0,
            "content": "那么，问题(8.3.9)的对偶问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.195,
                0.506,
                0.217
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {z} \\quad \\Phi_ {k} (z).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.233,
                0.691,
                0.251
            ],
            "angle": 0,
            "content": "设对偶问题的逼近最优解为 \\(z^{k + 1}\\) ，那么根据问题(8.3.9)的最优性条件，"
        },
        {
            "type": "equation",
            "bbox": [
                0.32,
                0.264,
                0.589,
                0.324
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{l} x ^ {k + 1} = \\operatorname {p r o x} _ {\\mu t _ {k} \\| \\cdot \\| _ {1}} \\left(x ^ {k} - t _ {k} A ^ {\\mathrm {T}} z ^ {k + 1}\\right), \\\\ y ^ {k + 1} = \\frac {1}{t _ {k} + 1} (y ^ {k} + t _ {k} z ^ {k + 1}). \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.339,
                0.737,
                0.377
            ],
            "angle": 0,
            "content": "综上，在第 \\(k\\) 步迭代，LASSO 问题 (8.3.7) 的近似点算法的迭代格式写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.32,
                0.38,
                0.737,
                0.472
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{l} z ^ {k + 1} \\approx \\underset {z} {\\arg \\max } \\quad \\Phi_ {k} (z), \\\\ x ^ {k + 1} = \\operatorname {p r o x} _ {\\mu t _ {k} \\| \\cdot \\| _ {1}} \\left(x ^ {k} - t _ {k} A ^ {\\mathrm {T}} z ^ {k + 1}\\right), \\\\ y ^ {k + 1} = \\frac {1}{t _ {k} + 1} \\left(y ^ {k} + t _ {k} z ^ {k + 1}\\right). \\end{array} \\right. \\tag {8.3.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.482,
                0.738,
                0.582
            ],
            "angle": 0,
            "content": "因为 \\(\\Phi_{k}(z)\\) 的最大值点的显式表达式是未知的，所以需要使用迭代算法近似求解．根据 \\(\\Phi_{k}(z)\\) 的连续可微性，我们可以调用梯度法进行求解．另外，还可以证明 \\(\\Phi_{k}(z)\\) 是半光滑的，从而调用半光滑牛顿法来更有效地求解，相关内容可以参考[119]．为了保证算法(8.3.10)的收敛性，根据文章[163]，我们采用以下不精确收敛准则："
        },
        {
            "type": "equation",
            "bbox": [
                0.194,
                0.597,
                0.522,
                0.633
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| \\nabla \\Phi_ {k} \\left(z ^ {k + 1}\\right) \\right\\| _ {2} \\leqslant \\sqrt {\\frac {\\alpha_ {k}}{t _ {k}}} \\varepsilon_ {k}, \\quad \\varepsilon_ {k} \\geqslant 0, \\sum_ {k = 1} ^ {\\infty} \\varepsilon_ {k} <   \\infty ,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.194,
                0.636,
                0.717,
                0.673
            ],
            "angle": 0,
            "content": "\\[\n\\| \\nabla \\Phi_ {k} (z ^ {k + 1}) \\| _ {2} \\leqslant \\sqrt {\\frac {\\alpha_ {k}}{t _ {k}}} \\delta_ {k} \\| (x ^ {k + 1}, y ^ {k + 1}) - (x ^ {k}, y ^ {k}) \\| _ {2}, \\quad \\delta_ {k} \\geqslant 0, \\sum_ {k = 1} ^ {\\infty} \\delta_ {k} <   + \\infty ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.688,
                0.737,
                0.725
            ],
            "angle": 0,
            "content": "其中 \\(\\varepsilon_{k}, \\delta_{k}\\) 是人为设定的参数，\\(\\alpha_{k}\\) 为 \\(\\Phi_{k}\\) 的强凹参数（即 \\(-\\Phi_{k}\\) 的强凸参数）的一个估计."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.731,
                0.739,
                0.853
            ],
            "angle": 0,
            "content": "我们用同第6.2节中一样的 \\(A\\) 和 \\(b\\)，分别取 \\(\\mu = 10^{-2}, 10^{-3}\\)，利用近似点算法来求解问题，取固定步长 \\(t_k \\stackrel{\\mathrm{def}}{=} t = 10^3\\)，其中子问题利用梯度下降法进行不精确求解，精度参数设置为 \\(\\alpha_k = \\frac{t}{t + 1}\\)，\\(\\varepsilon_k = \\delta_k = \\frac{8}{k^2}\\)，结果如图8.5所示。可以看到：近似点算法收敛所需要的外部迭代数很少，主要计算都在内迭代求解更新 \\(z\\) 的子问题上。对于 \\(z\\) 的子问题求解，我们利用了半光滑牛顿法来进行加速。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "380"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "image",
            "bbox": [
                0.356,
                0.154,
                0.73,
                0.379
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.4,
                0.39,
                0.684,
                0.407
            ],
            "angle": 0,
            "content": "图8.5 近似点算法求解LASSO问题"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.448,
                0.431,
                0.465
            ],
            "angle": 0,
            "content": "2. 逆协方差矩阵估计"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.493,
                0.71,
                0.51
            ],
            "angle": 0,
            "content": "第三章介绍了逆协方差矩阵估计问题，该问题可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.416,
                0.541,
                0.825,
                0.565
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X} \\quad \\langle S, X \\rangle - \\ln \\det  X + \\lambda \\| X \\| _ {1}, \\tag {8.3.11}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.59,
                0.825,
                0.627
            ],
            "angle": 0,
            "content": "其中 \\(S\\) 是已知的对称矩阵, 通常由样本协方差矩阵得到. 引入变量 \\(Y\\), 问题(8.3.11)可以等价转化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.294,
                0.655,
                0.825,
                0.681
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X, Y} \\psi (X, Y) \\stackrel {\\text {d e f}} {=} - \\ln \\det X + \\langle S, X \\rangle + \\lambda \\| Y \\| _ {1} + I _ {\\mathcal {D}} (X, Y), \\tag {8.3.12}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.708,
                0.698,
                0.726
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{D} = \\{(X,Y)\\mid X - Y = 0\\}\\) ， \\(I_{\\mathcal{D}}\\) 为集合 \\(\\mathcal{D}\\) 的示性函数"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.737,
                0.613,
                0.754
            ],
            "angle": 0,
            "content": "在第 \\(k\\) 步迭代，近似点算法求解如下问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.339,
                0.778,
                0.825,
                0.812
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X, Y} \\psi (X, Y) + \\frac {1}{2 t _ {k}} \\left(\\| X - X ^ {k} \\| _ {F} ^ {2} + \\| Y - Y ^ {k} \\| _ {F} ^ {2}\\right). \\tag {8.3.13}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "类似于 LASSO 问题的求解，我们通过求解问题 (8.3.13) 的对偶问题来构造"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "8.3 近似点算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "381"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.684,
                0.174
            ],
            "angle": 0,
            "content": "逼近解 \\(\\left(X^{k + 1},Y^{k + 1}\\right)\\) . 引入乘子 \\(Z\\) ，问题(8.3.13)的对偶函数可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.196,
                0.183,
                0.713,
                0.466
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\Phi_ {k} (Z) = \\inf  _ {X} \\left\\{- \\ln \\det X + \\langle Z, X \\rangle + \\frac {1}{2 t _ {k}} \\| X - X ^ {k} \\| _ {F} ^ {2} \\right\\} \\\\ + \\inf  _ {Y} \\left\\{\\lambda \\| Y \\| _ {1} + \\langle S - Z, Y \\rangle + \\frac {1}{2 t _ {k}} \\| Y - Y ^ {k} \\| _ {F} ^ {2} \\right\\} \\\\ = \\inf  _ {X} \\left\\{- \\ln \\det X + \\frac {1}{2 t _ {k}} \\| X - X ^ {k} + t _ {k} Z \\| _ {F} ^ {2} \\right\\} \\\\ - \\frac {1}{2 t _ {k}} \\left(\\| X ^ {k} - t _ {k} Z \\| _ {F} ^ {2} - \\| X ^ {k} \\| _ {F} ^ {2}\\right) \\\\ + \\inf  _ {Y} \\left\\{\\lambda \\| Y \\| _ {1} + \\frac {1}{2 t _ {k}} \\| Y - Y ^ {k} + t _ {k} (S - Z) \\| _ {F} ^ {2} \\right\\} \\\\ - \\frac {1}{2 t _ {k}} \\left(\\| Y _ {k} - t _ {k} (S - Z) \\| _ {F} ^ {2} - \\| Y ^ {k} \\| _ {F} ^ {2}\\right) \\\\ = \\Gamma_ {t _ {k}} ^ {1} \\left(X ^ {k} - t _ {k} Z\\right) - \\frac {1}{2 t _ {k}} \\left(\\| X ^ {k} - t _ {k} Z \\| _ {F} ^ {2} - \\| X ^ {k} \\| _ {F} ^ {2}\\right) \\\\ + \\lambda \\Gamma_ {\\lambda t _ {k}} ^ {2} (Y ^ {k} - t _ {k} (S - Z)) - \\frac {1}{2 t _ {k}} \\left(\\| Y _ {k} - t _ {k} (S - Z) \\| _ {F} ^ {2} - \\| Y ^ {k} \\| _ {F} ^ {2}\\right), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.472,
                0.216,
                0.488
            ],
            "angle": 0,
            "content": "其中，"
        },
        {
            "type": "equation",
            "bbox": [
                0.292,
                0.487,
                0.615,
                0.522
            ],
            "angle": 0,
            "content": "\\[\n\\Gamma_ {t _ {k}} ^ {1} (A) = \\inf  _ {X} \\left\\{- \\ln \\det  X + \\frac {1}{2 t _ {k}} \\| X - A \\| _ {F} ^ {2} \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.525,
                0.736,
                0.562
            ],
            "angle": 0,
            "content": "对于对称矩阵 \\(A\\) ，记其特征值分解为 \\(A = Q\\mathrm{Diag}(d_1,d_2,\\dots ,d_n)Q^{\\mathrm{T}}\\) ，以及定义函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.212,
                0.568,
                0.694,
                0.6
            ],
            "angle": 0,
            "content": "\\[\nq _ {t _ {k}} ^ {+} (v) = \\frac {1}{2} (\\sqrt {v ^ {2} + 4 t _ {k}} + v), q _ {t _ {k}} ^ {-} (v) = \\frac {1}{2} (\\sqrt {v ^ {2} + 4 t _ {k}} - v), v \\in \\mathbb {R},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.606,
                0.211,
                0.621
            ],
            "angle": 0,
            "content": "并记"
        },
        {
            "type": "equation",
            "bbox": [
                0.289,
                0.623,
                0.62,
                0.642
            ],
            "angle": 0,
            "content": "\\[\nA ^ {+} = Q \\operatorname {D i a g} \\left(q _ {t _ {k}} ^ {+} \\left(d _ {1}\\right), q _ {t _ {k}} ^ {+} \\left(d _ {2}\\right), \\dots , q _ {t _ {k}} ^ {+} \\left(d _ {n}\\right)\\right) Q ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.29,
                0.647,
                0.62,
                0.667
            ],
            "angle": 0,
            "content": "\\[\nA ^ {-} = Q \\operatorname {D i a g} \\left(q _ {t _ {k}} ^ {-} \\left(d _ {1}\\right), q _ {t _ {k}} ^ {-} \\left(d _ {2}\\right), \\dots , q _ {t _ {k}} ^ {-} \\left(d _ {n}\\right)\\right) Q ^ {\\mathrm {T}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.671,
                0.69,
                0.69
            ],
            "angle": 0,
            "content": "那么, \\(\\Gamma_{t_k}^1 (A)\\) 表达式中的最小值在 \\(X = A^{+}\\) 处取得, 相应的最小值为"
        },
        {
            "type": "equation",
            "bbox": [
                0.318,
                0.697,
                0.59,
                0.729
            ],
            "angle": 0,
            "content": "\\[\n\\Gamma_ {t _ {k}} ^ {1} (A) = - t _ {k} \\ln \\det  (A ^ {+}) + \\frac {1}{2} \\| A ^ {-} \\| _ {F} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.735,
                0.382,
                0.751
            ],
            "angle": 0,
            "content": "其是连续可微的，且梯度为"
        },
        {
            "type": "equation",
            "bbox": [
                0.373,
                0.763,
                0.534,
                0.782
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {A} \\Gamma_ {t _ {k}} ^ {1} (A) = A - A ^ {+}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.794,
                0.64,
                0.81
            ],
            "angle": 0,
            "content": "同样地，类似于 LASSO 问题中 \\(\\ell_1\\) 范数的最小化问题，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.251,
                0.816,
                0.657,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\Gamma_ {\\mu t _ {k}} ^ {2} (U) = \\inf  _ {Y} \\left\\{\\| Y \\| _ {1} + \\frac {1}{2 \\lambda t _ {k}} \\| Y - U \\| _ {2} ^ {2} \\right\\} = \\sum_ {i j} q _ {\\lambda t _ {k}} (U _ {i j}),\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "382"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.156,
                0.825,
                0.193
            ],
            "angle": 0,
            "content": "且相应的极小点为 \\(Y = \\mathrm{prox}_{\\lambda t_k\\| \\cdot \\| _1}(U)\\) .函数 \\(\\Gamma_{\\mu t_k}^2 (U)\\) 关于 \\(U\\) 是连续可微的，其梯度为"
        },
        {
            "type": "equation",
            "bbox": [
                0.416,
                0.194,
                0.667,
                0.23
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {U} \\Gamma_ {\\mu t _ {k}} ^ {2} (U) = \\frac {U - \\operatorname* {p r o x} _ {\\lambda t _ {k} \\| \\cdot \\| _ {1}} (U)}{\\lambda t _ {k}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.238,
                0.541,
                0.254
            ],
            "angle": 0,
            "content": "那么，问题(8.3.13)的对偶问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.489,
                0.272,
                0.594,
                0.294
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {Z} \\quad \\Phi_ {k} (Z).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.307,
                0.644,
                0.324
            ],
            "angle": 0,
            "content": "在第 \\(k\\) 步迭代, 问题(8.3.11)的近似点算法的格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.389,
                0.335,
                0.693,
                0.419
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{ \\begin{array}{l} Z ^ {k + 1} \\approx \\underset {Z} {\\arg \\max } \\Phi_ {k} (Z), \\\\ X ^ {k + 1} = \\operatorname {p r o x} _ {- t _ {k} \\ln \\det  (\\cdot)} \\left(X ^ {k} - t _ {k} Z ^ {k + 1}\\right), \\\\ Y ^ {k + 1} = \\operatorname {p r o x} _ {\\lambda t _ {k} \\| \\cdot \\| _ {1}} \\left(Y ^ {k} - t _ {k} (S - Z ^ {k + 1})\\right). \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.43,
                0.825,
                0.467
            ],
            "angle": 0,
            "content": "注意到 \\(\\Phi_{k}\\) 不是强凹函数，我们往往会减去近似点项以保证强凹性，即将上式的第一步改成求解优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.478,
                0.738,
                0.511
            ],
            "angle": 0,
            "content": "\\[\nZ ^ {k + 1} \\approx \\underset {Z} {\\arg \\max } \\quad \\hat {\\Phi} _ {k} (Z) \\stackrel {{\\mathrm {d e f}}} {{=}} \\Phi_ {k} (Z) - \\frac {1}{2 t _ {k}} \\| Z - Z ^ {k} \\| _ {F} ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.526,
                0.825,
                0.605
            ],
            "angle": 0,
            "content": "因为 \\(\\hat{\\Phi}_k(Z)\\) 的最大值点的显式表达式是未知的，所以需要使用迭代算法近似求解．根据 \\(\\hat{\\Phi}_k(Z)\\) 的连续可微性，我们可以调用梯度法进行求解．另外，还可以证明 \\(\\hat{\\Phi}_k(Z)\\) 是半光滑的，从而调用半光滑牛顿法来更有效地求解[203,213]．根据文章[163]，我们采用以下不精确收敛准则："
        },
        {
            "type": "equation",
            "bbox": [
                0.282,
                0.617,
                0.805,
                0.693
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\| \\nabla \\hat {\\Phi} _ {k} (Z ^ {k + 1}) \\| _ {2} \\leqslant \\sqrt {\\frac {\\alpha}{t _ {k}}} \\varepsilon_ {k}, \\varepsilon_ {k} \\geqslant 0, \\sum_ {k = 1} ^ {\\infty} \\varepsilon_ {k} <   + \\infty . \\\\ \\| \\nabla \\hat {\\Phi} _ {k} (Z ^ {k + 1}) \\| _ {2} \\leqslant \\sqrt {\\frac {\\alpha}{t _ {k}}} \\delta_ {k} \\| (X ^ {k + 1}, Y ^ {k + 1}) - (X ^ {k}, Y ^ {k}) \\| _ {F}, \\delta_ {k} \\geqslant 0, \\sum_ {k = 1} ^ {\\infty} \\delta_ {k} <   + \\infty , \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.707,
                0.825,
                0.754
            ],
            "angle": 0,
            "content": "其中 \\(\\varepsilon_{k}, \\delta_{k}\\) 是人为设定的参数，\\(\\alpha\\) 为 \\(\\hat{\\Phi}_{k}\\) 的强凹参数的一个估计，可取为 \\(\\frac{1}{t_{k}}\\)。更多内容可以参考 [203, 213]。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.783,
                0.421,
                0.801
            ],
            "angle": 0,
            "content": "8.3.4 收敛性分析"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "这一小节给出近似点算法的收敛性分析. 首先我们给出近似点算法收敛的基本定理."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "8.3 近似点算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "383"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.195
            ],
            "angle": 0,
            "content": "定理8.7 设 \\(\\psi\\) 是适当的闭凸函数（从而 \\(\\mathrm{prox}_{t\\psi}(x)\\) 对任意的 \\(x\\) 存在且唯一），最优值 \\(\\psi^{*}\\) 有限且在点 \\(x^{*}\\) 处可达，则对近似点算法有"
        },
        {
            "type": "equation",
            "bbox": [
                0.317,
                0.199,
                0.592,
                0.254
            ],
            "angle": 0,
            "content": "\\[\n\\psi (x ^ {k}) - \\psi^ {*} \\leqslant \\frac {\\| x ^ {0} - x ^ {*} \\| _ {2} ^ {2}}{2 \\sum_ {i = 1} ^ {k} t _ {i}}, \\quad \\forall k \\geqslant 1.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.273,
                0.737,
                0.332
            ],
            "angle": 0,
            "content": "证明．由于近似点算法可看做近似点梯度法的特殊情况，我们的证明也将沿用近似点梯度法中的分析过程，这里仅仅指出关键步骤的不同之处．由于 \\(f(x) = 0\\) ，所以对任意的 \\(t > 0\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.282,
                0.336,
                0.627,
                0.367
            ],
            "angle": 0,
            "content": "\\[\nf (x - t G _ {t} (x)) \\leqslant t \\nabla f (x) ^ {\\mathrm {T}} G _ {t} (x) + \\frac {t}{2} \\| G _ {t} (x) \\| _ {2} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.371,
                0.694,
                0.389
            ],
            "angle": 0,
            "content": "其中 \\(G_{t}(x)\\) 的定义见 (8.1.9) 式．运用近似点梯度法中的证明，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.276,
                0.394,
                0.632,
                0.424
            ],
            "angle": 0,
            "content": "\\[\nt _ {i} \\left(\\psi \\left(x ^ {i}\\right) - \\psi^ {*}\\right) \\leqslant \\frac {1}{2} \\left(\\| x ^ {i - 1} - x ^ {*} \\| _ {2} ^ {2} - \\| x ^ {i} - x ^ {*} \\| _ {2} ^ {2}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.429,
                0.453,
                0.448
            ],
            "angle": 0,
            "content": "且 \\(\\{\\psi (x^i)\\}\\) 是单调下降的序列．因此"
        },
        {
            "type": "equation",
            "bbox": [
                0.241,
                0.451,
                0.668,
                0.495
            ],
            "angle": 0,
            "content": "\\[\n\\left(\\sum_ {i = 1} ^ {k} t _ {i}\\right) \\left(\\psi \\left(x ^ {k}\\right) - \\psi^ {*}\\right) \\leqslant \\sum_ {i = 1} ^ {k} t _ {i} \\left(\\psi \\left(x ^ {i}\\right) - \\psi^ {*}\\right) \\leqslant \\frac {1}{2} \\| x ^ {0} - x ^ {*} \\| _ {2} ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.499,
                0.414,
                0.516
            ],
            "angle": 0,
            "content": "这样就完成了对定理8.7的证明"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.528,
                0.74,
                0.693
            ],
            "angle": 0,
            "content": "上述定理使得我们可以通过每次迭代的步长来控制算法的收敛性．若 \\(\\sum_{i = 1}^{k}t_{i}\\rightarrow +\\infty\\) ，则算法收敛．特别地，如果步长 \\(t_i\\) 固定或有正下界，则收敛速度为 \\(\\mathcal{O}\\left(\\frac{1}{k}\\right)\\) 与定理8.3不同，由于 \\(f(x) = 0\\) ，我们无需对步长 \\(t_i\\) 提出额外的要求，即每一步的 \\(t_i\\) 可以为任意值．理论上可以取充分大的 \\(t_i\\) 使得近似点算法经过较少的迭代步即可收敛，但这种做法实际的意义并不大．这是因为过大的 \\(t_i\\) 会导致子问题难以求解，从近似点算法的形式也可以看出，当\\(t_i = +\\infty\\) 时子问题与原问题是等价的."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.697,
                0.49,
                0.713
            ],
            "angle": 0,
            "content": "类似地，可以给出加速版本的收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.723,
                0.737,
                0.782
            ],
            "angle": 0,
            "content": "定理8.8 设 \\(\\psi\\) 是适当的闭凸函数, 最优值 \\(\\psi^{*}\\) 有限且在点 \\(x^{*}\\) 处达到. 假设参数 \\(t_{k}, \\gamma_{k}\\) 按照第8.3.1小节中的策略1或者策略2选取, 那么迭代序列 \\(\\{\\psi(x^{k})\\}\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.295,
                0.785,
                0.612,
                0.843
            ],
            "angle": 0,
            "content": "\\[\n\\psi \\left(x ^ {k}\\right) - \\psi^ {*} \\leqslant \\frac {2 \\| x ^ {0} - x ^ {*} \\| _ {2} ^ {2}}{\\left(2 \\sqrt {t _ {1}} + \\sum_ {i = 2} ^ {k} \\sqrt {t _ {i}}\\right) ^ {2}}, \\quad k \\geqslant 1.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "384"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.825,
                0.195
            ],
            "angle": 0,
            "content": "证明．在 \\(f(x) = 0\\) 的情况下使用Nesterov加速算法中的证明，这里仅指出关键步骤的不同之处．由于 \\(f(x) = 0,\\) 对任意的 \\(t > 0\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.341,
                0.206,
                0.741,
                0.236
            ],
            "angle": 0,
            "content": "\\[\nf (x) \\leqslant f (y) + \\nabla f (y) ^ {\\mathrm {T}} (x - y) + \\frac {1}{2 t} \\| x - y \\| _ {2} ^ {2}, \\quad \\forall x, y,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.246,
                0.332,
                0.263
            ],
            "angle": 0,
            "content": "于是结论"
        },
        {
            "type": "equation",
            "bbox": [
                0.435,
                0.26,
                0.649,
                0.295
            ],
            "angle": 0,
            "content": "\\[\n\\psi \\left(x ^ {k}\\right) - \\psi^ {*} \\leqslant \\frac {\\gamma_ {k} ^ {2}}{2 t _ {k}} \\| x ^ {0} - x ^ {*} \\| _ {2} ^ {2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.3,
                0.61,
                0.332
            ],
            "angle": 0,
            "content": "依然成立．对于固定步长 \\(t_k = t\\) 和 \\(\\gamma_{k} = \\frac{2}{k + 1}\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.479,
                0.341,
                0.605,
                0.377
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\gamma_ {k} ^ {2}}{2 t _ {k}} = \\frac {2}{(k + 1) ^ {2} t};\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.387,
                0.496,
                0.404
            ],
            "angle": 0,
            "content": "而对于变步长，根据(8.2.5)式，"
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.413,
                0.641,
                0.471
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\gamma_ {k} ^ {2}}{2 t _ {k}} \\leqslant \\frac {2}{\\left(2 \\sqrt {t _ {1}} + \\sum_ {i = 2} ^ {k} \\sqrt {t _ {i}}\\right) ^ {2}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.479,
                0.825,
                0.517
            ],
            "angle": 0,
            "content": "对于以上两种参数选择策略，分别将对应不等式代入 (8.3.4) 式就得到定理中的结论."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.526,
                0.825,
                0.613
            ],
            "angle": 0,
            "content": "这个定理意味着当 \\(\\sum_{i=1}^{k} \\sqrt{t_i} \\to +\\infty\\) 时算法收敛，且步长 \\(t_i\\) 取固定值或有正下界时，其收敛速度可达到 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\)。同样地，即使理论上可以取 \\(t_i\\) 为任意大的值，实际上仍需控制其上界以便子问题可快速求解。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.638,
                0.522,
                0.656
            ],
            "angle": 0,
            "content": "8.3.5 Moreau-Yosida 正则化"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.669,
                0.825,
                0.707
            ],
            "angle": 0,
            "content": "这一小节介绍Moreau-Yosida正则化，并通过此概念从另一个角度理解近似点算法．首先我们给出Moreau-Yosida正则化的定义."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.72,
                0.825,
                0.779
            ],
            "angle": 0,
            "content": "定义8.4 (Moreau-Yosida正则化) 设 \\(f(x)\\) 是适当的闭凸函数, \\(t > 0\\) 为给定的常数, 则 \\(f\\) 的以 \\(t\\) 为参数的 Moreau-Yosida 正则化 \\(f_{(t)}(x)\\) (又称为 Moreau envelope) 定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.373,
                0.79,
                0.824,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f _ {(t)} (x) = \\inf  _ {u} \\left\\{f (u) + \\frac {1}{2 t} \\| u - x \\| _ {2} ^ {2} \\right\\} \\tag {8.3.14} \\\\ = f \\left(\\operatorname {p r o x} _ {t f} (x)\\right) + \\frac {1}{2 t} \\| \\operatorname {p r o x} _ {t f} (x) - x \\| _ {2} ^ {2}. \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.301,
                0.133
            ],
            "angle": 0,
            "content": "8.3 近似点算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "385"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.739,
                0.195
            ],
            "angle": 0,
            "content": "根据定义，Moreau-Yosida正则化实际上是将最优解 \\(u = \\mathrm{prox}_{tf}(x)\\) 代回了与之对应的优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.355,
                0.2,
                0.554,
                0.232
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {u} f (u) + \\frac {1}{2 t} \\| u - x \\| _ {2} ^ {2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.237,
                0.741,
                0.275
            ],
            "angle": 0,
            "content": "中而得到的关于 \\(x\\) 的函数. 容易验证, \\(f_{(t)}(x)\\) 的定义域为 \\(\\mathbb{R}^n\\) (利用定理8.1), 且为凸函数 (定理2.13中的性质(8))."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.279,
                0.731,
                0.298
            ],
            "angle": 0,
            "content": "当 \\(f\\) 具有特殊形式时，\\(f_{(t)}(x)\\) 可以计算出显式解．下面给出一些例子."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.307,
                0.596,
                0.324
            ],
            "angle": 0,
            "content": "例8.6 示性函数与 \\(\\ell_1\\) 范数的Moreau-Yosida正则化"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.327,
                0.741,
                0.373
            ],
            "angle": 0,
            "content": "(1) \\(f\\) 是集合 \\(C\\) 上的示性函数，即 \\(f(x) = I_{C}(x)\\)，则 \\(f_{(t)}(x) = \\frac{1}{2t}\\mathrm{dist}^2 (x,C)\\)，其中 \\(\\mathrm{dist}(x,C)\\) 是点 \\(x\\) 与集合 \\(C\\) 的欧几里得距离："
        },
        {
            "type": "equation",
            "bbox": [
                0.337,
                0.379,
                0.608,
                0.485
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f _ {(t)} (x) = \\inf  _ {u} \\left\\{I _ {C} (u) + \\frac {1}{2 t} \\| u - x \\| _ {2} ^ {2} \\right\\} \\\\ = \\inf  _ {u \\in C} \\left\\{\\frac {1}{2 t} \\| u - x \\| _ {2} ^ {2} \\right\\} \\\\ = \\frac {1}{2 t} \\mathrm {d i s t} ^ {2} (x, C). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.495,
                0.566,
                0.514
            ],
            "angle": 0,
            "content": "(2) 设 \\( f(x) = \\| x\\| _1 \\)，则 \\( f_{(t)}(x) \\) 是Huber损失函数："
        },
        {
            "type": "equation",
            "bbox": [
                0.4,
                0.522,
                0.55,
                0.559
            ],
            "angle": 0,
            "content": "\\[\nf _ {(t)} (x) = \\sum_ {k = 1} ^ {n} \\phi_ {t} (x _ {k}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.566,
                0.251,
                0.582
            ],
            "angle": 0,
            "content": "其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.58,
                0.586,
                0.64
            ],
            "angle": 0,
            "content": "\\[\n\\phi_ {t} (z) = \\left\\{ \\begin{array}{l l} \\frac {z ^ {2}}{2 t}, & | z | \\leqslant t, \\\\ | z | - \\frac {t}{2}, & | z | > t. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.643,
                0.638,
                0.661
            ],
            "angle": 0,
            "content": "注意到此时 \\(f_{(t)}(x)\\) 定义式中右端目标函数是分量可分的，"
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.668,
                0.62,
                0.783
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f _ {(t)} (x) = \\min  _ {u} \\left\\{\\| u \\| _ {1} + \\frac {1}{2 t} \\| u - x \\| _ {2} ^ {2} \\right\\} \\\\ = \\sum_ {k = 1} ^ {n} \\min  _ {u _ {k}} \\left\\{\\left| u _ {k} \\right| + \\frac {1}{2 t} \\left(u _ {k} - x _ {k}\\right) ^ {2} \\right\\} \\\\ = \\sum_ {k = 1} ^ {n} \\phi_ {t} (x _ {k}). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.795,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "在例8.6中我们观察到，虽然函数 \\( f(x) \\) 可能为不光滑甚至不连续的函数，其Moreau-Yosida正则化 \\( f_{(t)}(x) \\) 却是光滑的．实际上，这对一般的适当闭凸函数也是成立的，即有如下结果："
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "386"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.827,
                0.196
            ],
            "angle": 0,
            "content": "定理8.9 (Moreau-Yosida正则化的可微性) 设 \\(f(x)\\) 为适当的闭凸函数, \\(f_{(t)}(x)\\) 为其Moreau-Yosida正则化, 则 \\(f_{(t)}(x)\\) 在全空间可微, 且其梯度为"
        },
        {
            "type": "equation",
            "bbox": [
                0.442,
                0.216,
                0.641,
                0.252
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f _ {(t)} (x) = \\frac {x - \\operatorname {p r o x} _ {t f} (x)}{t}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.275,
                0.561,
                0.294
            ],
            "angle": 0,
            "content": "证明. 考虑 \\(f_{(t)}(x)\\) 的共轭函数 \\(f_{(t)}^{*}(y)\\):"
        },
        {
            "type": "equation",
            "bbox": [
                0.357,
                0.317,
                0.723,
                0.488
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f _ {(t)} ^ {*} (y) = \\sup  _ {x} \\left\\{y ^ {T} x - f _ {(t)} (x) \\right\\} \\\\ = \\sup  _ {x} \\left\\{y ^ {T} x - \\inf  _ {u} \\left\\{f (u) + \\frac {1}{2 t} \\| u - x \\| _ {2} ^ {2} \\right\\} \\right\\} \\\\ = \\sup  _ {x} \\sup  _ {u} \\left\\{y ^ {T} x - f (u) - \\frac {1}{2 t} \\| u - x \\| _ {2} ^ {2} \\right\\} \\\\ = \\sup  _ {u} \\left\\{y ^ {T} u - f (u) \\right\\} + \\frac {t}{2} \\| y \\| _ {2} ^ {2} \\\\ = f ^ {*} (y) + \\frac {t}{2} \\| y \\| _ {2} ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.508,
                0.825,
                0.547
            ],
            "angle": 0,
            "content": "根据 [149] 中的结论, \\(f_{(t)}\\) 满足自共轭性, 即 \\(f_{(t)} = f_{(t)}^{**}\\). 利用这个性质可以对 \\(f_{(t)}(x)\\) 的形式进行改写:"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.567,
                0.687,
                0.641
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f _ {(t)} (x) = f _ {(t)} ^ {* *} (x) = \\sup  _ {y} \\left\\{x ^ {\\mathrm {T}} y - f _ {(t)} ^ {*} (y) \\right\\} \\\\ = \\sup  _ {y} \\left\\{x ^ {\\mathrm {T}} y - f ^ {*} (y) - \\frac {t}{2} \\| y \\| _ {2} ^ {2} \\right\\}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.661,
                0.825,
                0.7
            ],
            "angle": 0,
            "content": "上式右端函数的最大值点 \\(y\\) 存在唯一，且满足 \\(x \\in \\partial f_{(t)}^{*}(y)\\)。根据命题8.2知，对任意 \\(x\\) 存在唯一的 \\(y\\) 使得 \\(y \\in \\partial f_{(t)}(x)\\)，即 \\(f_{(t)}(x)\\) 是可微函数。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.707,
                0.719,
                0.725
            ],
            "angle": 0,
            "content": "为了推导 \\(\\nabla f_{(t)}(x)\\) ，我们首先对 \\(f_{(t)}(x)\\) 的形式进行改写："
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.745,
                0.72,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f _ {(t)} (x) = \\inf  _ {u} \\left\\{f (u) + \\frac {1}{2 t} \\| u - x \\| _ {2} ^ {2} \\right\\} \\\\ = \\frac {\\| x \\| _ {2} ^ {2}}{2 t} - \\frac {1}{t} \\sup  _ {u} \\left\\{x ^ {\\mathrm {T}} u - t f (u) - \\frac {1}{2} \\| u \\| _ {2} ^ {2} \\right\\} \\\\ = \\frac {\\| x \\| _ {2} ^ {2}}{2 t} - \\frac {1}{t} \\left(t f + \\frac {1}{2} \\| \\cdot \\| _ {2} ^ {2}\\right) ^ {*} (x). \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "387"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.38,
                0.174
            ],
            "angle": 0,
            "content": "再次利用命题8.2可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.241,
                0.179,
                0.736,
                0.325
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\nabla f _ {(t)} (x) = \\frac {x}{t} - \\frac {1}{t} \\nabla \\left(t f + \\frac {1}{2} \\| \\cdot \\| _ {2} ^ {2}\\right) ^ {*} (x) \\\\ = \\frac {x}{t} - \\frac {1}{t} \\operatorname {a r g m a x} _ {u} \\left\\{x ^ {\\mathrm {T}} u - t f (u) - \\frac {1}{2} \\| u \\| _ {2} ^ {2} \\right\\} \\tag {8.3.15} \\\\ = \\frac {x}{t} - \\frac {1}{t} \\underset {u} {\\operatorname {a r g m i n}} \\left\\{t f (u) + \\frac {1}{2} \\| u - x \\| _ {2} ^ {2} \\right\\} \\\\ = \\frac {x - \\operatorname {p r o x} _ {t f} (x)}{t}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.721,
                0.33,
                0.737,
                0.342
            ],
            "angle": 0,
            "content": "□"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.358,
                0.739,
                0.425
            ],
            "angle": 0,
            "content": "定理8.9说明了Moreau-Yosida正则化是原函数 \\(f\\) 的一个光滑化函数，且拥有光滑化参数 \\(t\\)。实际上，利用本章后面的结果（引理8.5）可进一步说明 \\(\\nabla f_{(t)}(x)\\) 是利普希茨连续的（参数为 \\(\\frac{1}{t}\\)）。如果考虑如下优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.294,
                0.429,
                0.737,
                0.464
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad f _ {(t)} (x) = \\inf  _ {u} \\left\\{f (u) + \\frac {1}{2 t} \\| u - x \\| _ {2} ^ {2} \\right\\}, \\tag {8.3.16}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.468,
                0.74,
                0.526
            ],
            "angle": 0,
            "content": "该问题和最小化 \\( f(x) \\) 同解，但相比直接最小化 \\( f(x) \\) 来说，\\( f_{(t)}(x) \\) 具有可微且梯度利普希茨连续的优点。我们可以对 (8.3.16) 式使用固定步长 \\( (t_k = t) \\) 的梯度下降法求解："
        },
        {
            "type": "equation",
            "bbox": [
                0.316,
                0.536,
                0.591,
                0.557
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - t \\nabla f _ {(t)} (x ^ {k}) = \\operatorname {p r o x} _ {t f} (x ^ {k}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.565,
                0.738,
                0.624
            ],
            "angle": 0,
            "content": "这就是近似点算法的迭代格式 (8.3.3). 综上所述, 近似点算法可以理解为先使用 Moreau-Yosida 正则化对目标函数进行光滑化, 之后再对光滑化的目标函数应用梯度下降导出的算法."
        },
        {
            "type": "title",
            "bbox": [
                0.337,
                0.653,
                0.571,
                0.674
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.691,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "在许多实际的优化问题中，人们所考虑的目标函数虽然有成千上万的自变量，对这些变量联合求解目标函数的极小值通常很困难，但这些自变量具有某种“可分离”的形式：当固定其中若干变量时，函数的结构会得到极大的简化。这种特殊的形式使得人们可以将原问题拆分成数个只有少数自变量的子问题。分块坐标下降法（block coordinate descent，BCD）正是利用了这样的思想来求解这种具有特殊结构的优化问题，在多数实际问题中有良好的数值表现。本节介绍分块坐标下降法的基本迭代格式和一些最近的收敛性结果，同时给出一些例子来说明其在具体问题上的应用。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "388"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.402,
                0.174
            ],
            "angle": 0,
            "content": "8.4.1 问题描述"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.187,
                0.495,
                0.204
            ],
            "angle": 0,
            "content": "考虑具有如下形式的问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.211,
                0.825,
                0.247
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathcal {X}} F (x _ {1}, x _ {2}, \\dots , x _ {s}) = f (x _ {1}, x _ {2}, \\dots , x _ {s}) + \\sum_ {i = 1} ^ {s} r _ {i} (x _ {i}), \\tag {8.4.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.254,
                0.827,
                0.312
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{X}\\) 是函数的可行域，这里将自变量 \\(x\\) 拆分成 \\(s\\) 个变量块 \\(x_{1}, x_{2}, \\dots, x_{s}\\)，每个变量块 \\(x_{i} \\in \\mathbb{R}^{n_{i}}\\)。函数 \\(f\\) 是关于 \\(x\\) 的可微函数，每个 \\(r_{i}(x_{i})\\) 关于 \\(x_{i}\\) 是适当的闭凸函数，但不一定可微。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.316,
                0.825,
                0.417
            ],
            "angle": 0,
            "content": "在问题 (8.4.1) 中，目标函数 \\(F\\) 的性质体现在 \\(f\\) ，每个 \\(r_i\\) 以及自变量的分块上．通常情况下，\\(f\\) 对于所有变量块 \\(x_i\\) 不可分，但单独考虑每一块自变量时，\\(f\\) 有简单结构；\\(r_i\\) 只和第 \\(i\\) 个自变量块有关，因此 \\(r_i\\) 在目标函数中是一个可分项．求解问题 (8.4.1) 的难点在于如何利用分块结构处理不可分的函数 \\(f\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.426,
                0.825,
                0.464
            ],
            "angle": 0,
            "content": "注8.2 在给出问题(8.4.1)时，唯一引入凸性的部分是 \\(r_i\\)。其余部分没有引入凸性，可行域 \\(\\mathcal{X}\\) 不是一定是凸集，\\(f\\) 也不一定是凸函数。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.474,
                0.826,
                0.533
            ],
            "angle": 0,
            "content": "需要指出的是, 并非所有问题都适合按照问题 (8.4.1) 进行处理. 下面给出六个可以化成问题 (8.4.1) 的实际例子, 第 8.4.3 小节将介绍如何使用分块坐标下降法求解它们."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.543,
                0.825,
                0.642
            ],
            "angle": 0,
            "content": "例8.7（分组LASSO[179]）考虑线性模型 \\(b = a^{\\mathrm{T}}x + \\varepsilon\\) ，现在对 \\(x\\) 使用分组LASSO模型建模．设矩阵 \\(A\\in \\mathbb{R}^{n\\times p}\\) 和向量 \\(b\\in \\mathbb{R}^n\\) 分别由上述模型中自变量和响应变量的 \\(n\\) 组观测值组成．参数 \\(x = (x_{1},x_{2},\\dots ,x_{G})\\in \\mathbb{R}^{p}\\) 可以分成 \\(G\\) 组，且 \\(\\{x_i\\}_{i = 1}^G\\) 中只有少数的非零向量．则分组LASSO对应的优化问题可表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.399,
                0.642,
                0.684,
                0.679
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\frac {1}{2 n} \\| b - A x \\| _ {2} ^ {2} + \\lambda \\sum_ {i = 1} ^ {G} \\sqrt {p _ {i}} \\| x _ {i} \\| _ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.683,
                0.547,
                0.699
            ],
            "angle": 0,
            "content": "在这个例子中待优化的变量共有 \\(G\\) 块"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.71,
                0.829,
                0.728
            ],
            "angle": 0,
            "content": "例8.8(聚类问题)在第三章中我们介绍了 \\(K\\) -均值聚类问题的等价形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.319,
                0.745,
                0.468,
                0.769
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {\\Phi , H} \\| A - \\Phi H \\| _ {F} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.329,
                0.77,
                0.825,
                0.789
            ],
            "angle": 0,
            "content": "s.t. \\(\\Phi \\in \\mathbb{R}^{n\\times k}\\) ,每一行只有一个元素为1，其余为0, (8.4.2)"
        },
        {
            "type": "equation",
            "bbox": [
                0.368,
                0.791,
                0.446,
                0.807
            ],
            "angle": 0,
            "content": "\\[\nH \\in \\mathbb {R} ^ {k \\times p}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "这是一个矩阵分解问题，自变量总共有两块。注意到变量 \\(\\Phi\\) 取值在离散空间上，因此聚类问题不是凸问题。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "389"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.737,
                0.195
            ],
            "angle": 0,
            "content": "例8.9（低秩矩阵恢复[160]）设 \\(b \\in \\mathbb{R}^m\\) 是已知的观测向量，\\(\\mathcal{A}\\) 是线性映射．考虑求解下面的极小化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.292,
                0.203,
                0.617,
                0.235
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X, Y} \\frac {1}{2} \\| \\mathcal {A} (X Y) - b \\| _ {2} ^ {2} + \\alpha \\| X \\| _ {F} ^ {2} + \\beta \\| Y \\| _ {F} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.244,
                0.737,
                0.281
            ],
            "angle": 0,
            "content": "其中 \\(\\alpha, \\beta > 0\\) 为正则化参数。这里正则化的作用是消除解 \\((X,Y)\\) 在放缩意义下的不唯一性。在这个例子中自变量共有两块。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.294,
                0.559,
                0.312
            ],
            "angle": 0,
            "content": "类似的例子还有非负矩阵分解与非负张量分解"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.323,
                0.737,
                0.358
            ],
            "angle": 0,
            "content": "例 8.10 (非负矩阵分解 [148]) 设 \\(M\\) 是已知矩阵, 考虑求解如下极小化问题:"
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.358,
                0.609,
                0.389
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X, Y \\geqslant 0} \\frac {1}{2} \\| X Y - M \\| _ {F} ^ {2} + \\alpha r _ {1} (X) + \\beta r _ {2} (X).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.395,
                0.559,
                0.411
            ],
            "angle": 0,
            "content": "在这个例子中自变量共有两块，且均有非负的约束。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.423,
                0.737,
                0.461
            ],
            "angle": 0,
            "content": "例8.11（非负张量分解[194]）设 \\(\\mathcal{M}\\) 是已知张量，考虑求解如下极小化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.241,
                0.469,
                0.667,
                0.508
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {A _ {1}, A _ {2}, \\dots , A _ {N} \\geqslant 0} \\frac {1}{2} \\| \\mathcal {M} - A _ {1} \\circ A _ {2} \\circ \\dots \\circ A _ {N} \\| _ {F} ^ {2} + \\sum_ {i = 1} ^ {N} \\lambda_ {i} r _ {i} (A _ {i}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.517,
                0.672,
                0.534
            ],
            "angle": 0,
            "content": "其中“\\(\\circ\\)”表示张量的外积运算。在这个例子中自变量的块数为 \\(N\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.546,
                0.655,
                0.563
            ],
            "angle": 0,
            "content": "在第四章中我们提到了字典学习问题，它也具有形式(8.4.1)."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.576,
                0.737,
                0.633
            ],
            "angle": 0,
            "content": "例8.12（字典学习）设 \\(A \\in \\mathbb{R}^{m \\times n}\\) 为 \\(n\\) 个观测，每个观测的信号维数是 \\(m\\)，现在我们要从 \\(A\\) 中学习出一个字典 \\(D \\in \\mathbb{R}^{m \\times k}\\) 和系数矩阵 \\(X \\in \\mathbb{R}^{k \\times n}\\)，使之为如下问题的解："
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.641,
                0.574,
                0.692
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {D, X} \\frac {1}{2 n} \\| D X - A \\| _ {F} ^ {2} + \\lambda \\| X \\| _ {1}, \\\\ \\begin{array}{l} \\text {s . t .} \\quad \\| D \\| _ {F} \\leqslant 1. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.703,
                0.731,
                0.721
            ],
            "angle": 0,
            "content": "在这里自变量有两块，分别为 \\(D\\) 和 \\(X\\) ，此外对 \\(D\\) 还存在球约束 \\(\\| D\\| _F\\leqslant 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.732,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "上述的所有例子中，函数 \\( f \\) 关于变量全体一般是非凸的，这使得求解问题(8.4.1)变得很有挑战性。首先，应用在非凸问题上的算法的收敛性不易分析，很多针对凸问题设计的算法通常会失效；其次，目标函数的整体结构十分复杂，这使得变量的更新需要很大计算量。对于这类问题，我们最终的目标是要设计一种算法，它具有简单的变量更新格式，同时具有一定的（全局）收敛性。而分块坐标下降法则是处理这类问题较为有效的算法。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "390"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.402,
                0.174
            ],
            "angle": 0,
            "content": "8.4.2 算法结构"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.188,
                0.827,
                0.268
            ],
            "angle": 0,
            "content": "考虑问题(8.4.1)，我们所感兴趣的分块坐标下降法具有如下更新方式：按照 \\(x_{1}, x_{2}, \\dots, x_{s}\\) 的次序依次固定其他 \\((s - 1)\\) 块变量极小化 \\(F\\)，完成一块变量的极小化后，它的值便立即被更新到变量空间中，更新下一块变量时将使用每个变量最新的值．根据这种更新方式定义辅助函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.388,
                0.283,
                0.696,
                0.304
            ],
            "angle": 0,
            "content": "\\[\nf _ {i} ^ {k} (x _ {i}) = f (x _ {1} ^ {k}, \\dots , x _ {i - 1} ^ {k}, x _ {i}, x _ {i + 1} ^ {k - 1}, \\dots , x _ {s} ^ {k - 1}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.318,
                0.825,
                0.399
            ],
            "angle": 0,
            "content": "其中 \\(x_{j}^{k}\\) 表示在第 \\(k\\) 次迭代中第 \\(j\\) 块自变量的值，\\(x_{i}\\) 是函数的自变量。函数 \\(f_{i}^{k}\\) 表示在第 \\(k\\) 次迭代更新第 \\(i\\) 块变量时所需要考虑的目标函数的光滑部分。考虑第 \\(i\\) 块变量时前 \\((i - 1)\\) 块变量已经完成更新，因此上标为 \\(k\\)，而后面下标从 \\((i + 1)\\) 起的变量仍为旧的值，因此上标为 \\((k - 1)\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.403,
                0.682,
                0.42
            ],
            "angle": 0,
            "content": "在每一步更新中，通常使用以下三种更新格式之一："
        },
        {
            "type": "equation",
            "bbox": [
                0.295,
                0.434,
                0.825,
                0.466
            ],
            "angle": 0,
            "content": "\\[\nx _ {i} ^ {k} = \\underset {x _ {i} \\in \\mathcal {X} _ {i} ^ {k}} {\\arg \\min } \\left\\{f _ {i} ^ {k} \\left(x _ {i}\\right) + r _ {i} \\left(x _ {i}\\right) \\right\\}, \\tag {8.4.3}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.295,
                0.47,
                0.825,
                0.51
            ],
            "angle": 0,
            "content": "\\[\nx _ {i} ^ {k} = \\underset {x _ {i} \\in \\mathcal {X} _ {i} ^ {k}} {\\arg \\min } \\left\\{f _ {i} ^ {k} \\left(x _ {i}\\right) + \\frac {L _ {i} ^ {k - 1}}{2} \\| x _ {i} - x _ {i} ^ {k - 1} \\| _ {2} ^ {2} + r _ {i} \\left(x _ {i}\\right) \\right\\}, \\tag {8.4.4}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.295,
                0.514,
                0.825,
                0.555
            ],
            "angle": 0,
            "content": "\\[\nx _ {i} ^ {k} = \\underset {x _ {i} \\in \\mathcal {X} _ {i} ^ {k}} {\\arg \\min } \\left\\{\\langle \\hat {g} _ {i} ^ {k}, x _ {i} - \\hat {x} _ {i} ^ {k - 1} \\rangle + \\frac {L _ {i} ^ {k - 1}}{2} \\| x _ {i} - \\hat {x} _ {i} ^ {k - 1} \\| _ {2} ^ {2} + r _ {i} (x _ {i}) \\right\\}, \\tag {8.4.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.568,
                0.412,
                0.586
            ],
            "angle": 0,
            "content": "其中 \\(L_{i}^{k} > 0\\) 为常数，"
        },
        {
            "type": "equation",
            "bbox": [
                0.345,
                0.602,
                0.737,
                0.622
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {X} _ {i} ^ {k} = \\left\\{x \\in \\mathbb {R} ^ {n _ {i}} \\mid \\left(x _ {1} ^ {k}, \\dots , x _ {i - 1} ^ {k}, x, x _ {i + 1} ^ {k - 1}, \\dots , x _ {s} ^ {k - 1}\\right) \\in \\mathcal {X} \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.637,
                0.576,
                0.655
            ],
            "angle": 0,
            "content": "在更新格式(8.4.5)中，\\(\\hat{x}_i^{k-1}\\) 采用外推定义："
        },
        {
            "type": "equation",
            "bbox": [
                0.417,
                0.67,
                0.825,
                0.69
            ],
            "angle": 0,
            "content": "\\[\n\\hat {x} _ {i} ^ {k - 1} = x _ {i} ^ {k - 1} + \\omega_ {i} ^ {k - 1} \\left(x _ {i} ^ {k - 1} - x _ {i} ^ {k - 2}\\right), \\tag {8.4.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.704,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(\\omega_{i}^{k}\\geqslant 0\\) 为外推的权重， \\(\\hat{g}_i^k\\stackrel {\\mathrm{def}}{=}\\nabla f_i^k (\\hat{x}_i^{k - 1})\\) 为外推点处的梯度.在(8.4.6)式中取权重 \\(\\omega_{i}^{k} = 0\\) 即可得到不带外推的更新格式，此时计算(8.4.5)等价于进行一次近似点梯度法的更新．在(8.4.5)式使用外推是为了加快分块坐标下降法的收敛速度．我们可以通过如下的方式理解这三种格式：格式(8.4.3)是最直接的，即固定其他分量然后对单一变量求极小；格式(8.4.4)则是增加了一个近似点项 \\(\\frac{L_i^{k - 1}}{2}\\| x_i - x_i^{k - 1}\\| _2^2\\) 来限制下一步迭代不应该与当前位置相距过远，增加近似点项的作用是使得算法能够收敛；格式(8.4.5)首先对 \\(f_{i}^{k}(x)\\) 进行线"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "391"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.157,
                0.736,
                0.195
            ],
            "angle": 0,
            "content": "性化以简化子问题的求解，在此基础上引入了Nesterov加速算法的技巧加快收敛."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.2,
                0.737,
                0.236
            ],
            "angle": 0,
            "content": "为了直观地说明分块坐标下降法的迭代过程，我们给出一个简单的例子."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.258,
                0.498,
                0.275
            ],
            "angle": 0,
            "content": "例8.13 考虑二元二次函数的优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.285,
                0.293,
                0.622,
                0.312
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad f (x, y) = x ^ {2} - 2 x y + 1 0 y ^ {2} - 4 x - 2 0 y,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.332,
                0.737,
                0.39
            ],
            "angle": 0,
            "content": "现在对变量 \\(x, y\\) 使用分块坐标下降法求解。当固定 \\(y\\) 时，可知当 \\(x = 2 + y\\) 时函数取极小值；当固定 \\(x\\) 时，可知当 \\(y = 1 + \\frac{x}{10}\\) 时函数取极小值。故采用格式(8.4.3)的分块坐标下降法为"
        },
        {
            "type": "equation",
            "bbox": [
                0.389,
                0.409,
                0.737,
                0.428
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = 2 + y ^ {k}, \\tag {8.4.7}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.389,
                0.432,
                0.737,
                0.466
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = 1 + \\frac {x ^ {k + 1}}{1 0}. \\tag {8.4.8}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.479,
                0.739,
                0.579
            ],
            "angle": 0,
            "content": "图8.6描绘了当初始点为 \\((x, y) = (0.5, 0.2)\\) 时的迭代点轨迹，可以看到在进行了7次迭代后迭代点与最优解已充分接近。回忆一下我们在例6.2中曾经对一个类似的问题使用过梯度法，而梯度法的收敛相当缓慢。一个直观的解释是：对于比较病态的问题，由于分块坐标下降法是对逐个分量处理，它能较好地捕捉目标函数的各向异性，而梯度法则会受到很大影响。"
        },
        {
            "type": "image",
            "bbox": [
                0.254,
                0.608,
                0.665,
                0.727
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.326,
                0.756,
                0.582,
                0.773
            ],
            "angle": 0,
            "content": "图8.6 分块坐标下降法迭代轨迹"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "结合上述更新格式(8.4.3)-(8.4.5)可以得到分块坐标下降法的基本框架，详见算法8.10."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "392"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "code_caption",
            "bbox": [
                0.26,
                0.159,
                0.456,
                0.175
            ],
            "angle": 0,
            "content": "算法8.10分块坐标下降法"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.178,
                0.751,
                0.199
            ],
            "angle": 0,
            "content": "1. 初始化：选择两组初始点 \\((x_{1}^{-1}, x_{2}^{-1}, \\dots, x_{s}^{-1}) = (x_{1}^{0}, x_{2}^{0}, \\dots, x_{s}^{0})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.203,
                0.426,
                0.216
            ],
            "angle": 0,
            "content": "2. for \\( k = 1,2,\\dots \\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.223,
                0.44,
                0.237
            ],
            "angle": 0,
            "content": "3. for \\(i = 1,2,\\dots\\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.242,
                0.651,
                0.26
            ],
            "angle": 0,
            "content": "4. 使用格式(8.4.3)或(8.4.4)或(8.4.5)更新 \\(x_{i}^{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.265,
                0.367,
                0.277
            ],
            "angle": 0,
            "content": "5. end for"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.285,
                0.469,
                0.3
            ],
            "angle": 0,
            "content": "6. if满足停机条件then"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.304,
                0.56,
                0.322
            ],
            "angle": 0,
            "content": "7. 返回 \\((x_1^k, x_2^k, \\dots, x_s^k)\\)，算法终止"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.327,
                0.357,
                0.34
            ],
            "angle": 0,
            "content": "8. end if"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.348,
                0.35,
                0.36
            ],
            "angle": 0,
            "content": "9. end for"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.178,
                0.651,
                0.36
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.421,
                0.827,
                0.644
            ],
            "angle": 0,
            "content": "算法8.10的子问题可采用三种不同的更新格式，一般来说这三种格式会产生不同的迭代序列，可能会收敛到不同的解，坐标下降算法的数值表现也不相同。格式(8.4.3)是最直接的更新方式，它严格保证了整个迭代过程的目标函数值是下降的。然而由于 \\(f\\) 的形式复杂，子问题求解难度较大。在收敛性方面，格式(8.4.3)在强凸问题上可保证目标函数收敛到极小值，但在非凸问题上不一定收敛。格式(8.4.4)(8.4.5)则是对格式(8.4.3)的修正，不保证迭代过程目标函数的单调性，但可以改善收敛性结果。使用格式(8.4.4)可使得算法收敛性在函数 \\(F\\) 为非严格凸时有所改善。格式(8.4.5)实质上为目标函数的一阶泰勒展开近似，在一些测试问题上有更好的表现，可能的原因是使用一阶近似可以避开一些局部极小值点。此外，格式(8.4.5)的计算量很小，比较容易实现。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.659,
                0.828,
                0.802
            ],
            "angle": 0,
            "content": "在实际的应用中，三种更新格式都有适用的问题，如果子问题可以写出显式解，则使用分块坐标下降算法可以节省相当一部分计算量。在每一步更新中，三种迭代格式(8.4.3)一(8.4.5)对不同自变量块可以混合使用，不必仅仅局限于一种。但对于同一个变量块，在整个迭代中应该使用相同的格式。例如在之后介绍的字典学习问题中，若对变量 \\(D\\) 使用格式(8.4.3)，对变量 \\(X\\) 使用格式(8.4.5)，则两个子问题都有显式解。因此更新格式的混用使得分块坐标下降法变得更加灵活。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.815,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "值得注意的是，对于非凸函数 \\( f(x) \\)，分块坐标下降法（算法8.10）可能失效。Powell在1973年就给出了一个使用格式(8.4.3)但不收敛的例子[157]。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "393"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.157,
                0.327,
                0.173
            ],
            "angle": 0,
            "content": "例8.14 令函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.208,
                0.18,
                0.7,
                0.218
            ],
            "angle": 0,
            "content": "\\[\nF \\left(x _ {1}, x _ {2}, x _ {3}\\right) = - x _ {1} x _ {2} - x _ {2} x _ {3} - x _ {3} x _ {1} + \\sum_ {i = 1} ^ {3} \\left[ \\left(x _ {i} - 1\\right) _ {+} ^ {2} + \\left(- x _ {i} - 1\\right) _ {+} ^ {2} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.226,
                0.736,
                0.245
            ],
            "angle": 0,
            "content": "其中 \\((x_{i} - 1)_{+}^{2}\\) 的含义为先对 \\((x_{i} - 1)\\) 取正部再平方．设 \\(\\varepsilon >0\\) ，初始点取为"
        },
        {
            "type": "equation",
            "bbox": [
                0.336,
                0.251,
                0.572,
                0.281
            ],
            "angle": 0,
            "content": "\\[\nx ^ {0} = \\left(- 1 - \\varepsilon , 1 + \\frac {\\varepsilon}{2}, - 1 - \\frac {\\varepsilon}{4}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.286,
                0.348,
                0.303
            ],
            "angle": 0,
            "content": "容易验证迭代序列满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.271,
                0.308,
                0.637,
                0.34
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k} = (- 1) ^ {k} \\cdot (- 1, 1, - 1) + (- \\frac {1}{8}) ^ {k} \\cdot \\left(- \\varepsilon , \\frac {\\varepsilon}{2}, - \\frac {\\varepsilon}{4}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.345,
                0.737,
                0.381
            ],
            "angle": 0,
            "content": "这个迭代序列有两个聚点 \\((-1, 1, -1)\\) 与 \\((1, -1, 1)\\)，但这两个点都不是 \\(F\\) 的稳定点。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.393,
                0.738,
                0.431
            ],
            "angle": 0,
            "content": "以上例子表明，分块坐标下降法的收敛性需要更多的假设，对非凸函数使用此方法可能会失败."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.456,
                0.315,
                0.475
            ],
            "angle": 0,
            "content": "8.4.3 应用举例"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.487,
                0.339,
                0.505
            ],
            "angle": 0,
            "content": "1. LASSO问题求解"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.517,
                0.619,
                0.534
            ],
            "angle": 0,
            "content": "下面介绍如何使用分块坐标下降法来求解 LASSO 问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.539,
                0.737,
                0.57
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\mu \\| x \\| _ {1} + \\frac {1}{2} \\| A x - b \\| ^ {2}. \\tag {8.4.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.576,
                0.738,
                0.614
            ],
            "angle": 0,
            "content": "由于目标函数的 \\(\\| x\\| _1\\) 部分是可分的，因此第 \\(i\\) 块变量即为 \\(x\\) 的第 \\(i\\) 个分量.为了方便，在考虑第 \\(i\\) 块的更新时，将自变量 \\(x\\) 记为"
        },
        {
            "type": "equation",
            "bbox": [
                0.417,
                0.62,
                0.487,
                0.665
            ],
            "angle": 0,
            "content": "\\[\nx = \\left[ \\begin{array}{c} x _ {i} \\\\ \\tilde {x} _ {i} \\end{array} \\right]\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.671,
                0.738,
                0.707
            ],
            "angle": 0,
            "content": "其中 \\(\\bar{x}_i\\) 为 \\(x\\) 去掉第 \\(i\\) 个分量而形成的列向量．而相应地，矩阵 \\(A\\) 在第 \\(i\\) 块的更新记为"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.708,
                0.51,
                0.735
            ],
            "angle": 0,
            "content": "\\[\nA = \\left[ \\begin{array}{c c} a _ {i} & \\bar {A} _ {i} \\end{array} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.739,
                0.738,
                0.776
            ],
            "angle": 0,
            "content": "其中 \\(\\bar{A}_i\\) 为矩阵 \\(A\\) 去掉第 \\(i\\) 列而形成的矩阵。这里为了方便表示，同时将 \\(x\\) 和 \\(A\\) 的分量顺序进行了调整，但调整后的问题依然和原问题是等价的。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.78,
                0.738,
                0.817
            ],
            "angle": 0,
            "content": "以下我们推导分块坐标下降法的更新格式。在第 \\(i\\) 块的更新中，考虑直接极小化的格式(8.4.3)，原问题可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.824,
                0.737,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x _ {i}} \\quad \\mu | x _ {i} | + \\mu \\| \\bar {x} _ {i} \\| _ {1} + \\frac {1}{2} \\| a _ {i} x _ {i} - (b - \\bar {A} _ {i} \\bar {x} _ {i}) \\| ^ {2}. \\tag {8.4.10}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "394"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.796,
                0.174
            ],
            "angle": 0,
            "content": "做替换 \\(c_{i} = b - \\bar{A}_{i}\\bar{x}_{i}\\) ，并注意到仅与 \\(\\bar{x}_i\\) 有关的项是常数，原问题等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.383,
                0.184,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x _ {i}} \\quad f _ {i} \\left(x _ {i}\\right) \\stackrel {\\text {d e f}} {=} \\mu \\left| x _ {i} \\right| + \\frac {1}{2} \\| a _ {i} \\| ^ {2} x _ {i} ^ {2} - a _ {i} ^ {\\mathrm {T}} c _ {i} x _ {i}. \\tag {8.4.11}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.228,
                0.576,
                0.245
            ],
            "angle": 0,
            "content": "对函数(8.4.11)，可直接写出它的最小值点"
        },
        {
            "type": "equation",
            "bbox": [
                0.366,
                0.255,
                0.826,
                0.349
            ],
            "angle": 0,
            "content": "\\[\nx _ {i} ^ {k} = \\underset {x _ {i}} {\\arg \\min } f _ {i} (x _ {i}) = \\left\\{ \\begin{array}{l l} \\frac {a _ {i} ^ {\\mathrm {T}} c _ {i} - \\mu_ {i}}{\\| a _ {i} \\| ^ {2}}, & a _ {i} ^ {\\mathrm {T}} c _ {i} > \\mu , \\\\ \\frac {a _ {i} ^ {\\mathrm {T}} c _ {i} + \\mu_ {i}}{\\| a _ {i} \\| ^ {2}}, & a _ {i} ^ {\\mathrm {T}} c _ {i} <   - \\mu , \\\\ 0, & \\text {其 他 .} \\end{array} \\right. \\tag {8.4.12}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.36,
                0.692,
                0.377
            ],
            "angle": 0,
            "content": "因此可写出 LASSO 问题的分块坐标下降法，见算法 8.11."
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.394,
                0.569,
                0.41
            ],
            "angle": 0,
            "content": "算法8.11 LASSO问题的分块坐标下降法"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.415,
                0.609,
                0.432
            ],
            "angle": 0,
            "content": "1. 输入 \\(A, b\\)，参数 \\(\\mu\\)。初始化 \\(x^0 = 0\\)，\\(k \\gets 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.436,
                0.487,
                0.451
            ],
            "angle": 0,
            "content": "2. while 未达到收敛准则 do"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.457,
                0.458,
                0.472
            ],
            "angle": 0,
            "content": "3. for \\(i = 1,2,\\dots ,n\\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.478,
                0.469,
                0.493
            ],
            "angle": 0,
            "content": "4. 根据定义计算 \\(\\bar{x}_i, c_i\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.498,
                0.508,
                0.515
            ],
            "angle": 0,
            "content": "5. 使用公式(8.4.12)计算 \\(x_{i}^{k}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.52,
                0.368,
                0.533
            ],
            "angle": 0,
            "content": "6. end for"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.54,
                0.385,
                0.554
            ],
            "angle": 0,
            "content": "7. \\(k\\gets k + 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.56,
                0.371,
                0.574
            ],
            "angle": 0,
            "content": "8. end while"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.415,
                0.609,
                0.574
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.602,
                0.825,
                0.702
            ],
            "angle": 0,
            "content": "我们用同第6.2节中一样的 \\(A\\) 和 \\(b\\)，分别取 \\(\\mu = 10^{-2}, 10^{-3}\\)，并调用连续化坐标下降法进行求解，其中停机准则和参数 \\(\\mu\\) 的连续化设置和第6.2节中的光滑化梯度法一致，结果如图8.7所示。可以看到，在结合连续化策略之后，坐标下降法可以很快地收敛到问题的解。相比其他算法，坐标下降法不需要调节步长参数。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.728,
                0.416,
                0.744
            ],
            "angle": 0,
            "content": "2. K-均值聚类算法"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.758,
                0.806,
                0.775
            ],
            "angle": 0,
            "content": "下面对聚类问题(8.4.2)使用分块坐标下降法进行求解。其目标函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.341,
                0.789,
                0.49,
                0.813
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {\\Phi , H} \\| A - \\Phi H \\| _ {F} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.35,
                0.815,
                0.741,
                0.833
            ],
            "angle": 0,
            "content": "s.t. \\(\\Phi \\in \\mathbb{R}^{n\\times k}\\) ,每一行只有一个元素为1，其余为0,"
        },
        {
            "type": "equation",
            "bbox": [
                0.391,
                0.836,
                0.47,
                0.852
            ],
            "angle": 0,
            "content": "\\[\nH \\in \\mathbb {R} ^ {k \\times p}.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "395"
        },
        {
            "type": "image",
            "bbox": [
                0.269,
                0.154,
                0.642,
                0.376
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.293,
                0.388,
                0.614,
                0.404
            ],
            "angle": 0,
            "content": "图8.7 分块坐标下降法求解LASSO问题"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.434,
                0.656,
                0.452
            ],
            "angle": 0,
            "content": "接下来分别讨论在固定 \\(\\varPhi\\) 和 \\(H\\) 的条件下如何极小化另一块变量"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.456,
                0.662,
                0.475
            ],
            "angle": 0,
            "content": "当固定 \\(H\\) 时, 设 \\(\\Phi\\) 的每一行为 \\(\\phi_{i}^{\\mathrm{T}}\\), 那么根据矩阵分块乘法,"
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.486,
                0.619,
                0.58
            ],
            "angle": 0,
            "content": "\\[\nA - \\Phi H = \\left[ \\begin{array}{c} a _ {1} ^ {\\mathrm {T}} \\\\ a _ {2} ^ {\\mathrm {T}} \\\\ \\vdots \\\\ a _ {n} ^ {\\mathrm {T}} \\end{array} \\right] - \\left[ \\begin{array}{c} \\phi_ {1} ^ {\\mathrm {T}} \\\\ \\phi_ {2} ^ {\\mathrm {T}} \\\\ \\vdots \\\\ \\phi_ {n} ^ {\\mathrm {T}} \\end{array} \\right] H = \\left[ \\begin{array}{c} a _ {1} ^ {\\mathrm {T}} - \\phi_ {1} ^ {\\mathrm {T}} H \\\\ a _ {2} ^ {\\mathrm {T}} - \\phi_ {2} ^ {\\mathrm {T}} H \\\\ \\vdots \\\\ a _ {n} ^ {\\mathrm {T}} - \\phi_ {n} ^ {\\mathrm {T}} H \\end{array} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.592,
                0.738,
                0.673
            ],
            "angle": 0,
            "content": "注意到 \\(\\phi_{i}\\) 只有一个分量为1，其余分量为0，不妨设其第 \\(j\\) 个分量为1，此时 \\(\\phi_{i}^{\\mathrm{T}}H\\) 相当于将 \\(H\\) 的第 \\(j\\) 行取出，因此 \\(\\| a_i^{\\mathrm{T}} - \\phi_i^{\\mathrm{T}}H\\|\\) 为 \\(a_{i}^{\\mathrm{T}}\\) 与 \\(H\\) 的第 \\(j\\) 个行向量的距离．我们的最终目的是极小化 \\(\\| A - \\Phi H\\| _F^2\\) ，所以 \\(j\\) 应该选矩阵 \\(H\\) 中距离 \\(a_{i}^{\\mathrm{T}}\\) 最近的那一行，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.327,
                0.684,
                0.579,
                0.745
            ],
            "angle": 0,
            "content": "\\[\n\\Phi_ {i j} = \\left\\{ \\begin{array}{l l} 1, & j = \\underset {l} {\\arg \\min } \\| a _ {i} - h _ {l} \\|, \\\\ 0, & \\text {其 他}. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.757,
                0.399,
                0.775
            ],
            "angle": 0,
            "content": "其中 \\(h_l^{\\mathrm{T}}\\) 表示矩阵 \\(H\\) 的第 \\(l\\) 行"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.779,
                0.707,
                0.801
            ],
            "angle": 0,
            "content": "当固定 \\(\\Phi\\) 时，此时考虑 \\(H\\) 的每一行 \\(h_j^{\\mathrm{T}}\\) ，根据目标函数的等价性有"
        },
        {
            "type": "equation",
            "bbox": [
                0.335,
                0.814,
                0.574,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| A - \\Phi H \\right\\| _ {F} ^ {2} = \\sum_ {j = 1} ^ {k} \\sum_ {a \\in S _ {j}} \\left\\| a - h _ {j} \\right\\| ^ {2},\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "396"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.802,
                0.174
            ],
            "angle": 0,
            "content": "因此只需要对每个 \\(h_j\\) 求最小即可。设 \\(\\bar{a}_j\\) 是目前第 \\(j\\) 类所有点的均值，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.305,
                0.187,
                0.776,
                0.295
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\sum_ {a \\in S _ {j}} \\| a - h _ {j} \\| ^ {2} = \\sum_ {a \\in S _ {j}} \\| a - \\bar {a} _ {j} + \\bar {a} _ {j} - h _ {j} \\| ^ {2} \\\\ = \\sum_ {a \\in S _ {j}} \\left(\\| a - \\bar {a} _ {j} \\| ^ {2} + \\| \\bar {a} _ {j} - h _ {j} \\| ^ {2} + 2 \\langle a - \\bar {a} _ {j}, \\bar {a} _ {j} - h _ {j} \\rangle\\right) \\\\ = \\sum_ {a \\in S _ {j}} \\left(\\| a - \\bar {a} _ {j} \\| ^ {2} + \\| \\bar {a} _ {j} - h _ {j} \\| ^ {2}\\right), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.309,
                0.825,
                0.356
            ],
            "angle": 0,
            "content": "这里利用了交叉项 \\(\\sum_{a\\in S_j}\\left\\langle a - \\bar{a}_j,\\bar{a}_j - h_j\\right\\rangle = 0\\) 的事实．因此容易看出，此时 \\(h_j\\) 直接取为 \\(\\bar{a}_j\\) 即可达到最小值."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.361,
                0.825,
                0.397
            ],
            "angle": 0,
            "content": "综上，我们得到了针对聚类问题的分块坐标下降法，它每一次迭代分为两步："
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.413,
                0.811,
                0.43
            ],
            "angle": 0,
            "content": "(1) 固定参考点 \\(H\\), 将每个样本点分到和其最接近的参考点代表的类中;"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.445,
                0.825,
                0.482
            ],
            "angle": 0,
            "content": "(2) 固定聚类方式 \\(\\Phi\\), 重新计算每个类所有点的均值并将其作为新的参考点."
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.413,
                0.825,
                0.482
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.498,
                0.825,
                0.535
            ],
            "angle": 0,
            "content": "这个过程恰好就是经典的 K-均值聚类算法，因此可以得到结论：K-均值聚类算法本质上是一个分块坐标下降法。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.562,
                0.397,
                0.578
            ],
            "angle": 0,
            "content": "3. 非负矩阵分解"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.592,
                0.825,
                0.629
            ],
            "angle": 0,
            "content": "非负矩阵分解问题[148]也可以使用分块坐标下降法求解．现在考虑最基本的非负矩阵分解问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.46,
                0.639,
                0.825,
                0.671
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X, Y \\geqslant 0} \\quad \\frac {1}{2} \\| X Y - M \\| _ {F} ^ {2}. \\tag {8.4.13}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.683,
                0.418,
                0.699
            ],
            "angle": 0,
            "content": "它的一个等价形式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.709,
                0.825,
                0.742
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X, Y} \\frac {1}{2} \\| X Y - M \\| _ {F} ^ {2} + I _ {\\geqslant 0} (X) + I _ {\\geqslant 0} (Y), \\tag {8.4.14}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.753,
                0.825,
                0.79
            ],
            "angle": 0,
            "content": "其中 \\(I_{\\geqslant 0}(\\cdot)\\) 为集合 \\(\\{X \\mid X \\geqslant 0\\}\\) 的示性函数。不难验证问题(8.4.14)具有形式(8.4.1)。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "以下考虑求解方法。注意到 \\(X\\) 和 \\(Y\\) 耦合在一起，在固定 \\(Y\\) 的条件下，我们无法直接按照格式(8.4.3)或格式(8.4.4)的形式给出子问题的显式解。若要采用这两种格式需要额外设计算法求解子问题，最终会产生较大计算量。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "397"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.202
            ],
            "angle": 0,
            "content": "但我们总能使用格式(8.4.5)来对子问题进行线性化，从而获得比较简单的更新格式。令 \\( f(X, Y) = \\frac{1}{2} \\| XY - M \\|_F^2 \\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.29,
                0.212,
                0.737,
                0.245
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\partial f}{\\partial X} = (X Y - M) Y ^ {T}, \\quad \\frac {\\partial f}{\\partial Y} = X ^ {T} (X Y - M). \\tag {8.4.15}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.255,
                0.739,
                0.293
            ],
            "angle": 0,
            "content": "注意到在格式(8.4.5)中，当 \\(r_i(X)\\) 为凸集示性函数时即是求解到该集合的投影，因此得到分块坐标下降法如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.293,
                0.305,
                0.737,
                0.334
            ],
            "angle": 0,
            "content": "\\[\nX ^ {k + 1} = \\max  \\left\\{X ^ {k} - t _ {k} ^ {x} \\left(X ^ {k} Y ^ {k} - M\\right) \\left(Y ^ {k}\\right) ^ {\\mathrm {T}}, 0 \\right\\}, \\tag {8.4.16}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.299,
                0.331,
                0.612,
                0.349
            ],
            "angle": 0,
            "content": "\\[\nY ^ {k + 1} = \\max  \\left\\{Y ^ {k} - t _ {k} ^ {y} \\left(X ^ {k}\\right) ^ {\\mathrm {T}} \\left(X ^ {k} Y ^ {k} - M\\right), 0 \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.363,
                0.592,
                0.398
            ],
            "angle": 0,
            "content": "其中 \\(t_k^x,t_k^y\\) 是步长，分别对应格式(8.4.5)中的 \\(\\frac{1}{L_i^k},i = 1,2.\\)"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.417,
                0.274,
                0.433
            ],
            "angle": 0,
            "content": "4. 字典学习"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.447,
                0.737,
                0.484
            ],
            "angle": 0,
            "content": "第三章提到了字典学习问题(3.9.1)，在实际中带关于变量 \\(D\\) 的罚函数的形式也很常见："
        },
        {
            "type": "equation",
            "bbox": [
                0.295,
                0.495,
                0.737,
                0.527
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\frac {1}{2 n} \\| D X - A \\| _ {F} ^ {2} + \\lambda \\| X \\| _ {1} + \\frac {\\mu}{2} \\| D \\| _ {F} ^ {2}. \\tag {8.4.17}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.538,
                0.738,
                0.6
            ],
            "angle": 0,
            "content": "注意问题(8.4.17)和原问题(3.9.1)的区别是使用罚函数 \\(\\frac{\\mu}{2} \\| D \\|_F^2\\) 代替 \\(F\\) 范数约束 \\(\\| D \\|_F \\leqslant 1\\)，在一定条件下它们是等价的。现在我们考虑使用分块坐标下降法来求解问题(8.4.17)。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.604,
                0.7,
                0.621
            ],
            "angle": 0,
            "content": "优化问题(8.4.17)的变量总共有两块，当固定变量 \\(D\\) 时，考虑函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.322,
                0.631,
                0.585,
                0.663
            ],
            "angle": 0,
            "content": "\\[\nf _ {D} (X) = \\frac {1}{2 n} \\| D X - A \\| _ {F} ^ {2} + \\lambda \\| X \\| _ {1}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.673,
                0.738,
                0.711
            ],
            "angle": 0,
            "content": "注意到对 \\(f_{D}(X)\\) 直接极小化是 \\(n\\) 个LASSO问题，无法求出显式解，为此我们可以使用格式(8.4.5).通过直接计算可得 \\(f_{D}(X)\\) 中光滑部分的梯度为"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.721,
                0.534,
                0.752
            ],
            "angle": 0,
            "content": "\\[\nG = \\frac {1}{n} D ^ {\\mathrm {T}} (D X - A),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.763,
                0.341,
                0.779
            ],
            "angle": 0,
            "content": "因此格式(8.4.5)等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.274,
                0.79,
                0.635,
                0.826
            ],
            "angle": 0,
            "content": "\\[\nX ^ {k + 1} = \\operatorname {p r o x} _ {t _ {k} \\lambda \\| \\cdot \\| _ {1}} \\left(X ^ {k} - \\frac {t _ {k}}{n} (D ^ {k}) ^ {\\mathrm {T}} (D ^ {k} X ^ {k} - A)\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.287,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(t_k\\) 为步长."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "398"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.157,
                0.51,
                0.174
            ],
            "angle": 0,
            "content": "当固定变量 \\(X\\) 时，考虑函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.407,
                0.184,
                0.677,
                0.217
            ],
            "angle": 0,
            "content": "\\[\nf _ {X} (D) = \\frac {1}{2 n} \\| D X - A \\| _ {F} ^ {2} + \\frac {\\mu}{2} \\| D \\| _ {F} ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.225,
                0.825,
                0.263
            ],
            "angle": 0,
            "content": "注意到对 \\(f_{X}(D)\\) 直接极小化是 \\(m\\) 个岭回归问题，可求出显式解，所以我们可以使用格式(8.4.3). 计算关于 \\(D^{\\mathrm{T}}\\) 的梯度为"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.273,
                0.689,
                0.304
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {D ^ {\\mathrm {T}}} f _ {X} (D) = \\frac {1}{n} X \\left(X ^ {\\mathrm {T}} D ^ {\\mathrm {T}} - A ^ {\\mathrm {T}}\\right) + \\mu D ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.314,
                0.436,
                0.332
            ],
            "angle": 0,
            "content": "令梯度为零向量，可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.347,
                0.64,
                0.366
            ],
            "angle": 0,
            "content": "\\[\nD = A X ^ {T} \\left(X X ^ {T} + n \\mu I\\right) ^ {- 1}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.381,
                0.825,
                0.419
            ],
            "angle": 0,
            "content": "因为 \\(X \\in \\mathbb{R}^{k \\times n}\\), 其中 \\(k \\ll n\\), 所以 \\(XX^{\\mathrm{T}}\\) 是一个比较小的矩阵, 可以方便地求出它的逆. 故格式(8.4.3)等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.385,
                0.434,
                0.697,
                0.453
            ],
            "angle": 0,
            "content": "\\[\nD ^ {k + 1} = A \\left(X ^ {k + 1}\\right) ^ {\\mathrm {T}} \\left(X ^ {k + 1} \\left(X ^ {k + 1}\\right) ^ {\\mathrm {T}} + n \\mu I\\right) ^ {- 1}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.469,
                0.741,
                0.485
            ],
            "angle": 0,
            "content": "若先更新 \\(X\\) 再更新 \\(D\\), 则最终可以得到如下的分块坐标下降法:"
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.496,
                0.826,
                0.531
            ],
            "angle": 0,
            "content": "\\[\nX ^ {k + 1} = \\operatorname {p r o x} _ {t _ {k} \\lambda \\| \\cdot \\| _ {1}} \\left(X ^ {k} - \\frac {t _ {k}}{n} (D ^ {k}) ^ {\\mathrm {T}} (D ^ {k} X ^ {k} - A)\\right), \\tag {8.4.18}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.335,
                0.535,
                0.826,
                0.555
            ],
            "angle": 0,
            "content": "\\[\nD ^ {k + 1} = A \\left(X ^ {k + 1}\\right) ^ {\\mathrm {T}} \\left(X ^ {k + 1} \\left(X ^ {k + 1}\\right) ^ {\\mathrm {T}} + n \\mu I\\right) ^ {- 1}. \\tag {8.4.19}\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.579,
                0.467,
                0.596
            ],
            "angle": 0,
            "content": "5. 最大割问题的非凸松弛"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.609,
                0.825,
                0.647
            ],
            "angle": 0,
            "content": "第四章提到最大割问题的半定松弛(4.5.8)，在实际算法设计中也会考虑一种基于半定松弛的非凸松弛："
        },
        {
            "type": "text",
            "bbox": [
                0.318,
                0.661,
                0.528,
                0.677
            ],
            "angle": 0,
            "content": "(半定松弛) min \\(\\langle C, X \\rangle\\),"
        },
        {
            "type": "equation",
            "bbox": [
                0.426,
                0.686,
                0.635,
                0.702
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & X _ {i i} = 1,   i = 1, 2, \\dots , n, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.71,
                0.519,
                0.725
            ],
            "angle": 0,
            "content": "\\[\nX \\succeq 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.319,
                0.734,
                0.55,
                0.753
            ],
            "angle": 0,
            "content": "(非凸松弛) \\(\\min \\left\\langle C, V^{\\mathrm{T}} V \\right\\rangle,\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.426,
                0.759,
                0.71,
                0.776
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & v _ {i} \\in \\mathbb {R} ^ {p}, \\| v _ {i} \\| = 1, i = 1, 2, \\dots , n, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.784,
                0.611,
                0.801
            ],
            "angle": 0,
            "content": "\\[\nV = \\left[ v _ {1}, v _ {2}, \\dots , v _ {n} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.815,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "比较两种松弛方式可知，非凸松弛通过引入分解 \\(X = V^{\\mathrm{T}}V\\) 并限制 \\(V\\) 的每一列的 \\(\\ell_2\\) 范数为1，将半定松弛中的 \\(X\\) 对角线元素为1以及 \\(X\\) 半正定的约束"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "399"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.195
            ],
            "angle": 0,
            "content": "消去了．但这两个问题一般不等价，当 \\(p\\) 充分大时二者等价．实际计算中通常选取一个较小的 \\(p\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.202,
                0.737,
                0.261
            ],
            "angle": 0,
            "content": "问题(8.4.20)中的非凸松弛有个自然的分块结构：矩阵 \\(V\\) 是按列分成 \\(n\\) 块的，因此可以用分块坐标下降法求解．以格式(8.4.3)为例，取定 \\(i\\) ，固定其余 \\(v_{j}, j \\neq i\\) ，我们只考虑目标函数和 \\(v_{i}\\) 相关的部分．因为目标函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.21,
                0.277,
                0.699,
                0.395
            ],
            "angle": 0,
            "content": "\\[\n\\mathrm {T r} \\left(\\left[ \\begin{array}{c c c c c} C _ {1 1} & \\dots & C _ {1 i} & \\dots & C _ {1 n} \\\\ \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\ C _ {i 1} & \\dots & C _ {i i} & \\dots & C _ {i n} \\\\ \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\ C _ {n 1} & \\dots & C _ {n i} & \\dots & C _ {n n} \\end{array} \\right] \\left[ \\begin{array}{c c c c c} v _ {1} ^ {\\mathrm {T}} v _ {1} & \\dots & v _ {1} ^ {\\mathrm {T}} v _ {i} & \\dots & v _ {1} ^ {\\mathrm {T}} v _ {n} \\\\ \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\ v _ {i} ^ {\\mathrm {T}} v _ {1} & \\dots & v _ {i} ^ {\\mathrm {T}} v _ {i} & \\dots & v _ {i} ^ {\\mathrm {T}} v _ {n} \\\\ \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\ v _ {n} ^ {\\mathrm {T}} v _ {1} & \\dots & v _ {n} ^ {\\mathrm {T}} v _ {i} & \\dots & v _ {n} ^ {\\mathrm {T}} v _ {n} \\end{array} \\right]\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.41,
                0.541,
                0.427
            ],
            "angle": 0,
            "content": "根据以上矩阵分块示意图可知和 \\(v_{i}\\) 有关的部分为"
        },
        {
            "type": "equation",
            "bbox": [
                0.351,
                0.447,
                0.556,
                0.48
            ],
            "angle": 0,
            "content": "\\[\nC _ {i i} v _ {i} ^ {\\mathrm {T}} v _ {i} + \\sum_ {j \\neq i} \\left(C _ {i j} + C _ {j i}\\right) v _ {i} ^ {\\mathrm {T}} v _ {j}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.497,
                0.737,
                0.555
            ],
            "angle": 0,
            "content": "注意到约束 \\(\\| v_{i}\\| = 1\\) ，因此上式中的第一项是常数，可以忽略；同时最大割问题中的 \\(C\\) 是对称矩阵，因此 \\(C_{ij} = C_{ji}\\) 。结合以上两点，最终在第 \\(i\\) 步我们求解的子问题是："
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.563,
                0.737,
                0.598
            ],
            "angle": 0,
            "content": "\\[\n\\min  f _ {i} \\left(v _ {i}\\right) = \\left(\\sum_ {j \\neq i} C _ {j i} v _ {j} ^ {\\mathrm {T}}\\right) v _ {i}, \\tag {8.4.21}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.358,
                0.6,
                0.466,
                0.616
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\text {s . t .} \\quad \\| v _ {i} \\| = 1. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.634,
                0.303,
                0.65
            ],
            "angle": 0,
            "content": "根据柯西不等式，"
        },
        {
            "type": "equation",
            "bbox": [
                0.274,
                0.666,
                0.637,
                0.711
            ],
            "angle": 0,
            "content": "\\[\n\\left(\\sum_ {j \\neq i} C _ {j i} v _ {j} ^ {\\mathrm {T}}\\right) v _ {i} \\geqslant - \\left\\| \\sum_ {j \\neq i} C _ {j i} v _ {j} \\right\\| \\| v _ {i} \\| = - \\left\\| \\sum_ {j \\neq i} C _ {j i} v _ {j} \\right\\|,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.728,
                0.314,
                0.743
            ],
            "angle": 0,
            "content": "等号取到当且仅当"
        },
        {
            "type": "equation",
            "bbox": [
                0.393,
                0.749,
                0.514,
                0.822
            ],
            "angle": 0,
            "content": "\\[\nv_{i} = \\frac{-\\sum_{j\\neq i}C_{ji}v_{j}}{\\left\\| \\sum_{j\\neq i}C_{ji}v_{j}\\right\\|}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.657,
                0.853
            ],
            "angle": 0,
            "content": "因此我们可得到求解最大割问题的分块坐标下降法，见算法8.12."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "400"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "code_caption",
            "bbox": [
                0.259,
                0.158,
                0.6,
                0.175
            ],
            "angle": 0,
            "content": "算法8.12最大割问题的分块坐标下降法[191]"
        },
        {
            "type": "algorithm",
            "bbox": [
                0.269,
                0.179,
                0.49,
                0.34
            ],
            "angle": 0,
            "content": "1. 初始化 \\( v_{i} \\) 且使得 \\( \\| v_{i}\\| = 1 \\)  \n2. while 未达到收敛准则 do  \n3. for \\( i = 1,2,\\dots ,n \\) do  \n4. 计算 \\( b_{i} = \\sum_{j\\neq i}C_{ji}v_{j} \\)  \n5. 更新 \\( v_{i} = -\\frac{b_{i}}{\\|b_{i}\\|} \\)  \n6. end for  \n7. end while"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.363,
                0.825,
                0.402
            ],
            "angle": 0,
            "content": "同理为了增加算法的稳定性，也可考虑使用格式(8.4.4)来求解此问题，读者可自行推导相关算法。"
        },
        {
            "type": "title",
            "bbox": [
                0.259,
                0.426,
                0.429,
                0.444
            ],
            "angle": 0,
            "content": "*8.4.4 收敛性分析"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.457,
                0.825,
                0.515
            ],
            "angle": 0,
            "content": "本小节对格式 (8.4.5) 在 \\(s = 2\\) 且非凸的情况下进行收敛性分析。这一分析技术主要的工具是 Kurdyka-Lojasiewicz（在后面的分析中简记为 KL）性质。感兴趣的读者可以参考文献 [24]。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.52,
                0.747,
                0.536
            ],
            "angle": 0,
            "content": "为了叙述简便，我们重新对问题(8.4.1)定义记号。考虑问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.548,
                0.825,
                0.569
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad \\Psi (x, y) \\stackrel {\\text {d e f}} {=} f (x) + g (y) + H (x, y), \\quad (x, y) \\in \\mathbb {R} ^ {n} \\times \\mathbb {R} ^ {m}, \\tag {8.4.22}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.582,
                0.825,
                0.619
            ],
            "angle": 0,
            "content": "其中 \\(f\\) 和 \\(g\\) 为适当闭函数，\\(H\\) 为其定义域上的连续可微函数．注意 \\(f\\) 和 \\(g\\) 不再是凸函数，这和上一小节的问题有所区别."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.623,
                0.825,
                0.661
            ],
            "angle": 0,
            "content": "对问题(8.4.22)，格式(8.4.5)化为如下基本形式（取 \\(\\hat{g}_i^{k + 1}\\) 为 \\(\\nabla_{x}H(x^{k},y^{k})\\) 或 \\(\\nabla_yH(x^k,y^k)\\) ， \\(\\hat{x}_i^k\\) 为 \\(x^{k}\\) 或 \\(y^{k}\\) )："
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.674,
                0.825,
                0.695
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} \\in \\operatorname {p r o x} _ {c _ {k} f} \\left(x ^ {k} - c _ {k} \\nabla_ {x} H \\left(x ^ {k}, y ^ {k}\\right)\\right), \\tag {8.4.23}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.699,
                0.825,
                0.722
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} \\in \\operatorname {p r o x} _ {d _ {k} g} \\left(y ^ {k} - d _ {k} \\nabla_ {y} H \\left(x ^ {k + 1}, y ^ {k}\\right)\\right). \\tag {8.4.24}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.732,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(c_{k}, d_{k}\\) 为步长参数，对应 (8.4.5) 中的 \\(L_{i}^{k}\\)，具体取法和函数 \\(\\Psi(x, y)\\) 本身的性质有关，在后面的讨论中会给出．格式 (8.4.23)，格式 (8.4.24) 与格式 (8.4.5) 有所区别，由于 \\(f\\) 和 \\(g\\) 不是凸函数，相应地 \\(\\operatorname{prox}_{f}\\) 和 \\(\\operatorname{prox}_{g}\\) 是集合函数，在迭代过程中只要求 \\(x^{k+1}\\) 和 \\(y^{k+1}\\) 是相应集合中的一个元素即可．为了保证 \\(\\operatorname{prox}_{f}\\) 和 \\(\\operatorname{prox}_{g}\\) 是良定义的，我们对 \\(f\\) 和 \\(g\\) 还需要提出下界有限的假设."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.119,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "401"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.215
            ],
            "angle": 0,
            "content": "注8.3 由于自变量只有两块，对光滑部分 \\(H\\) 我们采用的是线性化处理，因此格式(8.4.23)(8.4.24)又称为近似点交替线性化方法．当变量只有一块时，该方法退化成非凸情形的近似点梯度法，收敛性可类似地建立."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.234,
                0.737,
                0.272
            ],
            "angle": 0,
            "content": "为了分析算法的收敛性，我们先给出目标问题(8.4.22)所要满足的一个假设."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.292,
                0.571,
                0.309
            ],
            "angle": 0,
            "content": "假设8.2 在问题(8.4.22)中，函数 \\(f, g, H\\) 满足："
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.327,
                0.774,
                0.371
            ],
            "angle": 0,
            "content": "(1) \\(f\\colon \\mathbb{R}^n\\to (-\\infty , + \\infty ],g\\colon \\mathbb{R}^m\\to (-\\infty , + \\infty ]\\) 均为适当下半连续函数， \\(\\inf_{\\mathbb{R}^n\\times \\mathbb{R}^m}\\Psi >\\) \\(-\\infty ,\\inf_{\\mathbb{R}^n}f > - \\infty\\) ，以及 \\(\\inf_{\\mathbb{R}^m}g > - \\infty\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.386,
                0.737,
                0.444
            ],
            "angle": 0,
            "content": "(2) \\(H: \\mathbb{R}^n \\times \\mathbb{R}^m \\to \\mathbb{R}\\) 是连续可微函数, 且 \\(\\nabla H\\) 在有界集上是联合利普希茨连续的. 即对于任意的 \\(B_1 \\times B_2 \\subset \\mathbb{R}^n \\times \\mathbb{R}^m\\), 存在 \\(L > 0\\) 使得对于任意的 \\((x_i, y_i) \\in B_1 \\times B_2, i = 1, 2\\) 有"
        },
        {
            "type": "list",
            "bbox": [
                0.181,
                0.327,
                0.774,
                0.444
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.248,
                0.463,
                0.737,
                0.525
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| \\left(\\nabla_ {x} H \\left(x _ {1}, y _ {1}\\right) - \\nabla_ {x} H \\left(x _ {2}, y _ {2}\\right), \\nabla_ {y} H \\left(x _ {1}, y _ {1}\\right) - \\nabla_ {y} H \\left(x _ {2}, y _ {2}\\right)\\right) \\right\\| \\\\ \\leqslant L \\| \\left(x _ {1} - x _ {2}, y _ {1} - y _ {2}\\right) \\|. \\tag {8.4.25} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.552,
                0.737,
                0.59
            ],
            "angle": 0,
            "content": "根据假设8.2中的(2)，在有界集上 \\(H\\) 关于每个分量都是梯度 \\(L\\)-利普希茨连续的，且参数与另一分量无关．即"
        },
        {
            "type": "equation",
            "bbox": [
                0.299,
                0.609,
                0.612,
                0.649
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| \\nabla_ {x} H (x _ {1}, y) - \\nabla_ {x} H (x _ {2}, y) \\right\\| \\leqslant L \\| x _ {1} - x _ {2} \\|, \\\\ \\| \\nabla_ {y} H (x, y _ {1}) - \\nabla_ {y} H (x, y _ {2}) \\| \\leqslant L \\| y _ {1} - y _ {2} \\|. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.678,
                0.566,
                0.695
            ],
            "angle": 0,
            "content": "在假设8.2下，可以直接写出 \\(\\Psi (x,y)\\) 的次微分："
        },
        {
            "type": "equation",
            "bbox": [
                0.227,
                0.714,
                0.737,
                0.733
            ],
            "angle": 0,
            "content": "\\[\n\\partial \\Psi (x, y) = (\\nabla_ {x} H (x, y) + \\partial f (x), \\nabla_ {y} H (x, y) + \\partial g (y)), \\tag {8.4.26}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.752,
                0.737,
                0.788
            ],
            "angle": 0,
            "content": "其中“+”表示为集合间的加法。注意，这里的次微分应使用非凸函数的定义5.3。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.795,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "有了以上基本概念的铺垫，我们可以分析近似点交替线性化方法在问题 (8.4.22) 上的收敛性了。分析过程主要分为三个步骤：推导每一步迭代的函数值充分下降量，证明子列的收敛性，证明全序列的收敛性。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "402"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.157,
                0.362,
                0.174
            ],
            "angle": 0,
            "content": "1. 充分下降"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.187,
                0.826,
                0.245
            ],
            "angle": 0,
            "content": "第一步是推导算法的充分下降量. 下面的引理揭示了近似点交替线性化方法每一步迭代的充分下降性，它本质上是邻近算子的性质和梯度利普希茨连续函数性质的结合."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.256,
                0.827,
                0.333
            ],
            "angle": 0,
            "content": "引理8.1设 \\(h\\colon \\mathbb{R}^d\\to \\mathbb{R}\\) 是连续可微函数，梯度 \\(\\nabla h\\) 是利普希茨连续的，相应的常数为 \\(L_{h}\\) ， \\(\\sigma :\\mathbb{R}^{d}\\rightarrow (-\\infty , + \\infty ]\\) 是适当下半连续函数且 \\(\\inf_{\\mathbb{R}^d}\\sigma > - \\infty .\\) 固定 \\(t <   \\frac{1}{L_h}\\) ，则对任意的 \\(u\\in \\mathbf{dom}\\sigma\\) 和 \\(\\tilde{u}\\in \\mathrm{prox}_{t\\sigma}(u - t\\nabla h(u))\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.318,
                0.339,
                0.825,
                0.375
            ],
            "angle": 0,
            "content": "\\[\nh (\\tilde {u}) + \\sigma (\\tilde {u}) \\leqslant h (u) + \\sigma (u) - \\frac {1}{2} \\left(\\frac {1}{t} - L _ {h}\\right) \\| \\tilde {u} - u \\| ^ {2}. \\tag {8.4.27}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.382,
                0.741,
                0.399
            ],
            "angle": 0,
            "content": "证明. 首先根据 \\(\\sigma\\) 的假设, \\(\\tilde{u}\\) 是良定义的. 根据 \\(\\tilde{u}\\) 的最优性, 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.373,
                0.406,
                0.712,
                0.437
            ],
            "angle": 0,
            "content": "\\[\n\\langle \\tilde {u} - u, \\nabla h (u) \\rangle + \\frac {1}{2 t} \\| \\tilde {u} - u \\| ^ {2} + \\sigma (\\tilde {u}) \\leqslant \\sigma (u).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.444,
                0.469,
                0.46
            ],
            "angle": 0,
            "content": "再结合二次上界 (2.2.3)，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.318,
                0.467,
                0.765,
                0.567
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} h (\\tilde {u}) + \\sigma (\\tilde {u}) \\leqslant h (u) + \\langle \\tilde {u} - u, \\nabla h (u) \\rangle + \\frac {L _ {h}}{2} \\| \\tilde {u} - u \\| ^ {2} + \\sigma (\\tilde {u}) \\\\ \\leqslant h (u) + \\frac {L _ {h}}{2} \\| \\tilde {u} - u \\| ^ {2} + \\sigma (u) - \\frac {1}{2 t} \\| \\tilde {u} - u \\| ^ {2} \\\\ = h (u) + \\sigma (u) - \\frac {1}{2} \\left(\\frac {1}{t} - L _ {h}\\right) \\| \\tilde {u} - u \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.575,
                0.434,
                0.591
            ],
            "angle": 0,
            "content": "即可得到(8.4.27)成立"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.6,
                0.825,
                0.677
            ],
            "angle": 0,
            "content": "在引理8.1的证明中没有利用到 \\(t < \\frac{1}{L_h}\\) 这个条件，这是因为(8.4.27)式对任意的 \\(t > 0\\) 均成立．要求 \\(t < \\frac{1}{L_h}\\) 是为了让每一步有充分的下降量，从而使得近似点梯度迭代是一个下降算法."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.681,
                0.825,
                0.718
            ],
            "angle": 0,
            "content": "利用以上引理可推导出近似点交替线性化方法的单步充分下降量. 在问题 (8.4.22) 中, 定义迭代点序列"
        },
        {
            "type": "equation",
            "bbox": [
                0.454,
                0.73,
                0.629,
                0.749
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k} = \\left(x ^ {k}, y ^ {k}\\right), \\quad \\forall k \\geqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.761,
                0.604,
                0.779
            ],
            "angle": 0,
            "content": "则对于序列 \\(\\{z^k\\}\\) 我们有如下的充分下降定理："
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.789,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "定理8.10（充分下降）在假设8.2的条件下， \\(\\{z^k\\}\\) 为迭代格式(8.4.23)(8.4.24)产生的迭代序列，且假设 \\(z^k\\) 有界．取步长 \\(c_{k} = d_{k} = \\frac{1}{\\gamma L}\\) ，其中 \\(\\gamma >1\\) 是常数， \\(L\\) 为 \\(\\nabla H\\) 的利普希茨系数，则以下结论成立："
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "403"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.157,
                0.597,
                0.174
            ],
            "angle": 0,
            "content": "(1) 迭代点处的函数值序列 \\(\\{\\Psi(z^k)\\}\\) 是单调下降的，且"
        },
        {
            "type": "equation",
            "bbox": [
                0.278,
                0.182,
                0.737,
                0.211
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\rho_ {1}}{2} \\| z ^ {k + 1} - z ^ {k} \\| ^ {2} \\leqslant \\Psi (z ^ {k}) - \\Psi (z ^ {k + 1}), \\quad \\forall k \\geqslant 0, \\tag {8.4.28}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.217,
                0.366,
                0.234
            ],
            "angle": 0,
            "content": "其中 \\(\\rho_{1} = (\\gamma - 1)L\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.245,
                0.486,
                0.265
            ],
            "angle": 0,
            "content": "(2) 序列 \\(\\left\\{\\|z^{k+1} - z^k\\|\\right\\}_{k=1}^{\\infty}\\) 平方可和，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.226,
                0.272,
                0.737,
                0.309
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {k = 1} ^ {\\infty} \\left(\\| x ^ {k + 1} - x ^ {k} \\| ^ {2} + \\| y ^ {k + 1} - y ^ {k} \\| ^ {2}\\right) = \\sum_ {k = 1} ^ {\\infty} \\| z ^ {k + 1} - z ^ {k} \\| ^ {2} <   + \\infty , \\tag {8.4.29}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.317,
                0.455,
                0.341
            ],
            "angle": 0,
            "content": "并由此推出 \\(\\lim_{k\\to \\infty}\\| z^{k + 1} - z^k\\| = 0.\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.347,
                0.738,
                0.385
            ],
            "angle": 0,
            "content": "证明．(1)根据假设8.2的(2)， \\(H(x,y)\\) 关于每个分量都是利普希茨连续的，由引理8.1可得到每一步关于 \\(x^{k}\\) 和 \\(y^{k}\\) 的下降量估计："
        },
        {
            "type": "equation",
            "bbox": [
                0.302,
                0.395,
                0.653,
                0.485
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} H \\left(x ^ {k + 1}, y ^ {k}\\right) + f \\left(x ^ {k + 1}\\right) \\\\ \\leqslant H \\left(x ^ {k}, y ^ {k}\\right) + f \\left(x ^ {k}\\right) - \\frac {1}{2} \\left(\\frac {1}{c _ {k}} - L\\right) \\| x ^ {k + 1} - x ^ {k} \\| ^ {2} \\\\ = H \\left(x ^ {k}, y ^ {k}\\right) + f \\left(x ^ {k}\\right) - \\frac {1}{2} (\\gamma - 1) L \\| x ^ {k + 1} - x ^ {k} \\| ^ {2}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.493,
                0.251,
                0.509
            ],
            "angle": 0,
            "content": "以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.294,
                0.519,
                0.66,
                0.609
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} H \\left(x ^ {k + 1}, y ^ {k + 1}\\right) + g \\left(y ^ {k + 1}\\right) \\\\ \\leqslant H \\left(x ^ {k + 1}, y ^ {k}\\right) + g \\left(y ^ {k}\\right) - \\frac {1}{2} \\left(\\frac {1}{d _ {k}} - L\\right) \\| y ^ {k + 1} - y ^ {k} \\| ^ {2} \\\\ = H \\left(x ^ {k + 1}, y ^ {k}\\right) + g \\left(y ^ {k}\\right) - \\frac {1}{2} (\\gamma - 1) L \\| y ^ {k + 1} - y ^ {k} \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.618,
                0.578,
                0.635
            ],
            "angle": 0,
            "content": "将上述两个不等式相加，消去 \\(H(x^{k + 1},y^k)\\) ，得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.24,
                0.647,
                0.737,
                0.742
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\Psi (z ^ {k}) - \\Psi (z ^ {k + 1}) \\\\ = H \\left(x ^ {k}, y ^ {k}\\right) + f \\left(x ^ {k}\\right) + g \\left(y ^ {k}\\right) - H \\left(x ^ {k + 1}, y ^ {k + 1}\\right) - f \\left(x ^ {k + 1}\\right) - g \\left(y ^ {k + 1}\\right) \\\\ \\geqslant \\frac {1}{2} (\\gamma - 1) L \\left(\\| x ^ {k + 1} - x ^ {k} \\| ^ {2} + \\| y ^ {k + 1} - y ^ {k} \\| ^ {2}\\right). \\tag {8.4.30} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.754,
                0.319,
                0.77
            ],
            "angle": 0,
            "content": "由此立即可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.78,
                0.736,
                0.807
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\rho_ {1}}{2} \\left\\| z ^ {k + 1} - z ^ {k} \\right\\| ^ {2} \\leqslant \\Psi \\left(z ^ {k}\\right) - \\Psi \\left(z ^ {k + 1}\\right). \\tag {8.4.31}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.815,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "此外，容易得知迭代点处的函数值 \\(\\{\\Psi(z^k)\\}\\) 关于 \\(k\\) 是单调递减的。根据假设 \\(\\inf \\Psi > -\\infty\\) 可知 \\(\\Psi(z^k)\\) 单调下降收敛到一个有限的数 \\(\\Psi^*\\)。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "404"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.157,
                0.682,
                0.174
            ],
            "angle": 0,
            "content": "(2) 设 \\(N\\) 为任意的整数，在 (8.4.31) 式中对 \\(k\\) 求和，得"
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.185,
                0.779,
                0.224
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {k = 0} ^ {N - 1} \\| z ^ {k + 1} - z ^ {k} \\| ^ {2} \\leqslant \\frac {2}{\\rho_ {1}} (\\Psi (z ^ {0}) - \\Psi (z ^ {N})) \\leqslant \\frac {2}{\\rho_ {1}} (\\Psi (z ^ {0}) - \\Psi^ {*}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.237,
                0.657,
                0.274
            ],
            "angle": 0,
            "content": "令 \\(N\\to \\infty\\) 即可得 \\(\\sum_{k = 0}^{\\infty}\\| z^{k + 1} - z^k\\| ^2 < + \\infty\\) ，从而"
        },
        {
            "type": "equation",
            "bbox": [
                0.485,
                0.285,
                0.825,
                0.31
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\| z ^ {k + 1} - z ^ {k} \\| = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.321,
                0.825,
                0.38
            ],
            "angle": 0,
            "content": "定理8.10表明进行一轮近似点交替线性化迭代后，函数值下降量的下界可被相邻迭代点之间的距离控制。几乎所有下降类的算法在一定条件下都会满足这个性质。到此我们完成了算法收敛性分析的第一个步骤。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.405,
                0.379,
                0.422
            ],
            "angle": 0,
            "content": "2. 次梯度上界"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.435,
                0.825,
                0.515
            ],
            "angle": 0,
            "content": "在上一步中我们证明了迭代点处的函数值 \\(\\Psi^k\\) 最终会收敛到某个值, 但是这个值和局部最优解的关系还没有明确说明. 而序列 \\(\\{z^k\\}\\) 的收敛性质在上面的定理中也没有体现. 在这一部分我们将讨论序列 \\(\\{z^k\\}\\) 是否会趋于某个临界点, 这是收敛性框架中的第二个步骤."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.529,
                0.825,
                0.567
            ],
            "angle": 0,
            "content": "引理8.2(次梯度上界）在假设8.2的条件下，设 \\(\\{z^k\\}\\) 是迭代格式(8.4.23)(8.4.24)产生的有界序列，对任意的整数 \\(k\\) ，定义"
        },
        {
            "type": "equation",
            "bbox": [
                0.305,
                0.578,
                0.825,
                0.611
            ],
            "angle": 0,
            "content": "\\[\nA _ {x} ^ {k} = \\frac {1}{c _ {k - 1}} \\left(x ^ {k - 1} - x ^ {k}\\right) + \\nabla_ {x} H \\left(x ^ {k}, y ^ {k}\\right) - \\nabla_ {x} H \\left(x ^ {k - 1}, y ^ {k - 1}\\right), \\tag {8.4.32}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.622,
                0.297,
                0.636
            ],
            "angle": 0,
            "content": "以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.312,
                0.647,
                0.825,
                0.68
            ],
            "angle": 0,
            "content": "\\[\nA _ {y} ^ {k} = \\frac {1}{d _ {k - 1}} \\left(y ^ {k - 1} - y ^ {k}\\right) + \\nabla_ {y} H \\left(x ^ {k}, y ^ {k}\\right) - \\nabla_ {y} H \\left(x ^ {k}, y ^ {k - 1}\\right). \\tag {8.4.33}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.691,
                0.481,
                0.712
            ],
            "angle": 0,
            "content": "则有 \\((A_x^k, A_y^k) \\in \\partial \\Psi(x^k, y^k)\\) 且"
        },
        {
            "type": "equation",
            "bbox": [
                0.375,
                0.725,
                0.825,
                0.747
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| \\left(A _ {x} ^ {k}, A _ {y} ^ {k}\\right) \\right\\| \\leqslant \\left\\| A _ {x} ^ {k} \\right\\| + \\left\\| A _ {y} ^ {k} \\right\\| \\leqslant \\rho_ {2} \\left\\| z ^ {k} - z ^ {k - 1} \\right\\|, \\tag {8.4.34}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.761,
                0.625,
                0.779
            ],
            "angle": 0,
            "content": "其中 \\(\\rho_{2} = (2\\gamma +3)L\\) ， \\(\\gamma\\) 和 \\(L\\) 的定义同定理8.10"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.792,
                0.571,
                0.809
            ],
            "angle": 0,
            "content": "证明. 由迭代格式 (8.4.23), 当更新 \\(x^{k}\\) 时,"
        },
        {
            "type": "equation",
            "bbox": [
                0.269,
                0.82,
                0.814,
                0.857
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k} = \\underset {x \\in \\mathbb {R} ^ {n}} {\\arg \\min } \\left\\{\\left\\langle x - x ^ {k - 1}, \\nabla_ {x} H (x ^ {k - 1}, y ^ {k - 1}) \\right\\rangle + \\frac {1}{2 c _ {k - 1}} \\| x - x ^ {k - 1} \\| ^ {2} + f (x) \\right\\},\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "405"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.348,
                0.174
            ],
            "angle": 0,
            "content": "由一阶最优性条件可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.287,
                0.176,
                0.622,
                0.21
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {x} H (x ^ {k - 1}, y ^ {k - 1}) + \\frac {1}{c _ {k - 1}} (x ^ {k} - x ^ {k - 1}) + u ^ {k} = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.215,
                0.547,
                0.233
            ],
            "angle": 0,
            "content": "其中 \\(u^k \\in \\partial f(x^k)\\) 为 \\(f\\) 的一个次梯度．因此我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.301,
                0.237,
                0.605,
                0.271
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {x} H (x ^ {k - 1}, y ^ {k - 1}) + u ^ {k} = \\frac {1}{c _ {k - 1}} (x ^ {k - 1} - x ^ {k}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.276,
                0.487,
                0.294
            ],
            "angle": 0,
            "content": "同理，由迭代格式 (8.4.24) 可知关于 \\(y^{k}\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.309,
                0.298,
                0.599,
                0.332
            ],
            "angle": 0,
            "content": "\\[\n\\nabla_ {y} H (x ^ {k}, y ^ {k - 1}) + v ^ {k} = \\frac {1}{d _ {k - 1}} (y ^ {k - 1} - y ^ {k}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.336,
                0.445,
                0.355
            ],
            "angle": 0,
            "content": "其中 \\(v^{k}\\in \\partial g(y^{k})\\) 为 \\(g\\) 的一个次梯度"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.357,
                0.55,
                0.376
            ],
            "angle": 0,
            "content": "由 \\(A_{x}^{k}, A_{y}^{k}\\) 的定义和 \\(\\partial \\Psi\\) 的表达式 (8.4.26) 可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.311,
                0.383,
                0.595,
                0.404
            ],
            "angle": 0,
            "content": "\\[\nA _ {x} ^ {k} = \\nabla_ {x} H (x ^ {k}, y ^ {k}) + u ^ {k} \\in \\partial_ {x} \\Psi (x ^ {k}, y ^ {k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.313,
                0.406,
                0.592,
                0.43
            ],
            "angle": 0,
            "content": "\\[\nA _ {y} ^ {k} = \\nabla_ {y} H (x ^ {k}, y ^ {k}) + v ^ {k} \\in \\partial_ {y} \\Psi (x ^ {k}, y ^ {k}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.436,
                0.667,
                0.455
            ],
            "angle": 0,
            "content": "即有 \\(\\left(A_{x}^{k}, A_{y}^{k}\\right) \\in \\partial \\Psi\\left(x^{k}, y^{k}\\right)\\), 我们需要证明的第一个结论因此成立."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.457,
                0.737,
                0.496
            ],
            "angle": 0,
            "content": "下面估计 \\(A_x^k\\) 和 \\(A_y^k\\) 的模长。这里需要借助假设 8.2 的 (2)，即 \\(\\nabla H\\) 在有界集上关于 \\((x, y)\\) 是联合利普希茨连续的。因此对 \\(\\| A_x^k\\|\\) 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.223,
                0.5,
                0.682,
                0.653
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| A _ {x} ^ {k} \\right\\| \\leqslant \\frac {1}{c _ {k - 1}} \\left\\| x ^ {k - 1} - x ^ {k} \\right\\| + \\left\\| \\nabla_ {x} H \\left(x ^ {k}, y ^ {k}\\right) - \\nabla_ {x} H \\left(x ^ {k - 1}, y ^ {k - 1}\\right) \\right\\| \\\\ \\leqslant \\frac {1}{c _ {k - 1}} \\| x ^ {k - 1} - x ^ {k} \\| + L \\left(\\| x ^ {k - 1} - x ^ {k} \\| + \\| y ^ {k - 1} - y ^ {k} \\|\\right) \\\\ = \\left(L + \\frac {1}{c _ {k - 1}}\\right) \\| x ^ {k - 1} - x ^ {k} \\| + L \\| y ^ {k - 1} - y ^ {k} \\| \\\\ = (\\gamma + 1) L \\left\\| x ^ {k - 1} - x ^ {k} \\right\\| + L \\left\\| y ^ {k - 1} - y ^ {k} \\right\\| \\\\ \\leqslant (\\gamma + 2) L \\left\\| z ^ {k - 1} - z ^ {k} \\right\\|. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.66,
                0.737,
                0.698
            ],
            "angle": 0,
            "content": "其中，第二个不等式是根据 \\(\\nabla H\\) 的利普希茨连续性，最后一个不等式是将\\(\\| x^{k - 1} - x^k\\|\\) 和 \\(\\| y^{k - 1} - y^k\\|\\) 统一放大为 \\(\\| z^{k - 1} - z^k\\|\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.701,
                0.733,
                0.72
            ],
            "angle": 0,
            "content": "另一方面，对 \\(\\| A_y^k\\|\\) 的估计只需要用到 \\(\\nabla H\\) 关于 \\(y\\) 的利普希茨连续性："
        },
        {
            "type": "equation",
            "bbox": [
                0.233,
                0.725,
                0.673,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| A _ {y} ^ {k} \\right\\| \\leqslant \\frac {1}{d _ {k - 1}} \\left\\| y ^ {k} - y ^ {k - 1} \\right\\| + \\left\\| \\nabla_ {y} H \\left(x ^ {k}, y ^ {k}\\right) - \\nabla_ {y} H \\left(x ^ {k}, y ^ {k - 1}\\right) \\right\\| \\\\ \\leqslant \\frac {1}{d _ {k - 1}} \\| y ^ {k} - y ^ {k - 1} \\| + L \\| y ^ {k} - y ^ {k - 1} \\| \\\\ = \\left(\\frac {1}{d _ {k - 1}} + L\\right) \\| y ^ {k} - y ^ {k - 1} \\| \\\\ \\leqslant (\\gamma + 1) L \\left\\| z ^ {k} - z ^ {k - 1} \\right\\|. \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "406"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.486,
                0.174
            ],
            "angle": 0,
            "content": "结合这两个估计我们最终得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.189,
                0.826,
                0.211
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| \\left(A _ {x} ^ {k}, A _ {y} ^ {k}\\right) \\right\\| \\leqslant \\left\\| A _ {x} ^ {k} \\right\\| + \\left\\| A _ {y} ^ {k} \\right\\| \\leqslant (2 \\gamma + 3) L \\left\\| z ^ {k} - z ^ {k - 1} \\right\\| = \\rho_ {2} \\left\\| z ^ {k} - z ^ {k - 1} \\right\\|.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.226,
                0.825,
                0.305
            ],
            "angle": 0,
            "content": "从以上分析知道，随着迭代的进行，\\(\\partial \\Psi(z^k)\\) 将会包含一个模长不断趋于0的向量，这暗示着某种收敛性．在迭代序列 \\(\\{z^k\\}\\) 是有界的这一假设下，由于有界序列一定有收敛的子列，因此猜想 \\(\\{z^k\\}\\) 的极限点应该和 \\(\\Psi\\) 的临界点有一定的关系．实际上，我们有如下引理："
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.32,
                0.825,
                0.377
            ],
            "angle": 0,
            "content": "引理8.3（极限点集的性质）定义 \\(\\omega(z^0)\\) 为近似点交替线性化方法从点 \\(z^0\\) 出发产生迭代序列的所有极限点集，且 \\(\\{z^k\\}\\) 是有界序列，则以下结论成立："
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.393,
                0.734,
                0.411
            ],
            "angle": 0,
            "content": "(1) \\(\\varnothing \\neq \\omega (z^0)\\subset \\operatorname {crit}\\Psi\\) ，其中crit \\(\\Psi\\) 定义为 \\(\\Psi\\) 所有的临界点；"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.426,
                0.56,
                0.444
            ],
            "angle": 0,
            "content": "(2) \\(z^k\\) 与集合 \\(\\omega(z^0)\\) 的距离趋于 0, 即"
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.393,
                0.734,
                0.444
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.474,
                0.46,
                0.826,
                0.485
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\operatorname {d i s t} \\left(z ^ {k}, \\omega \\left(z ^ {0}\\right)\\right) = 0; \\tag {8.4.35}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.503,
                0.497,
                0.521
            ],
            "angle": 0,
            "content": "(3) \\(\\omega(z^0)\\) 是非空的连通紧集；"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.536,
                0.55,
                0.554
            ],
            "angle": 0,
            "content": "(4) \\(\\Psi\\) 在 \\(\\omega(z^0)\\) 上是一个有限的常数."
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.503,
                0.55,
                0.554
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.569,
                0.825,
                0.688
            ],
            "angle": 0,
            "content": "引理8.3的证明比较烦琐，读者可在[24]中找到严格的证明．该引理表明从点 \\(z^0\\) 出发产生的点列 \\(\\{z^k\\}\\) 的极限点都是 \\(\\Psi\\) 的临界点（次梯度集含有零向量）．至此我们已经得到了迭代序列 \\(\\{z^k\\}\\) 的子列收敛性，这至少保证了算法在迭代过程中与临界点越来越接近．一个自然的问题就是： \\(\\{z^k\\}\\) 全序列在何种条件下收敛？这就要进入理论分析的第三个步骤：利用函数的KL性质."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.716,
                0.515,
                0.733
            ],
            "angle": 0,
            "content": "3. 利用KL性质证明全序列收敛"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.747,
                0.825,
                0.786
            ],
            "angle": 0,
            "content": "证明 \\(\\{z^k\\}\\) 的全序列收敛性需要引入与KL性质相关的一些定义和概念.为了记号方便，给定实数 \\(\\alpha \\leqslant \\beta\\) ，定义 \\([\\alpha ,\\beta ]\\) 关于函数 \\(\\sigma\\) 的原像为"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.801,
                0.689,
                0.82
            ],
            "angle": 0,
            "content": "\\[\n[ \\alpha \\leqslant \\sigma \\leqslant \\beta ] = \\{x \\in \\mathbb {R} ^ {d} \\mid \\alpha \\leqslant \\sigma (x) \\leqslant \\beta \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.836,
                0.695,
                0.854
            ],
            "angle": 0,
            "content": "可类似地定义 \\([\\alpha < \\sigma < \\beta]\\). 接下来引入 \\(\\Phi_{\\eta}\\) 函数类的概念."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "407"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.737,
                0.194
            ],
            "angle": 0,
            "content": "定义8.5 \\((\\Phi_{\\eta}\\) 函数类）定义 \\(\\Phi_{\\eta}\\) 是凹连续函数 \\(\\varphi :[0,\\eta)\\to \\mathbb{R}_{+}\\) 的集合且满足如下条件："
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.205,
                0.289,
                0.222
            ],
            "angle": 0,
            "content": "(1) \\(\\varphi (0) = 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.234,
                0.512,
                0.252
            ],
            "angle": 0,
            "content": "(2) \\(\\varphi\\) 在 \\((0, \\eta)\\) 内连续可微，在点 0 处连续；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.262,
                0.485,
                0.28
            ],
            "angle": 0,
            "content": "(3) 对任意的 \\(s \\in (0, \\eta)\\)，都有 \\(\\varphi'(s) > 0\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.205,
                0.512,
                0.28
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.29,
                0.486,
                0.307
            ],
            "angle": 0,
            "content": "根据上面的定义我们可引入KL性质"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.317,
                0.737,
                0.355
            ],
            "angle": 0,
            "content": "定义8.6 (Kurdyka-Łojasiewicz (KL)性质）设 \\(\\sigma\\) ： \\(\\mathbb{R}^d\\to (-\\infty , + \\infty ]\\) 是适当下半连续函数."
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.363,
                0.739,
                0.404
            ],
            "angle": 0,
            "content": "(1) 称函数 \\(\\sigma\\) 在给定点 \\(\\bar{u} \\in \\mathbf{dom} \\partial \\sigma \\stackrel{\\mathrm{def}}{=} \\{u \\mid \\partial \\sigma(u) \\neq \\emptyset\\}\\) 处具有 \\(KL\\) 性质, 若存在 \\(\\eta \\in (0, +\\infty]\\) 和 \\(\\bar{u}\\) 的一个邻域 \\(U\\) 以及函数 \\(\\varphi \\in \\Phi_{\\eta}\\), 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.413,
                0.602,
                0.432
            ],
            "angle": 0,
            "content": "\\[\n\\forall u \\in U \\cap [ \\sigma (\\bar {u}) <   \\sigma <   \\sigma (\\bar {u}) + \\eta ],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.443,
                0.344,
                0.459
            ],
            "angle": 0,
            "content": "以下不等式成立："
        },
        {
            "type": "equation",
            "bbox": [
                0.336,
                0.47,
                0.612,
                0.49
            ],
            "angle": 0,
            "content": "\\[\n\\varphi^ {\\prime} (\\sigma (u) - \\sigma (\\bar {u})) \\cdot \\operatorname {d i s t} (0, \\partial \\sigma (u)) \\geqslant 1,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.501,
                0.524,
                0.517
            ],
            "angle": 0,
            "content": "其中 \\(\\mathrm{dist}(x,S)\\) 表示点 \\(x\\) 到集合 \\(S\\) 的距离."
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.529,
                0.68,
                0.546
            ],
            "angle": 0,
            "content": "(2) 若 \\(\\sigma\\) 在 \\(\\mathbf{dom} \\partial \\sigma\\) 上处处满足 KL 性质，则称 \\(\\sigma\\) 是一个 KL 函数."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.556,
                0.739,
                0.636
            ],
            "angle": 0,
            "content": "一大类函数都具有KL性质，该性质刻画了函数本身在给定点 \\(\\bar{u}\\) 处的某种行为．显然，如果点 \\(\\bar{u}\\) 不是函数 \\(\\sigma\\) 的临界点，那么KL性质在点 \\(\\bar{u}\\) 处自然成立．因此KL性质成立的不平凡情形是 \\(\\bar{u}\\) 是 \\(\\sigma\\) 的临界点，即 \\(0\\in \\partial \\sigma (\\bar{u})\\) 在这种情况下KL性质保证了“函数 \\(\\sigma\\) 可被锐化”．直观上来说，令"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.646,
                0.547,
                0.666
            ],
            "angle": 0,
            "content": "\\[\n\\tilde {\\varphi} (u) = \\varphi (\\sigma (u) - \\sigma (\\bar {u})),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.676,
                0.434,
                0.693
            ],
            "angle": 0,
            "content": "KL性质在某种条件下可以改写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.704,
                0.527,
                0.722
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {d i s t} (0, \\partial \\tilde {\\varphi} (u)) \\geqslant 1,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.732,
                0.739,
                0.812
            ],
            "angle": 0,
            "content": "其中 \\(u\\) 的取法需要保证 \\(\\sigma(u) > \\sigma(\\bar{u})\\). 以上性质表明, 无论 \\(u\\) 多么接近临界点 \\(\\bar{u}\\), 函数 \\(\\tilde{\\varphi}(u)\\) 的次梯度的模长均大于 1. 所以 KL 性质也被称为是函数 \\(\\sigma\\) 在重参数化子 \\(\\varphi\\) 下的一个锐化, 这种几何性质在分析一阶算法的收敛性时起到关键的作用."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.738,
                0.854
            ],
            "angle": 0,
            "content": "由于非凸问题有多个临界点，有时单个点 \\(\\bar{u}\\) 处的KL性质是不够的，我们需要引入一致KL性质."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "408"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "引理8.4（一致KL性质）设 \\(\\Omega\\) 是紧集，\\(\\sigma: \\mathbb{R}^d \\to (-\\infty, +\\infty]\\) 是适当下半连续函数，在 \\(\\Omega\\) 上为常数且在 \\(\\Omega\\) 的每个点处都满足KL性质，则存在 \\(\\varepsilon > 0, \\eta > 0, \\varphi \\in \\Phi_\\eta\\) 使得对任意 \\(\\bar{u} \\in \\Omega\\) 和所有满足以下条件的 \\(u\\)："
        },
        {
            "type": "equation",
            "bbox": [
                0.348,
                0.23,
                0.735,
                0.249
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{u \\in \\mathbb {R} ^ {d}: \\operatorname {d i s t} (u, \\Omega) <   \\varepsilon \\right\\} \\cap \\left[ \\sigma (\\bar {u}) <   \\sigma <   \\sigma (\\bar {u}) + \\eta \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.264,
                0.28,
                0.28
            ],
            "angle": 0,
            "content": "有"
        },
        {
            "type": "equation",
            "bbox": [
                0.41,
                0.284,
                0.673,
                0.302
            ],
            "angle": 0,
            "content": "\\[\n\\varphi^ {\\prime} (\\sigma (u) - \\sigma (\\bar {u})) \\mathrm {d i s t} (0, \\partial \\sigma (u)) \\geqslant 1.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.315,
                0.826,
                0.391
            ],
            "angle": 0,
            "content": "证明. 因为 \\(\\mathbb{R}^d\\) 上的紧集可以由有限多个开集覆盖，因此该问题可在有限个点上进行讨论. 设 \\(\\mu\\) 是 \\(\\sigma\\) 在 \\(\\Omega\\) 上的取值. 由于 \\(\\Omega\\) 是紧集，根据有限覆盖定理，存在有限多个开球 \\(B(u_i, \\varepsilon_i)\\) （其中 \\(u_i \\in \\Omega, i = 1, 2, \\dots, p\\)）使得 \\(\\Omega \\subset \\bigcup_{i=1}^{p} B(u_i, \\varepsilon_i)\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.392,
                0.825,
                0.447
            ],
            "angle": 0,
            "content": "现在考虑这些点 \\(u_{i}\\) ：在点 \\(u_{i}\\) 上KL性质成立，设 \\(\\varphi_{i}:[0,\\eta_{i})\\to \\mathbb{R}_{+}\\) 是对应的重参数化子，则对任意 \\(u\\in B(u_i,\\varepsilon_i)\\cap [\\mu <  \\sigma <  \\mu +\\eta_i]\\) ，有逐点的KL性质："
        },
        {
            "type": "equation",
            "bbox": [
                0.423,
                0.453,
                0.825,
                0.471
            ],
            "angle": 0,
            "content": "\\[\n\\varphi_ {i} ^ {\\prime} (\\sigma (u) - \\mu) \\operatorname {d i s t} (0, \\partial \\sigma (u)) \\geqslant 1. \\tag {8.4.36}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.481,
                0.431,
                0.497
            ],
            "angle": 0,
            "content": "取充分小的 \\(\\varepsilon > 0\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.367,
                0.508,
                0.715,
                0.548
            ],
            "angle": 0,
            "content": "\\[\nU _ {\\varepsilon} \\stackrel {{\\mathrm {d e f}}} {{=}} \\left\\{u \\in \\mathbb {R} ^ {d} \\mid \\operatorname {d i s t} (u, \\Omega) \\leqslant \\varepsilon \\right\\} \\subset \\bigcup_ {i = 1} ^ {p} B \\left(u _ {i}, \\varepsilon_ {i}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.559,
                0.414,
                0.581
            ],
            "angle": 0,
            "content": "取 \\(\\eta = \\min_{i}\\eta_{i}\\) ，以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.409,
                0.593,
                0.673,
                0.623
            ],
            "angle": 0,
            "content": "\\[\n\\varphi (s) = \\int_ {0} ^ {s} \\max  _ {i} \\varphi_ {i} ^ {\\prime} (t) d t, \\quad s \\in [ 0, \\eta).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.634,
                0.393,
                0.651
            ],
            "angle": 0,
            "content": "容易验证 \\(\\varphi \\in \\Phi_{\\eta}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.654,
                0.825,
                0.691
            ],
            "angle": 0,
            "content": "对任意的 \\(u \\in U_{\\varepsilon} \\cap [\\mu < \\sigma < \\mu + \\eta]\\), \\(u\\) 必定落在某个球 \\(B(u_{i_0}, \\varepsilon_{i_0})\\) 中, 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.31,
                0.703,
                0.773,
                0.754
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\varphi^ {\\prime} (\\sigma (u) - \\mu) \\mathrm {d i s t} (0, \\partial \\sigma (u)) = \\max  _ {i} \\varphi_ {i} ^ {\\prime} (\\sigma (u) - \\mu) \\mathrm {d i s t} (0, \\partial \\sigma (u)) \\\\ \\geqslant \\varphi_ {i _ {0}} ^ {\\prime} (\\sigma (u) - \\mu) \\mathrm {d i s t} (0, \\partial \\sigma (u)) \\geqslant 1. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.765,
                0.419,
                0.781
            ],
            "angle": 0,
            "content": "即一致KL性质成立"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "注意，该引理和普通KL性质的区别是 \\(\\varphi, \\eta\\) 的取法对 \\(\\bar{u}\\) 是一致的，不再依赖于 \\(\\bar{u}\\) 的具体位置。同时 \\(u\\) 选择的范围也相应地扩大了。有了上面的准备工作，我们可以利用KL性质证明 \\(\\{z^k\\}\\) 的全序列收敛性。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.335,
                0.133
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "409"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.737,
                0.194
            ],
            "angle": 0,
            "content": "定理8.11（有限长度性质）设 \\(\\Psi\\) 是 \\(KL\\) 函数，且假设8.2满足，则以下结论成立："
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.21,
                0.414,
                0.227
            ],
            "angle": 0,
            "content": "(1) 序列 \\(\\{z^k\\}\\) 的长度有限，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.24,
                0.737,
                0.276
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {k = 1} ^ {\\infty} \\left\\| z ^ {k + 1} - z ^ {k} \\right\\| <   + \\infty . \\tag {8.4.37}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.296,
                0.57,
                0.315
            ],
            "angle": 0,
            "content": "(2) 序列 \\(\\left\\{z^{k}\\right\\}\\) 收敛到 \\(\\Psi\\) 的一个临界点 \\(z^{*} = (x^{*}, y^{*})\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.329,
                0.737,
                0.387
            ],
            "angle": 0,
            "content": "证明. 由于 \\(\\{z^k\\}\\) 是有界序列，存在收敛子列 \\(\\{z^{k_q}\\} \\to \\bar{z},q\\to \\infty\\) ，和之前的推导类似，不管全序列 \\(\\{z^k\\}\\) 收敛性如何，对应的函数值列 \\(\\{\\Psi (z^k)\\}\\) 总是收敛的，且"
        },
        {
            "type": "equation",
            "bbox": [
                0.383,
                0.392,
                0.737,
                0.416
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {k \\rightarrow \\infty} \\Psi (z ^ {k}) = \\Psi (\\bar {z}). \\tag {8.4.38}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.426,
                0.737,
                0.485
            ],
            "angle": 0,
            "content": "以下不妨设 \\(\\Psi (\\bar{z}) < \\Psi (z^k),\\forall k\\in \\mathbb{N}\\) ，这是因为若存在 \\(\\bar{k}\\) 使得 \\(\\Psi (z^{\\bar{k}}) = \\Psi (\\bar{z})\\) 由充分下降性(8.4.28)可知 \\(z^{\\bar{k} +1} = z^{\\bar{k}}\\) ，进而有 \\(z^{k} = z^{\\bar{k}},\\forall k > \\bar{k}\\) 结论自然成立."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.49,
                0.737,
                0.528
            ],
            "angle": 0,
            "content": "由极限(8.4.38)和极限点集 \\(\\omega (z^0)\\) 的性质(8.4.35)，可知对任意的 \\(\\varepsilon ,\\eta >0\\) ，存在充分大的正整数 \\(l\\) ，使得对任意的 \\(k > l\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.303,
                0.544,
                0.606,
                0.563
            ],
            "angle": 0,
            "content": "\\[\n\\Psi (z ^ {k}) <   \\Psi (\\bar {z}) + \\eta , \\quad \\mathrm {d i s t} (z ^ {k}, \\omega (z ^ {0})) <   \\varepsilon .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.58,
                0.737,
                0.618
            ],
            "angle": 0,
            "content": "以上的分析说明当 \\(k\\) 充分大时，迭代点序列最终会满足一致KL性质的前提．下面就在这个结论下分别证明定理8.11的两个结论."
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.636,
                0.738,
                0.675
            ],
            "angle": 0,
            "content": "(1) 根据临界点的性质, \\(\\omega\\left(z^{0}\\right)\\) 是非空紧集, 且 \\(\\Psi\\) 在 \\(\\omega\\left(z^{0}\\right)\\) 上是常数. 在引理8.4中令 \\(\\Omega=\\omega\\left(z^{0}\\right)\\), 对任意的 \\(k+l\\),"
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.69,
                0.737,
                0.711
            ],
            "angle": 0,
            "content": "\\[\n\\varphi^ {\\prime} \\left(\\Psi \\left(z ^ {k}\\right) - \\Psi (\\bar {z})\\right) \\operatorname {d i s t} \\left(0, \\partial \\Psi \\left(z ^ {k}\\right)\\right) \\geqslant 1. \\tag {8.4.39}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.727,
                0.351,
                0.743
            ],
            "angle": 0,
            "content": "根据引理8.2可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.302,
                0.759,
                0.645,
                0.78
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {d i s t} \\left(0, \\partial \\Psi \\left(z ^ {k}\\right)\\right) \\leqslant \\left\\| \\left(A _ {x} ^ {k}, A _ {y} ^ {k}\\right) \\right\\| \\leqslant \\rho_ {2} \\| z ^ {k} - z ^ {k - 1} \\|.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.795,
                0.334,
                0.811
            ],
            "angle": 0,
            "content": "代入KL性质有"
        },
        {
            "type": "equation",
            "bbox": [
                0.333,
                0.822,
                0.737,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\varphi^ {\\prime} \\left(\\Psi \\left(z ^ {k}\\right) - \\Psi (\\bar {z})\\right) \\geqslant \\frac {1}{\\rho_ {2}} \\| z ^ {k} - z ^ {k - 1} \\| ^ {- 1}. \\tag {8.4.40}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "410"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.3,
                0.157,
                0.478,
                0.174
            ],
            "angle": 0,
            "content": "另外，由 \\(\\varphi\\) 的凹性，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.412,
                0.189,
                0.823,
                0.231
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\varphi \\left(\\Psi \\left(z ^ {k}\\right) - \\Psi (\\bar {z})\\right) - \\varphi \\left(\\Psi \\left(z ^ {k + 1}\\right) - \\Psi (\\bar {z})\\right) \\\\ \\geqslant \\varphi^ {\\prime} (\\Psi (z ^ {k}) - \\Psi (\\bar {z})) (\\Psi (z ^ {k}) - \\Psi (z ^ {k + 1})). \\tag {8.4.41} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.3,
                0.25,
                0.459,
                0.266
            ],
            "angle": 0,
            "content": "为了表示方便，定义"
        },
        {
            "type": "equation",
            "bbox": [
                0.395,
                0.282,
                0.729,
                0.302
            ],
            "angle": 0,
            "content": "\\[\n\\Delta_ {p, q} = \\varphi \\big (\\Psi (z ^ {p}) - \\Psi (\\bar {z}) \\big) - \\varphi \\big (\\Psi (z ^ {q}) - \\Psi (\\bar {z}) \\big),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.317,
                0.561,
                0.334
            ],
            "angle": 0,
            "content": "其中 \\(p, q\\) 为任意正整数. 定义常数"
        },
        {
            "type": "equation",
            "bbox": [
                0.51,
                0.345,
                0.613,
                0.379
            ],
            "angle": 0,
            "content": "\\[\nC = \\frac {2 \\rho_ {2}}{\\rho_ {1}} > 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.395,
                0.824,
                0.432
            ],
            "angle": 0,
            "content": "根据不等式 (8.4.41)，使用 (8.4.40) 式和定理 8.10 分别估计不等号右边的两项，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.371,
                0.447,
                0.823,
                0.54
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\Delta_ {k, k + 1} \\geqslant \\varphi^ {\\prime} \\left(\\Psi \\left(z ^ {k}\\right) - \\Psi (\\bar {z})\\right) \\left(\\Psi \\left(z ^ {k}\\right) - \\Psi \\left(z ^ {k + 1}\\right)\\right) \\\\ \\geqslant \\frac {1}{\\rho_ {2}} \\| z ^ {k} - z ^ {k - 1} \\| ^ {- 1} \\cdot \\frac {\\rho_ {1}}{2} \\| z ^ {k + 1} - z ^ {k} \\| ^ {2} \\tag {8.4.42} \\\\ = \\frac {\\left\\| z ^ {k + 1} - z ^ {k} \\right\\| ^ {2}}{C \\left\\| z ^ {k} - z ^ {k - 1} \\right\\|}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.3,
                0.551,
                0.356,
                0.567
            ],
            "angle": 0,
            "content": "等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.567,
                0.696,
                0.593
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| z ^ {k + 1} - z ^ {k} \\right\\| \\leqslant \\sqrt {C \\Delta_ {k , k + 1} \\left\\| z ^ {k} - z ^ {k - 1} \\right\\|}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.599,
                0.825,
                0.638
            ],
            "angle": 0,
            "content": "根据基本不等式 \\(2\\sqrt{ab} \\leqslant a + b, \\forall a, b > 0\\) ，我们取 \\(a = \\| z^k - z^{k-1}\\|\\), \\(b = C\\Delta_{k,k+1}\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.42,
                0.654,
                0.824,
                0.672
            ],
            "angle": 0,
            "content": "\\[\n2 \\| z ^ {k + 1} - z ^ {k} \\| \\leqslant \\| z ^ {k} - z ^ {k - 1} \\| + C \\Delta_ {k, k + 1}. \\tag {8.4.43}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.694,
                0.825,
                0.731
            ],
            "angle": 0,
            "content": "对任意的 \\(k > l\\) ，在(8.4.43)中把 \\(k\\) 替换成 \\(i\\) 并对 \\(i = l + 1, l + 2, \\dots, k\\) 求和，得"
        },
        {
            "type": "equation",
            "bbox": [
                0.336,
                0.744,
                0.789,
                0.824
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} 2 \\sum_ {i = l + 1} ^ {k} \\| z ^ {i + 1} - z ^ {i} \\| \\leqslant \\sum_ {i = l + 1} ^ {k} \\| z ^ {i} - z ^ {i - 1} \\| + C \\sum_ {i = l + 1} ^ {k} \\Delta_ {i, i + 1} \\\\ \\leqslant \\sum_ {i = l + 1} ^ {k} \\| z ^ {i + 1} - z ^ {i} \\| + \\| z ^ {l + 1} - z ^ {l} \\| + C \\Delta_ {l + 1, k + 1}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.836,
                0.607,
                0.855
            ],
            "angle": 0,
            "content": "最后一个不等式是因为 \\(\\Delta_{p,q} + \\Delta_{q,r} = \\Delta_{p,r}\\)"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.4 分块坐标下降法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.119,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "411"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.157,
                0.644,
                0.174
            ],
            "angle": 0,
            "content": "注意到上式不等号右边刚好可以和左边部分抵消，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.25,
                0.184,
                0.698,
                0.275
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\sum_ {i = l + 1} ^ {k} \\left\\| z ^ {i + 1} - z ^ {i} \\right\\| \\\\ \\leqslant \\| z ^ {l + 1} - z ^ {l} \\| + C \\left(\\varphi \\left(\\Psi \\left(z ^ {l + 1}\\right) - \\Psi (\\bar {z})\\right) - \\varphi \\left(\\Psi \\left(z ^ {k + 1}\\right) - \\Psi (\\bar {z})\\right)\\right) \\\\ \\leqslant \\| z ^ {l + 1} - z ^ {l} \\| + C \\varphi (\\Psi (z ^ {l + 1}) - \\Psi (\\bar {z})). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.286,
                0.697,
                0.302
            ],
            "angle": 0,
            "content": "不等式右边是有界的数且与 \\(k\\) 无关，由级数收敛的定义立即可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.31,
                0.559,
                0.347
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {k = 1} ^ {\\infty} \\left| \\left| z ^ {k + 1} - z ^ {k} \\right| \\right| <   + \\infty .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.361,
                0.737,
                0.399
            ],
            "angle": 0,
            "content": "(2) 在 (8.4.37) 的前提下 \\(\\{z^k\\}\\) 全序列收敛是显然的。这等价于证明 \\(\\{z^k\\}\\) 是柯西列。对任意 \\(q > p > l\\)，"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.409,
                0.567,
                0.449
            ],
            "angle": 0,
            "content": "\\[\nz ^ {q} - z ^ {p} = \\sum_ {k = p} ^ {q - 1} (z ^ {k + 1} - z ^ {k}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.459,
                0.344,
                0.475
            ],
            "angle": 0,
            "content": "根据三角不等式，"
        },
        {
            "type": "equation",
            "bbox": [
                0.3,
                0.482,
                0.651,
                0.527
            ],
            "angle": 0,
            "content": "\\[\n\\| z ^ {q} - z ^ {p} \\| = \\left\\| \\sum_ {k = p} ^ {q - 1} (z ^ {k + 1} - z ^ {k}) \\right\\| \\leqslant \\sum_ {k = p} ^ {q - 1} \\| z ^ {k + 1} - z ^ {k} \\|,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.537,
                0.738,
                0.59
            ],
            "angle": 0,
            "content": "而 \\(\\| z^{k + 1} - z^k\\|\\) 的可和性意味着 \\(\\sum_{k = l + 1}^{\\infty}\\| z^{k + 1} - z^k\\|\\) 趋于0．因此 \\(\\{z^k\\}\\) 是一个柯西列，算法产生的迭代序列有全序列收敛性. □"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.602,
                0.737,
                0.66
            ],
            "angle": 0,
            "content": "定理8.11的(1)有别于定理8.10的(2):后者只得到了 \\(\\| z^{k + 1} - z^k\\|\\) 平方可和的结论，而前者则说明从 \\(z^0\\) 出发，迭代序列的轨迹长度是有限的．这个结论显然比定理8.10中的要强，也是推导全序列收敛的关键."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.684,
                0.411,
                0.7
            ],
            "angle": 0,
            "content": "4. 一般问题的收敛性分析框架"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.714,
                0.737,
                0.771
            ],
            "angle": 0,
            "content": "总结以上三个步骤，我们实际上建立了一大类非凸问题优化算法收敛性分析的框架．给定函数 \\(\\Psi : \\mathbb{R}^d \\to (-\\infty, +\\infty]\\) 为适当下半连续函数，考虑极小化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.776,
                0.501,
                0.8
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {z \\in \\mathbb {R} ^ {d}} \\quad \\Psi (z).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.805,
                0.717,
                0.823
            ],
            "angle": 0,
            "content": "对任意的一般性算法 \\(\\mathcal{A}\\)，假定算法 \\(\\mathcal{A}\\) 以如下方式产生迭代序列 \\(\\{z^k\\}_{k\\in \\mathbb{N}}\\)："
        },
        {
            "type": "equation",
            "bbox": [
                0.307,
                0.835,
                0.598,
                0.853
            ],
            "angle": 0,
            "content": "\\[\nz ^ {0} \\in \\mathbb {R} ^ {d}, \\quad z ^ {k + 1} = \\mathcal {A} (z ^ {k}), \\quad k = 0, 1, \\dots .\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "412"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "我们的最终目标是要证明算法 \\(\\mathcal{A}\\) 产生的全序列收敛到 \\(\\Psi\\) 的一个稳定点。注意，在一般的分析框架下能比较容易地得到序列 \\(\\{z^k\\}\\) 的子列收敛性，若函数满足KL性质，则可以得到迭代序列的全序列收敛性。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.22,
                0.63,
                0.236
            ],
            "angle": 0,
            "content": "我们采用了如下步骤来证明该算法的收敛性："
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.252,
                0.825,
                0.289
            ],
            "angle": 0,
            "content": "(1) 充分下降：算法 \\(\\mathcal{A}\\) 本质上是一个下降算法，且每一步的下降量有一个下界估计："
        },
        {
            "type": "equation",
            "bbox": [
                0.378,
                0.305,
                0.744,
                0.324
            ],
            "angle": 0,
            "content": "\\[\n\\rho_ {1} \\| z ^ {k + 1} - z ^ {k} \\| ^ {2} \\leqslant \\Psi (z ^ {k}) - \\Psi (z ^ {k + 1}), \\quad k = 0, 1, \\dots ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.341,
                0.573,
                0.358
            ],
            "angle": 0,
            "content": "其中 \\(\\rho_{1}\\) 是和迭代次数 \\(k\\) 无关的常数"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.374,
                0.825,
                0.411
            ],
            "angle": 0,
            "content": "(2) 每次迭代时次梯度的上界: 假设算法 \\(\\mathcal{A}\\) 的迭代序列有界, 则在这一步需要找到另一个常数 \\(\\rho_{2}\\) 使得次梯度有一个上界估计:"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.427,
                0.763,
                0.447
            ],
            "angle": 0,
            "content": "\\[\n\\| w ^ {k + 1} \\| \\leqslant \\rho_ {2} \\| z ^ {k + 1} - z ^ {k} \\|, \\quad w ^ {k} \\in \\partial \\Psi (z ^ {k}), \\quad k = 0, 1, \\dots .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.469,
                0.825,
                0.506
            ],
            "angle": 0,
            "content": "(3) 应用KL性质：假设 \\(\\Psi\\) 是一个KL函数，证明迭代序列 \\(\\{z^k\\}\\) 是一个柯西列."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.522,
                0.826,
                0.622
            ],
            "angle": 0,
            "content": "前两个步骤是证明多数算法的基本步骤，当这两个性质成立时，对任意的算法 \\(\\mathcal{A}\\) 产生的迭代序列的聚点集合都为非空连通紧集，且这些聚点都是 \\(\\Psi\\) 的临界点。此外，前两个性质的成立依赖于算法本身的理论性质，而KL性质本质上是函数自身的性质，不依赖算法 \\(\\mathcal{A}\\) 的结构。因此第三步是推导全序列收敛的关键。"
        },
        {
            "type": "title",
            "bbox": [
                0.461,
                0.655,
                0.621,
                0.676
            ],
            "angle": 0,
            "content": "8.5 对偶算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.695,
                0.826,
                0.815
            ],
            "angle": 0,
            "content": "对于复合优化问题，许多实际问题的原始问题有时候比较难以处理，这时候一个非常重要的技巧是考虑它的对偶问题。本节将讲解两种算法：一种是把前面提到的算法应用到对偶问题上，例如对偶近似点梯度法；另一种是同时把原始问题和对偶问题结合起来考虑，例如原始－对偶混合梯度类的算法。我们将看到这两种算法的思想将极大地丰富求解问题的手段。为了方便起见，这一节主要考虑如下形式的问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.416,
                0.832,
                0.825,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\left(\\mathrm {P}\\right) \\min  _ {x \\in \\mathbb {R} ^ {n}} \\psi (x) = f (x) + h (A x), \\tag {8.5.1}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "8.5 对偶算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "413"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.195
            ],
            "angle": 0,
            "content": "其中 \\(f, h\\) 都是闭凸函数，\\(A \\in \\mathbb{R}^{m \\times n}\\) 为实数矩阵。通过引入约束 \\(y = Ax\\)，可以写出与问题(8.5.1)等价的约束优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.38,
                0.203,
                0.737,
                0.228
            ],
            "angle": 0,
            "content": "\\[\n\\min  f (x) + h (y), \\tag {8.5.2}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.392,
                0.227,
                0.492,
                0.242
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & y = A x. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.251,
                0.528,
                0.268
            ],
            "angle": 0,
            "content": "对约束 \\(y = Ax\\) 引入乘子 \\(z\\) ，得到拉格朗日函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.276,
                0.627,
                0.322
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} L (x, y, z) = f (x) + h (y) - z ^ {\\mathrm {T}} (y - A x) \\\\ = \\left(f (x) + \\left(A ^ {\\mathrm {T}} z\\right) ^ {\\mathrm {T}} x\\right) + \\left(h (y) - z ^ {\\mathrm {T}} y\\right). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.328,
                0.6,
                0.345
            ],
            "angle": 0,
            "content": "利用共轭函数的定义(2.6.1)，可计算拉格朗日对偶问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.302,
                0.354,
                0.737,
                0.379
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {z} \\phi (z) = - f ^ {*} (- A ^ {\\mathrm {T}} z) - h ^ {*} (z). \\tag {8.5.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.385,
                0.737,
                0.422
            ],
            "angle": 0,
            "content": "特别值得注意的是，本章讲解的算法不局限于求解上述形式的问题，在学习过程中注意灵活推广到求解其他形式的问题。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.447,
                0.391,
                0.465
            ],
            "angle": 0,
            "content": "8.5.1 对偶近似点梯度法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.478,
                0.738,
                0.577
            ],
            "angle": 0,
            "content": "在实际应用中，我们会发现许多时候对偶问题的求解会比原始问题的求解容易很多，这时候可以把之前讲过的各种算法，例如梯度下降算法、近似点梯度算法、增广拉格朗日函数法等，应用到对偶问题上。本小节我们将近似点梯度算法应用到对偶问题上，得到对偶近似点梯度法，还将讨论与其等价的、针对原始问题设计的算法。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.581,
                0.738,
                0.68
            ],
            "angle": 0,
            "content": "对偶问题(8.5.3)是无约束的复合优化形式，因此可以考虑第8.1节的近似点梯度算法．要使用该算法，首先要解决的问题是对偶问题的目标函数\\(\\phi (z)\\) 是否是“可微函数 \\(+\\) 凸函数”的复合形式．如果假设原始问题中 \\(f(x)\\) 是闭的强凸函数（强凸参数为 \\(\\mu\\)），下面的引理说明其共轭函数是定义在全空间 \\(\\mathbb{R}^n\\) 上的梯度利普希茨连续函数："
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.691,
                0.738,
                0.761
            ],
            "angle": 0,
            "content": "引理8.5（强凸函数共轭函数的性质）设 \\(f(x)\\) 是适当且闭的强凸函数，其强凸参数为 \\(\\mu > 0\\) ，\\(f^{*}(y)\\) 是 \\(f(x)\\) 的共轭函数，则 \\(f^{*}(y)\\) 在全空间 \\(\\mathbb{R}^n\\) 上有定义，且 \\(f^{*}(y)\\) 是梯度 \\(\\frac{1}{\\mu}\\)-利普希茨连续的可微函数。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.767,
                0.738,
                0.826
            ],
            "angle": 0,
            "content": "证明. 显然对任意的 \\(y \\in \\mathbb{R}^{n}\\), 函数 \\(f(x) - x^{\\mathrm{T}}y\\) 是强凸函数, 根据定理 8.1 的证明过程立即知道对任意的 \\(y \\in \\mathbb{R}^{n}\\), 存在唯一的 \\(x \\in \\operatorname{dom} f\\), 使得 \\(f^{*}(y) = x^{\\mathrm{T}}y - f(x)\\). 根据凸优化问题的一阶最优性条件可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.326,
                0.835,
                0.579,
                0.855
            ],
            "angle": 0,
            "content": "\\[\ny \\in \\partial f (x) \\Leftrightarrow f ^ {*} (y) = x ^ {\\mathrm {T}} y - f (x).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "414"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.193
            ],
            "angle": 0,
            "content": "此外根据定理 2.15 可知 \\(f(x)\\) 的二次共轭为其本身, 于是对同一组 \\(x, y\\) 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.348,
                0.197,
                0.734,
                0.226
            ],
            "angle": 0,
            "content": "\\[\nx ^ {T} y - f ^ {*} (y) = f (x) = f ^ {* *} (x) = \\sup  _ {y} \\left\\{x ^ {T} y - f ^ {*} (y) \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.232,
                0.745,
                0.25
            ],
            "angle": 0,
            "content": "这说明 \\(y\\) 也使得 \\(x^{\\mathrm{T}}y - f^{*}(y)\\) 取到最大值．根据一阶最优性条件，"
        },
        {
            "type": "equation",
            "bbox": [
                0.497,
                0.263,
                0.585,
                0.281
            ],
            "angle": 0,
            "content": "\\[\nx \\in \\partial f ^ {*} (y).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.294,
                0.771,
                0.311
            ],
            "angle": 0,
            "content": "再根据 \\(x\\) 的唯一性容易推出 \\(\\partial f^{*}(y)\\) 中只含一个元素，故 \\(f^{*}(y)\\) 可微"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.312,
                0.825,
                0.358
            ],
            "angle": 0,
            "content": "下证其为梯度 \\(\\frac{1}{\\mu}\\)-利普希茨连续的. 对任意的 \\(y_{1}, y_{2}\\), 存在唯一的 \\(x_{1}, x_{2} \\in \\mathbf{dom} f\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.44,
                0.362,
                0.642,
                0.38
            ],
            "angle": 0,
            "content": "\\[\ny _ {1} \\in \\partial f (x _ {1}), \\quad y _ {2} \\in \\partial f (x _ {2}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.384,
                0.607,
                0.412
            ],
            "angle": 0,
            "content": "根据次梯度性质以及 \\(f(x) - \\frac{\\mu}{2}\\| x\\|^2\\) 是凸函数，"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.422,
                0.684,
                0.442
            ],
            "angle": 0,
            "content": "\\[\nf (x _ {2}) \\geqslant f (x _ {1}) + \\left(y _ {1} - \\mu x _ {1}\\right) ^ {\\mathrm {T}} \\left(x _ {2} - x _ {1}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.446,
                0.684,
                0.466
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x _ {1}\\right) \\geqslant f \\left(x _ {2}\\right) + \\left(y _ {2} - \\mu x _ {2}\\right) ^ {\\mathrm {T}} \\left(x _ {1} - x _ {2}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.476,
                0.401,
                0.492
            ],
            "angle": 0,
            "content": "将上述两式相加得"
        },
        {
            "type": "equation",
            "bbox": [
                0.411,
                0.505,
                0.672,
                0.525
            ],
            "angle": 0,
            "content": "\\[\n\\left(y _ {1} - y _ {2}\\right) ^ {\\mathrm {T}} \\left(x _ {1} - x _ {2}\\right) \\geqslant \\mu \\| x _ {1} - x _ {2} \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.536,
                0.785,
                0.555
            ],
            "angle": 0,
            "content": "根据 \\(x\\) 和 \\(y\\) 的关系我们有 \\(x_{1} = \\nabla f^{*}(y_{1}), x_{2} = \\nabla f^{*}(y_{2})\\)，代入上式可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.318,
                0.565,
                0.767,
                0.586
            ],
            "angle": 0,
            "content": "\\[\n\\left(y _ {1} - y _ {2}\\right) ^ {\\mathrm {T}} \\left(\\nabla f ^ {*} \\left(y _ {1}\\right) - \\nabla f ^ {*} \\left(y _ {2}\\right)\\right) \\geqslant \\mu \\| \\nabla f ^ {*} \\left(y _ {1}\\right) - \\nabla f ^ {*} \\left(y _ {2}\\right) \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.594,
                0.825,
                0.641
            ],
            "angle": 0,
            "content": "注意到这正是 \\(\\nabla f^{*}(y)\\) 的余强制性，根据引理6.1可知 \\(\\nabla f^{*}(y)\\) 是 \\(\\frac{1}{\\mu}\\)-利普希茨连续的. □"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.654,
                0.825,
                0.72
            ],
            "angle": 0,
            "content": "经过上面的推导，我们知道 \\(\\nabla f^{*}\\) 是利普希茨连续函数，因此在对偶问题(8.5.3)中 \\(f^{*}(-A^{\\mathrm{T}}z)\\) 是梯度 \\(\\frac{1}{\\mu}\\| A\\| _2^2\\) 利普希茨连续的函数，这是因为对于任意的 \\(z_{1},z_{2}\\)，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.319,
                0.726,
                0.765,
                0.797
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| A \\nabla f ^ {*} \\left(- A ^ {T} z _ {1}\\right) - A \\nabla f ^ {*} \\left(- A ^ {T} z _ {2}\\right) \\right\\| \\leqslant \\frac {1}{\\mu} \\| A \\| _ {2} \\| A ^ {T} \\left(z _ {1} - z _ {2}\\right) \\| \\\\ \\leqslant \\frac {\\left\\| A \\right\\| _ {2} ^ {2}}{\\mu} \\| z _ {1} - z _ {2} \\|. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.806,
                0.751,
                0.822
            ],
            "angle": 0,
            "content": "考虑在对偶问题上应用近似点梯度算法，每次迭代更新如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.401,
                0.834,
                0.825,
                0.855
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = \\operatorname {p r o x} _ {t h ^ {*}} \\left(z ^ {k} + t A \\nabla f ^ {*} \\left(- A ^ {\\mathrm {T}} z ^ {k}\\right)\\right), \\tag {8.5.4}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.283,
                0.133
            ],
            "angle": 0,
            "content": "8.5 对偶算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "415"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.215
            ],
            "angle": 0,
            "content": "这里注意对偶问题是取最大值，因此邻近算子内部应该取上升方向。引入变量 \\(x^{k + 1} = \\nabla f^{*}(-A^{\\mathrm{T}}z^{k})\\) ，利用共轭函数的性质得 \\(-A^{\\mathrm{T}}z^{k}\\in \\partial f(x^{k + 1})\\) 。因此迭代格式(8.5.4)等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.224,
                0.736,
                0.258
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\underset {x} {\\arg \\min } \\left\\{f (x) + \\left(A ^ {\\mathrm {T}} z ^ {k}\\right) ^ {\\mathrm {T}} x \\right\\}, \\tag {8.5.5}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.327,
                0.259,
                0.537,
                0.278
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = \\operatorname {p r o x} _ {t h ^ {*}} \\left(z ^ {k} + t A x ^ {k + 1}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.289,
                0.741,
                0.348
            ],
            "angle": 0,
            "content": "迭代格式(8.5.5)仅仅将计算 \\(\\nabla f^{*}(-A^{\\mathrm{T}}z^{k})\\) 化成了一个共轭函数的求解问题，本质上和迭代格式(8.5.4)是一样的。但下面的分析会提供另一种角度来理解对偶近似点梯度法。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.352,
                0.724,
                0.369
            ],
            "angle": 0,
            "content": "我们先引入有关邻近算子和共轭函数的一个重要性质：Moreau分解."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.38,
                0.737,
                0.417
            ],
            "angle": 0,
            "content": "引理8.6 (Moreau分解) 设 \\(f\\) 是定义在 \\(\\mathbb{R}^n\\) 上的适当的闭凸函数，则对任意的 \\(x \\in \\mathbb{R}^n\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.352,
                0.422,
                0.737,
                0.442
            ],
            "angle": 0,
            "content": "\\[\nx = \\operatorname {p r o x} _ {f} (x) + \\operatorname {p r o x} _ {f ^ {*}} (x); \\tag {8.5.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.449,
                0.269,
                0.464
            ],
            "angle": 0,
            "content": "或更一般地，"
        },
        {
            "type": "equation",
            "bbox": [
                0.325,
                0.464,
                0.737,
                0.493
            ],
            "angle": 0,
            "content": "\\[\nx = \\operatorname {p r o x} _ {\\lambda f} (x) + \\lambda \\operatorname {p r o x} _ {\\lambda^ {- 1} f ^ {*}} \\left(\\frac {x}{\\lambda}\\right), \\tag {8.5.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.496,
                0.368,
                0.512
            ],
            "angle": 0,
            "content": "其中 \\(\\lambda > 0\\) 为任意正实数."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.525,
                0.739,
                0.604
            ],
            "angle": 0,
            "content": "引理8.6的证明并不复杂，主要思路和引理8.5完全相同，细节留给读者完成．Moreau分解的结论非常漂亮，它表明：对任意的闭凸函数 \\(f\\) ，空间 \\(\\mathbb{R}^n\\) 上的恒等映射总可以分解成两个函数 \\(f\\) 与 \\(f^{*}\\) 邻近算子的和．根据Moreau分解的一般形式（取 \\(\\lambda = t,f = h^{*}\\) ，并注意到 \\(h^{**} = h\\) )，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.22,
                0.613,
                0.682,
                0.684
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} z ^ {k} + t A x ^ {k + 1} = \\operatorname {p r o x} _ {t h ^ {*}} \\left(z ^ {k} + t A x ^ {k + 1}\\right) + t \\operatorname {p r o x} _ {t ^ {- 1} h} \\left(\\frac {z ^ {k}}{t} + A x ^ {k + 1}\\right) \\\\ = z ^ {k + 1} + t \\operatorname {p r o x} _ {t ^ {- 1} h} \\left(\\frac {z ^ {k}}{t} + A x ^ {k + 1}\\right), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.692,
                0.63,
                0.708
            ],
            "angle": 0,
            "content": "由此给出对偶近似点梯度法等价的针对原始问题的更新格式："
        },
        {
            "type": "equation",
            "bbox": [
                0.2,
                0.717,
                0.459,
                0.746
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\underset {x} {\\arg \\min } \\left\\{f (x) + \\left(z ^ {k}\\right) ^ {\\mathrm {T}} A x \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.201,
                0.749,
                0.737,
                0.823
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} y ^ {k + 1} = \\operatorname {p r o x} _ {t ^ {- 1} h} \\left(\\frac {z ^ {k}}{t} + A x ^ {k + 1}\\right) \\tag {8.5.8} \\\\ = \\underset {y} {\\arg \\min } \\left\\{h (y) - \\left(z ^ {k}\\right) ^ {\\mathrm {T}} \\left(y - A x ^ {k + 1}\\right) + \\frac {t}{2} \\| A x ^ {k + 1} - y \\| _ {2} ^ {2} \\right\\}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.202,
                0.828,
                0.414,
                0.848
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = z ^ {k} + t \\left(A x ^ {k + 1} - y ^ {k + 1}\\right).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "416"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.79,
                0.174
            ],
            "angle": 0,
            "content": "如果我们写出约束优化问题(8.5.2)的拉格朗日函数和增广拉格朗日函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.186,
                0.631,
                0.203
            ],
            "angle": 0,
            "content": "\\[\nL (x, y, z) = f (x) + h (y) - z ^ {\\mathrm {T}} (y - A x),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.343,
                0.208,
                0.743,
                0.238
            ],
            "angle": 0,
            "content": "\\[\nL _ {t} (x, y, z) = f (x) + h (y) - z ^ {\\mathrm {T}} (y - A x) + \\frac {t}{2} \\| y - A x \\| ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.245,
                0.515,
                0.261
            ],
            "angle": 0,
            "content": "则迭代格式(8.5.8)可以等价地写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.436,
                0.273,
                0.629,
                0.299
            ],
            "angle": 0,
            "content": "\\[\nx^{k + 1} = \\operatorname *{arg  min}_{x}L(x,y^{k},z^{k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.436,
                0.305,
                0.824,
                0.334
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = \\underset {y} {\\arg \\min } L _ {t} \\left(x ^ {k + 1}, y, z ^ {k}\\right), \\tag {8.5.9}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.437,
                0.34,
                0.645,
                0.358
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = z ^ {k} + t \\left(A x ^ {k + 1} - y ^ {k + 1}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.369,
                0.825,
                0.51
            ],
            "angle": 0,
            "content": "迭代格式(8.5.9)又称为交替极小化方法：第一步迭代为在拉格朗日函数中关于 \\(x\\) 求极小，第二步迭代为在增广拉格朗日函数中关于 \\(y\\) 求极小，第三步迭代为更新拉格朗日乘子。交替极小化方法与下一节介绍的交替方向乘子法有非常相似的结构。上面的分析表明，对偶近似点梯度法等价于对原始问题(8.5.2)使用交替极小化方法。若 \\(f\\) 可分，可将 \\(x\\) 的求解划分成几个独立的子问题求解；在 \\(z\\) 的更新中，步长 \\(t\\) 可以为常数，或者由线搜索决定。在上述更新框架下，还可以考虑引入加速版本的近似点梯度算法。"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.514,
                0.782,
                0.53
            ],
            "angle": 0,
            "content": "下面我们给出四个例子来说明如何拆分和使用对偶近似点梯度法"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.543,
                0.687,
                0.559
            ],
            "angle": 0,
            "content": "例 8.15 (正则化范数近似) 假设 \\(f\\) 是强凸函数, 考虑"
        },
        {
            "type": "equation",
            "bbox": [
                0.455,
                0.573,
                0.627,
                0.59
            ],
            "angle": 0,
            "content": "\\[\n\\min  f (x) + \\| A x - b \\|,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.604,
                0.823,
                0.641
            ],
            "angle": 0,
            "content": "其中 \\(\\| \\cdot \\|\\) 是任意一种范数。对应原始问题(8.5.1)我们有 \\(h(y) = \\| y - b\\|\\)，可计算出 \\(h(y)\\) 的共轭函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.44,
                0.651,
                0.641,
                0.702
            ],
            "angle": 0,
            "content": "\\[\nh ^ {*} (z) = \\left\\{ \\begin{array}{l l} b ^ {\\mathrm {T}} z, & \\| z \\| _ {*} \\leqslant 1 \\\\ + \\infty , & \\text {其 他}, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.711,
                0.647,
                0.728
            ],
            "angle": 0,
            "content": "其中 \\(\\| \\cdot \\|_{*}\\) 表示 \\(\\| \\cdot \\|\\) 的对偶范数．从而对偶问题为："
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.74,
                0.642,
                0.767
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {\\| z \\| _ {*} \\leqslant 1} - f ^ {*} (- A ^ {T} z) - b ^ {T} z,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.776,
                0.528,
                0.792
            ],
            "angle": 0,
            "content": "应用对偶近似点梯度法，更新如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.411,
                0.803,
                0.674,
                0.829
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\underset {x} {\\arg \\min } \\left\\{f (x) + (A ^ {\\mathrm {T}} z ^ {k}) ^ {\\mathrm {T}} x \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.413,
                0.836,
                0.666,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = \\mathcal {P} _ {\\| z \\| _ {*} \\leqslant 1} (z ^ {k} + t (A x ^ {k + 1} - b)).\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "8.5 对偶算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "417"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.157,
                0.465,
                0.174
            ],
            "angle": 0,
            "content": "例8.16 假设 \\(f\\) 是强凸函数，考虑"
        },
        {
            "type": "equation",
            "bbox": [
                0.361,
                0.18,
                0.548,
                0.218
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad f (x) + \\sum_ {i = 1} ^ {p} \\| B _ {i} x \\| _ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.224,
                0.429,
                0.261
            ],
            "angle": 0,
            "content": "即 \\(h(y_{1},y_{2},\\dots ,y_{p}) = \\sum_{i = 1}^{p}\\| y_{i}\\|_{2}\\) ，且"
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.266,
                0.554,
                0.297
            ],
            "angle": 0,
            "content": "\\[\nA = \\left[ \\begin{array}{c c c c} B _ {1} ^ {\\mathrm {T}} & B _ {1} ^ {\\mathrm {T}} & \\dots & B _ {p} ^ {\\mathrm {T}} \\end{array} \\right] ^ {\\mathrm {T}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.301,
                0.737,
                0.339
            ],
            "angle": 0,
            "content": "假定 \\(B_{i}\\in \\mathbb{R}^{m_{i}\\times n}\\) ，记 \\(C_i\\) 是 \\(\\mathbb{R}_{m_i}\\) 中的单位欧几里得球，根据 \\(\\| \\cdot \\| _2\\) 的共轭函数定义，对偶问题形式如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.343,
                0.556,
                0.387
            ],
            "angle": 0,
            "content": "\\[\n\\max  _ {\\| z _ {i} \\| _ {2} \\leqslant 1} - f ^ {*} \\left(- \\sum_ {i = 1} ^ {p} B _ {i} ^ {T} z _ {i}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.391,
                0.424,
                0.407
            ],
            "angle": 0,
            "content": "从而对偶近似点梯度法更新如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.306,
                0.41,
                0.601,
                0.478
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} x ^ {k + 1} = \\underset {x} {\\arg \\min } \\left\\{f (x) + \\left(\\sum_ {i = 1} ^ {p} B _ {i} ^ {\\mathrm {T}} z _ {i}\\right) ^ {\\mathrm {T}} x \\right\\}, \\\\ z _ {i} ^ {k + 1} = \\mathcal {P} _ {C _ {i}} \\left(z _ {i} ^ {k} + t B _ {i} x ^ {k + 1}\\right), i = 1, 2, \\dots , p. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.484,
                0.635,
                0.501
            ],
            "angle": 0,
            "content": "例 8.17 (在凸集交上的极小化) 假设 \\( f \\) 是强凸函数, 考虑"
        },
        {
            "type": "equation",
            "bbox": [
                0.342,
                0.51,
                0.564,
                0.548
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  f (x), \\\\ \\begin{array}{l l} \\text {s . t .} & x \\in C _ {1} \\cap C _ {2} \\cap \\dots \\cap C _ {m}, \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.557,
                0.719,
                0.592
            ],
            "angle": 0,
            "content": "其中 \\(C_{i}\\) 为闭凸集，易于计算投影．记 \\(h(y_{1},y_{2},\\dots ,y_{m}) = \\sum_{i = 1}^{m}I_{C_{i}}(y_{i})\\) ，以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.371,
                0.597,
                0.537,
                0.629
            ],
            "angle": 0,
            "content": "\\[\nA = \\left[ \\begin{array}{c c c c} I & I & \\dots & I \\end{array} \\right] ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.633,
                0.356,
                0.648
            ],
            "angle": 0,
            "content": "从而对偶问题形式如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.653,
                0.586,
                0.697
            ],
            "angle": 0,
            "content": "\\[\n\\max _ {z _ {i} \\in C _ {i}} - f ^ {*} \\left(- \\sum_ {i = 1} ^ {m} z _ {i}\\right) - \\sum_ {i = 1} ^ {m} I _ {C _ {i}} ^ {*} (z _ {i}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.7,
                0.737,
                0.738
            ],
            "angle": 0,
            "content": "利用共轭函数的性质可知 \\(I_{C_i}^*(z_i)\\) 是集合 \\(C_i\\) 的支撑函数，其显式表达式不易求出。因此我们利用Moreau分解将迭代格式写成交替极小化方法的形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.29,
                0.741,
                0.582,
                0.794
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\underset {x} {\\arg \\min } \\left\\{f (x) + \\left(\\sum_ {i = 1} ^ {m} z _ {i}\\right) ^ {\\mathrm {T}} x \\right\\},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.291,
                0.794,
                0.603,
                0.831
            ],
            "angle": 0,
            "content": "\\[\ny _ {i} ^ {k + 1} = \\mathcal {P} _ {C _ {i}} \\left(\\frac {z _ {i} ^ {k}}{t} + x ^ {k + 1}\\right), \\quad i = 1, 2, \\dots , m,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.293,
                0.834,
                0.616,
                0.855
            ],
            "angle": 0,
            "content": "\\[\nz _ {i} ^ {k + 1} = z _ {i} ^ {k} + t \\left(x ^ {k + 1} - y _ {i} ^ {k + 1}\\right), \\quad i = 1, 2, \\dots , m.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "418"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.157,
                0.824,
                0.194
            ],
            "angle": 0,
            "content": "例8.18（可分问题的拆分）假设 \\(f_{i}\\) 是强凸函数，\\(h_i^*\\) 有易于计算的邻近算子．考虑"
        },
        {
            "type": "equation",
            "bbox": [
                0.341,
                0.203,
                0.741,
                0.243
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\sum_ {j = 1} ^ {n} f _ {j} \\left(x _ {j}\\right) + \\sum_ {i = 1} ^ {m} h _ {i} \\left(A _ {i 1} x _ {1} + A _ {i 2} x _ {2} + \\dots + A _ {i n} \\bar {x} _ {N}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.253,
                0.424,
                0.268
            ],
            "angle": 0,
            "content": "其对偶问题形式如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.278,
                0.759,
                0.317
            ],
            "angle": 0,
            "content": "\\[\n\\max  - \\sum_ {i = 1} ^ {m} h _ {i} ^ {*} (z _ {i}) - \\sum_ {j = 1} ^ {n} f _ {j} ^ {*} (- A _ {1 j} ^ {\\mathrm {T}} z _ {1} - A _ {2 j} ^ {\\mathrm {T}} z _ {2} - \\dots - A _ {m j} ^ {\\mathrm {T}} z _ {m}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.327,
                0.477,
                0.342
            ],
            "angle": 0,
            "content": "对偶近似点梯度法更新如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.328,
                0.351,
                0.754,
                0.441
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} x _ {j} ^ {k + 1} = \\underset {x _ {j}} {\\arg \\min } \\left\\{f _ {j} \\left(x _ {j}\\right) + \\left(\\sum_ {i = 1} ^ {m} A _ {i j} z _ {i} ^ {k}\\right) ^ {\\mathrm {T}} x _ {j} \\right\\}, \\quad j = 1, 2, \\dots , n, \\\\ z _ {i} ^ {k + 1} = \\operatorname {p r o x} _ {t h _ {i} ^ {*}} \\left(z _ {i} + t \\sum_ {j = 1} ^ {n} A _ {i j} x _ {j} ^ {k + 1}\\right), \\quad i = 1, 2, \\dots , m. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.458,
                0.538,
                0.477
            ],
            "angle": 0,
            "content": "8.5.2 原始-对偶混合梯度算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.489,
                0.825,
                0.734
            ],
            "angle": 0,
            "content": "本小节介绍另外一种非常重要的算法——原始-对偶混合梯度(primal-dual hybrid gradient, PDHG)算法．对于给定的优化问题，我们可以从原始问题或对偶问题出发来设计相应算法求解．那么能否将两个方面结合起来呢？PDHG算法就是借鉴了这样的思想．相比于直接在原始问题上或者对偶问题上应用优化算法求解，PDHG算法在每次迭代时同时考虑原始变量和对偶变量的更新．这使得它在一定程度上可以有效避免单独针对原始问题或对偶问题求解算法中可能出现的问题，例如，原始问题梯度为零向量或不可微，对偶问题形式复杂等．本小节将介绍原始-对偶混合梯度算法以及它的一个变形——Chambolle-Pock算法．在这里需要注意，由于本书在之前引入了线性规划的原始-对偶算法，本节中讲述的算法的名称很容易和它的混淆．为了避免歧义我们使用PDHG算法来特指本节的原始-对偶混合梯度法，读者应当注意这个区别."
        },
        {
            "type": "text",
            "bbox": [
                0.291,
                0.738,
                0.816,
                0.755
            ],
            "angle": 0,
            "content": "PDHG 算法的构造要从鞍点问题谈起. 我们仍然考虑原始问题(8.5.1):"
        },
        {
            "type": "equation",
            "bbox": [
                0.46,
                0.769,
                0.623,
                0.787
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad f (x) + h (A x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.801,
                0.824,
                0.818
            ],
            "angle": 0,
            "content": "其中 \\(f, h\\) 是适当的闭凸函数. 由于 \\(h\\) 有自共轭性, 我们将问题(8.5.1)变形为"
        },
        {
            "type": "equation",
            "bbox": [
                0.31,
                0.829,
                0.826,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\left(L _ {P D}\\right) \\quad \\min  _ {x} \\max  _ {z} \\quad \\psi_ {P D} (x, z) \\stackrel {\\text {d e f}} {=} f (x) - h ^ {*} (z) + z ^ {T} A x. \\tag {8.5.10}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.283,
                0.133
            ],
            "angle": 0,
            "content": "8.5 对偶算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "419"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.737,
                0.195
            ],
            "angle": 0,
            "content": "可以看到此时问题(8.5.1)变成了一个极小-极大问题，即关于变量 \\(x\\) 求极小，关于变量 \\(z\\) 求极大，这是一个典型的鞍点问题。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.199,
                0.737,
                0.235
            ],
            "angle": 0,
            "content": "另一种常用的鞍点问题定义方式是直接利用带约束的问题(8.5.2)构造拉格朗日函数．问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.305,
                0.252,
                0.603,
                0.277
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}, y \\in \\mathbb {R} ^ {m}} f (x) + h (y), \\quad \\text {s . t .} \\quad y = A x.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.288,
                0.373,
                0.305
            ],
            "angle": 0,
            "content": "相应的鞍点问题形式如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.285,
                0.319,
                0.737,
                0.345
            ],
            "angle": 0,
            "content": "\\[\n\\left(L _ {P}\\right) \\min  _ {x, y} \\max  _ {z} f (x) + h (y) + z ^ {\\mathrm {T}} (A x - y). \\tag {8.5.11}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.357,
                0.701,
                0.376
            ],
            "angle": 0,
            "content": "类似地，在对偶问题(8.5.3)中引入变量 \\(w = -A^{\\mathrm{T}}z\\)，则可定义鞍点问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.235,
                0.389,
                0.737,
                0.415
            ],
            "angle": 0,
            "content": "\\[\n\\left(L _ {D}\\right) \\min  _ {p} \\max  _ {w, z} - f ^ {*} (w) - h ^ {*} (z) + p ^ {\\mathrm {T}} \\left(w + A ^ {\\mathrm {T}} z\\right). \\tag {8.5.12}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.426,
                0.739,
                0.505
            ],
            "angle": 0,
            "content": "鞍点问题是关于某些变量求极小的同时关于另一些变量求极大，直接求解比较困难。PDHG算法的思想就是分别对两类变量应用近似点梯度算法。以求解问题(8.5.10)为例，PDHG算法交替更新原始变量以及对偶变量，其迭代格式如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.202,
                0.514,
                0.737,
                0.584
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} z ^ {k + 1} = \\arg \\max  _ {z} \\left\\{- h ^ {*} (z) + \\left\\langle A x ^ {k}, z - z ^ {k} \\right\\rangle - \\frac {1}{2 \\delta_ {k}} \\| z - z ^ {k} \\| _ {2} ^ {2} \\right\\} \\\\ = \\operatorname {p r o x} _ {\\delta_ {k} h ^ {*}} \\left(z ^ {k} + \\delta_ {k} A x ^ {k}\\right), \\tag {8.5.13} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.205,
                0.578,
                0.648,
                0.639
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} x ^ {k + 1} = \\arg \\min  _ {x} \\left\\{f (x) + \\left(z ^ {k + 1}\\right) ^ {\\mathrm {T}} A \\left(x - x ^ {k}\\right) + \\frac {1}{2 \\alpha_ {k}} \\| x - x ^ {k} \\| _ {2} ^ {2} \\right\\} \\\\ = \\operatorname {p r o x} _ {\\alpha_ {k} f} \\left(x ^ {k} - \\alpha_ {k} A ^ {\\mathrm {T}} z ^ {k + 1}\\right), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.649,
                0.738,
                0.728
            ],
            "angle": 0,
            "content": "其中 \\(\\alpha_{k}, \\delta_{k}\\) 分别为原始变量和对偶变量的更新步长。它在第一步固定原始变量 \\(x^{k}\\) 针对对偶变量做梯度上升，在第二步固定更新后的对偶变量 \\(z^{k+1}\\) 针对原始变量做梯度下降。在这里注意，原始变量和对偶变量的更新顺序是无关紧要的，若先更新原始变量，其等价于在另一初值下先更新对偶变量。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.732,
                0.739,
                0.79
            ],
            "angle": 0,
            "content": "事实上，PDHG算法在 \\(\\alpha_{k} = +\\infty, \\delta_{k} = +\\infty\\) 这两种特殊情况下等价于近似点梯度算法分别应用于对偶问题和原始问题。首先考虑 \\(\\alpha_{k} = +\\infty\\) 的情形。此时交换两步迭代的顺序，PDHG算法(8.5.13)可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.801,
                0.737,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} x ^ {k + 1} = \\underset {x} {\\arg \\min } \\left\\{f (x) + \\left(z ^ {k}\\right) ^ {\\mathrm {T}} A x \\right\\}, \\tag {8.5.14} \\\\ z ^ {k + 1} = \\operatorname {p r o x} _ {\\delta_ {k} h ^ {*}} \\left(z ^ {k} + \\delta_ {k} A x ^ {k + 1}\\right). \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "420"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.825,
                0.194
            ],
            "angle": 0,
            "content": "可以看到，其实质上就是迭代格式(8.5.5)，其中 \\(\\delta_{k}\\) 是步长．类似地，我们也可以写出 \\(\\delta_{k} = +\\infty\\) 的情形："
        },
        {
            "type": "equation",
            "bbox": [
                0.417,
                0.204,
                0.825,
                0.237
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = \\underset {z} {\\arg \\min } \\left\\{h ^ {*} (z) - \\left(A x ^ {k}\\right) ^ {\\mathrm {T}} z \\right\\}, \\tag {8.5.15}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.416,
                0.237,
                0.654,
                0.257
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {\\alpha_ {k} f} \\left(x ^ {k} - \\alpha_ {k} A ^ {\\mathrm {T}} z ^ {k + 1}\\right)).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.265,
                0.825,
                0.323
            ],
            "angle": 0,
            "content": "假定 \\(h\\) 和 \\(h^*\\) 都是可微的，由迭代格式(8.5.15)的第一式的最优性条件，可知 \\(Ax^k = \\nabla h^*(z^{k+1})\\)。再利用共轭函数的性质，有 \\(z^{k+1} = \\nabla h(Ax^k)\\)，所以 \\(x^{k+1}\\) 的更新等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.404,
                0.334,
                0.678,
                0.356
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {\\alpha_ {k} f} \\left(x ^ {k} - \\alpha_ {k} A ^ {\\mathrm {T}} \\nabla h \\left(A x ^ {k}\\right)\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.365,
                0.611,
                0.382
            ],
            "angle": 0,
            "content": "这实际上就是对原始问题应用近似点梯度算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.386,
                0.825,
                0.444
            ],
            "angle": 0,
            "content": "PDHG算法的收敛性需要比较强的条件，在有些情形下未必收敛。这里再介绍PDHG算法的一个变形——Chambolle-Pock算法[41]。它与PDHG算法的区别在于多了一个外推步，具体的迭代格式如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.454,
                0.635,
                0.474
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = \\operatorname {p r o x} _ {\\delta_ {k} h ^ {*}} \\left(z ^ {k} + \\delta_ {k} A y ^ {k}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.478,
                0.824,
                0.499
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {\\alpha_ {k} f} \\left(x ^ {k} - \\alpha_ {k} A ^ {\\mathrm {T}} z ^ {k + 1}\\right), \\tag {8.5.16}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.504,
                0.562,
                0.522
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = 2 x ^ {k + 1} - x ^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.532,
                0.825,
                0.58
            ],
            "angle": 0,
            "content": "我们会在后面的小节中证明，当取常数步长 \\(\\alpha_{k} = t,\\delta_{k} = s\\) 时，该算法的收敛性在 \\(\\sqrt{st} < \\frac{1}{\\|A\\|_2}\\) 的条件下成立."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.598,
                0.401,
                0.616
            ],
            "angle": 0,
            "content": "8.5.3 应用举例"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.63,
                0.426,
                0.646
            ],
            "angle": 0,
            "content": "1. LASSO问题求解"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.66,
                0.431,
                0.675
            ],
            "angle": 0,
            "content": "考虑 LASSO 问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.681,
                0.684,
                0.713
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\psi (x) \\stackrel {\\text {d e f}} {=} \\mu \\| x \\| _ {1} + \\frac {1}{2} \\| A x - b \\| _ {2} ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.72,
                0.825,
                0.763
            ],
            "angle": 0,
            "content": "我们取 \\(f(x) = \\mu \\| x\\| _1\\) 和 \\(h(x) = \\frac{1}{2}\\| x - b\\| _2^2\\) ，相应的鞍点问题(8.5.10)形式如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.415,
                0.768,
                0.667,
                0.791
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} \\max  _ {z \\in \\mathbb {R} ^ {m}} f (x) - h ^ {*} (z) + z ^ {\\mathrm {T}} A x.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.795,
                0.424,
                0.811
            ],
            "angle": 0,
            "content": "根据共轭函数的定义，"
        },
        {
            "type": "equation",
            "bbox": [
                0.35,
                0.818,
                0.734,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nh ^ {*} (z) = \\sup  _ {y \\in \\mathbb {R} ^ {m}} \\left\\{y ^ {\\mathrm {T}} z - \\frac {1}{2} \\| y - b \\| _ {2} ^ {2} \\right\\} = \\frac {1}{2} \\| z \\| _ {2} ^ {2} + b ^ {\\mathrm {T}} z.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.283,
                0.133
            ],
            "angle": 0,
            "content": "8.5 对偶算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "421"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.544,
                0.175
            ],
            "angle": 0,
            "content": "应用PDHG算法， \\(x^{k + 1}\\) 和 \\(z^{k + 1}\\) 的更新格式分别为"
        },
        {
            "type": "equation",
            "bbox": [
                0.241,
                0.181,
                0.667,
                0.214
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = \\operatorname {p r o x} _ {\\delta_ {k} h ^ {*}} \\left(z ^ {k} + \\delta_ {k} A x ^ {k}\\right) = \\frac {1}{\\delta_ {k} + 1} \\left(z ^ {k} + \\delta_ {k} A x ^ {k} - \\delta_ {k} b\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.241,
                0.217,
                0.498,
                0.24
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {\\alpha_ {k} \\mu \\| \\cdot \\| _ {1}} \\left(x ^ {k} - \\alpha_ {k} A ^ {\\mathrm {T}} z ^ {k + 1}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.247,
                0.68,
                0.264
            ],
            "angle": 0,
            "content": "这里 \\(\\delta_{k},\\alpha_{k}\\) 为步长．同样地，可以写出Chambolle-Pock算法格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.326,
                0.269,
                0.577,
                0.304
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = \\frac {1}{\\delta_ {k} + 1} \\left(z ^ {k} + \\delta_ {k} A y ^ {k} - \\delta_ {k} b\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.326,
                0.307,
                0.582,
                0.329
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {\\alpha_ {k} \\mu \\| \\cdot \\| _ {1}} \\left(x ^ {k} - \\alpha_ {k} A ^ {\\mathrm {T}} z ^ {k + 1}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.326,
                0.333,
                0.465,
                0.353
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = 2 x ^ {k + 1} - x ^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.362,
                0.741,
                0.448
            ],
            "angle": 0,
            "content": "我们用同第6.2节中一样的 \\(A\\) 和 \\(b\\)，并取 \\(\\mu = 10^{-3}\\)，分别采用带连续化策略的 PDHG 算法和 Chambolle-Pock 算法来进行求解，这里取 \\(\\delta_k = 1\\) 和 \\(\\alpha_k = \\frac{1}{\\|A\\|_2^2}\\)。停机准则和参数 \\(\\mu\\) 的连续化设置与第6.2节中的光滑化梯度法一致。结果如图8.8所示。可以看到，尽管 PDHG 算法在某些情况下没有收敛"
        },
        {
            "type": "image",
            "bbox": [
                0.268,
                0.457,
                0.641,
                0.681
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.216,
                0.694,
                0.694,
                0.712
            ],
            "angle": 0,
            "content": "图8.8 PDHG算法和Chambolle-Pock算法求解LASSO问题"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.732,
                0.641,
                0.749
            ],
            "angle": 0,
            "content": "性保证，但它在这个例子上比Chambolle-Pock算法稍快一些."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.772,
                0.294,
                0.789
            ],
            "angle": 0,
            "content": "2. TV-\\(L^1\\) 模型"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.802,
                0.739,
                0.82
            ],
            "angle": 0,
            "content": "考虑去噪情形下的 \\(\\mathrm{TV - }L^{1}\\) 模型(3.11.6)(即 \\(\\mathcal{A}\\) 为矩阵空间的恒等算子）："
        },
        {
            "type": "equation",
            "bbox": [
                0.34,
                0.831,
                0.568,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {U \\in \\mathbb {R} ^ {n \\times n}} \\| U \\| _ {T V} + \\lambda \\| U - B \\| _ {1},\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "422"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.194
            ],
            "angle": 0,
            "content": "其中 \\(\\|U\\|_{TV}\\) 为全变差，其定义参见 (3.11.3) 式，即可以用离散的梯度（线性）算子 \\(D: \\mathbb{R}^{n \\times n} \\to \\mathbb{R}^{n \\times n \\times 2}\\) 表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.442,
                0.203,
                0.641,
                0.234
            ],
            "angle": 0,
            "content": "\\[\n\\| U \\| _ {T V} = \\sum_ {1 \\leqslant i, j \\leqslant n} \\| (D U) _ {i j} \\| _ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.241,
                0.482,
                0.258
            ],
            "angle": 0,
            "content": "对任意的 \\(W, V \\in \\mathbb{R}^{n \\times n \\times 2}\\)，记"
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.267,
                0.741,
                0.299
            ],
            "angle": 0,
            "content": "\\[\n\\| W \\| = \\sum_ {1 \\leqslant i, j \\leqslant n} \\| w _ {i j} \\| _ {2}, \\quad \\langle W, V \\rangle = \\sum_ {1 \\leqslant i, j \\leqslant n, 1 \\leqslant k \\leqslant 2} w _ {i, j, k} v _ {i, j, k},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.305,
                0.816,
                0.324
            ],
            "angle": 0,
            "content": "其中 \\(w_{ij} \\in \\mathbb{R}^2\\) 且 \\(\\|\\cdot\\|\\) 定义了 \\(\\mathbb{R}^{n \\times n \\times 2}\\) 上的一种范数。利用 \\(\\|\\cdot\\|\\) 的定义，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.478,
                0.333,
                0.605,
                0.352
            ],
            "angle": 0,
            "content": "\\[\n\\| U \\| _ {T V} = \\| D U \\|.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.361,
                0.677,
                0.378
            ],
            "angle": 0,
            "content": "对应于问题(8.5.1)，我们取 \\(D\\) 为相应的线性算子，并取"
        },
        {
            "type": "equation",
            "bbox": [
                0.306,
                0.387,
                0.774,
                0.406
            ],
            "angle": 0,
            "content": "\\[\nf (U) = \\lambda \\| U - B \\| _ {1}, U \\in \\mathbb {R} ^ {n \\times n}, h (W) = \\| W \\|, W \\in \\mathbb {R} ^ {n \\times n \\times 2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.416,
                0.489,
                0.433
            ],
            "angle": 0,
            "content": "相应的鞍点问题(8.5.10)如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.362,
                0.443,
                0.72,
                0.467
            ],
            "angle": 0,
            "content": "\\[\n\\left(L _ {P D}\\right) \\min  _ {U \\in \\mathbb {R} ^ {n \\times n}} \\max  _ {V \\in \\mathbb {R} ^ {n \\times n \\times 2}} f (U) - h ^ {*} (V) + \\langle V, D U \\rangle .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.473,
                0.424,
                0.489
            ],
            "angle": 0,
            "content": "根据共轭函数的定义，"
        },
        {
            "type": "equation",
            "bbox": [
                0.307,
                0.494,
                0.775,
                0.553
            ],
            "angle": 0,
            "content": "\\[\nh ^ {*} (V) = \\sup  _ {U \\in \\mathbb {R} ^ {n \\times n \\times 2}} \\quad \\{\\langle U, V \\rangle - \\| U \\| \\} = \\left\\{ \\begin{array}{l l} 0, & \\max  _ {i, j} \\| v _ {i j} \\| _ {2} \\leqslant 1, \\\\ + \\infty , & \\text {其 他}. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.558,
                0.825,
                0.599
            ],
            "angle": 0,
            "content": "记 \\(\\mathcal{V} = \\{V\\in \\mathbb{R}^{n\\times n\\times 2}:\\max_{ij}\\| v_{ij}\\| _2\\leqslant 1\\}\\) ，其示性函数记为 \\(I_{\\mathcal{V}}(V)\\) ，则问题 \\((\\mathrm{L}_{\\mathrm{PD}})\\) 可以整理为"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.603,
                0.684,
                0.626
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {U} \\max  _ {V} f (U) + \\langle V, D U \\rangle - I _ {\\mathcal {V}} (V).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.63,
                0.54,
                0.647
            ],
            "angle": 0,
            "content": "应用PDHG算法，则 \\(V^{k + 1}\\) 的更新为"
        },
        {
            "type": "equation",
            "bbox": [
                0.336,
                0.656,
                0.825,
                0.678
            ],
            "angle": 0,
            "content": "\\[\nV ^ {k + 1} = \\operatorname {p r o x} _ {s I _ {\\mathcal {V}}} \\left(V ^ {k} + s D U ^ {k}\\right) = \\mathcal {P} _ {\\mathcal {V}} \\left(V ^ {k} + s D U ^ {k}\\right), \\tag {8.5.17}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.684,
                0.65,
                0.702
            ],
            "angle": 0,
            "content": "即 \\(V^{k} + sDU^{k}\\) 在 \\(\\mathcal{V}\\) 上的投影，而 \\(U^{k + 1}\\) 的更新如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.71,
                0.804,
                0.85
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} U ^ {k + 1} = \\operatorname {p r o x} _ {t f} (U ^ {k} + t G V ^ {k + 1}) \\\\ = \\arg \\min  _ {U} \\left\\{\\lambda \\| U - B \\| _ {1} + \\left\\langle V ^ {k + 1}, D U \\right\\rangle + \\frac {1}{2 t} \\| U - U ^ {k} \\| _ {F} ^ {2} \\right\\} \\\\ \\Longleftrightarrow (U ^ {k + 1}) _ {i j} = \\left\\{ \\begin{array}{l l} (U ^ {k} + t G V ^ {k + 1}) _ {i j} - t \\lambda , & (U ^ {k} + t G V ^ {k + 1}) _ {i j} > b _ {i j} + t \\lambda , \\\\ (U ^ {k} + t G V ^ {k + 1}) _ {i j} + t \\lambda , & (U ^ {k} + t G V ^ {k + 1}) _ {i j} <   b _ {i j} - t \\lambda , \\\\ b _ {i j}, & | (U ^ {k} + t G V ^ {k + 1}) _ {i j} - b _ {i j} | \\leqslant t \\lambda , \\end{array} \\right. \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "8.5 对偶算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "423"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.568,
                0.174
            ],
            "angle": 0,
            "content": "其中 \\(G:\\mathbb{R}^{n\\times n\\times 2}\\to \\mathbb{R}^{n\\times n}\\) 为离散的散度算子，其满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.264,
                0.186,
                0.646,
                0.206
            ],
            "angle": 0,
            "content": "\\[\n\\langle V, D U \\rangle = - \\langle G V, U \\rangle , \\quad \\forall U \\in \\mathbb {R} ^ {n \\times n}, V \\in \\mathbb {R} ^ {n \\times n \\times 2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.217,
                0.736,
                0.256
            ],
            "angle": 0,
            "content": "若应用Chambolle-Pock算法，那么 \\(U^{k + 1}\\) 的更新保持不变，仅需调整 \\(V^{k + 1}\\) 的更新为 \\(V^{k} + sD(2U^{k + 1} - U^{k})\\) 在 \\(\\mathcal{V}\\) 上的投影."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.279,
                0.31,
                0.296
            ],
            "angle": 0,
            "content": "3. 图像填充模型"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.31,
                0.28,
                0.325
            ],
            "angle": 0,
            "content": "考虑问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.324,
                0.57,
                0.355
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {U \\in \\mathbb {R} ^ {n \\times n}} \\| U \\| _ {T V} + \\frac {\\lambda}{2} \\| U - B \\| _ {F} ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.359,
                0.697,
                0.376
            ],
            "angle": 0,
            "content": "类似于上一个例子中的分析，我们取 \\(D\\) 为相应的线性算子，并取"
        },
        {
            "type": "equation",
            "bbox": [
                0.228,
                0.383,
                0.679,
                0.415
            ],
            "angle": 0,
            "content": "\\[\nf (U) = \\frac {\\lambda}{2} \\| U - B \\| _ {F} ^ {2}, U \\in \\mathbb {R} ^ {n \\times n}, h (W) = \\| W \\|, W \\in \\mathbb {R} ^ {n \\times n \\times 2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.422,
                0.373,
                0.438
            ],
            "angle": 0,
            "content": "一般的鞍点问题叙述如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.283,
                0.452,
                0.627,
                0.476
            ],
            "angle": 0,
            "content": "\\[\n\\left(L _ {P D}\\right) \\max  _ {U} \\max  _ {V} f (U) + \\langle V, D U \\rangle - I _ {\\mathcal {V}} (V),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.485,
                0.738,
                0.523
            ],
            "angle": 0,
            "content": "其中 \\(\\nu\\) 与 \\(\\mathrm{TV - }L^{1}\\) 模型中的定义一致．应用PDHG算法，则 \\(V^{k + 1}\\) 的更新为(8.5.17)式．引入离散的散度算子 \\(G\\) ， \\(U^{k + 1}\\) 的更新如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.194,
                0.535,
                0.715,
                0.632
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} U ^ {k + 1} = \\operatorname {p r o x} _ {t f} (U ^ {k} + t G V ^ {k + 1}) \\\\ = \\arg \\min  _ {U} \\left\\{\\frac {\\lambda}{2} \\| U - B \\| _ {F} ^ {2} + \\left\\langle V ^ {k + 1}, D U \\right\\rangle + \\frac {1}{2 t} \\| U - U ^ {k} \\| _ {F} ^ {2} \\right\\} \\\\ \\Longleftrightarrow (U ^ {k + 1}) _ {i j} = \\frac {(U ^ {k} + t G V ^ {k + 1}) _ {i j} + t \\lambda b _ {i j}}{1 + t \\lambda}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.639,
                0.67,
                0.656
            ],
            "angle": 0,
            "content": "同样地，Chambolle-Pock算法的更新表达式也可类似地推出"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.68,
                0.327,
                0.696
            ],
            "angle": 0,
            "content": "4. 图像反卷积模型"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.71,
                0.28,
                0.726
            ],
            "angle": 0,
            "content": "考虑问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.723,
                0.578,
                0.755
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {U \\in \\mathbb {R} ^ {n \\times n}} \\| U \\| _ {T V} + \\frac {\\lambda}{2} \\| \\mathcal {A} U - B \\| _ {F} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.759,
                0.674,
                0.776
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{A}U = K_{\\mathcal{A}} * U\\) 为卷积算子，且 \\(K_{\\mathcal{A}}\\) 是 \\(\\mathcal{A}\\) 的卷积核对应的矩阵。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.78,
                0.738,
                0.817
            ],
            "angle": 0,
            "content": "类似于 \\(\\mathrm{TV - }L^{1}\\) 模型中的分析，对应于问题(8.5.1)，取 \\(D\\) 为相应的线性算子，并取"
        },
        {
            "type": "equation",
            "bbox": [
                0.22,
                0.825,
                0.686,
                0.857
            ],
            "angle": 0,
            "content": "\\[\nf (U) = \\frac {\\lambda}{2} \\| \\mathcal {A} U - B \\| _ {F} ^ {2}, U \\in \\mathbb {R} ^ {n \\times n}, h (W) = \\| W \\|, W \\in \\mathbb {R} ^ {n \\times n \\times 2}.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "424"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.529,
                0.174
            ],
            "angle": 0,
            "content": "类似地，一般的鞍点问题叙述如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.362,
                0.192,
                0.722,
                0.216
            ],
            "angle": 0,
            "content": "\\[\n\\left(L _ {P D}\\right) \\min  _ {U} \\max  _ {V} f (U) + \\langle V, D U \\rangle - I _ {V} (V),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.23,
                0.531,
                0.246
            ],
            "angle": 0,
            "content": "其中 \\(\\nu\\) 与 \\(\\mathrm{TV} - L^{1}\\) 模型中的定义一致"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.251,
                0.824,
                0.269
            ],
            "angle": 0,
            "content": "应用PDHG算法，则 \\(V^{k + 1}\\) 的更新仍为(8.5.17)式，而 \\(U^{k + 1}\\) 的更新为："
        },
        {
            "type": "equation",
            "bbox": [
                0.306,
                0.282,
                0.776,
                0.343
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} U ^ {k + 1} = \\operatorname {p r o x} _ {t f} (U ^ {k} + t G V ^ {k + 1}) \\\\ = \\arg \\min  _ {U} \\left\\{\\frac {\\lambda}{2} \\| \\mathcal {A} U - B \\| _ {F} ^ {2} + \\frac {1}{2 t} \\| U - (U ^ {k} + t G V ^ {k + 1}) \\| _ {F} ^ {2} \\right\\}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.357,
                0.825,
                0.394
            ],
            "angle": 0,
            "content": "其中 \\(G\\) 为离散的散度算子。根据凸二次函数的最优性条件，可知 \\(U^{k+1}\\) 满足如下方程："
        },
        {
            "type": "equation",
            "bbox": [
                0.348,
                0.406,
                0.735,
                0.437
            ],
            "angle": 0,
            "content": "\\[\n\\lambda \\mathcal {A} ^ {*} (\\mathcal {A} U ^ {k + 1} - B) + \\frac {1}{t} (U ^ {k + 1} - (U ^ {k} + t G V ^ {k + 1})) = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.449,
                0.825,
                0.507
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{A}^*\\) 是 \\(\\mathcal{A}\\) 的共轭算子, 且其卷积核对应的矩阵为 \\(K_{\\mathcal{A}^*}\\). 由于 \\(\\mathcal{A}U = K_{\\mathcal{A}} * U\\) 具有卷积的形式, 我们可以利用快速傅里叶变换 \\(\\mathcal{F}\\) 和其逆变换 \\(\\mathcal{F}^{-1}\\) 来快速求解上面的线性方程组. 根据"
        },
        {
            "type": "equation",
            "bbox": [
                0.385,
                0.526,
                0.698,
                0.544
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {F} (\\mathcal {A} U) = \\mathcal {F} (K _ {\\mathcal {A}} * U) = \\mathcal {F} (K _ {\\mathcal {A}}) \\odot \\mathcal {F} (U),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.562,
                0.51,
                0.578
            ],
            "angle": 0,
            "content": "其中 \\(\\odot\\) 表示逐分量相乘，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.379,
                0.591,
                0.701,
                0.653
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathcal {F} \\left(K _ {\\mathcal {A} ^ {*}}\\right) \\odot \\left(\\mathcal {F} \\left(K _ {\\mathcal {A}}\\right) \\odot \\mathcal {F} \\left(U ^ {k + 1}\\right) - \\mathcal {F} (B)\\right) + \\\\ \\frac {1}{t \\lambda} \\mathcal {F} \\big (U ^ {k + 1} - \\big (U ^ {k} + t G V ^ {k + 1} \\big) \\big) = 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.666,
                0.685,
                0.684
            ],
            "angle": 0,
            "content": "利用关系式 \\(\\mathcal{F}(K_{\\mathcal{A}^*}) = \\overline{\\mathcal{F}(K_{\\mathcal{A}})}\\) ，可得 \\(U^{k + 1}\\) 的显式表达式"
        },
        {
            "type": "equation",
            "bbox": [
                0.335,
                0.697,
                0.748,
                0.74
            ],
            "angle": 0,
            "content": "\\[\nU ^ {k + 1} = \\mathcal {F} ^ {- 1} \\left(\\frac {\\mathcal {F} (U ^ {k} + t G V ^ {k + 1}) + t \\lambda \\mathcal {F} (B) \\odot \\overline {{\\mathcal {F} (K _ {\\mathcal {A}})}}}{1 + t \\lambda | \\mathcal {F} (K _ {\\mathcal {A}}) | ^ {2}}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.753,
                0.674,
                0.77
            ],
            "angle": 0,
            "content": "以上表达式中除 \\(\\mathcal{F},\\mathcal{F}^{-1},G\\) 外，其余均为逐分量的运算"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.803,
                0.421,
                0.82
            ],
            "angle": 0,
            "content": "8.5.4 收敛性分析"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.836,
                0.796,
                0.853
            ],
            "angle": 0,
            "content": "本小节给出对偶近似点梯度算法和Chambolle-Pock算法的收敛性."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "8.5 对偶算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "425"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.157,
                0.411,
                0.174
            ],
            "angle": 0,
            "content": "1. 对偶近似点梯度法的收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.187,
                0.739,
                0.246
            ],
            "angle": 0,
            "content": "对偶近似点梯度法函数值的收敛性可以从近似点梯度算法的收敛性得到（定理8.3）。这里我们给一个更强的结果，即迭代点收敛到原始问题和对偶问题的解，证明可参考[64]定理2.2以及[48]定理3.4。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.257,
                0.747,
                0.343
            ],
            "angle": 0,
            "content": "定理8.12(对偶近似点梯度法的收敛性）给定初始值 \\(z^0,x^0\\) ，序列 \\(\\{(x^{k},z^{k})\\}\\) 由迭代格式(8.5.5)生成．假设 \\(f\\) 是强凸的闭函数（强凸参数为 \\(\\mu\\) ）且步长\\(t\\in \\left(0,\\frac{\\mu}{\\|A\\|_2^2}\\right]\\) ，则 \\(\\{z^k\\}\\) 收敛到对偶问题 \\((D)\\) 的解， \\(\\{x^k\\}\\) 收敛到原始问题 \\((P)\\) 的唯一解."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.357,
                0.739,
                0.442
            ],
            "angle": 0,
            "content": "定理8.12主要说明了原始变量和对偶变量同时收敛到相应问题的最优解，但它并没有说明原始变量的收敛速度．而通过近似点梯度算法的收敛性结果，我们容易得知对偶变量 \\(\\{z^k\\}\\) 在函数值的意义下具有 \\(\\mathcal{O}\\left(\\frac{1}{k}\\right)\\) 的收敛速度."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.465,
                0.452,
                0.483
            ],
            "angle": 0,
            "content": "2. *Chambolle-Pock 算法的收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.495,
                0.739,
                0.555
            ],
            "angle": 0,
            "content": "下面讨论Chambolle-Pock算法(8.5.16)的收敛性．由于PDHG类算法针对鞍点问题设计，在讨论收敛性时也应该在鞍点的意义下讨论．对问题(8.5.10)，设 \\(X,Z\\) 分别为变量 \\(x,z\\) 的取值空间，若点 \\((\\hat{x},\\hat{z})\\) 满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.264,
                0.568,
                0.644,
                0.586
            ],
            "angle": 0,
            "content": "\\[\n\\psi_ {\\mathrm {P D}} (x, \\hat {z}) \\geqslant \\psi_ {\\mathrm {P D}} (\\hat {x}, \\hat {z}) \\geqslant \\psi_ {\\mathrm {P D}} (\\hat {x}, z), \\quad \\forall x \\in X, z \\in Z,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.6,
                0.646,
                0.617
            ],
            "angle": 0,
            "content": "称 \\((\\hat{x},\\hat{z})\\) 是问题(8.5.10)的一个鞍点，其中 \\(\\psi_{\\mathrm{PD}}\\) 的定义见该问题"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.621,
                0.738,
                0.657
            ],
            "angle": 0,
            "content": "为了更方便地刻画点 \\((x, z)\\) 的最优性，我们引入部分原始-对偶间隙的概念."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.671,
                0.739,
                0.709
            ],
            "angle": 0,
            "content": "定义8.7(部分原始-对偶间隙）对任意子集 \\(B_{1}\\times B_{2}\\subset X\\times Z\\) ，定义部分原始-对偶间隙为"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.723,
                0.737,
                0.748
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {G} _ {B _ {1} \\times B _ {2}} (x, z) = \\max  _ {z ^ {\\prime} \\in B _ {2}} \\psi_ {\\mathrm {P D}} \\left(x, z ^ {\\prime}\\right) - \\min  _ {x ^ {\\prime} \\in B _ {1}} \\psi_ {\\mathrm {P D}} \\left(x ^ {\\prime}, z\\right). \\tag {8.5.18}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.757,
                0.535,
                0.774
            ],
            "angle": 0,
            "content": "不难验证，只要鞍点 \\((\\hat{x},\\hat{z})\\in B_1\\times B_2\\) ，就有"
        },
        {
            "type": "equation",
            "bbox": [
                0.192,
                0.787,
                0.737,
                0.853
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathcal {G} _ {B _ {1} \\times B _ {2}} (x, z) \\geqslant \\psi_ {\\mathrm {P D}} (x, \\hat {z}) - \\psi_ {\\mathrm {P D}} (\\hat {x}, z) \\\\ = \\left(\\psi_ {\\mathrm {P D}} (x, \\hat {z}) - \\psi_ {\\mathrm {P D}} (\\hat {x}, \\hat {z})\\right) + \\left(\\psi_ {\\mathrm {P D}} (\\hat {x}, \\hat {z}) - \\psi_ {\\mathrm {P D}} (\\hat {x}, z)\\right) \\tag {8.5.19} \\\\ \\geqslant 0 + 0 = 0, \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "426"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.367,
                0.173
            ],
            "angle": 0,
            "content": "并且在鞍点处"
        },
        {
            "type": "equation",
            "bbox": [
                0.479,
                0.177,
                0.604,
                0.195
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {G} _ {B _ {1} \\times B _ {2}} (\\hat {x}, \\hat {z}) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.204,
                0.825,
                0.241
            ],
            "angle": 0,
            "content": "此外，容易验证当点 \\((\\hat{x},\\hat{z})\\in \\mathbf{int}\\bigl (B_1\\times B_2\\bigr)\\) 且满足 \\(\\mathcal{G}_{B_1\\times B_2}(\\hat{x},\\hat{z}) = 0\\) 时， \\((\\hat{x},\\hat{z})\\) 是一个鞍点."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.246,
                0.746,
                0.263
            ],
            "angle": 0,
            "content": "有了上面的铺垫，我们给出Chambolle-Pock算法的收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.273,
                0.825,
                0.339
            ],
            "angle": 0,
            "content": "定理8.13 (Chambolle-Pock算法的收敛性[41]) 设 \\(f, h\\) 为闭凸函数，问题(8.5.10)存在鞍点 \\((\\hat{x}, \\hat{z})\\)。在迭代格式(8.5.16)中取步长 \\(\\alpha_{k} = t, \\delta_{k} = s\\)，且满足 \\(st < \\frac{1}{L} (L = \\|A\\|_{2}^{2})\\)，则该迭代格式产生的序列 \\(\\{(x^{k}, z^{k})\\}\\) 具有以下性质："
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.347,
                0.499,
                0.364
            ],
            "angle": 0,
            "content": "(1) \\(\\forall k, (x^{k}, z^{k})\\) 有界，且满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.371,
                0.824,
                0.408
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\| x ^ {k} - \\hat {x} \\| ^ {2}}{2 t} + \\frac {\\| z ^ {k} - \\hat {z} \\| ^ {2}}{2 s} \\leqslant C \\left(\\frac {\\| x ^ {0} - \\hat {x} \\| ^ {2}}{2 t} + \\frac {\\| z ^ {0} - \\hat {z} \\| ^ {2}}{2 s}\\right), \\tag {8.5.20}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.3,
                0.416,
                0.502,
                0.434
            ],
            "angle": 0,
            "content": "其中常数 \\(C \\leqslant (1 - Lst)^{-1}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.442,
                0.825,
                0.493
            ],
            "angle": 0,
            "content": "(2) 记 \\(\\overline{x}_N = \\frac{1}{N}\\sum_{k=1}^{N}x^k\\), \\(\\overline{z}_N = \\frac{1}{N}\\sum_{k=1}^{N}z^k\\), 则对于任意有界区域 \\(B_1 \\times B_2 \\subset X \\times Z\\), 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.456,
                0.494,
                0.824,
                0.525
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {G} _ {B _ {1} \\times B _ {2}} \\left(\\bar {x} _ {N}, \\bar {z} _ {N}\\right) \\leqslant \\frac {D \\left(B _ {1} , B _ {2}\\right)}{N}, \\tag {8.5.21}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.301,
                0.529,
                0.336,
                0.544
            ],
            "angle": 0,
            "content": "其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.378,
                0.542,
                0.747,
                0.581
            ],
            "angle": 0,
            "content": "\\[\nD \\left(B _ {1}, B _ {2}\\right) = \\sup  _ {(x, z) \\in B _ {1} \\times B _ {2}} \\left\\{\\frac {\\| x - x ^ {0} \\| ^ {2}}{2 t} + \\frac {\\| z - z ^ {0} \\| ^ {2}}{2 s} \\right\\};\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.586,
                0.788,
                0.604
            ],
            "angle": 0,
            "content": "进一步地，序列 \\(\\{(\\overline{x}_N,\\overline{z}_N)\\}_{N = 1}^{\\infty}\\) 的聚点为问题(8.5.10)的一个鞍点；"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.615,
                0.722,
                0.632
            ],
            "angle": 0,
            "content": "(3) 存在问题(8.5.10)一个鞍点 \\((x^{*}, z^{*})\\) 使得 \\(x^{k} \\to x^{*}, z^{k} \\to z^{*}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.644,
                0.644,
                0.661
            ],
            "angle": 0,
            "content": "证明. 为了方便推导，首先考虑算法的一般格式："
        },
        {
            "type": "equation",
            "bbox": [
                0.448,
                0.671,
                0.824,
                0.698
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = \\operatorname {p r o x} _ {s h ^ {*}} \\left(z ^ {k} + s A \\bar {x}\\right), \\tag {8.5.22}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.448,
                0.696,
                0.637,
                0.715
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {t f} \\left(x ^ {k} - t A ^ {\\mathrm {T}} \\bar {z}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.724,
                0.825,
                0.782
            ],
            "angle": 0,
            "content": "这里和算法(8.5.16)不同的是，我们使用 \\(\\bar{x},\\bar{z}\\) 来表示更新 \\(x,z\\) 时的参考点．当它们取特定值时，以上格式可以为PDHG算法或Chambolle-Pock算法．根据邻近算子的性质，"
        },
        {
            "type": "equation",
            "bbox": [
                0.425,
                0.789,
                0.654,
                0.822
            ],
            "angle": 0,
            "content": "\\[\n- A ^ {\\mathrm {T}} \\bar {z} + \\frac {x ^ {k} - x ^ {k + 1}}{t} \\in \\partial f (x ^ {k + 1}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.449,
                0.824,
                0.658,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nA \\bar {x} + \\frac {z ^ {k} - z ^ {k + 1}}{s} \\in \\partial h ^ {*} (z ^ {k + 1}).\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.283,
                0.133
            ],
            "angle": 0,
            "content": "8.5 对偶算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "427"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.547,
                0.174
            ],
            "angle": 0,
            "content": "根据次梯度的定义，对于任意的 \\((x,z)\\in X\\times Z\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.201,
                0.182,
                0.737,
                0.22
            ],
            "angle": 0,
            "content": "\\[\nf (x) \\geqslant f \\left(x ^ {k + 1}\\right) + \\frac {1}{t} \\left(x - x ^ {k + 1}\\right) ^ {\\mathrm {T}} \\left(x ^ {k} - x ^ {k + 1}\\right) - \\left(x - x ^ {k + 1}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\bar {z}, \\tag {8.5.23}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.201,
                0.215,
                0.646,
                0.246
            ],
            "angle": 0,
            "content": "\\[\nh ^ {*} (z) \\geqslant h ^ {*} (z ^ {k + 1}) + \\frac {1}{s} (z - z ^ {k + 1}) ^ {\\mathrm {T}} (z ^ {k} - z ^ {k + 1}) + (z - z ^ {k + 1}) ^ {\\mathrm {T}} A \\bar {x}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.254,
                0.737,
                0.291
            ],
            "angle": 0,
            "content": "将上述两个不等式相加，并引入二次项可整理得到（具体细节请读者自行推导）"
        },
        {
            "type": "equation",
            "bbox": [
                0.216,
                0.298,
                0.737,
                0.441
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\frac {\\| x - x ^ {k} \\| ^ {2}}{2 t} + \\frac {\\| z - z ^ {k} \\| ^ {2}}{2 s} - \\frac {\\| x - x ^ {k + 1} \\| ^ {2}}{2 t} - \\frac {\\| z - z ^ {k + 1} \\| ^ {2}}{2 s} \\\\ \\geqslant \\left[ f \\left(x ^ {k + 1}\\right) - h ^ {*} (z) + \\left(x ^ {k + 1}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z \\right] \\\\ - \\left[ f (x) - h ^ {*} \\left(z ^ {k + 1}\\right) + x ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z ^ {k + 1} \\right] \\tag {8.5.24} \\\\ + \\frac {\\| x ^ {k} - x ^ {k + 1} \\| ^ {2}}{2 t} + \\frac {\\| z ^ {k} - z ^ {k + 1} \\| ^ {2}}{2 s} \\\\ + \\left(x ^ {k + 1} - \\bar {x}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k + 1} - z\\right) - \\left(x ^ {k + 1} - x\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k + 1} - \\bar {z}\\right). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.451,
                0.738,
                0.509
            ],
            "angle": 0,
            "content": "在后续推导中可以看到，上述不等式右端最后两项表达式在证明收敛性过程中有重要作用。将迭代格式 (8.5.16) 代入不等式 (8.5.24) 中，即取 \\(\\bar{x} = 2x^{k} - x^{k-1}, \\bar{z} = z^{k+1}\\)，那么不等式 (8.5.24) 右端最后两项表达式"
        },
        {
            "type": "equation",
            "bbox": [
                0.23,
                0.519,
                0.737,
                0.661
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left(x ^ {k + 1} - \\bar {x}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k + 1} - z\\right) - \\left(x ^ {k + 1} - x\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k + 1} - \\bar {z}\\right) \\\\ = \\left(x ^ {k + 1} - x ^ {k} - \\left(x ^ {k} - x ^ {k - 1}\\right)\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k + 1} - z\\right) \\\\ = \\left(x ^ {k + 1} - x ^ {k}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k + 1} - z\\right) - \\left(x ^ {k} - x ^ {k - 1}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k} - z\\right) \\tag {8.5.25} \\\\ - \\left(x ^ {k} - x ^ {k - 1}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k + 1} - z ^ {k}\\right) \\\\ \\geqslant \\left(x ^ {k + 1} - x ^ {k}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k + 1} - z\\right) - \\left(x ^ {k} - x ^ {k - 1}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k} - z\\right) \\\\ - \\sqrt {L} \\| x ^ {k} - x ^ {k - 1} \\| \\| z ^ {k + 1} - z ^ {k} \\|, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.673,
                0.738,
                0.72
            ],
            "angle": 0,
            "content": "应用柯西不等式即得到最后的不等号. 又利用 \\(2ab \\leqslant \\alpha a^{2} + \\frac{b^{2}}{\\alpha}\\) 对任意的 \\(\\alpha > 0\\) 均成立, 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.302,
                0.728,
                0.737,
                0.787
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\sqrt {L} \\| x ^ {k} - x ^ {k - 1} \\| \\| z ^ {k + 1} - z ^ {k} \\| \\\\ \\leqslant \\frac {\\sqrt {L} \\alpha t}{2 t} \\| x ^ {k} - x ^ {k - 1} \\| ^ {2} + \\frac {\\sqrt {L} s}{2 \\alpha s} \\| z ^ {k + 1} - z ^ {k} \\| ^ {2}, \\tag {8.5.26} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.795,
                0.291,
                0.829
            ],
            "angle": 0,
            "content": "取 \\(\\alpha = \\sqrt{\\frac{s}{t}}\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.352,
                0.828,
                0.556,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\sqrt {L} \\alpha t = \\sqrt {L} \\frac {s}{\\alpha} = \\sqrt {L s t} <   1,\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "428"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.763,
                0.174
            ],
            "angle": 0,
            "content": "从而合并 (8.5.24) 式和 (8.5.25) 式得到，对于任意的 \\((x, z) \\in X \\times Z\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.298,
                0.185,
                0.824,
                0.32
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\frac {\\| x - x ^ {k} \\| ^ {2}}{2 t} + \\frac {\\| z - z ^ {k} \\| ^ {2}}{2 s} - \\frac {\\| x - x ^ {k + 1} \\| ^ {2}}{2 t} - \\frac {\\| z - z ^ {k + 1} \\| ^ {2}}{2 s} \\\\ \\geqslant \\left[ f \\left(x ^ {k + 1}\\right) - h ^ {*} (z) + \\left(x ^ {k + 1}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z \\right] - \\left[ f (x) - h ^ {*} \\left(z ^ {k + 1}\\right) + x ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z ^ {k + 1} \\right] \\\\ + \\left(1 - \\sqrt {L s t}\\right) \\frac {\\| z ^ {k} - z ^ {k + 1} \\| ^ {2}}{2 s} + \\frac {\\| x ^ {k} - x ^ {k + 1} \\| ^ {2}}{2 t} - \\sqrt {L s t} \\frac {\\| x ^ {k - 1} - x ^ {k} \\| ^ {2}}{2 t} \\\\ + \\left(x ^ {k + 1} - x ^ {k}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k + 1} - z\\right) - \\left(x ^ {k} - x ^ {k - 1}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k} - z\\right). \\tag {8.5.27} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.334,
                0.823,
                0.35
            ],
            "angle": 0,
            "content": "将上述不等式中的 \\(k\\) 从 0 遍历至 \\(N - 1\\) 并求和, 消掉不等式两边共同项后有"
        },
        {
            "type": "equation",
            "bbox": [
                0.279,
                0.362,
                0.824,
                0.518
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\sum_ {k = 1} ^ {N} \\left\\{\\left[ f (x ^ {k}) - h ^ {*} (z) + (x ^ {k}) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z \\right] - \\left[ f (x) - h ^ {*} (z ^ {k}) + x ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z ^ {k} \\right] \\right\\} \\\\ + \\frac {\\| x - x ^ {N} \\| ^ {2}}{2 t} + \\frac {\\| z - z ^ {N} \\| ^ {2}}{2 s} + (1 - \\sqrt {L s t}) \\sum_ {k = 1} ^ {N} \\frac {\\| z ^ {k} - z ^ {k - 1} \\| ^ {2}}{2 s} \\tag {8.5.28} \\\\ + \\left(1 - \\sqrt {L s t}\\right) \\sum_ {k = 1} ^ {N - 1} \\frac {\\| x ^ {k} - x ^ {k - 1} \\| ^ {2}}{2 t} + \\frac {\\| x ^ {N} - x ^ {N - 1} \\| ^ {2}}{2 t} \\\\ \\leqslant \\frac {\\| x - x ^ {0} \\| ^ {2}}{2 t} + \\frac {\\| z - z ^ {0} \\| ^ {2}}{2 s} + (x ^ {N} - x ^ {N - 1}) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} (z ^ {N} - z), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.528,
                0.824,
                0.573
            ],
            "angle": 0,
            "content": "其中约定 \\(x^{-1} = x^{0}\\) . 再一次应用柯西不等式, 以及 \\(2ab \\leqslant \\alpha a^{2} + \\frac{b^{2}}{\\alpha}\\) 对任意的 \\(\\alpha > 0\\) 均成立, 可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.326,
                0.586,
                0.761,
                0.641
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left(x ^ {N} - x ^ {N - 1}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {N} - z\\right) \\leqslant \\left\\| x ^ {N} - x ^ {N - 1} \\right\\| \\left(\\sqrt {L} \\left\\| z ^ {N} - z \\right\\|\\right) \\\\ \\leqslant \\frac {\\| x ^ {N} - x ^ {N - 1} \\| ^ {2}}{2 t} + \\frac {L s t \\| z - z ^ {N} \\| ^ {2}}{2 s}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.651,
                0.489,
                0.667
            ],
            "angle": 0,
            "content": "不等式(8.5.28)可进一步整理为"
        },
        {
            "type": "equation",
            "bbox": [
                0.292,
                0.679,
                0.824,
                0.853
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\sum_ {k = 1} ^ {N} \\left\\{\\left[ f (x ^ {k}) - h ^ {*} (z) + (x ^ {k}) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z \\right] - \\left[ f (x) - h ^ {*} (z ^ {k}) + x ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z ^ {k} \\right] \\right\\} \\\\ + \\frac {\\| x - x ^ {N} \\| ^ {2}}{2 t} + (1 - L s t) \\frac {\\| z - z ^ {N} \\| ^ {2}}{2 s} + (1 - \\sqrt {L s t}) \\sum_ {k = 1} ^ {N} \\frac {\\| z ^ {k} - z ^ {k - 1} \\| ^ {2}}{2 s} \\\\ + \\left(1 - \\sqrt {L s t}\\right) \\sum_ {k = 1} ^ {N - 1} \\frac {\\| x ^ {k} - x ^ {k - 1} \\| ^ {2}}{2 t} \\\\ \\leqslant \\frac {\\| x - x ^ {0} \\| ^ {2}}{2 t} + \\frac {\\| z - z ^ {0} \\| ^ {2}}{2 s}. \\tag {8.5.29} \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.133
            ],
            "angle": 0,
            "content": "8.5 对偶算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "429"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.468,
                0.174
            ],
            "angle": 0,
            "content": "若取 \\((x,z) = (\\hat{x},\\hat{z})\\) ，则由鞍点性质可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.231,
                0.189,
                0.677,
                0.21
            ],
            "angle": 0,
            "content": "\\[\n\\left[ f \\left(x ^ {k}\\right) - h ^ {*} (\\hat {z}) + \\left(x ^ {k}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\hat {z} \\right] - \\left[ f (\\hat {x}) - h ^ {*} \\left(z ^ {k}\\right) + \\hat {x} ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z ^ {k} \\right] \\geqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.226,
                0.538,
                0.243
            ],
            "angle": 0,
            "content": "进而(8.5.29)左边每一项都是正的，结论(1)成立"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.246,
                0.651,
                0.264
            ],
            "angle": 0,
            "content": "从(8.5.29)出发，利用 \\(f,h^{*}\\) 的凸性，以及 \\(\\overline{x}_N,\\overline{z}_N\\) 的定义，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.206,
                0.277,
                0.737,
                0.397
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left[ f \\left(\\bar {x} _ {N}\\right) - h ^ {*} (z) + \\left(\\bar {x} _ {N}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z \\right] - \\left[ f (x) - h ^ {*} \\left(\\bar {z} _ {N}\\right) + x ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\bar {z} _ {N} \\right] \\\\ \\leqslant \\frac {1}{N} \\sum_ {k = 1} ^ {N} \\left\\{\\left[ f (x ^ {k}) - h ^ {*} (z) + (x ^ {k}) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z \\right] - \\left[ f (x) - h ^ {*} (z ^ {k}) + x ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z ^ {k} \\right] \\right\\} \\\\ \\leqslant \\frac {1}{N} \\left(\\frac {\\| x - x ^ {0} \\| ^ {2}}{2 t} + \\frac {\\| z - z ^ {0} \\| ^ {2}}{2 s}\\right). \\tag {8.5.30} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.413,
                0.739,
                0.493
            ],
            "angle": 0,
            "content": "从而结论 (2) 中(8.5.21)式成立．由(1)知 \\(\\{(x^{k},z^{k})\\}\\) 是有界序列，因此其均值列 \\(\\{(\\overline{x}_N,\\overline{z}_N)\\}\\) 也为有界序列．记 \\((x^{\\sharp},z^{\\sharp})\\) 为序列 \\(\\{(\\overline{x}_N,\\overline{z}_N)\\}\\) 的聚点，利用\\(f,h^{*}\\) 的凸性以及闭性（下半连续性），对(8.5.30)式左右同时取下极限，可知对任意的 \\((x,z)\\in X\\times Z\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.202,
                0.508,
                0.738,
                0.53
            ],
            "angle": 0,
            "content": "\\[\n\\left[ f \\left(x ^ {\\sharp}\\right) - h ^ {*} (z) + \\left(x ^ {\\sharp}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z \\right] - \\left[ f (x) - h ^ {*} \\left(z ^ {\\sharp}\\right) + x ^ {\\mathrm {T}} A ^ {\\mathrm {T}} z ^ {\\sharp} \\right] \\leqslant 0. \\tag {8.5.31}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.544,
                0.649,
                0.561
            ],
            "angle": 0,
            "content": "从而 \\((x^{\\sharp},z^{\\sharp})\\) 也是问题(8.5.10)的一个鞍点．至此仅剩(3)待证明"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.565,
                0.74,
                0.667
            ],
            "angle": 0,
            "content": "为了证明 \\(\\{(x^k,z^k)\\}\\) 全序列收敛到问题(8.5.10)的鞍点，我们采用的大致思路为：先说明其子列收敛，然后再利用(8.5.27)式估计序列中其他点到子列极限点的误差（进而证明全序列收敛)，最后说明该极限点是鞍点．根据结论(1)， \\(\\{(x^{k},z^{k})\\}\\) 是有界点列，因此存在子列 \\(\\{(x^{k_l},z^{k_l})\\}\\) 收敛于 \\((x^{*},z^{*})\\) ．在(8.5.27)式中令 \\((x,z) = (x^{*},z^{*})\\) ，并将 \\(k\\) 从 \\(k_{l}\\) 取至 \\(N - 1,N > k_{l}\\) 并求和，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.237,
                0.677,
                0.672,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\frac {\\| x ^ {*} - x ^ {N} \\| ^ {2}}{2 t} + \\frac {\\| z ^ {*} - z ^ {N} \\| ^ {2}}{2 s} \\\\ + \\left(1 - \\sqrt {L s t}\\right) \\sum_ {k = k _ {l} + 1} ^ {N} \\frac {\\| z ^ {k} - z ^ {k - 1} \\| ^ {2}}{2 s} - \\frac {\\| x ^ {k _ {l}} - x ^ {k _ {l} - 1} \\| ^ {2}}{2 t} \\\\ + \\left(1 - \\sqrt {L s t}\\right) \\sum_ {k = k _ {l}} ^ {N - 1} \\frac {\\| x ^ {k} - x ^ {k - 1} \\| ^ {2}}{2 t} + \\frac {\\| x ^ {N} - x ^ {N - 1} \\| ^ {2}}{2 t} \\\\ + \\left(x ^ {N} - x ^ {N - 1}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {N} - z ^ {*}\\right) - \\left(x ^ {k _ {l}} - x ^ {k _ {l} - 1}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k _ {l}} - z ^ {*}\\right) \\\\ \\leqslant \\frac {\\| x ^ {*} - x ^ {k _ {l}} \\| ^ {2}}{2 t} + \\frac {\\| z ^ {*} - z ^ {k _ {l}} \\| ^ {2}}{2 s}. \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "430"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.707,
                0.174
            ],
            "angle": 0,
            "content": "去掉上式中不等式左边的求和项（正项），我们有如下估计："
        },
        {
            "type": "equation",
            "bbox": [
                0.315,
                0.178,
                0.771,
                0.269
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\frac {\\| x ^ {*} - x ^ {N} \\| ^ {2}}{2 t} + \\frac {\\| z ^ {*} - z ^ {N} \\| ^ {2}}{2 s} \\\\ \\leqslant \\frac {\\| x ^ {*} - x ^ {k _ {l}} \\| ^ {2}}{2 t} + \\frac {\\| z ^ {*} - z ^ {k _ {l}} \\| ^ {2}}{2 s} + \\frac {\\| x ^ {k _ {l}} - x ^ {k _ {l} - 1} \\| ^ {2}}{2 t} - \\frac {\\| x ^ {N} - x ^ {N - 1} \\| ^ {2}}{2 t} \\\\ + \\left(x ^ {k _ {l}} - x ^ {k _ {l} - 1}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {k _ {l}} - z ^ {*}\\right) - \\left(x ^ {N} - x ^ {N - 1}\\right) ^ {\\mathrm {T}} A ^ {\\mathrm {T}} \\left(z ^ {N} - z ^ {*}\\right). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.277,
                0.315,
                0.293
            ],
            "angle": 0,
            "content": "注意到"
        },
        {
            "type": "equation",
            "bbox": [
                0.437,
                0.294,
                0.613,
                0.312
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k _ {l}} \\rightarrow x ^ {*}, \\quad \\left(x ^ {k _ {l}} \\text {的 定 义}\\right)\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.386,
                0.318,
                0.67,
                0.336
            ],
            "angle": 0,
            "content": "\\[\nx ^ {N} - x ^ {N - 1} \\rightarrow 0, \\quad (\\text {由} (8. 5. 2 9) \\text {式 推 出})\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.432,
                0.343,
                0.687,
                0.361
            ],
            "angle": 0,
            "content": "\\[\n\\{z ^ {k} \\} \\text {有 界 ， (本 定 理 中 (1) 的 结 论)}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.367,
                0.825,
                0.426
            ],
            "angle": 0,
            "content": "所以当 \\(N \\to \\infty\\) 时有, \\(x^{N} \\rightarrow x^{*}, z^{N} \\rightarrow z^{*}\\), 全序列收敛性得证. 最后, 由全序列收敛可知均值 \\((\\overline{x}_{N}, \\overline{z}_{N})\\) 也收敛到 \\((x^{*}, z^{*})\\), 根据 (1) 的结论和极限的唯一性立即得到 \\((x^{\\sharp}, z^{\\sharp}) = (x^{*}, z^{*})\\), 即收敛到问题(8.5.10)的一个鞍点. \\(\\square\\)"
        },
        {
            "type": "title",
            "bbox": [
                0.425,
                0.456,
                0.657,
                0.476
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.494,
                0.825,
                0.718
            ],
            "angle": 0,
            "content": "统计学、机器学习和科学计算中出现了很多结构复杂且可能非凸、非光滑的优化问题。交替方向乘子法很自然地提供了一个适用范围广泛、容易理解和实现、可靠性不错的解决方案。该方法是在20世纪70年代发展起来的，与许多其他算法等价或密切相关，如对偶分解、乘子方法、Douglas-Rachford Splitting 方法、Dykstra 交替投影方法、Bregman 对于带 \\(\\ell_{1}\\) 范数问题的迭代算法、近似点算法等。本节首先介绍交替方向乘子法的基本算法；在介绍了 Douglas-Rachford Splitting 方法之后，说明将其应用在对偶问题上与将交替方向乘子法应用在原始问题上等价；然后给出交替方向乘子法的一些变形技巧，以及它和其他一些算法的关系；接着给出大量实际问题中的例子，并展示如何用交替方向乘子法来求解这些问题；最后给出交替方向乘子法的收敛性证明。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.743,
                0.46,
                0.761
            ],
            "angle": 0,
            "content": "8.6.1 交替方向乘子法"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.774,
                0.459,
                0.79
            ],
            "angle": 0,
            "content": "本节考虑如下凸问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.452,
                0.799,
                0.825,
                0.847
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x _ {1}, x _ {2}} f _ {1} \\left(x _ {1}\\right) + f _ {2} \\left(x _ {2}\\right), \\tag {8.6.1} \\\\ \\begin{array}{l l} \\text {s . t .} & A _ {1} x _ {1} + A _ {2} x _ {2} = b, \\end{array} \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "431"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.257
            ],
            "angle": 0,
            "content": "其中 \\(f_{1}, f_{2}\\) 是适当的闭凸函数，但不要求是光滑的，\\(x_{1} \\in \\mathbb{R}^{n}, x_{2} \\in \\mathbb{R}^{m}, A_{1} \\in \\mathbb{R}^{p \\times n}, A_{2} \\in \\mathbb{R}^{p \\times m}, b \\in \\mathbb{R}^{p}\\)。这个问题的特点是目标函数可以分成彼此分离的两块，但是变量被线性约束结合在一起。常见的一些无约束和带约束的优化问题都可以表示成这一形式。下面的一些例子将展示如何把某些一般的优化问题转化为适用交替方向乘子法求解的标准形式。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.269,
                0.515,
                0.285
            ],
            "angle": 0,
            "content": "例8.19 可以分成两块的无约束优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.298,
                0.531,
                0.322
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} f _ {1} (x) + f _ {2} (x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.33,
                0.737,
                0.367
            ],
            "angle": 0,
            "content": "为了将此问题转化为标准形式(8.6.1)，需要将目标函数改成可分的形式。我们可以通过引入一个新的变量 \\(z\\) 并令 \\(x = z\\)，将问题转化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.378,
                0.531,
                0.424
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x, z} \\quad f _ {1} (x) + f _ {2} (z), \\\\ \\begin{array}{l l} \\text {s . t .} & x - z = 0. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.437,
                0.498,
                0.452
            ],
            "angle": 0,
            "content": "例8.20 带线性变换的无约束优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.367,
                0.467,
                0.539,
                0.489
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} f _ {1} (x) + f _ {2} (A x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.498,
                0.657,
                0.515
            ],
            "angle": 0,
            "content": "类似地，我们可以引入一个新的变量 \\(z\\) ，令 \\(z = Ax\\) ，则问题变为"
        },
        {
            "type": "equation",
            "bbox": [
                0.375,
                0.525,
                0.531,
                0.571
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x, z} \\quad f _ {1} (x) + f _ {2} (z), \\\\ \\begin{array}{l l} \\text {s . t .} & A x - z = 0. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.584,
                0.48,
                0.6
            ],
            "angle": 0,
            "content": "对比问题(8.6.1)可知 \\(A_{1} = A\\) 和 \\(A_{2} = -I\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.613,
                0.446,
                0.628
            ],
            "angle": 0,
            "content": "例8.21 凸集上的约束优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.396,
                0.64,
                0.51,
                0.685
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x} f (x), \\\\ \\begin{array}{l l} \\text {s . t .} & A x \\in C, \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.697,
                0.737,
                0.734
            ],
            "angle": 0,
            "content": "其中 \\(C \\subset \\mathbb{R}^{n}\\) 为凸集．对于集合约束 \\(Ax \\in C\\) ，我们可以用示性函数 \\(I_{C}(\\cdot)\\) 将其添加到目标函数中，那么问题可以转化为例8.20中的形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.369,
                0.748,
                0.537,
                0.771
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} f (x) + I _ {C} (A x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.779,
                0.45,
                0.795
            ],
            "angle": 0,
            "content": "其中 \\(I_{C}(z)\\) 是集合 \\(C\\) 的示性函数，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.364,
                0.805,
                0.542,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nI _ {C} (z) = \\left\\{ \\begin{array}{l l} 0, & z \\in C, \\\\ + \\infty , & \\text {其 他}. \\end{array} \\right.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "432"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.158,
                0.544,
                0.172
            ],
            "angle": 0,
            "content": "再引入约束 \\(z = Ax\\) ，那么问题转化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.466,
                0.189,
                0.616,
                0.213
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, z} f (x) + I _ {C} (z),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.477,
                0.221,
                0.605,
                0.235
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A x - z = 0. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.254,
                0.735,
                0.271
            ],
            "angle": 0,
            "content": "例8.22 全局一致性问题（global consensus problem）[30]"
        },
        {
            "type": "equation",
            "bbox": [
                0.482,
                0.285,
                0.601,
                0.323
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\sum_ {i = 1} ^ {N} \\phi_ {i} (x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.337,
                0.692,
                0.352
            ],
            "angle": 0,
            "content": "令 \\(x = z\\) ，并将 \\(x\\) 复制 \\(N\\) 份，分别为 \\(x_{i}\\) ，那么问题转化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.42,
                0.366,
                0.543,
                0.404
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x _ {i}, z} \\sum_ {i = 1} ^ {N} \\phi_ {i} (x _ {i}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.43,
                0.412,
                0.662,
                0.427
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & x _ {i} - z = 0, i = 1, 2, \\dots , N. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.444,
                0.825,
                0.479
            ],
            "angle": 0,
            "content": "在这里注意，从形式上看全局一致性问题仍然具有问题(8.6.1)的结构：如果令"
        },
        {
            "type": "equation",
            "bbox": [
                0.463,
                0.486,
                0.62,
                0.506
            ],
            "angle": 0,
            "content": "\\[\nx = \\left(x _ {1} ^ {\\mathrm {T}}, x _ {2} ^ {\\mathrm {T}}, \\dots , x _ {N} ^ {\\mathrm {T}}\\right) ^ {\\mathrm {T}}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.52,
                0.299,
                0.534
            ],
            "angle": 0,
            "content": "以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.536,
                0.655,
                0.574
            ],
            "angle": 0,
            "content": "\\[\nf _ {1} (x) = \\sum_ {i = 1} ^ {N} \\phi_ {i} \\left(x _ {i}\\right), \\quad f _ {2} (z) = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.583,
                0.4,
                0.598
            ],
            "angle": 0,
            "content": "则此问题可以化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.457,
                0.603,
                0.612,
                0.626
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, z} \\quad f _ {1} (x) + f _ {2} (z),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.634,
                0.625,
                0.65
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A _ {1} x - A _ {2} z = 0, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.662,
                0.447,
                0.678
            ],
            "angle": 0,
            "content": "其中矩阵 \\(A_{1}, A_{2}\\) 定义为："
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.689,
                0.675,
                0.783
            ],
            "angle": 0,
            "content": "\\[\nA _ {1} = \\left[ \\begin{array}{c c c c} I & & & \\\\ & I & & \\\\ & & \\ddots & \\\\ & & & I \\end{array} \\right], \\quad A _ {2} = \\left[ \\begin{array}{c} I \\\\ I \\\\ \\vdots \\\\ I \\end{array} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.795,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "在全局一致性问题的例子中，我们将问题重写为具有两个变量块的形式，而不是简单地将问题(8.6.1)推广为多个变量块的形式。这样做是有一定原因的，我们将在应用举例部分给出解答。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "433"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.157,
                0.534,
                0.174
            ],
            "angle": 0,
            "content": "例8.23 共享问题（Sharing Problem）[30]"
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.184,
                0.562,
                0.227
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x _ {i}} \\quad \\sum_ {i = 1} ^ {N} f _ {i} \\left(x _ {i}\\right) + g \\left(\\sum_ {i = 1} ^ {N} x _ {i}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.237,
                0.735,
                0.271
            ],
            "angle": 0,
            "content": "为了使目标函数可分，我们将 \\(g\\) 的变量 \\(x_{i}\\) 分别复制一份为 \\(z_{i}\\)，那么问题转化为"
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.271,
                0.548,
                0.314
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x _ {i}, z _ {i}} \\sum_ {i = 1} ^ {N} f _ {i} (x _ {i}) + g \\left(\\sum_ {i = 1} ^ {N} z _ {i}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.339,
                0.32,
                0.577,
                0.337
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & x _ {i} - z _ {i} = 0, i = 1, 2, \\dots , N. \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.345,
                0.484,
                0.361
            ],
            "angle": 0,
            "content": "容易验证此问题也具有问题(8.6.1)的形式"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.375,
                0.737,
                0.412
            ],
            "angle": 0,
            "content": "下面给出交替方向乘子法（alternating direction method of multipliers, ADMM）的迭代格式，首先写出问题(8.6.1)的增广拉格朗日函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.233,
                0.424,
                0.736,
                0.475
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} L _ {\\rho} \\left(x _ {1}, x _ {2}, y\\right) = f _ {1} \\left(x _ {1}\\right) + f _ {2} \\left(x _ {2}\\right) + y ^ {\\mathrm {T}} \\left(A _ {1} x _ {1} + A _ {2} x _ {2} - b\\right) \\tag {8.6.2} \\\\ + \\frac {\\rho}{2} \\| A _ {1} x _ {1} + A _ {2} x _ {2} - b \\| _ {2} ^ {2}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.484,
                0.737,
                0.522
            ],
            "angle": 0,
            "content": "其中 \\(\\rho > 0\\) 是二次罚项的系数。常见的求解带约束问题的增广拉格朗日函数法为如下更新："
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.536,
                0.737,
                0.565
            ],
            "angle": 0,
            "content": "\\[\n\\left(x _ {1} ^ {k + 1}, x _ {2} ^ {k + 1}\\right) = \\underset {x _ {1}, x _ {2}} {\\arg \\min } L _ {\\rho} \\left(x _ {1}, x _ {2}, y ^ {k}\\right), \\tag {8.6.3}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.336,
                0.569,
                0.737,
                0.59
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = y ^ {k} + \\tau \\rho \\left(A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b\\right), \\tag {8.6.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.604,
                0.737,
                0.682
            ],
            "angle": 0,
            "content": "其中 \\(\\tau\\) 为步长。在实际求解中，第一步迭代(8.6.3)同时对 \\(x_{1}\\) 和 \\(x_{2}\\) 进行优化有时候比较困难，而固定一个变量求解关于另一个变量的极小问题可能比较简单，因此我们可以考虑对 \\(x_{1}\\) 和 \\(x_{2}\\) 交替求极小，这就是交替方向乘子法的基本思路。其迭代格式可以总结如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.31,
                0.696,
                0.737,
                0.725
            ],
            "angle": 0,
            "content": "\\[\nx _ {1} ^ {k + 1} = \\underset {x _ {1}} {\\arg \\min } L _ {\\rho} \\left(x _ {1}, x _ {2} ^ {k}, y ^ {k}\\right), \\tag {8.6.5}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.311,
                0.73,
                0.737,
                0.758
            ],
            "angle": 0,
            "content": "\\[\nx _ {2} ^ {k + 1} = \\underset {x _ {2}} {\\arg \\min } L _ {\\rho} \\left(x _ {1} ^ {k + 1}, x _ {2}, y ^ {k}\\right), \\tag {8.6.6}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.311,
                0.764,
                0.737,
                0.784
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = y ^ {k} + \\tau \\rho \\left(A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b\\right), \\tag {8.6.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.796,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(\\tau\\) 为步长，通常取值于 \\(\\left(0, \\frac{1 + \\sqrt{5}}{2}\\right]\\)。关于这样选择步长的收敛性，我们将在证明收敛性的小节中介绍。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "434"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.318
            ],
            "angle": 0,
            "content": "观察交替方向乘子法的迭代格式，第一步固定 \\(x_{2},y\\) 对 \\(x_{1}\\) 求极小；第二步固定 \\(x_{1},y\\) 对 \\(x_{2}\\) 求极小；第三步更新拉格朗日乘子 \\(y\\) 。这一迭代格式和之前讨论的交替极小化方法 (8.5.9) 非常相似。它们的区别是交替极小化方法的第一步是针对拉格朗日函数求极小，而ADMM的第一步将其换成了增广拉格朗日函数。虽然从形式上看两个算法只是略有差别，但这种改变会带来截然不同的算法表现。ADMM的一个最直接的改善就是去掉了目标函数\\(f_{1}(x)\\) 强凸的要求，其本质还是由于它引入了二次罚项。而在交替极小化方法中我们要求 \\(f(x)\\) 为强凸函数。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.322,
                0.825,
                0.401
            ],
            "angle": 0,
            "content": "需要注意的是，虽然交替方向乘子法引入了二次罚项，但对一般的闭凸函数 \\( f_{1} \\) 和 \\( f_{2} \\)，迭代(8.6.5)和迭代(8.6.6)在某些特殊情况下仍然不是良定义的。本节假设每个子问题的解均是存在且唯一的，但读者应当注意到这个假设对一般的闭凸函数是不成立的。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.405,
                0.825,
                0.504
            ],
            "angle": 0,
            "content": "与无约束优化问题不同，交替方向乘子法针对的问题(8.6.1)是带约束的优化问题，因此算法的收敛准则应当借助约束优化问题的最优性条件（KKT条件）．因为 \\(f_{1}, f_{2}\\) 均为闭凸函数，约束为线性约束，所以当 Slater 条件成立时，可以使用凸优化问题的 KKT 条件来作为交替方向乘子法的收敛准则. 问题(8.6.1)的拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.343,
                0.518,
                0.738,
                0.536
            ],
            "angle": 0,
            "content": "\\[\nL \\left(x _ {1}, x _ {2}, y\\right) = f _ {1} \\left(x _ {1}\\right) + f _ {2} \\left(x _ {2}\\right) + y ^ {\\mathrm {T}} \\left(A _ {1} x _ {1} + A _ {2} x _ {2} - b\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.55,
                0.825,
                0.587
            ],
            "angle": 0,
            "content": "根据定理 5.13, 若 \\(x_{1}^{*}, x_{2}^{*}\\) 为问题(8.6.1)的最优解, \\(y^{*}\\) 为对应的拉格朗日乘子,则以下条件满足:"
        },
        {
            "type": "equation",
            "bbox": [
                0.4,
                0.601,
                0.825,
                0.619
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial_ {x _ {1}} L \\left(x _ {1} ^ {*}, x _ {2} ^ {*}, y ^ {*}\\right) = \\partial f _ {1} \\left(x _ {1} ^ {*}\\right) + A _ {1} ^ {\\mathrm {T}} y ^ {*}, \\tag {8.6.8a}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.401,
                0.625,
                0.825,
                0.644
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial_ {x _ {2}} L \\left(x _ {1} ^ {*}, x _ {2} ^ {*}, y ^ {*}\\right) = \\partial f _ {2} \\left(x _ {2} ^ {*}\\right) + A _ {2} ^ {\\mathrm {T}} y ^ {*}, \\tag {8.6.8b}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.651,
                0.825,
                0.668
            ],
            "angle": 0,
            "content": "\\[\nA _ {1} x _ {1} ^ {*} + A _ {2} x _ {2} ^ {*} = b. \\tag {8.6.8c}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.681,
                0.825,
                0.78
            ],
            "angle": 0,
            "content": "在这里条件(8.6.8c)又称为原始可行性条件，条件(8.6.8a)和条件(8.6.8b)又称为对偶可行性条件。由于问题中只含等式约束，KKT条件中的互补松弛条件可以不加考虑。在ADMM迭代中，我们得到的迭代点实际为 \\((x_1^k, x_2^k, y^k)\\)，因此收敛准则应当针对 \\((x_1^k, x_2^k, y^k)\\) 检测条件(8.6.8)。接下来讨论如何具体计算这些收敛准则。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.785,
                0.825,
                0.822
            ],
            "angle": 0,
            "content": "一般来说，原始可行性条件(8.6.8c)在迭代中是不满足的，为了检测这个条件，需要计算原始可行性残差"
        },
        {
            "type": "equation",
            "bbox": [
                0.459,
                0.835,
                0.624,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nr ^ {k} = A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} ^ {k} - b\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "435"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.195
            ],
            "angle": 0,
            "content": "的模长,这一计算是比较容易的.下面来看两个对偶可行性条件.考虑ADMM迭代更新 \\(x_{2}\\) 的步骤"
        },
        {
            "type": "equation",
            "bbox": [
                0.249,
                0.203,
                0.66,
                0.248
            ],
            "angle": 0,
            "content": "\\[\nx _ {2} ^ {k} = \\underset {x} {\\arg \\min } \\left\\{f _ {2} (x) + \\frac {\\rho}{2} \\left\\| A _ {1} x _ {1} ^ {k} + A _ {2} x - b + \\frac {y ^ {k - 1}}{\\rho} \\right\\| ^ {2} \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.256,
                0.692,
                0.273
            ],
            "angle": 0,
            "content": "假设这一子问题有显式解或能够精确求解，根据最优性条件不难推出"
        },
        {
            "type": "equation",
            "bbox": [
                0.281,
                0.287,
                0.737,
                0.308
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial f _ {2} \\left(x _ {2} ^ {k}\\right) + A _ {2} ^ {\\mathrm {T}} \\left[ y ^ {k - 1} + \\rho \\left(A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} ^ {k} - b\\right) \\right]. \\tag {8.6.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.321,
                0.737,
                0.359
            ],
            "angle": 0,
            "content": "注意到当ADMM步长 \\(\\tau = 1\\) 时，根据迭代(8.6.7)可知上式方括号中的表达式就是 \\(y^{k}\\)，最终我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.378,
                0.361,
                0.529,
                0.381
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial f _ {2} \\left(x _ {2} ^ {k}\\right) + A _ {2} ^ {\\mathrm {T}} y ^ {k},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.39,
                0.739,
                0.47
            ],
            "angle": 0,
            "content": "这恰好就是条件(8.6.8b).上面的分析说明在ADMM迭代过程中，若 \\(x_{2}\\) 的更新能取到精确解且步长 \\(\\tau = 1\\) ，对偶可行性条件(8.6.8b)是自然成立的，因此无需针对条件(8.6.8b)单独验证最优性条件.然而，在迭代过程中条件(8.6.8a)却不能自然满足．实际上，由 \\(x_{1}\\) 的更新公式"
        },
        {
            "type": "equation",
            "bbox": [
                0.243,
                0.478,
                0.665,
                0.515
            ],
            "angle": 0,
            "content": "\\[\nx _ {1} ^ {k} = \\underset {x} {\\arg \\min } \\left\\{f _ {1} (x) + \\frac {\\rho}{2} \\| A _ {1} x + A _ {2} x _ {2} ^ {k - 1} - b + \\frac {y ^ {k - 1}}{\\rho} \\| ^ {2} \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.525,
                0.486,
                0.541
            ],
            "angle": 0,
            "content": "假设子问题能精确求解，根据最优性条件"
        },
        {
            "type": "equation",
            "bbox": [
                0.273,
                0.556,
                0.635,
                0.577
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial f _ {1} (x _ {1} ^ {k}) + A _ {1} ^ {\\mathrm {T}} [ \\rho (A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} ^ {k - 1} - b) + y ^ {k - 1} ].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.59,
                0.738,
                0.627
            ],
            "angle": 0,
            "content": "注意，这里 \\(x_{2}\\) 上标是 \\(k - 1\\) ，因此根据ADMM的第三式(8.6.7)，同样取\\(\\tau = 1\\) ，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.308,
                0.642,
                0.737,
                0.663
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial f _ {1} \\left(x _ {1} ^ {k}\\right) + A _ {1} ^ {\\mathrm {T}} \\left(y ^ {k} + A _ {2} \\left(x _ {2} ^ {k - 1} - x _ {2} ^ {k}\\right)\\right). \\tag {8.6.10}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.674,
                0.738,
                0.712
            ],
            "angle": 0,
            "content": "对比条件(8.6.8a)可知多出来的项为 \\(A_1^{\\mathrm{T}}A_2(x_2^{k - 1} - x_2^k)\\)，因此要检测对偶可行性只需要检测残差"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.715,
                0.537,
                0.736
            ],
            "angle": 0,
            "content": "\\[\ns ^ {k} = A _ {1} ^ {\\mathrm {T}} A _ {2} (x _ {2} ^ {k - 1} - x _ {2} ^ {k})\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.745,
                0.739,
                0.802
            ],
            "angle": 0,
            "content": "是否充分小, 这一检测同样也是比较容易的. 综上, 当 \\(x_{2}\\) 更新取到精确解且 \\(\\tau = 1\\) 时, 判断 ADMM 是否收敛只需要检测前述两个残差 \\(r^{k}, s^{k}\\) 是否充分小:"
        },
        {
            "type": "equation",
            "bbox": [
                0.243,
                0.815,
                0.61,
                0.834
            ],
            "angle": 0,
            "content": "\\[\n0 \\approx \\| r ^ {k} \\| = \\| A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} ^ {k} - b \\| \\quad (\\text {原 始 可 行 性}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.246,
                0.836,
                0.609,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n0 \\approx \\| s ^ {k} \\| = \\| A _ {1} ^ {\\mathrm {T}} A _ {2} (x _ {2} ^ {k - 1} - x _ {2} ^ {k}) \\| \\quad (\\text {对 偶 可 行 性}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.678,
                0.827,
                0.737,
                0.842
            ],
            "angle": 0,
            "content": "(8.6.11)"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "436"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.616,
                0.176
            ],
            "angle": 0,
            "content": "8.6.2 Douglas-Rachford Splitting 算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.187,
                0.825,
                0.224
            ],
            "angle": 0,
            "content": "Douglas-Rachford Splitting (DRS) 算法是一类非常重要的算子分裂算法。它可以用于求解下面的无约束优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.24,
                0.824,
                0.261
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad \\psi (x) = f (x) + h (x), \\tag {8.6.12}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.269,
                0.63,
                0.286
            ],
            "angle": 0,
            "content": "其中 \\(f\\) 和 \\(h\\) 是闭凸函数. DRS 算法的迭代格式是"
        },
        {
            "type": "equation",
            "bbox": [
                0.446,
                0.3,
                0.823,
                0.318
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {t h} \\left(z ^ {k}\\right), \\tag {8.6.13}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.446,
                0.324,
                0.823,
                0.344
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = \\operatorname {p r o x} _ {t f} \\left(2 x ^ {k + 1} - z ^ {k}\\right), \\tag {8.6.14}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.447,
                0.35,
                0.824,
                0.368
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = z ^ {k} + y ^ {k + 1} - x ^ {k + 1}, \\tag {8.6.15}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.381,
                0.825,
                0.419
            ],
            "angle": 0,
            "content": "其中 \\(t\\) 是一个正的常数．我们还可以通过一系列变形来得到DRS格式的等价迭代．首先在原始DRS格式中按照 \\(y,z,x\\) 的顺序进行更新，则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.453,
                0.432,
                0.632,
                0.451
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = \\operatorname {p r o x} _ {t f} (2 x ^ {k} - z ^ {k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.455,
                0.458,
                0.613,
                0.474
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = z ^ {k} + y ^ {k + 1} - x ^ {k},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.453,
                0.482,
                0.6,
                0.5
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {t h} \\left(z ^ {k + 1}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.511,
                0.825,
                0.549
            ],
            "angle": 0,
            "content": "引入辅助变量 \\(w^{k} = z^{k} - x^{k}\\) ，并注意到上面迭代中变量 \\(z^{k},z^{k + 1}\\) 可以消去，则得到DRS算法的等价迭代格式"
        },
        {
            "type": "equation",
            "bbox": [
                0.449,
                0.563,
                0.823,
                0.583
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = \\operatorname {p r o x} _ {t f} \\left(x ^ {k} - w ^ {k}\\right), \\tag {8.6.16}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.45,
                0.589,
                0.824,
                0.607
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {t h} \\left(w ^ {k} + y ^ {k + 1}\\right), \\tag {8.6.17}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.447,
                0.614,
                0.823,
                0.631
            ],
            "angle": 0,
            "content": "\\[\nw ^ {k + 1} = w ^ {k} + y ^ {k + 1} - x ^ {k + 1}. \\tag {8.6.18}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.644,
                0.634,
                0.661
            ],
            "angle": 0,
            "content": "DRS 格式还可以写成关于 \\(z^{k}\\) 的不动点迭代的形式"
        },
        {
            "type": "equation",
            "bbox": [
                0.493,
                0.676,
                0.824,
                0.692
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = T \\left(z ^ {k}\\right), \\tag {8.6.19}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.706,
                0.298,
                0.722
            ],
            "angle": 0,
            "content": "其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.368,
                0.727,
                0.718,
                0.747
            ],
            "angle": 0,
            "content": "\\[\nT (z) = z + \\operatorname {p r o x} _ {t f} \\left(2 \\operatorname {p r o x} _ {t h} (z) - z\\right) - \\operatorname {p r o x} _ {t h} (z).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.753,
                0.825,
                0.832
            ],
            "angle": 0,
            "content": "将DRS格式写成不动点迭代的形式是有好处的：第一，它去掉了迭代中的 \\(x^{k},y^{k}\\) 变量，使得算法形式更加简洁；第二，对不动点迭代的收敛性研究有一些常用的工具和技术手段，例如泛函分析中的压缩映射原理；第三，针对不动点迭代可写出很多种不同类型的加速算法."
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.836,
                0.734,
                0.853
            ],
            "angle": 0,
            "content": "下面的定理给出 \\(T\\) 的不动点与 \\(f + h\\) 的极小值之间的关系："
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "437"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.157,
                0.736,
                0.196
            ],
            "angle": 0,
            "content": "定理8.14 (1) 若 \\(z\\) 是(8.6.19)中 \\(T\\) 的一个不动点, 即 \\(z = T(z)\\), 则 \\(x = \\operatorname{prox}_{th}(z)\\) 是问题(8.6.12)的一个最小值点."
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.209,
                0.737,
                0.246
            ],
            "angle": 0,
            "content": "(2) 若 \\(x\\) 是问题(8.6.12)的一个最小值点, 则存在 \\(u \\in t\\partial f(x) \\cap (-t\\partial h(x))\\), 且 \\(x - u = T(x - u)\\), 即 \\(x - u\\) 是 \\(T\\) 的一个不动点."
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.261,
                0.214,
                0.276
            ],
            "angle": 0,
            "content": "证明."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.292,
                0.443,
                0.308
            ],
            "angle": 0,
            "content": "(1) 如果 \\(z\\) 是 \\(T\\) 的一个不动点，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.324,
                0.667,
                0.344
            ],
            "angle": 0,
            "content": "\\[\nz = T (z) = z + \\operatorname {p r o x} _ {t f} \\left(2 \\operatorname {p r o x} _ {t h} (z) - z\\right) - \\operatorname {p r o x} _ {t h} (z),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.359,
                0.373,
                0.376
            ],
            "angle": 0,
            "content": "令 \\(x = \\mathrm{prox}_{th}(z)\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.353,
                0.392,
                0.595,
                0.412
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {p r o x} _ {t f} (2 x - z) = x = \\operatorname {p r o x} _ {t h} (z).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.425,
                0.475,
                0.442
            ],
            "angle": 0,
            "content": "由邻近算子的定义和最优性条件得"
        },
        {
            "type": "equation",
            "bbox": [
                0.35,
                0.459,
                0.597,
                0.476
            ],
            "angle": 0,
            "content": "\\[\nx - z \\in t \\partial f (x), \\quad z - x \\in t \\partial h (x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.493,
                0.259,
                0.509
            ],
            "angle": 0,
            "content": "因此，"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.513,
                0.551,
                0.53
            ],
            "angle": 0,
            "content": "\\[\n0 \\in t \\partial f (x) + t \\partial h (x).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.209,
                0.541,
                0.737,
                0.579
            ],
            "angle": 0,
            "content": "根据凸优化问题的一阶充要条件知 \\(x = \\mathrm{prox}_{th}(z)\\) 是问题(8.6.12)的一个最小值点."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.593,
                0.638,
                0.61
            ],
            "angle": 0,
            "content": "(2) 因为 \\(x\\) 是问题(8.6.12)一个最小值点，根据一阶充要条件，"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.627,
                0.552,
                0.644
            ],
            "angle": 0,
            "content": "\\[\n0 \\in t \\partial f (x) + t \\partial h (x),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.66,
                0.6,
                0.678
            ],
            "angle": 0,
            "content": "这等价于存在 \\(u\\in t\\partial f(x)\\cap (-t\\partial h(x))\\) ．由定理8.2，"
        },
        {
            "type": "equation",
            "bbox": [
                0.357,
                0.692,
                0.619,
                0.711
            ],
            "angle": 0,
            "content": "\\[\nu \\in t \\partial f (x) \\Longleftrightarrow x = \\operatorname {p r o x} _ {t f} (x + u),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.717,
                0.619,
                0.737
            ],
            "angle": 0,
            "content": "\\[\nu \\in (- t \\partial h (x)) \\Longleftrightarrow x = \\operatorname {p r o x} _ {t h} (x - u),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.211,
                0.749,
                0.32,
                0.765
            ],
            "angle": 0,
            "content": "然后可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.266,
                0.782,
                0.68,
                0.802
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {p r o x} _ {t f} \\left(2 \\operatorname {p r o x} _ {t h} (x - u) - (x - u)\\right) - \\operatorname {p r o x} _ {t h} (x - u) = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.816,
                0.234,
                0.831
            ],
            "angle": 0,
            "content": "即"
        },
        {
            "type": "equation",
            "bbox": [
                0.404,
                0.837,
                0.543,
                0.853
            ],
            "angle": 0,
            "content": "\\[\nx - u = T (x - u).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.72,
                0.838,
                0.737,
                0.851
            ],
            "angle": 0,
            "content": "□"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "438"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.157,
                0.778,
                0.174
            ],
            "angle": 0,
            "content": "对于不动点迭代，我们可以通过添加松弛项来加快收敛速度，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.188,
                0.64,
                0.209
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = z ^ {k} + \\rho (T (z ^ {k}) - z ^ {k}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.224,
                0.825,
                0.262
            ],
            "angle": 0,
            "content": "其中，当 \\(1 < \\rho < 2\\) 时是超松弛，\\(0 < \\rho < 1\\) 是欠松弛。从而得到DRS算法的松弛版本"
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.277,
                0.577,
                0.297
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {t h} \\left(z ^ {k}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.303,
                0.638,
                0.323
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = \\operatorname {p r o x} _ {t f} (2 x ^ {k + 1} - z ^ {k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.328,
                0.642,
                0.347
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = z ^ {k} + \\rho (y ^ {k + 1} - x ^ {k + 1}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.363,
                0.367,
                0.38
            ],
            "angle": 0,
            "content": "其等价形式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.395,
                0.573,
                0.416
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = \\operatorname {p r o x} _ {t f} \\left(x ^ {k} - w ^ {k}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.422,
                0.687,
                0.441
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {t h} ((1 - \\rho) x ^ {k} + \\rho y ^ {k + 1} + w ^ {k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.447,
                0.681,
                0.465
            ],
            "angle": 0,
            "content": "\\[\nw ^ {k + 1} = w ^ {k} + \\rho y ^ {k + 1} + (1 - \\rho) x ^ {k} - x ^ {k + 1}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.481,
                0.825,
                0.519
            ],
            "angle": 0,
            "content": "DRS 算法和 ADMM 有一定的等价关系。考虑本章开始引入的可分的凸问题(8.6.1)："
        },
        {
            "type": "equation",
            "bbox": [
                0.454,
                0.534,
                0.618,
                0.556
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x _ {1}, x _ {2}} f _ {1} (x _ {1}) + f _ {2} (x _ {2}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.463,
                0.566,
                0.631,
                0.58
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & A _ {1} x _ {1} + A _ {2} x _ {2} = b, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.595,
                0.539,
                0.611
            ],
            "angle": 0,
            "content": "它的对偶问题为无约束复合优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.626,
                0.824,
                0.666
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {z} \\underbrace {b ^ {\\mathrm {T}} z + f _ {1} ^ {*} \\left(- A _ {1} ^ {\\mathrm {T}} z\\right)} _ {f (z)} + \\underbrace {f _ {2} ^ {*} \\left(- A _ {2} ^ {\\mathrm {T}} z\\right)} _ {h (z)}, \\tag {8.6.20}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.678,
                0.825,
                0.737
            ],
            "angle": 0,
            "content": "根据问题(8.6.20)的结构拆分出 \\(f(z)\\) 和 \\(h(z)\\), 我们对该问题使用 DRS 算法求解. 下面这个定理表明, 对原始问题(8.6.1)使用 ADMM 求解就等价于将 DRS 算法应用在对偶问题(8.6.20)上."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.751,
                0.824,
                0.788
            ],
            "angle": 0,
            "content": "定理8.15 对问题(8.6.20)应用迭代格式(8.6.16)一(8.6.18)等价于运用ADMM到问题(8.6.1)."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.803,
                0.623,
                0.819
            ],
            "angle": 0,
            "content": "证明. 对于迭代格式(8.6.16)，其最优性条件为"
        },
        {
            "type": "equation",
            "bbox": [
                0.373,
                0.835,
                0.71,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n0 \\in t b - t A _ {1} \\partial f _ {1} ^ {*} (- A _ {1} ^ {\\mathrm {T}} y ^ {k + 1}) - x ^ {k} + w ^ {k} + y ^ {k + 1},\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "439"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.156,
                0.495,
                0.175
            ],
            "angle": 0,
            "content": "上式等价于存在 \\(x_{1}^{k}\\in \\partial f_{1}^{*}(-A_{1}^{\\mathrm{T}}y^{k + 1})\\) ，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.343,
                0.189,
                0.737,
                0.208
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = x ^ {k} - w ^ {k} + t \\left(A _ {1} x _ {1} ^ {k} - b\\right). \\tag {8.6.21}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.221,
                0.462,
                0.24
            ],
            "angle": 0,
            "content": "根据命题 8.2, \\(-A_{1}^{\\mathrm{T}} y^{k+1} \\in \\partial f_{1}\\left(x_{1}^{k}\\right)\\), 故"
        },
        {
            "type": "equation",
            "bbox": [
                0.309,
                0.254,
                0.602,
                0.273
            ],
            "angle": 0,
            "content": "\\[\n- A _ {1} ^ {\\mathrm {T}} \\left(x ^ {k} - w ^ {k} + t \\left(A _ {1} x _ {1} ^ {k} - b\\right)\\right) \\in \\partial f _ {1} \\left(x _ {1} ^ {k}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.289,
                0.296,
                0.305
            ],
            "angle": 0,
            "content": "这就是如下更新"
        },
        {
            "type": "equation",
            "bbox": [
                0.213,
                0.314,
                0.697,
                0.352
            ],
            "angle": 0,
            "content": "\\[\nx _ {1} ^ {k} = \\arg \\min  _ {x _ {1}} \\left\\{f _ {1} (x _ {1}) + \\left(x ^ {k}\\right) ^ {\\mathrm {T}} \\left(A _ {1} x _ {1} - b\\right) + \\frac {t}{2} \\| A _ {1} x _ {1} - b - \\frac {w ^ {k}}{t} \\| _ {2} ^ {2} \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.363,
                0.283,
                0.379
            ],
            "angle": 0,
            "content": "的最优性条件."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.383,
                0.487,
                0.399
            ],
            "angle": 0,
            "content": "类似地，迭代(8.6.17)的最优性条件为"
        },
        {
            "type": "equation",
            "bbox": [
                0.299,
                0.414,
                0.611,
                0.433
            ],
            "angle": 0,
            "content": "\\[\n0 \\in t A _ {2} \\partial f _ {2} ^ {*} (- A _ {2} ^ {\\mathrm {T}} x ^ {k + 1}) + w ^ {k} + y ^ {k + 1} - x ^ {k + 1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.447,
                0.478,
                0.466
            ],
            "angle": 0,
            "content": "其等价于存在 \\(x_2^k \\in \\partial f_2^*(-A_2^{\\mathrm{T}} x^{k+1})\\)，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.335,
                0.481,
                0.736,
                0.499
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} + t \\left(A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} ^ {k} - b\\right). \\tag {8.6.22}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.513,
                0.583,
                0.532
            ],
            "angle": 0,
            "content": "同样地，根据命题8.2，\\(-A_2^{\\mathrm{T}}x^{k + 1}\\in \\partial f_2(x_2^k)\\)，所以可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.301,
                0.545,
                0.61,
                0.565
            ],
            "angle": 0,
            "content": "\\[\n- A _ {2} ^ {\\mathrm {T}} \\left(x ^ {k} + t \\left(A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} ^ {k} - b\\right)\\right) \\in \\partial f _ {2} \\left(x _ {2} ^ {k}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.58,
                0.245,
                0.596
            ],
            "angle": 0,
            "content": "其等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.221,
                0.606,
                0.687,
                0.642
            ],
            "angle": 0,
            "content": "\\[\nx _ {2} ^ {k} = \\arg \\min  _ {x _ {2}} \\left\\{f _ {2} (x _ {2}) + \\left(x ^ {k}\\right) ^ {\\mathrm {T}} \\left(A _ {2} x _ {2}\\right) + \\frac {t}{2} \\| A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} - b \\| _ {2} ^ {2} \\right\\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.653,
                0.737,
                0.692
            ],
            "angle": 0,
            "content": "由(8.6.21)式和(8.6.22)式可得 \\(w\\)-更新转化为 \\(w^{k+1} = -tA_2x_2^k\\). 令 \\(z^k = x^{k+1}\\), 总结上面的更新, 可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.218,
                0.703,
                0.691,
                0.8
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} x _ {1} ^ {k} = \\underset {x _ {1}} {\\arg \\min } \\left\\{f _ {1} (x _ {1}) + (z ^ {k - 1}) ^ {\\mathrm {T}} A _ {1} x _ {1} + \\frac {t}{2} \\| A _ {1} x _ {1} + A _ {2} x _ {2} ^ {k - 1} - b \\| \\right\\}, \\\\ x _ {2} ^ {k} = \\underset {x _ {2}} {\\arg \\min } \\left\\{f _ {2} (x _ {2}) + (z ^ {k - 1}) ^ {\\mathrm {T}} A _ {2} x _ {2} + \\frac {t}{2} \\| A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} - b \\| \\right\\}, \\\\ z ^ {k} = z ^ {k - 1} + t \\left(A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} ^ {k} - b\\right), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.816,
                0.728,
                0.833
            ],
            "angle": 0,
            "content": "这就是交替方向乘子法应用到问题(8.6.1)，其中罚因子 \\(\\rho = t\\) ，步长 \\(\\tau = 1\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.836,
                0.54,
                0.853
            ],
            "angle": 0,
            "content": "以上论证过程均可以反推，因此等价性成立"
        },
        {
            "type": "text",
            "bbox": [
                0.721,
                0.838,
                0.737,
                0.851
            ],
            "angle": 0,
            "content": "□"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "440"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.46,
                0.174
            ],
            "angle": 0,
            "content": "8.6.3 常见变形和技巧"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.188,
                0.825,
                0.225
            ],
            "angle": 0,
            "content": "本小节将给出交替方向乘子法的一些变形以及实现交替方向乘子法的一些技巧."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.253,
                0.345,
                0.269
            ],
            "angle": 0,
            "content": "1. 线性化"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.284,
                0.825,
                0.341
            ],
            "angle": 0,
            "content": "我们构造ADMM的初衷是将自变量拆分，最终使得关于 \\(x_{1}\\) 和 \\(x_{2}\\) 的子问题有显式解．但是在实际应用中，有时子问题并不容易求解，或者没有必要精确求解．那么如何寻找子问题的一个近似呢？"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.346,
                0.607,
                0.362
            ],
            "angle": 0,
            "content": "不失一般性，我们考虑第一个子问题，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.424,
                0.375,
                0.824,
                0.404
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x _ {1}} \\quad f _ {1} \\left(x _ {1}\\right) + \\frac {\\rho}{2} \\| A _ {1} x _ {1} - v ^ {k} \\| ^ {2}, \\tag {8.6.23}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.416,
                0.297,
                0.432
            ],
            "angle": 0,
            "content": "其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.46,
                0.43,
                0.824,
                0.466
            ],
            "angle": 0,
            "content": "\\[\nv ^ {k} = b - A _ {2} x _ {2} ^ {k} - \\frac {1}{\\rho} y ^ {k}. \\tag {8.6.24}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.472,
                0.825,
                0.53
            ],
            "angle": 0,
            "content": "当子问题不能显式求解时, 可采用线性化的方法 [147] 近似求解问题 (8.6.23). 线性化技巧实际上是使用近似点项对子问题目标函数进行二次近似. 当子问题目标函数可微时, 线性化将问题 (8.6.23) 变为"
        },
        {
            "type": "equation",
            "bbox": [
                0.281,
                0.54,
                0.801,
                0.577
            ],
            "angle": 0,
            "content": "\\[\nx _ {1} ^ {k + 1} = \\underset {x _ {1}} {\\arg \\min } \\left\\{\\left(\\nabla f _ {1} \\left(x _ {1} ^ {k}\\right) + \\rho A _ {1} ^ {\\mathrm {T}} \\left(A _ {1} x _ {1} ^ {k} - v ^ {k}\\right)\\right) ^ {\\mathrm {T}} x _ {1} + \\frac {1}{2 \\eta_ {k}} \\| x _ {1} - x ^ {k} \\| _ {2} ^ {2} \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.589,
                0.824,
                0.627
            ],
            "angle": 0,
            "content": "其中 \\(\\eta_{k}\\) 是步长参数，这等价于做一步梯度下降。当目标函数不可微时，可以考虑只将二次项线性化，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.291,
                0.637,
                0.791,
                0.673
            ],
            "angle": 0,
            "content": "\\[\nx _ {1} ^ {k + 1} = \\underset {x _ {1}} {\\arg \\min } \\left\\{f (x _ {1}) + \\rho \\left(A _ {1} ^ {\\mathrm {T}} \\left(A _ {1} x _ {1} ^ {k} - v ^ {k}\\right)\\right) ^ {\\mathrm {T}} x _ {1} + \\frac {1}{2 \\eta_ {k}} \\| x _ {1} - x ^ {k} \\| _ {2} ^ {2} \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.686,
                0.825,
                0.724
            ],
            "angle": 0,
            "content": "这等价于求解子问题(8.6.23)时做一步近似点梯度步．当然，若 \\(f_{1}(x_{1})\\) 是可微函数与不可微函数的和时，也可将其可微部分线性化."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.751,
                0.362,
                0.767
            ],
            "angle": 0,
            "content": "2. 缓存分解"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.775,
                0.825,
                0.819
            ],
            "angle": 0,
            "content": "如果目标函数中含二次函数，例如 \\( f_{1}(x_{1}) = \\frac{1}{2} \\|Cx_{1} - d\\|_{2}^{2} \\)，那么针对 \\( x_{1} \\) 的更新(8.6.5)等价于求解线性方程组"
        },
        {
            "type": "equation",
            "bbox": [
                0.411,
                0.834,
                0.673,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\left(C ^ {\\mathrm {T}} C + \\rho A _ {1} ^ {\\mathrm {T}} A _ {1}\\right) x _ {1} = C ^ {\\mathrm {T}} d + \\rho A _ {1} ^ {\\mathrm {T}} v ^ {k},\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.119,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "441"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.739,
                0.278
            ],
            "angle": 0,
            "content": "其中 \\(v^{k}\\) 的定义如(8.6.24)式．虽然子问题有显式解，但是每步求解的复杂度仍然比较高，这时候可以考虑用缓存分解的方法．首先对 \\(C^{\\mathrm{T}}C + \\rho A_{1}^{\\mathrm{T}}A_{1}\\) 进行Cholesky分解并缓存分解的结果，在每步迭代中只需要求解简单的三角形方程组；当 \\(\\rho\\) 发生更新时，就要重新进行分解．特别地，当 \\(C^{\\mathrm{T}}C + \\rho A_{1}^{\\mathrm{T}}A_{1}\\) 一部分容易求逆，另一部分是低秩的情形时，可以用SMW公式(B.1.2)来求逆."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.303,
                0.274,
                0.319
            ],
            "angle": 0,
            "content": "3. 优化转移"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.334,
                0.738,
                0.372
            ],
            "angle": 0,
            "content": "有时候为了方便求解子问题，可以用一个性质好的矩阵 \\(D\\) 近似二次项\\(A_1^{\\mathrm{T}}A_{1}\\) ，此时子问题(8.6.23)替换为"
        },
        {
            "type": "equation",
            "bbox": [
                0.169,
                0.382,
                0.759,
                0.415
            ],
            "angle": 0,
            "content": "\\[\nx _ {1} ^ {k + 1} = \\arg \\min  _ {x _ {1}} \\left\\{f _ {1} \\left(x _ {1}\\right) + \\frac {\\rho}{2} \\| A _ {1} x _ {1} - v ^ {k} \\| _ {2} ^ {2} + \\frac {\\rho}{2} \\left(x _ {1} - x ^ {k}\\right) ^ {\\mathrm {T}} \\left(D - A _ {1} ^ {\\mathrm {T}} A _ {1}\\right) \\left(x _ {1} - x ^ {k}\\right) \\right\\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.428,
                0.741,
                0.522
            ],
            "angle": 0,
            "content": "其中 \\(v^{k}\\) 的定义如(8.6.24)式，这种方法也称为优化转移。通过选取合适的 \\(D\\)，当计算 \\(\\underset{x_{1}}{\\operatorname{argmin}}\\left\\{f_{1}(x_{1}) + \\frac{\\rho}{2} x_{1}^{\\mathrm{T}} D x_{1}\\right\\}\\) 明显比计算 \\(\\underset{x_{1}}{\\operatorname{argmin}}\\left\\{f_{1}(x_{1}) + \\frac{\\rho}{2} x_{1}^{\\mathrm{T}} A_{1}^{\\mathrm{T}} A_{1} x_{1}\\right\\}\\) 要容易时，优化转移可以极大地简化子问题的计算。特别地，当 \\(D = \\frac{\\eta_{k}}{\\rho} I\\) 时，优化转移等价于做单步的近似点梯度步。"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.548,
                0.394,
                0.564
            ],
            "angle": 0,
            "content": "4. 二次罚项系数的动态调节"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.578,
                0.74,
                0.761
            ],
            "angle": 0,
            "content": "动态调节二次罚项系数在交替方向乘子法的实际应用中是一个非常重要的数值技巧。在介绍ADMM时我们引入了原始可行性和对偶可行性（分别用 \\(\\| r^k\\|\\) 和 \\(\\| s^k\\|\\) 度量），见(8.6.11)式。在实际求解过程中，二次罚项系数 \\(\\rho\\) 太大会导致原始可行性 \\(\\| r^k\\|\\) 下降很快，但是对偶可行性 \\(\\| s^k\\|\\) 下降很慢；二次罚项系数太小，则会有相反的效果。这样都会导致收敛比较慢或得到的解的可行性很差。一个自然的想法是在每次迭代时动态调节惩罚系数 \\(\\rho\\) 的大小，从而使得原始可行性和对偶可行性能够以比较一致的速度下降到零。这种做法通常可以改善算法在实际中的收敛效果，以及使算法表现更少地依赖于惩罚系数的初始选择。一个简单有效的方式是令"
        },
        {
            "type": "equation",
            "bbox": [
                0.336,
                0.772,
                0.569,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\rho^ {k + 1} = \\left\\{ \\begin{array}{l l} \\gamma_ {p} \\rho^ {k}, & \\| r ^ {k} \\| > \\mu \\| s ^ {k} \\|, \\\\ \\frac {\\rho^ {k}}{\\gamma_ {d}} & \\| s ^ {k} \\| > \\mu \\| r ^ {k} \\|, \\\\ \\rho^ {k}, & \\text {其 他}, \\end{array} \\right.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "442"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.827,
                0.381
            ],
            "angle": 0,
            "content": "其中 \\(\\mu > 1, \\gamma_{p} > 1, \\gamma_{d} > 1\\) 是参数. 常见的选择为 \\(\\mu = 10, \\gamma_{p} = \\gamma_{d} = 2\\). 该惩罚参数更新方式背后的想法是在迭代过程中, 将原始可行性 \\(\\| r^{k} \\|\\) 和对偶可行性 \\(\\| s^{k} \\|\\) 保持在彼此的 \\(\\mu\\) 倍内. 如果发现 \\(\\| r^{k} \\|\\) 或 \\(\\| s^{k} \\|\\) 下降过慢就应该相应增大或减小二次罚项系数 \\(\\rho^{k}\\). 但在改变 \\(\\rho^{k}\\) 的时候需要注意, 若之前利用了缓存分解的技巧, 此时分解需要重新计算. 更一般地, 我们可以考虑对每一个约束给一个不同的惩罚系数, 甚至可以将增广拉格朗日函数(8.6.2)中的二次项 \\(\\frac{\\rho}{2} \\| r \\|^{2}\\) 替换为 \\(\\frac{\\rho}{2} r^{\\mathrm{T}} P r\\), 其中 \\(P\\) 是一个对称正定矩阵. 如果 \\(P\\) 在整个迭代过程中是不变的, 我们可以将这个一般的交替方向乘子法解释为将标准的交替方向乘子法应用在修改后的初始问题上——等式约束 \\(A_{1} x_{1} + A_{2} x_{2} - b = 0\\) 替换为 \\(F(A_{1} x_{1} + A_{2} x_{2} - b) = 0\\), 其中 \\(F\\) 为 \\(P\\) 的 Cholesky 因子, 即 \\(P = F^{\\mathrm{T}} F\\), 且 \\(F\\) 是对角元为正数的上三角矩阵."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.439,
                0.346,
                0.456
            ],
            "angle": 0,
            "content": "5. 超松弛"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.48,
                0.825,
                0.518
            ],
            "angle": 0,
            "content": "另外一种想法是用超松弛的技巧，在(8.6.6)式与(8.6.7)式中，\\(A_{1}x_{1}^{k + 1}\\) 可以被替换为"
        },
        {
            "type": "equation",
            "bbox": [
                0.422,
                0.538,
                0.662,
                0.557
            ],
            "angle": 0,
            "content": "\\[\n\\alpha_ {k} A _ {1} x _ {1} ^ {k + 1} - (1 - \\alpha_ {k}) \\left(A _ {2} x _ {2} ^ {k} - b\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.585,
                0.825,
                0.643
            ],
            "angle": 0,
            "content": "其中 \\(\\alpha_{k} \\in (0,2)\\) 是一个松弛参数。当 \\(\\alpha_{k} > 1\\) 时，这种技巧称为超松弛；当 \\(\\alpha_{k} < 1\\) 时，这种技巧称为欠松弛。实验表明 \\(\\alpha_{k} \\in [1.5,1.8]\\) 的超松弛可以提高收敛速度。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.702,
                0.498,
                0.718
            ],
            "angle": 0,
            "content": "6. 多块与非凸问题的ADMM"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.743,
                0.824,
                0.781
            ],
            "angle": 0,
            "content": "在引入问题(8.6.1)时，我们提到了有两块变量 \\(x_{1}, x_{2}\\)。这个问题不难推广到有多块变量的情形："
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.806,
                0.825,
                0.852
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  _ {x _ {1}, x _ {2}, \\dots , x _ {N}} f _ {1} \\left(x _ {1}\\right) + f _ {2} \\left(x _ {2}\\right) + \\dots + f _ {N} \\left(x _ {N}\\right), \\tag {8.6.25} \\\\ \\begin{array}{l} \\text {s . t .} \\quad A _ {1} x _ {1} + A _ {2} x _ {2} + \\dots + A _ {N} x _ {N} = b. \\end{array} \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "443"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.196
            ],
            "angle": 0,
            "content": "这里 \\(f_{i}(x_{i})\\) 是闭凸函数，\\(x_{i} \\in \\mathbb{R}^{n_{i}}, A_{i} \\in \\mathbb{R}^{m \\times n_{i}}\\)。同样可以写出问题(8.6.25)的增广拉格朗日函数 \\(L_{\\rho}(x_{1}, x_{2}, \\dots, x_{N}, y)\\)，相应的多块ADMM迭代格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.251,
                0.213,
                0.516,
                0.241
            ],
            "angle": 0,
            "content": "\\[\nx_{1}^{k + 1} = \\operatorname *{arg  min}_{x}L_{\\rho}(x,x_{2}^{k},\\dots ,x_{N}^{k},y^{k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.251,
                0.245,
                0.53,
                0.274
            ],
            "angle": 0,
            "content": "\\[\nx_{2}^{k + 1} = \\operatorname *{arg  min}_{x}L_{\\rho}(x_{1}^{k + 1},x,\\dots ,x_{N}^{k},y^{k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.264,
                0.283,
                0.354,
                0.293
            ],
            "angle": 0,
            "content": "\\[\n\\dots\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.253,
                0.302,
                0.542,
                0.33
            ],
            "angle": 0,
            "content": "\\[\nx _ {N} ^ {k + 1} = \\underset {x} {\\arg \\min } L _ {\\rho} (x _ {1} ^ {k + 1}, x _ {2} ^ {k + 1}, \\dots , x, y ^ {k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.253,
                0.335,
                0.655,
                0.356
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = y ^ {k} + \\tau \\rho (A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} + \\dots + A _ {N} x _ {N} ^ {k + 1} - b),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.372,
                0.462,
                0.406
            ],
            "angle": 0,
            "content": "其中 \\(\\tau \\in \\left(0, \\frac{1}{2} (\\sqrt{5} + 1)\\right)\\) 为步长参数."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.407,
                0.739,
                0.487
            ],
            "angle": 0,
            "content": "针对非凸问题，ADMM 格式可能不是良定义的，即每个子问题可能不存在最小值或最小值点不唯一。若只考虑子问题解存在的情形，我们依然可以形式上利用 ADMM 格式(8.6.5)-(8.6.7)对非凸问题进行求解。这里 arg min 应该理解为选取子问题最小值点中的一个。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.492,
                0.741,
                0.551
            ],
            "angle": 0,
            "content": "和有两块变量的凸问题上的ADMM格式相比，多块（非凸）ADMM可能不具有收敛性．但在找到有效算法之前，这两种ADMM算法的变形都值得一试．它们在某些实际问题上也有不错的效果."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.589,
                0.315,
                0.608
            ],
            "angle": 0,
            "content": "8.6.4 应用举例"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.625,
                0.741,
                0.727
            ],
            "angle": 0,
            "content": "本小节给出一些交替方向乘子法的应用实例。在实际中，大多数问题并不直接具有问题(8.6.1)的形式。我们需要通过一系列拆分技巧将问题化成ADMM的标准形式，同时要求每一个子问题尽量容易求解。需要指出的是，对同一个问题可能有多种拆分方式，不同方式导出的最终算法可能差异巨大，读者应当选择最容易求解的拆分方式。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.762,
                0.305,
                0.779
            ],
            "angle": 0,
            "content": "1. LASSO 问题"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.796,
                0.321,
                0.812
            ],
            "angle": 0,
            "content": "LASSO问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.345,
                0.826,
                0.564,
                0.859
            ],
            "angle": 0,
            "content": "\\[\n\\min \\quad \\mu \\| x \\| _ {1} + \\frac {1}{2} \\| A x - b \\| ^ {2}.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "444"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.193
            ],
            "angle": 0,
            "content": "这是典型的无约束复合优化问题，我们可以很容易地将其写成ADMM标准问题形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.47,
                0.196,
                0.613,
                0.219
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, z} \\quad f (x) + h (z),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.479,
                0.228,
                0.566,
                0.242
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & x = z, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.25,
                0.825,
                0.294
            ],
            "angle": 0,
            "content": "其中 \\(f(x) = \\frac{1}{2}\\| Ax - b\\|^2\\)，\\(h(z) = \\mu \\| z\\| _1\\)。对于此问题，交替方向乘子法迭代格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.301,
                0.737,
                0.36
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} x ^ {k + 1} = \\arg \\min  _ {x} \\left\\{\\frac {1}{2} \\| A x - b \\| ^ {2} + \\frac {\\rho}{2} \\| x - z ^ {k} + \\frac {1}{\\rho} y ^ {k} \\| _ {2} ^ {2} \\right\\}, \\\\ = \\left(A ^ {\\mathrm {T}} A + \\rho I\\right) ^ {- 1} \\left(A ^ {\\mathrm {T}} b + \\rho z ^ {k} - y ^ {k}\\right), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.349,
                0.365,
                0.706,
                0.436
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} z ^ {k + 1} = \\arg \\min  _ {z} \\left\\{\\mu \\| z \\| _ {1} + \\frac {\\rho}{2} \\| x ^ {k + 1} - z + \\frac {1}{\\rho} y ^ {k} \\| ^ {2} \\right\\}, \\\\ = \\operatorname {p r o x} _ {(\\mu / \\rho) \\| \\cdot \\| _ {1}} \\left(x ^ {k + 1} + \\frac {1}{\\rho} y ^ {k}\\right), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.441,
                0.557,
                0.46
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = y ^ {k} + \\tau \\rho \\left(x ^ {k + 1} - z ^ {k + 1}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.471,
                0.825,
                0.653
            ],
            "angle": 0,
            "content": "注意, 因为 \\(\\rho > 0\\), 所以 \\(A^{\\mathrm{T}} A + \\rho I\\) 总是可逆的. \\(x\\) 迭代本质上是计算一个岭回归问题 (\\(\\ell_{2}\\) 范数平方正则化的最小二乘问题); 而对 \\(z\\) 的更新为 \\(\\ell_{1}\\) 范数的邻近算子, 同样有显式解. 在求解 \\(x\\) 迭代时, 若使用固定的罚因子 \\(\\rho\\), 我们可以缓存矩阵 \\(A^{\\mathrm{T}} A + \\rho I\\) 的初始分解, 从而减小后续迭代中的计算量. 需要注意的是, 在 LASSO 问题中, 矩阵 \\(A \\in \\mathbb{R}^{m \\times n}\\) 通常有较多的列 (即 \\(m \\ll n\\)), 因此 \\(A^{\\mathrm{T}} A \\in \\mathbb{R}^{n \\times n}\\) 是一个低秩矩阵, 二次罚项的作用就是将 \\(A^{\\mathrm{T}} A\\) 增加了一个正定项. 该 ADMM 主要运算量来自更新 \\(x\\) 变量时求解线性方程组, 复杂度为 \\(\\mathcal{O}(n^{3})\\) (若使用缓存分解技术或 SMW 公式 (B.1.2) 则可进一步降低每次迭代的运算量)."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.658,
                0.569,
                0.674
            ],
            "angle": 0,
            "content": "接下来考虑 LASSO 问题的对偶问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.467,
                0.681,
                0.824,
                0.713
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad b ^ {\\mathrm {T}} y + \\frac {1}{2} \\| y \\| ^ {2}, \\tag {8.6.26}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.478,
                0.712,
                0.613,
                0.73
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\text {s . t .} \\quad \\| A ^ {\\mathrm {T}} y \\| _ {\\infty} \\leqslant \\mu . \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.739,
                0.825,
                0.777
            ],
            "angle": 0,
            "content": "将 \\(\\| A^{\\mathrm{T}}y\\|_{\\infty}\\leqslant \\mu\\) 变成示性函数放在目标函数中，并引入约束 \\(A^{\\mathrm{T}}y + z = 0\\) ，可以得到如下等价问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.421,
                0.784,
                0.824,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\underbrace {b ^ {\\mathrm {T}} y + \\frac {1}{2} \\| y \\| ^ {2}} _ {f (y)} + \\underbrace {I _ {\\| z \\| _ {\\infty} \\leqslant \\mu} (z)} _ {h (z)}, \\tag {8.6.27} \\\\ \\begin{array}{l l} \\text {s . t .} & A ^ {\\mathrm {T}} y + z = 0. \\end{array} \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "445"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.156,
                0.666,
                0.174
            ],
            "angle": 0,
            "content": "对约束 \\(A^{\\mathrm{T}}y + z = 0\\) 引入乘子 \\(x\\) ，对偶问题的增广拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.192,
                0.183,
                0.713,
                0.215
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\rho} (y, z, x) = b ^ {T} y + \\frac {1}{2} \\| y \\| ^ {2} + I _ {\\| z \\| _ {\\infty} \\leqslant \\mu} (z) - x ^ {T} \\left(A ^ {T} y + z\\right) + \\frac {\\rho}{2} \\| A ^ {T} y + z \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.223,
                0.739,
                0.321
            ],
            "angle": 0,
            "content": "在这里我们故意引入符号 \\(x\\) 作为拉格朗日乘子，实际上可以证明（见习题8.18）\\(x\\) 恰好对应的是原始问题的自变量。以下说明如何求解每个子问题。当固定 \\(y, x\\) 时，对 \\(z\\) 的更新即向无穷范数球 \\(\\{z \\| z \\|_{\\infty} \\leqslant \\mu\\}\\) 做欧几里得投影，即将每个分量截断在区间 \\([- \\mu, \\mu]\\) 中；当固定 \\(z, x\\) 时，对 \\(y\\) 的更新即求解线性方程组"
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.325,
                0.584,
                0.345
            ],
            "angle": 0,
            "content": "\\[\n(I + \\rho A A ^ {\\mathrm {T}}) y = A (x ^ {k} - \\rho z ^ {k + 1}) - b.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.354,
                0.398,
                0.371
            ],
            "angle": 0,
            "content": "因此得到ADMM迭代格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.293,
                0.379,
                0.613,
                0.469
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} z ^ {k + 1} = \\mathcal {P} _ {\\| z \\| _ {\\infty} \\leqslant \\mu} \\left(\\frac {x ^ {k}}{\\rho} - A ^ {\\mathrm {T}} y ^ {k}\\right), \\\\ y ^ {k + 1} = \\left(I + \\rho A A ^ {\\mathrm {T}}\\right) ^ {- 1} \\left(A \\left(x ^ {k} - \\rho z ^ {k + 1}\\right) - b\\right), \\\\ x ^ {k + 1} = x ^ {k} - \\tau \\rho (A ^ {\\mathrm {T}} y ^ {k + 1} + z ^ {k + 1}). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.48,
                0.738,
                0.558
            ],
            "angle": 0,
            "content": "注意，虽然ADMM应用于对偶问题也需要求解一个线性方程组，但由于LASSO问题的特殊性 \\((m\\ll n)\\)，求解 \\(y\\) 更新的线性方程组需要的计算量是\\(\\mathcal{O}(m^3)\\)，使用缓存分解技巧后可进一步降低至 \\(\\mathcal{O}(m^2)\\)，这大大小小针对原始问题的ADMM."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.563,
                0.502,
                0.579
            ],
            "angle": 0,
            "content": "对原始问题，另一种可能的拆分方法是"
        },
        {
            "type": "equation",
            "bbox": [
                0.37,
                0.587,
                0.537,
                0.657
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\underbrace {\\mu \\| x \\| _ {1}} _ {f (x)} + \\underbrace {\\frac {1}{2} \\| z \\| ^ {2}} _ {h (z)}, \\\\ \\begin{array}{l} \\text {s . t .} \\quad z = A x - b, \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.669,
                0.416,
                0.686
            ],
            "angle": 0,
            "content": "可以写出其增广拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.208,
                0.694,
                0.698,
                0.725
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\rho} (x, z, y) = \\mu \\| x \\| _ {1} + \\frac {1}{2} \\| z \\| ^ {2} + y ^ {\\mathrm {T}} (A x - b - z) + \\frac {\\rho}{2} \\| A x - b - z \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.733,
                0.738,
                0.791
            ],
            "angle": 0,
            "content": "但是如果对这种拆分方式使用ADMM，求解 \\(x\\) 的更新本质上还是在求解一个LASSO问题！这种变形方式将问题绕回了起点，因此并不是一个实用的方法."
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.795,
                0.738,
                0.854
            ],
            "angle": 0,
            "content": "我们用同第6.2节中一样的 \\(A\\) 和 \\(b\\), 并取 \\(\\mu = 10^{-3}\\), 分别使用 ADMM 求解原始问题和对偶问题, 这里取 \\(\\tau = 1.618\\), 原始问题和对偶问题的参数 \\(\\rho\\) 分别为 0.01 和 100 , 终止条件设为 \\(\\left|f\\left(x^{k}\\right) - f\\left(x^{k-1}\\right)\\right|< 10^{-8}\\) 和最大迭代步"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "446"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.156,
                0.827,
                0.277
            ],
            "angle": 0,
            "content": "数 2000. 此外, 对于原始问题的 ADMM, 我们还添加终止条件 \\(\\| x^{k} - z^{k}\\| < 10^{-10}\\); 相应地, 对于对偶问题, 额外的终止条件取为 \\(\\| A^{\\mathrm{T}}y^{k} + z^{k}\\| < 10^{-10}\\). 算法结果见图8.9. 这里的 ADMM 没有使用连续化策略来调整 \\(\\mu\\), 因此可以看出 ADMM 相对其他算法的强大之处. 此外, 对于这个例子, 求解原始问题需要的迭代步数较少, 但求解对偶问题每一次迭代所需要的时间更短, 综合来看 ADMM 求解对偶问题时更快."
        },
        {
            "type": "image",
            "bbox": [
                0.356,
                0.292,
                0.727,
                0.517
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.411,
                0.529,
                0.673,
                0.545
            ],
            "angle": 0,
            "content": "图8.9 ADMM求解LASSO问题"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.607,
                0.431,
                0.623
            ],
            "angle": 0,
            "content": "2. 广义 LASSO 问题"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.641,
                0.5,
                0.657
            ],
            "angle": 0,
            "content": "广义LASSO问题的定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.671,
                0.825,
                0.703
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad \\mu \\| F x \\| _ {1} + \\frac {1}{2} \\| A x - b \\| ^ {2}. \\tag {8.6.28}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.716,
                0.825,
                0.774
            ],
            "angle": 0,
            "content": "在 LASSO 问题中, 增加 \\(\\|x\\|_1\\) 项是要保证 \\(x\\) 的稀疏性, 而对许多问题, \\(x\\) 本身不稀疏, 但在某种变换下是稀疏的. 一个重要的例子是当 \\(F \\in \\mathbb{R}^{(n-1) \\times n}\\) 是一阶差分矩阵"
        },
        {
            "type": "equation",
            "bbox": [
                0.455,
                0.78,
                0.629,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nF _ {i j} = \\left\\{ \\begin{array}{l l} 1, & j = i + 1, \\\\ - 1, & j = i, \\\\ 0, & \\text {其 他}, \\end{array} \\right.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "447"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.429,
                0.174
            ],
            "angle": 0,
            "content": "且 \\(A = I\\) 时，广义LASSO问题为"
        },
        {
            "type": "equation",
            "bbox": [
                0.319,
                0.185,
                0.589,
                0.224
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\frac {1}{2} \\| x - b \\| ^ {2} + \\mu \\sum_ {i = 1} ^ {n - 1} | x _ {i + 1} - x _ {i} |,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.234,
                0.744,
                0.314
            ],
            "angle": 0,
            "content": "这个问题就是图像去噪问题的 TV 模型 [165]；当 \\(A = I\\) 且 \\(F\\) 是二阶差分矩阵时，问题(8.6.28)被称为一范数趋势滤波[112]. 下面介绍如何使用 ADMM 求解问题(8.6.28). 通过引入约束 \\(Fx = z\\) ，我们将问题(8.6.28)写为交替方向乘子法所对应问题的形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.322,
                0.737,
                0.359
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x, z} \\quad \\frac {1}{2} \\| A x - b \\| ^ {2} + \\mu \\| z \\| _ {1}, \\tag {8.6.29}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.358,
                0.361,
                0.486,
                0.377
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & F x - z = 0, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.39,
                0.449,
                0.407
            ],
            "angle": 0,
            "content": "引入乘子 \\(y\\) ，其增广拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.219,
                0.418,
                0.689,
                0.45
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\rho} (x, z, y) = \\frac {1}{2} \\| A x - b \\| ^ {2} + \\mu \\| z \\| _ {1} + y ^ {\\mathrm {T}} (F x - z) + \\frac {\\rho}{2} \\| F x - z \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.459,
                0.402,
                0.475
            ],
            "angle": 0,
            "content": "此问题的 \\(x\\) 迭代是求解方程组"
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.485,
                0.612,
                0.523
            ],
            "angle": 0,
            "content": "\\[\n\\left(A ^ {\\mathrm {T}} A + \\rho F ^ {\\mathrm {T}} F\\right) x = A ^ {\\mathrm {T}} b + \\rho F ^ {\\mathrm {T}} \\left(z ^ {k} - \\frac {y ^ {k}}{\\rho}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.532,
                0.741,
                0.569
            ],
            "angle": 0,
            "content": "而 \\(z\\) 迭代依然通过 \\(\\ell_1\\) 范数的邻近算子．因此交替方向乘子法所产生的迭代为"
        },
        {
            "type": "equation",
            "bbox": [
                0.264,
                0.578,
                0.644,
                0.616
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\left(A ^ {\\mathrm {T}} A + \\rho F ^ {\\mathrm {T}} F\\right) ^ {- 1} \\left(A ^ {\\mathrm {T}} b + \\rho F ^ {\\mathrm {T}} \\left(z ^ {k} - \\frac {y ^ {k}}{\\rho}\\right)\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.266,
                0.617,
                0.526,
                0.654
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = \\operatorname {p r o x} _ {(\\mu / \\rho) \\| \\cdot \\| _ {1}} \\left(F x ^ {k + 1} + \\frac {y ^ {k}}{\\rho}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.266,
                0.655,
                0.49,
                0.677
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = y ^ {k} + \\tau \\rho (F x ^ {k + 1} - z ^ {k + 1}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.692,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "在这个问题中，ADMM迭代的主要计算量仍在 \\(x\\) 更新上．由于图像去噪问题涉及的变量数量可能有上百万，这一步迭代就要涉及求解一个百万量级的线性方程组．在这种数据规模下不适合再使用矩阵分解的方式求解方程组，而应当注意系数矩阵的特殊结构．对于全变差去噪问题，\\(A^{\\mathrm{T}}A + \\rho F^{\\mathrm{T}}F\\) 是三对角矩阵，所以此时 \\(x\\) 迭代可以在 \\(\\mathcal{O}(n)\\) 的时间复杂度内解决[85]；对于图像去模糊问题，\\(A\\) 是卷积算子，则利用傅里叶变换可将求解方程组的复杂度降低至 \\(\\mathcal{O}(n\\log n)\\)；对于一范数趋势滤波问题，\\(A^{\\mathrm{T}}A + \\rho F^{\\mathrm{T}}F\\) 是五对角矩阵，所以 \\(x\\) 迭代仍可以在 \\(\\mathcal{O}(n)\\) 的时间复杂度内解决."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "448"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.157,
                0.43,
                0.174
            ],
            "angle": 0,
            "content": "3. 逆协方差矩阵估计"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.189,
                0.825,
                0.226
            ],
            "angle": 0,
            "content": "第三章介绍了概率图模型和逆协方差矩阵估计问题. 该问题的基本形式是"
        },
        {
            "type": "equation",
            "bbox": [
                0.415,
                0.245,
                0.825,
                0.269
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X} \\quad \\langle S, X \\rangle - \\ln \\det  X + \\mu \\| X \\| _ {1}, \\tag {8.6.30}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.28,
                0.825,
                0.339
            ],
            "angle": 0,
            "content": "其中 \\(S\\) 是已知的对称矩阵, 通常由样本协方差矩阵得到. 变量 \\(X \\in S_{++}^{n}, \\| \\cdot \\|_{1}\\) 定义为矩阵所有元素绝对值的和. 文献 [167] 说明在这个问题上, 交替方向乘子法具有非常好的表现. 接下来我们说明如何应用交替方向乘子法."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.344,
                0.825,
                0.381
            ],
            "angle": 0,
            "content": "目标函数由光滑项和非光滑项组成，因此引入约束 \\(X = Z\\) 将问题的两部分分离："
        },
        {
            "type": "equation",
            "bbox": [
                0.418,
                0.385,
                0.663,
                0.44
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\underbrace {\\langle S , X \\rangle - \\ln \\det  X} _ {f (X)} + \\underbrace {\\mu \\| Z \\| _ {1}} _ {h (Z)}, \\\\ \\begin{array}{l l} \\text {s . t .} & X = Z. \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.454,
                0.75,
                0.47
            ],
            "angle": 0,
            "content": "引入乘子 \\(U\\) 作用在约束 \\(X - Z = 0\\) 上，可得增广拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.283,
                0.484,
                0.8,
                0.514
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\rho} (X, Z, U) = \\langle S, X \\rangle - \\ln \\det  X + \\mu \\| Z \\| _ {1} + \\langle U, X - Z \\rangle + \\frac {\\rho}{2} \\| X - Z \\| _ {F} ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.524,
                0.825,
                0.583
            ],
            "angle": 0,
            "content": "这里注意, 针对矩阵情形我们应当使用 \\(F\\) 范数替换 \\(\\ell_{2}\\) 范数作为增广拉格朗日函数法的罚项. 接下来就是分别写出 ADMM 子问题的显式解. 首先, 固定 \\(Z^{k}, U^{k}\\), 则 \\(X\\) 子问题是凸光滑问题, 对 \\(X\\) 求矩阵导数并令其为零,"
        },
        {
            "type": "equation",
            "bbox": [
                0.421,
                0.601,
                0.662,
                0.619
            ],
            "angle": 0,
            "content": "\\[\nS - X ^ {- 1} + U ^ {k} + \\rho (X - Z ^ {k}) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.637,
                0.825,
                0.674
            ],
            "angle": 0,
            "content": "这是一个关于 \\(X\\) 的矩阵方程，可以求出满足上述矩阵方程的唯一正定的 \\(X\\) 为"
        },
        {
            "type": "equation",
            "bbox": [
                0.42,
                0.68,
                0.664,
                0.7
            ],
            "angle": 0,
            "content": "\\[\nX ^ {k + 1} = Q \\operatorname {D i a g} (x _ {1}, x _ {2}, \\dots , x _ {n}) Q ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.712,
                0.738,
                0.73
            ],
            "angle": 0,
            "content": "其中 \\(Q\\) 包含矩阵 \\(S - \\rho Z^{k} + U^{k}\\) 的所有特征向量，\\(x_{i}\\) 的表达式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.456,
                0.743,
                0.627,
                0.78
            ],
            "angle": 0,
            "content": "\\[\nx _ {i} = \\frac {- d _ {i} + \\sqrt {d _ {i} ^ {2} + 4 \\rho}}{2 \\rho},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.794,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "\\(d_{i}\\) 为矩阵 \\(S - \\rho Z^{k} + U^{k}\\) 的第 \\(i\\) 个特征值．其次，固定 \\(X^{k + 1},U^k\\) ，则 \\(Z\\) 的更新为矩阵 \\(\\ell_1\\) 范数的邻近算子．最后是常规的乘子更新．读者可在习题8.4中推导相关结论."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "449"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.157,
                0.31,
                0.174
            ],
            "angle": 0,
            "content": "4. 矩阵分离问题"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.189,
                0.405,
                0.205
            ],
            "angle": 0,
            "content": "考虑矩阵分离问题(3.8.2)："
        },
        {
            "type": "equation",
            "bbox": [
                0.371,
                0.22,
                0.737,
                0.251
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X, S} \\| X \\| _ {*} + \\mu \\| S \\| _ {1}, \\tag {8.6.31}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.382,
                0.252,
                0.513,
                0.267
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & X + S = M, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.283,
                0.737,
                0.321
            ],
            "angle": 0,
            "content": "其中 \\(\\| \\cdot \\| _1\\) 与 \\(\\| \\cdot \\|_*\\) 分别表示矩阵 \\(\\ell_1\\) 范数与核范数. 引入乘子 \\(Y\\) 作用在约束 \\(X + S = M\\) 上，我们可以得到此问题的增广拉格朗日函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.205,
                0.334,
                0.737,
                0.381
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\rho} (X, S, Y) = \\| X \\| _ {*} + \\mu \\| S \\| _ {1} + \\langle Y, X + S - M \\rangle + \\frac {\\rho}{2} \\| X + S - M \\| _ {F} ^ {2}. \\tag {8.6.32}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.396,
                0.737,
                0.434
            ],
            "angle": 0,
            "content": "在第 \\((k + 1)\\) 步，交替方向乘子法分别求解关于 \\(X\\) 和 \\(S\\) 的子问题来更新得到\\(X^{k + 1}\\) 和 \\(S^{k + 1}\\) ，对于 \\(X\\) 子问题，"
        },
        {
            "type": "equation",
            "bbox": [
                0.251,
                0.448,
                0.655,
                0.597
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} X ^ {k + 1} = \\underset {X} {\\arg \\min } L _ {\\rho} (X, S ^ {k}, Y ^ {k}) \\\\ = \\arg \\min  _ {X} \\left\\{\\| X \\| _ {*} + \\frac {\\rho}{2} \\left\\| X + S ^ {k} - M + \\frac {Y ^ {k}}{\\rho} \\right\\| _ {F} ^ {2} \\right\\}, \\\\ = \\arg \\min  _ {X} \\left\\{\\frac {1}{\\rho} \\| X \\| _ {*} + \\frac {1}{2} \\left\\| X + S ^ {k} - M + \\frac {Y ^ {k}}{\\rho} \\right\\| _ {F} ^ {2} \\right\\}, \\\\ = U \\operatorname {D i a g} \\left(\\operatorname {p r o x} _ {(1 / \\rho) \\| \\cdot \\| _ {1}} (\\sigma (A))\\right) V ^ {\\mathrm {T}}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.61,
                0.737,
                0.663
            ],
            "angle": 0,
            "content": "其中 \\(A = M - S^{k} - \\frac{Y^{k}}{\\rho}\\) ， \\(\\sigma (A)\\) 为 \\(A\\) 的所有非零奇异值构成的向量并且\\(U\\mathrm{Diag}(\\sigma (A))V^{\\mathrm{T}}\\) 为 \\(A\\) 的约化奇异值分解．对于 \\(S\\) 子问题，"
        },
        {
            "type": "equation",
            "bbox": [
                0.251,
                0.677,
                0.653,
                0.79
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} S ^ {k + 1} = \\underset {S} {\\arg \\min } L _ {\\rho} (X ^ {k + 1}, S, Y ^ {k}) \\\\ = \\underset {S} {\\arg \\min } \\left\\{\\mu \\| S \\| _ {1} + \\frac {\\rho}{2} \\left\\| X ^ {k + 1} + S - M + \\frac {Y ^ {k}}{\\rho} \\right\\| _ {F} ^ {2} \\right\\} \\\\ = \\operatorname {p r o x} _ {(\\mu / \\rho) \\| \\cdot \\| _ {1}} \\left(M - X ^ {k + 1} - \\frac {Y ^ {k}}{\\rho}\\right). \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.802,
                0.45,
                0.818
            ],
            "angle": 0,
            "content": "对于乘子 \\(Y\\) ，依然使用常规更新，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.835,
                0.585,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nY ^ {k + 1} = Y ^ {k} + \\tau \\rho (X ^ {k + 1} + S ^ {k + 1} - M).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "450"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.538,
                0.174
            ],
            "angle": 0,
            "content": "那么，交替方向乘子法的迭代格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.396,
                0.186,
                0.691,
                0.211
            ],
            "angle": 0,
            "content": "\\[\nX ^ {k + 1} = U \\operatorname {D i a g} \\left(\\operatorname {p r o x} _ {(1 / \\rho) \\| \\cdot \\| _ {1}} (\\sigma (A))\\right) V ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.399,
                0.215,
                0.687,
                0.25
            ],
            "angle": 0,
            "content": "\\[\nS ^ {k + 1} = \\operatorname {p r o x} _ {(\\mu / \\rho) \\| \\cdot \\| _ {1}} \\left(M - L ^ {k + 1} - \\frac {Y ^ {k}}{\\rho}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.255,
                0.655,
                0.273
            ],
            "angle": 0,
            "content": "\\[\nY ^ {k + 1} = Y ^ {k} + \\tau \\rho (X ^ {k + 1} + S ^ {k + 1} - M).\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.296,
                0.448,
                0.313
            ],
            "angle": 0,
            "content": "5. 全局一致性优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.326,
                0.591,
                0.342
            ],
            "angle": 0,
            "content": "第8.6.1小节介绍了全局一致性优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.484,
                0.352,
                0.598,
                0.39
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad \\sum_ {i = 1} ^ {N} \\phi_ {i} (x)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.4,
                0.436,
                0.417
            ],
            "angle": 0,
            "content": "并给出了一个拆分方式"
        },
        {
            "type": "equation",
            "bbox": [
                0.424,
                0.428,
                0.543,
                0.464
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x _ {i}, z} \\sum_ {i = 1} ^ {N} \\phi_ {i} (x _ {i}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.432,
                0.473,
                0.662,
                0.487
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & x _ {i} - z = 0, i = 1, 2, \\dots , N, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.5,
                0.436,
                0.517
            ],
            "angle": 0,
            "content": "其增广拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.258,
                0.525,
                0.825,
                0.565
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\rho} \\left(x _ {1}, x _ {2}, \\dots , x _ {N}, z, y _ {1}, y _ {2}, \\dots , y _ {N}\\right) = \\sum_ {i = 1} ^ {N} \\phi_ {i} \\left(x _ {i}\\right) + \\sum_ {i = 1} ^ {N} y _ {i} ^ {\\mathrm {T}} \\left(x _ {i} - z\\right) + \\frac {\\rho}{2} \\sum_ {i = 1} ^ {N} \\| x _ {i} - z \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.575,
                0.48,
                0.593
            ],
            "angle": 0,
            "content": "固定 \\(z^k, y_i^k\\)，更新 \\(x_i\\) 的公式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.373,
                0.603,
                0.825,
                0.647
            ],
            "angle": 0,
            "content": "\\[\nx _ {i} ^ {k + 1} = \\underset {x} {\\arg \\min } \\left\\{\\phi_ {i} (x) + \\frac {\\rho}{2} \\left\\| x - z ^ {k} + \\frac {y _ {i} ^ {k}}{\\rho} \\right\\| ^ {2} \\right\\}. \\tag {8.6.33}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.654,
                0.825,
                0.734
            ],
            "angle": 0,
            "content": "在这里注意, 虽然表面上看增广拉格朗日函数有 \\((N + 1)\\) 个变量块, 但本质上还是两个变量块. 这是因为在更新某 \\(x_{i}\\) 时并没有利用其他 \\(x_{i}\\) 的信息, 所有 \\(x_{i}\\) 可以看成一个整体. 相应地, 所有乘子 \\(y_{i}\\) 也可以看成一个整体. 迭代式(8.6.33)的具体计算依赖于 \\(\\phi_{i}\\) 的形式, 在一般情况下更新 \\(x_{i}\\) 的表达式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.438,
                0.743,
                0.644,
                0.78
            ],
            "angle": 0,
            "content": "\\[\nx _ {i} ^ {k + 1} = \\operatorname {p r o x} _ {\\phi_ {i} / \\rho} \\left(z ^ {k} - \\frac {y _ {i} ^ {k}}{\\rho}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.789,
                0.757,
                0.807
            ],
            "angle": 0,
            "content": "固定 \\(x_{i}^{k + 1},y_{i}^{k}\\) ，问题关于 \\(z\\) 是二次函数，因此可以直接写出显式解："
        },
        {
            "type": "equation",
            "bbox": [
                0.438,
                0.817,
                0.644,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\left(x _ {i} ^ {k + 1} + \\frac {y _ {i} ^ {k}}{\\rho}\\right).\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "451"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.157,
                0.536,
                0.174
            ],
            "angle": 0,
            "content": "综上，该问题的交替方向乘子法迭代格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.182,
                0.607,
                0.218
            ],
            "angle": 0,
            "content": "\\[\nx _ {i} ^ {k + 1} = \\operatorname {p r o x} _ {\\phi_ {i} / \\rho} \\left(z ^ {k} - \\frac {y _ {i} ^ {k}}{\\rho}\\right), i = 1, 2, \\dots , N,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.289,
                0.22,
                0.49,
                0.259
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\left(x _ {i} ^ {k + 1} + \\frac {y _ {i} ^ {k}}{\\rho}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.262,
                0.621,
                0.283
            ],
            "angle": 0,
            "content": "\\[\ny _ {i} ^ {k + 1} = y _ {i} ^ {k} + \\tau \\rho \\left(x _ {i} ^ {k + 1} - z ^ {k + 1}\\right), i = 1, 2, \\dots , N.\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.305,
                0.378,
                0.323
            ],
            "angle": 0,
            "content": "6. 非凸集合上的优化问题"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.335,
                0.467,
                0.352
            ],
            "angle": 0,
            "content": "非凸集合上的优化问题可以表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.405,
                0.363,
                0.737,
                0.388
            ],
            "angle": 0,
            "content": "\\[\n\\min  f (x), \\tag {8.6.34}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.416,
                0.385,
                0.502,
                0.4
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & x \\in S, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.412,
                0.737,
                0.45
            ],
            "angle": 0,
            "content": "其中 \\(f\\) 是闭凸函数，但 \\(S\\) 是非凸集合．利用例8.21的技巧，对集合 \\(S\\) 引入示性函数 \\(I_S(z)\\) 并做拆分，可得到问题(8.6.34)的等价优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.379,
                0.462,
                0.737,
                0.487
            ],
            "angle": 0,
            "content": "\\[\n\\min  f (x) + I _ {S} (z), \\tag {8.6.35}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.484,
                0.505,
                0.498
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l l} \\text {s . t .} & x - z = 0, \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.51,
                0.347,
                0.526
            ],
            "angle": 0,
            "content": "其增广拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.262,
                0.536,
                0.644,
                0.565
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\rho} (x, z, y) = f (x) + I _ {S} (z) + y ^ {T} (x - z) + \\frac {\\rho}{2} \\| x - z \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.572,
                0.646,
                0.589
            ],
            "angle": 0,
            "content": "由于 \\(f\\) 是闭凸函数，固定 \\(z, y\\) 后对 \\(x\\) 求极小就是计算邻近算子："
        },
        {
            "type": "equation",
            "bbox": [
                0.352,
                0.597,
                0.554,
                0.634
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = \\operatorname {p r o x} _ {f / \\rho} \\left(z ^ {k} - \\frac {y ^ {k}}{\\rho}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.642,
                0.606,
                0.659
            ],
            "angle": 0,
            "content": "固定 \\(x, y\\)，对 \\(z\\) 求极小实际上是到非凸集合上的投影问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.238,
                0.667,
                0.667,
                0.707
            ],
            "angle": 0,
            "content": "\\[\nz ^ {k + 1} = \\underset {z \\in S} {\\arg \\min } \\frac {1}{2} \\left\\| z - \\left(x ^ {k + 1} + \\frac {y ^ {k}}{\\rho}\\right) \\right\\| ^ {2} = \\mathcal {P} _ {S} \\left(x ^ {k + 1} + \\frac {y ^ {k}}{\\rho}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.715,
                0.737,
                0.753
            ],
            "angle": 0,
            "content": "一般来说，由于 \\(S\\) 是非凸集合，计算 \\(\\mathcal{P}_S\\) 是比较困难的（例如不能保证存在性和唯一性），但是当 \\(S\\) 有特定结构时，到 \\(S\\) 上的投影可以精确求解。"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.764,
                0.738,
                0.823
            ],
            "angle": 0,
            "content": "(1) 基数: 如果 \\(S = \\{x \\mid \\| x \\|_0 \\leqslant c\\}\\), 其中 \\(\\| \\cdot \\|_0\\) 表示 \\(\\ell_0\\) 范数, 即非零元素的数目, 那么计算任意向量 \\(v\\) 到 \\(S\\) 中的投影就是保留 \\(v\\) 分量中绝对值从大到小排列的前 \\(c\\) 个, 其余分量变成 0. 假设 \\(v\\) 的各个分量满足"
        },
        {
            "type": "equation",
            "bbox": [
                0.383,
                0.836,
                0.565,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\left| v _ {i _ {1}} \\right| \\geqslant \\left| v _ {i _ {2}} \\right| \\geqslant \\dots \\geqslant \\left| v _ {i _ {n}} \\right|,\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "452"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.3,
                0.157,
                0.442,
                0.174
            ],
            "angle": 0,
            "content": "则投影算子可写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.43,
                0.183,
                0.695,
                0.233
            ],
            "angle": 0,
            "content": "\\[\n(\\mathcal {P} _ {S} (v)) _ {i} = \\left\\{ \\begin{array}{l l} v _ {i}, & i \\in \\{i _ {1}, i _ {2}, \\dots , i _ {c} \\}, \\\\ 0, & \\text {其 他}. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.247,
                0.825,
                0.325
            ],
            "angle": 0,
            "content": "(2) 低秩投影: 当变量 \\(x \\in \\mathbb{R}^{m \\times n}\\) 是矩阵时, 一种常见的非凸约束是在低秩矩阵空间中进行优化问题求解, 即 \\(S = \\{x \\mid \\operatorname{rank}(x) \\leqslant r\\}\\). 此时到低秩矩阵空间上的投影等价于对 \\(x\\) 做截断奇异值分解. 设 \\(x\\) 的奇异值分解为"
        },
        {
            "type": "equation",
            "bbox": [
                0.492,
                0.326,
                0.632,
                0.365
            ],
            "angle": 0,
            "content": "\\[\nx = \\sum_ {i = 1} ^ {\\min  \\{m, n \\}} \\sigma_ {i} u _ {i} v _ {i} ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.369,
                0.825,
                0.406
            ],
            "angle": 0,
            "content": "其中 \\(\\sigma_{1} \\geqslant \\dots \\geqslant \\sigma_{\\min \\{m, n\\}} \\geqslant 0\\) 为奇异值，\\(u_{i}, v_{i}\\) 为对应的左右奇异向量，则 \\(\\mathcal{P}_{S}\\) 的表达式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.492,
                0.406,
                0.632,
                0.441
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {P} _ {S} (x) = \\sum_ {i = 1} ^ {r} \\sigma_ {i} u _ {i} v _ {i} ^ {\\mathrm {T}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.451,
                0.825,
                0.509
            ],
            "angle": 0,
            "content": "(3) 布尔 (Bool) 约束: 如果限制变量 \\(x\\) 只能在 \\(\\{0,1\\}\\) 中取值, 即 \\(S = \\{0,1\\}^n\\), 容易验证 \\(\\mathcal{P}_S(v)\\) 就是简单地把 \\(v\\) 的每个分量 \\(v_i\\) 变为 0 和 1 中离它更近的数, 一种可能的实现方式是分别对它们做四舍五入操作."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.533,
                0.448,
                0.549
            ],
            "angle": 0,
            "content": "7. 非负矩阵分解和补全"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.563,
                0.825,
                0.621
            ],
            "angle": 0,
            "content": "非负矩阵分解和补全是一个非常重要的统计学习方法，它可以看作非负矩阵分解问题和低秩矩阵补全问题的结合，即已知一个非负矩阵的部分元素，求其非负分解."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.625,
                0.825,
                0.704
            ],
            "angle": 0,
            "content": "假设我们有从一个非负、秩为 \\(r\\) 的矩阵 \\(M \\in \\mathbb{R}^{m \\times n}\\) 中采样的部分元素 \\(M_{i,j}, (i,j) \\in \\Omega \\subset \\{1,2,\\dots,m\\} \\times \\{1,2,\\dots,n\\}\\)，目标是要找到非负矩阵 \\(X \\in \\mathbb{R}^{m \\times q}, Y \\in \\mathbb{R}^{q \\times n}\\) 使得 \\(\\| M - XY\\|_F^2\\) 极小。一般地，因为数据和应用问题的不同，\\(q\\) 可以等于、小于或者大于 \\(r\\)。定义矩阵 \\(P \\in \\mathbb{R}^{m \\times n}\\)："
        },
        {
            "type": "equation",
            "bbox": [
                0.459,
                0.713,
                0.623,
                0.763
            ],
            "angle": 0,
            "content": "\\[\nP _ {i j} = \\left\\{ \\begin{array}{l l} 1, & (i, j) \\in \\Omega , \\\\ 0, & \\text {其 他}, \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.771,
                0.614,
                0.788
            ],
            "angle": 0,
            "content": "则非负矩阵分解和补全问题可以写成如下形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.443,
                0.798,
                0.638,
                0.822
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {X, Y} \\| P \\odot (X Y - M) \\| _ {F} ^ {2},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.453,
                0.83,
                0.639,
                0.848
            ],
            "angle": 0,
            "content": "\\[\ns. t. \\quad X _ {i j} \\geqslant 0, Y _ {i j} \\geqslant 0, \\forall i, j.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "453"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.368,
                0.174
            ],
            "angle": 0,
            "content": "注意，这个问题是非凸的。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.188,
                0.646,
                0.205
            ],
            "angle": 0,
            "content": "为了利用交替方向乘子法的优势，我们考虑如下等价形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.348,
                0.235,
                0.528,
                0.266
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {U, V, X, Y, Z} \\frac {1}{2} \\| X Y - Z \\| _ {F} ^ {2},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.383,
                0.274,
                0.533,
                0.289
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\text {s . t .} \\quad X = U, Y = V, \\end{array}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.422,
                0.298,
                0.523,
                0.313
            ],
            "angle": 0,
            "content": "\\[\nU \\geqslant 0, V \\geqslant 0,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.422,
                0.323,
                0.558,
                0.34
            ],
            "angle": 0,
            "content": "\\[\nP \\odot (Z - M) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.372,
                0.739,
                0.431
            ],
            "angle": 0,
            "content": "对于等式约束 \\(X = U\\) 和 \\(Y = V\\) 分别引入拉格朗日乘子 \\(\\Lambda\\) 和 \\(\\Pi\\)，并将非负约束 \\(U \\geqslant 0, V \\geqslant 0\\) 和观测值约束 \\(P \\odot (Z - M) = 0\\) 放到约束中，则增广拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.295,
                0.467,
                0.737,
                0.552
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} L _ {\\alpha , \\beta} (X, Y, Z, U, V, \\Lambda , \\Pi) \\\\ = \\frac {1}{2} \\| X Y - Z \\| _ {F} ^ {2} + \\langle \\Lambda , X - U \\rangle + \\langle \\Pi , Y - V \\rangle \\\\ + \\frac {\\alpha}{2} \\| X - U \\| _ {F} ^ {2} + \\frac {\\beta}{2} \\| Y - V \\| _ {F} ^ {2}. \\tag {8.6.36} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.582,
                0.382,
                0.599
            ],
            "angle": 0,
            "content": "应用交替方向乘子法，可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.255,
                0.634,
                0.593,
                0.661
            ],
            "angle": 0,
            "content": "\\[\nX ^ {k + 1} = \\underset {X} {\\arg \\min } L _ {\\alpha , \\beta} (X, Y ^ {k}, Z ^ {k}, U ^ {k}, V ^ {k}, \\Lambda^ {k}, \\Pi^ {k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.256,
                0.667,
                0.606,
                0.694
            ],
            "angle": 0,
            "content": "\\[\nY ^ {k + 1} = \\underset {Y} {\\arg \\min } L _ {\\alpha , \\beta} (X ^ {k + 1}, Y, Z ^ {k}, U ^ {k}, V ^ {k}, \\Lambda^ {k}, \\Pi^ {k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.256,
                0.7,
                0.636,
                0.73
            ],
            "angle": 0,
            "content": "\\[\nZ^{k + 1} = \\operatorname *{arg  min}_{P\\odot (Z - M) = 0}L_{\\alpha ,\\beta}(X^{k + 1},Y^{k + 1},Z,U^{k},V^{k},\\Lambda^{k},\\Pi^{k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.256,
                0.736,
                0.64,
                0.763
            ],
            "angle": 0,
            "content": "\\[\nU^{k + 1} = \\operatorname *{arg  min}_{U\\geqslant 0}L_{\\alpha ,\\beta}(X^{k + 1},Y^{k + 1},Z^{k + 1},U,V^{k},\\Lambda^{k},\\Pi^{k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.256,
                0.77,
                0.653,
                0.798
            ],
            "angle": 0,
            "content": "\\[\nV^{k + 1} = \\operatorname *{arg  min}_{V\\geqslant 0}L_{\\alpha ,\\beta}(X^{k + 1},Y^{k + 1},Z^{k + 1},U^{k + 1},V,\\Lambda^{k},\\Pi^{k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.256,
                0.805,
                0.485,
                0.822
            ],
            "angle": 0,
            "content": "\\[\n\\Lambda^ {k + 1} = \\Lambda^ {k} + \\tau \\alpha (X ^ {k + 1} - U ^ {k + 1}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.256,
                0.829,
                0.487,
                0.847
            ],
            "angle": 0,
            "content": "\\[\n\\varPi^ {k + 1} = \\varPi^ {k} + \\tau \\beta (Y ^ {k + 1} - V ^ {k + 1}).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "454"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.157,
                0.453,
                0.174
            ],
            "angle": 0,
            "content": "将子问题求解，可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.334,
                0.189,
                0.697,
                0.211
            ],
            "angle": 0,
            "content": "\\[\nX ^ {k + 1} = \\left(Z ^ {k} \\left(Y ^ {k}\\right) ^ {\\mathrm {T}} + \\alpha U ^ {k} - \\Lambda^ {k}\\right) \\left(Y ^ {k} \\left(Y ^ {k}\\right) ^ {\\mathrm {T}} + \\alpha I\\right) ^ {- 1},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.337,
                0.214,
                0.748,
                0.235
            ],
            "angle": 0,
            "content": "\\[\nY ^ {k + 1} = ((X ^ {k + 1}) ^ {\\mathrm {T}} X ^ {k + 1} + \\beta I) ^ {- 1} ((X ^ {k + 1}) ^ {\\mathrm {T}} Z ^ {k} + \\beta V ^ {k} - \\Pi^ {k}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.337,
                0.238,
                0.644,
                0.259
            ],
            "angle": 0,
            "content": "\\[\nZ ^ {k + 1} = X ^ {k + 1} Y ^ {k + 1} + P \\odot (M - X ^ {k + 1} Y ^ {k + 1}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.337,
                0.262,
                0.535,
                0.299
            ],
            "angle": 0,
            "content": "\\[\nU ^ {k + 1} = \\mathcal {P} _ {+} \\left(X ^ {k + 1} + \\frac {\\Lambda^ {k}}{\\alpha}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.301,
                0.536,
                0.337
            ],
            "angle": 0,
            "content": "\\[\nV ^ {k + 1} = \\mathcal {P} _ {+} \\left(Y ^ {k + 1} + \\frac {\\Pi^ {k}}{\\beta}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.337,
                0.34,
                0.569,
                0.36
            ],
            "angle": 0,
            "content": "\\[\n\\Lambda^ {k + 1} = \\Lambda^ {k} + \\tau \\alpha (X ^ {k + 1} - U ^ {k + 1}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.364,
                0.572,
                0.385
            ],
            "angle": 0,
            "content": "\\[\n\\Pi^ {k + 1} = \\Pi^ {k} + \\tau \\beta (Y ^ {k + 1} - V ^ {k + 1}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.401,
                0.825,
                0.44
            ],
            "angle": 0,
            "content": "其中 \\((\\mathcal{P}_{+}(A))_{ij} = \\max \\{a_{ij},0\\}\\) ．注意，该格式为多块交替方向乘子法，其收敛性可能需要较强假设."
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.468,
                0.501,
                0.485
            ],
            "angle": 0,
            "content": "8. 多块交替方向乘子法的反例"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.499,
                0.825,
                0.537
            ],
            "angle": 0,
            "content": "这里给出一个多块交替方向乘子法的例子，并且从数值上说明若直接采用格式(8.6.25)，则算法未必收敛。"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.542,
                0.42,
                0.558
            ],
            "angle": 0,
            "content": "考虑最优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.422,
                0.575,
                0.824,
                0.611
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\begin{array}{c c} \\min  & 0, \\end{array} \\\\ \\begin{array}{l} \\text {m i n} \\quad 0, \\\\ \\text {s . t .} \\quad A _ {1} x _ {1} + A _ {2} x _ {2} + A _ {3} x _ {3} = 0, \\end{array} \\tag {8.6.37} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.627,
                0.825,
                0.707
            ],
            "angle": 0,
            "content": "其中 \\(A_{i} \\in \\mathbb{R}^{3}, i = 1,2,3\\) 为三维空间中的非零向量，\\(x_{i} \\in \\mathbb{R}, i = 1,2,3\\) 是自变量。问题(8.6.37)实际上就是求解三维空间中的线性方程组，若 \\(A_{1}, A_{2}, A_{3}\\) 之间线性无关，则问题(8.6.37)只有零解。此时容易计算出最优解对应的乘子为 \\(y = (0,0,0)^{\\mathrm{T}}\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.711,
                0.825,
                0.749
            ],
            "angle": 0,
            "content": "现在推导多块交替方向乘子法的格式．问题(8.6.37)的增广拉格朗日函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.287,
                0.762,
                0.797,
                0.79
            ],
            "angle": 0,
            "content": "\\[\nL _ {\\rho} (x, y) = 0 + y ^ {\\mathrm {T}} \\left(A _ {1} x _ {1} + A _ {2} x _ {2} + A _ {3} x _ {3}\\right) + \\frac {\\rho}{2} \\| A _ {1} x _ {1} + A _ {2} x _ {2} + A _ {3} x _ {3} \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.802,
                0.557,
                0.818
            ],
            "angle": 0,
            "content": "当固定 \\(x_{2}, x_{3}, y\\) 时，对 \\(x_{1}\\) 求最小可推出"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.835,
                0.686,
                0.854
            ],
            "angle": 0,
            "content": "\\[\nA _ {1} ^ {\\mathrm {T}} y + \\rho A _ {1} ^ {\\mathrm {T}} \\left(A _ {1} x _ {1} + A _ {2} x _ {2} + A _ {3} x _ {3}\\right) = 0,\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "455"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.245,
                0.173
            ],
            "angle": 0,
            "content": "整理可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.3,
                0.171,
                0.607,
                0.207
            ],
            "angle": 0,
            "content": "\\[\nx _ {1} = - \\frac {1}{\\| A _ {1} \\| ^ {2}} \\left(A _ {1} ^ {\\mathrm {T}} \\left(\\frac {y}{\\rho} + A _ {2} x _ {2} + A _ {3} x _ {3}\\right)\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.21,
                0.736,
                0.246
            ],
            "angle": 0,
            "content": "可类似地计算 \\(x_{2}, x_{3}\\) 的表达式，因此多块交替方向乘子法的迭代格式可以写为"
        },
        {
            "type": "equation",
            "bbox": [
                0.281,
                0.244,
                0.594,
                0.282
            ],
            "angle": 0,
            "content": "\\[\nx _ {1} ^ {k + 1} = - \\frac {1}{\\| A _ {1} \\| ^ {2}} A _ {1} ^ {\\mathrm {T}} \\left(\\frac {y ^ {k}}{\\rho} + A _ {2} x _ {2} ^ {k} + A _ {3} x _ {3} ^ {k}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.284,
                0.283,
                0.737,
                0.322
            ],
            "angle": 0,
            "content": "\\[\nx _ {2} ^ {k + 1} = - \\frac {1}{\\| A _ {2} \\| ^ {2}} A _ {2} ^ {\\mathrm {T}} \\left(\\frac {y ^ {k}}{\\rho} + A _ {1} x _ {1} ^ {k + 1} + A _ {3} x _ {3} ^ {k}\\right), \\tag {8.6.38}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.284,
                0.322,
                0.624,
                0.358
            ],
            "angle": 0,
            "content": "\\[\nx _ {3} ^ {k + 1} = - \\frac {1}{\\| A _ {3} \\| ^ {2}} A _ {3} ^ {\\mathrm {T}} \\left(\\frac {y ^ {k}}{\\rho} + A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1}\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.284,
                0.36,
                0.601,
                0.381
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = y ^ {k} + \\rho \\left(A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} + A _ {3} x _ {3} ^ {k + 1}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.387,
                0.737,
                0.446
            ],
            "angle": 0,
            "content": "对此问题而言，罚因子取不同值仅仅是将乘子 \\( y^{k} \\) 缩放了常数倍，所以罚因子 \\( \\rho \\) 的任意取法（包括动态调节）都是等价的。在数值实验中我们不妨取 \\( \\rho = 1 \\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.449,
                0.737,
                0.488
            ],
            "angle": 0,
            "content": "格式(8.6.38)的收敛性与 \\(A_{i}, i = 1,2,3\\) 的选取有关．为了方便，令 \\(A = [A_1,A_2,A_3]\\)，以及 \\(x = (x_{1},x_{2},x_{3})^{\\mathrm{T}}\\)，并选取 \\(A\\) 为"
        },
        {
            "type": "equation",
            "bbox": [
                0.303,
                0.494,
                0.605,
                0.563
            ],
            "angle": 0,
            "content": "\\[\n\\widetilde {A} = \\left[ \\begin{array}{c c c} {{1}} & {{1}} & {{2}} \\\\ {{0}} & {{1}} & {{1}} \\\\ {{0}} & {{0}} & {{1}} \\end{array} \\right] \\quad \\text {或} \\quad \\widehat {A} = \\left[ \\begin{array}{c c c} {{1}} & {{1}} & {{1}} \\\\ {{1}} & {{1}} & {{2}} \\\\ {{1}} & {{2}} & {{2}} \\end{array} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.568,
                0.738,
                0.689
            ],
            "angle": 0,
            "content": "在迭代中，自变量初值初值选为 \\((1,1,1)\\)，乘子选为 \\((0,0,0)\\)。图8.10记录了在两种不同 \\(A\\) 条件下，\\(x\\) 和 \\(y\\) 的 \\(\\ell_2\\) 范数随迭代的变化过程。可以看到，当 \\(A = \\widetilde{A}\\) 时数值结果表明迭代是发散的，而当 \\(A = \\widehat{A}\\) 时数值结果表明迭代是收敛的。另一个比较有趣的观察是，在图8.10（b）中 \\(\\| x\\|\\) 与 \\(\\| y\\|\\) 并不是单调下降的，而是在下降的同时有规律地振荡。实际上，文献[43]中具体解释了 \\(A\\) 取 \\(\\widetilde{A}\\) 会导致发散的原因。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.714,
                0.341,
                0.733
            ],
            "angle": 0,
            "content": "*8.6.5 收敛性分析"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.745,
                0.737,
                0.782
            ],
            "angle": 0,
            "content": "本节主要讨论交替方向乘子法 (8.6.5) — (8.6.7) 在问题 (8.6.1) 上的收敛性，更详细的讨论请参考 [45]. 在此之前我们先引入一些必要的假设."
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.786,
                0.737,
                0.824
            ],
            "angle": 0,
            "content": "假设8.3 (1) \\( f_{1}(x), f_{2}(x) \\) 均为闭凸函数，且每个ADMM迭代子问题存在唯一解；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.836,
                0.574,
                0.853
            ],
            "angle": 0,
            "content": "(2) 原始问题(8.6.1)的解集非空，且 Slater 条件满足。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "456"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "image",
            "bbox": [
                0.262,
                0.156,
                0.523,
                0.322
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.344,
                0.332,
                0.442,
                0.346
            ],
            "angle": 0,
            "content": "(a) 系数矩阵为 \\(\\widetilde{A}\\)"
        },
        {
            "type": "image",
            "bbox": [
                0.564,
                0.16,
                0.822,
                0.322
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.644,
                0.332,
                0.742,
                0.345
            ],
            "angle": 0,
            "content": "(b) 系数矩阵为 \\(\\widehat{A}\\)"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.408,
                0.365,
                0.675,
                0.382
            ],
            "angle": 0,
            "content": "图8.10 选取不同 \\(A\\) 时的数值结果"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.409,
                0.825,
                0.488
            ],
            "angle": 0,
            "content": "假设8.3给出的条件是很基本的，\\(f_{1}\\) 和 \\(f_{2}\\) 的凸性保证了要求解的问题是凸问题，每个子问题存在唯一解是为了保证迭代的良定义；而在Slater条件满足的情况下，原始问题的KKT对和最优解是对应的，因此可以很方便地使用KKT条件来讨论收敛性。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.492,
                0.829,
                0.51
            ],
            "angle": 0,
            "content": "由于原始问题解集非空, 不妨设 \\((x_{1}^{*}, x_{2}^{*}, y^{*})\\) 是 KKT 对, 即满足条件(8.6.8)"
        },
        {
            "type": "equation",
            "bbox": [
                0.325,
                0.524,
                0.758,
                0.544
            ],
            "angle": 0,
            "content": "\\[\n- A _ {1} ^ {\\mathrm {T}} y ^ {*} \\in \\partial f _ {1} \\left(x _ {1} ^ {*}\\right), \\quad - A _ {2} ^ {\\mathrm {T}} y ^ {*} \\in \\partial f _ {2} \\left(x _ {2} ^ {*}\\right), \\quad A _ {1} x _ {1} ^ {*} + A _ {2} x _ {2} ^ {*} = b.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.558,
                0.825,
                0.596
            ],
            "angle": 0,
            "content": "我们最终的目的是证明ADMM迭代序列 \\(\\{(x_1^k,x_2^k,y^k)\\}\\) 收敛到原始问题的一个KKT对，因此引入如下记号来表示当前迭代点和KKT对的误差："
        },
        {
            "type": "equation",
            "bbox": [
                0.401,
                0.609,
                0.682,
                0.632
            ],
            "angle": 0,
            "content": "\\[\n\\left(e _ {1} ^ {k}, e _ {2} ^ {k}, e _ {y} ^ {k}\\right) \\stackrel {\\text {d e f}} {=} \\left(x _ {1} ^ {k}, x _ {2} ^ {k}, y ^ {k}\\right) - \\left(x _ {1} ^ {*}, x _ {2} ^ {*}, y ^ {*}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.646,
                0.632,
                0.663
            ],
            "angle": 0,
            "content": "我们进一步引入如下辅助变量来简化之后的证明："
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.674,
                0.735,
                0.696
            ],
            "angle": 0,
            "content": "\\[\nu ^ {k} = - A _ {1} ^ {\\mathrm {T}} [ y ^ {k} + (1 - \\tau) \\rho (A _ {1} e _ {1} ^ {k} + A _ {2} e _ {2} ^ {k}) + \\rho A _ {2} (x _ {2} ^ {k - 1} - x _ {2} ^ {k}) ],\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.299,
                0.699,
                0.602,
                0.72
            ],
            "angle": 0,
            "content": "\\[\nv ^ {k} = - A _ {2} ^ {\\mathrm {T}} [ y ^ {k} + (1 - \\tau) \\rho (A _ {1} e _ {1} ^ {k} + A _ {2} e _ {2} ^ {k}) ],\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.72,
                0.825,
                0.758
            ],
            "angle": 0,
            "content": "\\[\n\\Psi_ {k} = \\frac {1}{\\tau \\rho} \\| e _ {y} ^ {k} \\| ^ {2} + \\rho \\| A _ {2} e _ {2} ^ {k} \\| ^ {2}, \\tag {8.6.39}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.762,
                0.66,
                0.781
            ],
            "angle": 0,
            "content": "\\[\n\\Phi_ {k} = \\Psi_ {k} + \\max \\left(1 - \\tau , 1 - \\tau^ {- 1}\\right) \\rho \\| A _ {1} e _ {1} ^ {k} + A _ {2} e _ {2} ^ {k} \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.794,
                0.825,
                0.833
            ],
            "angle": 0,
            "content": "其中 \\(u^{k}, v^{k}\\) 和每个子问题的最优性条件有很大联系，见(8.6.9)式和(8.6.10)式，而 \\(\\Psi_{k}\\) 和 \\(\\Phi_{k}\\) 则是误差向量 \\(e_{1}^{k}, e_{2}^{k}, e_{y}^{k}\\) 的某种度量。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.836,
                0.597,
                0.853
            ],
            "angle": 0,
            "content": "在这些记号的基础上，我们有如下结果："
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "457"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.156,
                0.737,
                0.194
            ],
            "angle": 0,
            "content": "引理8.7 假设 \\(\\{(x_1^k, x_2^k, y^k)\\}\\) 为交替方向乘子法产生一个迭代序列，那么，对任意的 \\(k \\geqslant 1\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.208,
                0.737,
                0.228
            ],
            "angle": 0,
            "content": "\\[\nu ^ {k} \\in \\partial f _ {1} \\left(x _ {1} ^ {k}\\right), v ^ {k} \\in \\partial f _ {2} \\left(x _ {2} ^ {k}\\right), \\tag {8.6.40}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.216,
                0.251,
                0.737,
                0.297
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\Phi_ {k} - \\Phi_ {k + 1} \\geqslant \\min  (\\tau , 1 + \\tau - \\tau^ {2}) \\rho \\| A _ {2} \\left(x _ {2} ^ {k} - x _ {2} ^ {k + 1}\\right) \\| ^ {2} \\\\ + \\min  (1, 1 + \\tau^ {- 1} - \\tau) \\rho \\| A _ {1} e _ {1} ^ {k + 1} + A _ {2} e _ {2} ^ {k + 1} \\| ^ {2}. \\tag {8.6.41} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.31,
                0.737,
                0.347
            ],
            "angle": 0,
            "content": "证明．先证明(8.6.40)式的两个结论．根据交替方向乘子法的迭代过程，对\\(x_{1}^{k + 1}\\) 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.268,
                0.361,
                0.637,
                0.38
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial f _ {1} \\left(x _ {1} ^ {k + 1}\\right) + A _ {1} ^ {\\mathrm {T}} y ^ {k} + \\rho A _ {1} ^ {\\mathrm {T}} \\left(A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k} - b\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.393,
                0.654,
                0.413
            ],
            "angle": 0,
            "content": "将 \\( y^{k} = y^{k + 1} - \\tau \\rho \\left(A_{1}x_{1}^{k + 1} + A_{2}x_{2}^{k + 1} - b\\right) \\) 代入上式，消去 \\( y^{k} \\) 就有"
        },
        {
            "type": "equation",
            "bbox": [
                0.17,
                0.422,
                0.737,
                0.449
            ],
            "angle": 0,
            "content": "\\[\n- A _ {1} ^ {\\mathrm {T}} \\left(y ^ {k + 1} + (1 - \\tau) \\rho \\left(A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b\\right) + \\rho A _ {2} \\left(x _ {2} ^ {k} - x _ {2} ^ {k + 1}\\right)\\right) \\in \\partial f _ {1} \\left(x _ {1} ^ {k + 1}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.459,
                0.737,
                0.498
            ],
            "angle": 0,
            "content": "根据 \\(u^k\\) 的定义自然有 \\(u^k \\in \\partial f_1(x_1^k)\\) （注意代回 \\(b = A_1x_1^* + A_2x_2^*\\)）类似地，对 \\(x_2^{k+1}\\) 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.26,
                0.511,
                0.645,
                0.53
            ],
            "angle": 0,
            "content": "\\[\n0 \\in \\partial f _ {2} \\left(x _ {2} ^ {k + 1}\\right) + A _ {2} ^ {\\mathrm {T}} y ^ {k} + \\rho A _ {2} ^ {\\mathrm {T}} \\left(A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.544,
                0.442,
                0.562
            ],
            "angle": 0,
            "content": "同样利用 \\(y^{k}\\) 的表达式消去 \\(y^{k}\\), 得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.235,
                0.573,
                0.672,
                0.599
            ],
            "angle": 0,
            "content": "\\[\n- A _ {2} ^ {\\mathrm {T}} \\left(y ^ {k + 1} + (1 - \\tau) \\rho \\left(A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b\\right)\\right) \\in \\partial f _ {2} \\left(x _ {2} ^ {k + 1}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.61,
                0.434,
                0.628
            ],
            "angle": 0,
            "content": "根据 \\(v^k\\) 的定义自然有 \\(v^{k}\\in \\partial f_{2}(x_{2}^{k})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.632,
                0.737,
                0.668
            ],
            "angle": 0,
            "content": "接下来证明不等式(8.6.41). 首先根据 \\(\\left(x_{1}^{*}, x_{2}^{*}, y^{*}\\right)\\) 的最优性条件以及关系式(8.6.40),"
        },
        {
            "type": "equation",
            "bbox": [
                0.31,
                0.668,
                0.597,
                0.689
            ],
            "angle": 0,
            "content": "\\[\nu ^ {k + 1} \\in \\partial f _ {1} (x _ {1} ^ {k + 1}), \\quad - A _ {1} ^ {\\mathrm {T}} y ^ {*} \\in \\partial f _ {1} (x _ {1} ^ {*}),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.313,
                0.693,
                0.595,
                0.713
            ],
            "angle": 0,
            "content": "\\[\nv ^ {k + 1} \\in \\partial f _ {2} \\left(x _ {2} ^ {k + 1}\\right), \\quad - A _ {2} ^ {\\mathrm {T}} y ^ {*} \\in \\partial f _ {2} \\left(x _ {2} ^ {*}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.72,
                0.336,
                0.735
            ],
            "angle": 0,
            "content": "根据凸函数的单调性，"
        },
        {
            "type": "equation",
            "bbox": [
                0.343,
                0.747,
                0.565,
                0.767
            ],
            "angle": 0,
            "content": "\\[\n\\left\\langle u ^ {k + 1} + A _ {1} ^ {\\mathrm {T}} y ^ {*}, x _ {1} ^ {k + 1} - x _ {1} ^ {*} \\right\\rangle \\geqslant 0,\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.771,
                0.564,
                0.792
            ],
            "angle": 0,
            "content": "\\[\n\\left\\langle v ^ {k + 1} + A _ {2} ^ {\\mathrm {T}} y ^ {*}, x _ {2} ^ {k + 1} - x _ {2} ^ {*} \\right\\rangle \\geqslant 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.804,
                0.664,
                0.821
            ],
            "angle": 0,
            "content": "将上述两个不等式相加，结合 \\(u^{k + 1}, v^{k + 1}\\) 的定义，并注意到恒等式"
        },
        {
            "type": "equation",
            "bbox": [
                0.194,
                0.834,
                0.737,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nA _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b = (\\tau \\rho) ^ {- 1} \\left(y ^ {k + 1} - y ^ {k}\\right) = (\\tau \\rho) ^ {- 1} \\left(e _ {y} ^ {k + 1} - e _ {y} ^ {k}\\right), \\tag {8.6.42}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "458"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.383,
                0.174
            ],
            "angle": 0,
            "content": "我们就可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.314,
                0.185,
                0.824,
                0.266
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\frac {1}{\\tau \\rho} \\left\\langle e _ {y} ^ {k + 1}, e _ {y} ^ {k} - e _ {y} ^ {k + 1} \\right\\rangle - (1 - \\tau) \\rho \\| A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b \\| ^ {2} \\\\ + \\rho \\left\\langle A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right), A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b \\right\\rangle \\tag {8.6.43} \\\\ - \\rho \\left\\langle A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right), A _ {2} e _ {2} ^ {k + 1} \\right\\rangle \\geqslant 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.279,
                0.784,
                0.296
            ],
            "angle": 0,
            "content": "不等式(8.6.43)的形式和不等式(8.6.41)还有一定差异，主要的差别就在"
        },
        {
            "type": "equation",
            "bbox": [
                0.393,
                0.31,
                0.688,
                0.332
            ],
            "angle": 0,
            "content": "\\[\n\\rho \\left\\langle A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right), A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b \\right\\rangle\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.345,
                0.727,
                0.362
            ],
            "angle": 0,
            "content": "这一项上．我们来估计这一项的上界．为了方便，引入新符号"
        },
        {
            "type": "equation",
            "bbox": [
                0.359,
                0.376,
                0.701,
                0.394
            ],
            "angle": 0,
            "content": "\\[\n\\nu^ {k + 1} = y ^ {k + 1} + (1 - \\tau) \\rho \\left(A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b\\right),\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.353,
                0.4,
                0.734,
                0.419
            ],
            "angle": 0,
            "content": "\\[\nM ^ {k + 1} = (1 - \\tau) \\rho \\left\\langle A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right), A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} ^ {k} - b \\right\\rangle ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.431,
                0.745,
                0.451
            ],
            "angle": 0,
            "content": "则 \\(-A_{2}^{\\mathrm{T}}\\nu^{k + 1}\\in \\partial f_{2}(x_{2}^{k + 1})\\) 以及 \\(-A_2^{\\mathrm{T}}\\nu^k\\in \\partial f_2(x_2^k)\\) .再利用单调性知"
        },
        {
            "type": "equation",
            "bbox": [
                0.423,
                0.465,
                0.824,
                0.484
            ],
            "angle": 0,
            "content": "\\[\n\\left\\langle - A _ {2} ^ {\\mathrm {T}} \\left(\\nu^ {k + 1} - \\nu^ {k}\\right), x _ {2} ^ {k + 1} - x _ {2} ^ {k} \\right\\rangle \\geqslant 0. \\tag {8.6.44}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.499,
                0.522,
                0.516
            ],
            "angle": 0,
            "content": "根据这些不等式关系我们最终得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.362,
                0.53,
                0.723,
                0.643
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\rho \\left\\langle A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right), A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b \\right\\rangle \\\\ = (1 - \\tau) \\rho \\left\\langle A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right), A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b \\right\\rangle \\\\ + \\left\\langle A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right), y ^ {k + 1} - y ^ {k} \\right\\rangle \\\\ = M ^ {k + 1} + \\left\\langle v ^ {k + 1} - v ^ {k}, A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right) \\right\\rangle \\\\ \\leqslant M ^ {k + 1}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.659,
                0.825,
                0.697
            ],
            "angle": 0,
            "content": "其中第一个等号利用了关系式(8.6.42)，第二个等号利用了 \\(\\nu^k\\) 的定义（注意\\(M^{k + 1}\\) 中 \\(x_{1}\\) 和 \\(x_{2}\\) 的上标的变化），最后的不等式则是直接应用(8.6.44)式"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.701,
                0.644,
                0.718
            ],
            "angle": 0,
            "content": "估计完这一项之后，不等式(8.6.43)可以放缩成"
        },
        {
            "type": "equation",
            "bbox": [
                0.341,
                0.73,
                0.746,
                0.786
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\frac {1}{\\tau \\rho} \\left\\langle e _ {y} ^ {k + 1}, e _ {y} ^ {k} - e _ {y} ^ {k + 1} \\right\\rangle - (1 - \\tau) \\rho \\| A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b \\| ^ {2} \\\\ + M ^ {k + 1} - \\rho \\left\\langle A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right), A _ {2} e _ {2} ^ {k + 1} \\right\\rangle \\geqslant 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.799,
                0.505,
                0.815
            ],
            "angle": 0,
            "content": "上式中含有内积项，利用恒等式"
        },
        {
            "type": "equation",
            "bbox": [
                0.3,
                0.826,
                0.787,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\langle a, b \\rangle = \\frac {1}{2} (\\| a \\| ^ {2} + \\| b \\| ^ {2} - \\| a - b \\| ^ {2}) = \\frac {1}{2} (\\| a + b \\| ^ {2} - \\| a \\| ^ {2} - \\| b \\| ^ {2}),\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.334,
                0.133
            ],
            "angle": 0,
            "content": "8.6 交替方向乘子法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "459"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.262,
                0.174
            ],
            "angle": 0,
            "content": "进一步得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.201,
                0.185,
                0.737,
                0.243
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\frac {1}{\\tau \\rho} \\left(\\| e _ {y} ^ {k} \\| ^ {2} - \\| e _ {y} ^ {k + 1} \\| ^ {2}\\right) - (2 - \\tau) \\rho \\| A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b \\| ^ {2} \\tag {8.6.45} \\\\ + 2 M ^ {k + 1} - \\rho \\| A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right) \\| ^ {2} - \\rho \\| A _ {2} e _ {2} ^ {k + 1} \\| ^ {2} + \\rho \\| A _ {2} e _ {2} ^ {k} \\| ^ {2} \\geqslant 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.257,
                0.737,
                0.295
            ],
            "angle": 0,
            "content": "此时除了 \\(M^{k + 1}\\) 中的项，(8.6.45)中的其他项均在不等式(8.6.41)中出现．由于\\(M^{k + 1}\\) 的符号和 \\(\\tau\\) 的取法有关，下面我们针对 \\(\\tau\\) 的两种取法进行讨论."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.299,
                0.58,
                0.317
            ],
            "angle": 0,
            "content": "情形一 \\(\\tau \\in (0,1]\\)，此时 \\(M^{k + 1}\\geqslant 0\\)，根据基本不等式，"
        },
        {
            "type": "equation",
            "bbox": [
                0.299,
                0.332,
                0.613,
                0.375
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} 2 \\left\\langle A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right), A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} ^ {k} - b \\right\\rangle \\\\ \\leqslant \\| A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right) \\| ^ {2} + \\| A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} ^ {k} - b \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.389,
                0.351,
                0.406
            ],
            "angle": 0,
            "content": "代入不等式(8.6.45)得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.198,
                0.418,
                0.737,
                0.511
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\frac {1}{\\tau \\rho} \\| e _ {y} ^ {k} \\| ^ {2} + \\rho \\| A _ {2} e _ {2} ^ {k} \\| ^ {2} + (1 - \\tau) \\rho \\| A _ {1} e _ {1} ^ {k} + A _ {2} e _ {2} ^ {k} \\| ^ {2} \\\\ - \\left[ \\frac {1}{\\tau \\rho} \\| e _ {y} ^ {k + 1} \\| ^ {2} + \\rho \\| A _ {2} e _ {2} ^ {k + 1} \\| ^ {2} + (1 - \\tau) \\rho \\| A _ {1} e _ {1} ^ {k + 1} + A _ {2} e _ {2} ^ {k + 1} \\| ^ {2} \\right] \\tag {8.6.46} \\\\ \\geqslant \\rho \\| A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b \\| ^ {2} + \\tau \\rho \\| A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right) \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.526,
                0.555,
                0.544
            ],
            "angle": 0,
            "content": "情形二 \\(\\tau > 1\\) ，此时 \\(M^{k + 1} < 0\\) ，根据基本不等式，"
        },
        {
            "type": "equation",
            "bbox": [
                0.286,
                0.558,
                0.624,
                0.611
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} - 2 \\left\\langle A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right), A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} ^ {k} - b \\right\\rangle \\\\ \\leqslant \\tau \\| A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right) \\| ^ {2} + \\frac {1}{\\tau} \\| A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} ^ {k} - b \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.623,
                0.419,
                0.64
            ],
            "angle": 0,
            "content": "同样代入不等式(8.6.45)可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.189,
                0.653,
                0.736,
                0.784
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\frac {1}{\\tau \\rho} \\| e _ {y} ^ {k} \\| ^ {2} + \\rho \\| A _ {2} e _ {2} ^ {k} \\| ^ {2} + \\left(1 - \\frac {1}{\\tau}\\right) \\rho \\| A _ {1} e _ {1} ^ {k} + A _ {2} e _ {2} ^ {k} \\| ^ {2} \\\\ - \\left[ \\frac {1}{\\tau \\rho} \\| e _ {y} ^ {k + 1} \\| ^ {2} + \\rho \\| A _ {2} e _ {2} ^ {k + 1} \\| ^ {2} + \\left(1 - \\frac {1}{\\tau}\\right) \\rho \\| A _ {1} e _ {1} ^ {k + 1} + A _ {2} e _ {2} ^ {k + 1} \\| ^ {2} \\right] \\tag {8.6.47} \\\\ \\geqslant \\left(1 + \\frac {1}{\\tau} - \\tau\\right) \\rho \\| A _ {1} x _ {1} ^ {k + 1} + A _ {2} x _ {2} ^ {k + 1} - b \\| ^ {2} \\\\ + (1 + \\tau - \\tau^ {2}) \\rho \\| A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right) \\| ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.799,
                0.739,
                0.856
            ],
            "angle": 0,
            "content": "整合(8.6.46)式和(8.6.47)式即可得到不等式 (8.6.41). 注意, 只有当 \\(\\tau \\in \\left(0, \\frac{1 + \\sqrt{5}}{2}\\right)\\) 时, (8.6.41) 式中不等号右侧的项才为非负."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "460"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "引理8.7中(8.6.40)式直接利用了每个子问题的最优性条件以及KKT条件，不等式(8.6.41)的证明比较复杂，它实际上是 \\([66]^{\\text{定理B.1}}\\) 的一个简化版本。这个不等式的直观解释是迭代点误差的某种度量 \\(\\Phi_{k}\\) 是单调有界的。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.219,
                0.675,
                0.236
            ],
            "angle": 0,
            "content": "有了引理8.7的基础，我们给出主要的收敛性定理"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.247,
                0.825,
                0.305
            ],
            "angle": 0,
            "content": "定理8.16 在假设8.3的条件下，进一步假定 \\(A_{1}, A_{2}\\) 列满秩．如果 \\(\\tau \\in \\left(0, \\frac{1 + \\sqrt{5}}{2}\\right)\\)，则序列 \\(\\{(x_{1}^{k}, x_{2}^{k}, y^{k})\\}\\) 收敛到原始问题的一个KKT对."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.31,
                0.764,
                0.326
            ],
            "angle": 0,
            "content": "证明. 引理 8.7 表明 \\(\\Phi_{k}\\) 都是有界列, 根据 \\(\\Phi_{k}\\) 的定义 (8.6.39) 可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.42,
                0.338,
                0.663,
                0.359
            ],
            "angle": 0,
            "content": "\\[\n\\| e _ {y} ^ {k} \\|, \\quad \\| A _ {2} e _ {2} ^ {k} \\|, \\quad \\| A _ {1} e _ {1} ^ {k} + A _ {2} e _ {2} ^ {k} \\|\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.37,
                0.419,
                0.386
            ],
            "angle": 0,
            "content": "均有界．根据不等式"
        },
        {
            "type": "equation",
            "bbox": [
                0.41,
                0.398,
                0.673,
                0.418
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| A _ {1} e _ {1} ^ {k} \\right\\| \\leqslant \\left\\| A _ {1} e _ {1} ^ {k} + A _ {2} e _ {2} ^ {k} \\right\\| + \\left\\| A _ {2} e _ {2} ^ {k} \\right\\|,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.429,
                0.825,
                0.468
            ],
            "angle": 0,
            "content": "可以进一步推出 \\(\\{\\| A_1e_1^k\\|\\}\\) 也是有界序列．注意到 \\(A_1^{\\mathrm{T}}A_1\\succ 0,A_2^{\\mathrm{T}}A_2\\succ 0\\) ，因此以上有界性也等价于 \\(\\{(x_1^k,x_2^k,y^k)\\}\\) 是有界序列."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.471,
                0.604,
                0.487
            ],
            "angle": 0,
            "content": "引理8.3的另一个直接结果就是无穷级数"
        },
        {
            "type": "equation",
            "bbox": [
                0.389,
                0.495,
                0.693,
                0.533
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {k = 0} ^ {\\infty} \\left\\| A _ {1} e _ {1} ^ {k} + A _ {2} e _ {2} ^ {k} \\right\\| ^ {2}, \\sum_ {k = 0} ^ {\\infty} \\left\\| A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right) \\right\\| ^ {2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.54,
                0.419,
                0.556
            ],
            "angle": 0,
            "content": "都是收敛的，这表明"
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.566,
                0.825,
                0.61
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| A _ {1} e _ {1} ^ {k} + A _ {2} e _ {2} ^ {k} \\right\\| = \\left\\| A _ {1} x _ {1} ^ {k} + A _ {2} x _ {2} ^ {k} - b \\right\\|\\rightarrow 0, \\tag {8.6.48} \\\\ \\left\\| A _ {2} \\left(x _ {2} ^ {k + 1} - x _ {2} ^ {k}\\right)\\right\\|\\rightarrow 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.619,
                0.825,
                0.658
            ],
            "angle": 0,
            "content": "利用这些结果我们就可以推导收敛性了．首先证明迭代点子列的收敛性．由于 \\(\\{(x_1^k,x_2^k,y^k)\\}\\) 是有界序列，因此它存在一个收敛子列，设"
        },
        {
            "type": "equation",
            "bbox": [
                0.438,
                0.667,
                0.645,
                0.689
            ],
            "angle": 0,
            "content": "\\[\n\\left(x _ {1} ^ {k _ {j}}, x _ {2} ^ {k _ {j}}, y ^ {k _ {j}}\\right)\\rightarrow \\left(x _ {1} ^ {\\infty}, x _ {2} ^ {\\infty}, y ^ {\\infty}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.699,
                0.825,
                0.738
            ],
            "angle": 0,
            "content": "使用(8.6.39)式中的 \\(u^k\\) 和 \\(v^k\\) 的定义以及(8.6.48)式可得 \\(\\{u^k\\}\\) 与 \\(\\{v^k\\}\\) 相应的子列也收敛："
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.746,
                0.825,
                0.776
            ],
            "angle": 0,
            "content": "\\[\nu ^ {\\infty} \\stackrel {\\text {d e f}} {=} \\lim  _ {j \\rightarrow \\infty} u ^ {k _ {j}} = - A _ {1} ^ {\\mathrm {T}} y ^ {\\infty}, \\quad v ^ {\\infty} = \\lim  _ {j \\rightarrow \\infty} v ^ {k _ {j}} = - A _ {2} ^ {\\mathrm {T}} y ^ {\\infty}. \\tag {8.6.49}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.785,
                0.825,
                0.823
            ],
            "angle": 0,
            "content": "从(8.6.40)式我们知道对于任意的 \\(k \\geqslant 1\\)，有 \\(u^k \\in \\partial f_1(x_1^k)\\)，\\(v^k \\in \\partial f_2(x_2^k)\\)。利用定理2.19中次梯度映射的图像是闭集可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.393,
                0.835,
                0.691,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n- A _ {1} y ^ {\\infty} \\in \\partial f _ {1} \\left(x _ {1} ^ {\\infty}\\right), \\quad - A _ {2} y ^ {\\infty} \\in \\partial f _ {2} \\left(x _ {2} ^ {\\infty}\\right).\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "461"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.157,
                0.351,
                0.174
            ],
            "angle": 0,
            "content": "由(8.6.48)的第一式可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.257,
                0.185,
                0.65,
                0.215
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {j \\rightarrow \\infty} \\| A _ {1} x _ {1} ^ {k _ {j}} + A _ {2} x _ {2} ^ {k _ {j}} - b \\| = \\| A _ {1} x _ {1} ^ {\\infty} + A _ {2} x _ {2} ^ {\\infty} - b \\| = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.224,
                0.737,
                0.263
            ],
            "angle": 0,
            "content": "这表明 \\((x_{1}^{\\infty}, x_{2}^{\\infty}, y^{\\infty})\\) 是原始问题的一个KKT对. 因此上述分析中的 \\((x_{1}^{*}, x_{2}^{*}, y^{*})\\) 均可替换为 \\((x_{1}^{\\infty}, x_{2}^{\\infty}, y^{\\infty})\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.266,
                0.738,
                0.306
            ],
            "angle": 0,
            "content": "为了说明 \\(\\{(x_1^k,x_2^k,y^k)\\}\\) 全序列的收敛性，我们注意到 \\(\\Phi_{k}\\) 是单调下降的，且对子列 \\(\\{\\Phi_{k_j}\\}\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.186,
                0.319,
                0.72,
                0.404
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\lim  _ {j \\rightarrow \\infty} \\Phi_ {k _ {j}} \\\\ = \\lim  _ {j \\rightarrow \\infty} \\left(\\frac {1}{\\tau \\rho} \\| e _ {y} ^ {k _ {j}} \\| ^ {2} + \\rho \\| A _ {2} e _ {2} ^ {k _ {j}} \\| ^ {2} + \\max  \\left\\{1 - \\tau , 1 - \\frac {1}{\\tau} \\right\\} \\rho \\| A _ {1} e _ {1} ^ {k _ {j}} + A _ {2} e _ {2} ^ {k _ {j}} \\| ^ {2}\\right) \\\\ = 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.417,
                0.737,
                0.455
            ],
            "angle": 0,
            "content": "由于单调序列的子列收敛等价于全序列收敛，因此 \\(\\lim_{k\\to \\infty}\\Phi_k = 0\\) ，从而可以立即得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.193,
                0.464,
                0.713,
                0.569
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} 0 \\leqslant \\lim  _ {k \\rightarrow \\infty} \\sup  _ {\\tau \\rho} \\frac {1}{\\| e _ {y} ^ {k} \\| ^ {2}} \\leqslant \\lim  _ {k \\rightarrow \\infty} \\Phi_ {k} = 0, \\\\ 0\\leqslant \\limsup_{k\\to \\infty}\\rho \\| A_{2}e_{2}^{k}\\|^{2}\\leqslant \\limsup_{k\\to \\infty}\\Phi_{k} = 0, \\\\ 0 \\leqslant \\lim  _ {k \\rightarrow \\infty} \\left\\{\\max  \\left\\{1 - \\tau , 1 - \\frac {1}{\\tau} \\right\\} \\rho \\| A _ {1} e _ {1} ^ {k} + A _ {2} e _ {2} ^ {k} \\| ^ {2} \\right\\} \\leqslant \\lim  _ {k \\rightarrow \\infty} \\Phi_ {k} = 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.579,
                0.228,
                0.595
            ],
            "angle": 0,
            "content": "这说明"
        },
        {
            "type": "equation",
            "bbox": [
                0.278,
                0.597,
                0.632,
                0.619
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| e _ {y} ^ {k} \\right\\|\\rightarrow 0, \\quad \\left\\| A _ {2} e _ {2} ^ {k} \\right\\|\\rightarrow 0, \\quad \\left\\| A _ {1} e _ {1} ^ {k} + A _ {2} e _ {2} ^ {k} \\right\\|\\rightarrow 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.627,
                0.245,
                0.643
            ],
            "angle": 0,
            "content": "进一步有"
        },
        {
            "type": "equation",
            "bbox": [
                0.238,
                0.657,
                0.668,
                0.686
            ],
            "angle": 0,
            "content": "\\[\n0 \\leqslant \\lim  _ {k \\rightarrow \\infty} \\sup  _ {\\| A _ {1} e _ {1} ^ {k} \\| \\leqslant \\lim  _ {k \\rightarrow \\infty} \\left(\\| A _ {2} e _ {2} ^ {k} \\| + \\| A _ {1} e _ {1} ^ {k} + A _ {2} e _ {2} ^ {k} \\|\\right) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.697,
                0.643,
                0.715
            ],
            "angle": 0,
            "content": "注意到 \\(A_1^{\\mathrm{T}}A_1\\succ 0\\) ， \\(A_{2}^{\\mathrm{T}}A_{2}\\succ 0\\) ，所以最终我们得到全序列收敛："
        },
        {
            "type": "equation",
            "bbox": [
                0.355,
                0.727,
                0.552,
                0.748
            ],
            "angle": 0,
            "content": "\\[\n\\left(x _ {1} ^ {k}, x _ {2} ^ {k}, y ^ {k}\\right)\\rightarrow \\left(x _ {1} ^ {\\infty}, x _ {2} ^ {\\infty}, y ^ {\\infty}\\right).\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.35,
                0.777,
                0.558,
                0.799
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.816,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "随着大数据时代的来临，以及机器学习和深度学习等人工智能领域的发展，许多大规模的优化问题随之产生，它们对传统优化理论和算法都产"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "462"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.827,
                0.277
            ],
            "angle": 0,
            "content": "生了巨大的挑战。幸运的是，这些问题往往与概率和统计学科有很大联系，由此促使了随机优化算法的广泛使用。随机算法的思想可以追溯到Monro-Robbins算法[162]，相比传统优化算法，随机优化算法能极大地节省每步迭代的运算量，从而使得算法在大规模数据中变得可行。本节将介绍随机梯度算法的基本形式和收敛性理论，以及当前在深度学习领域广泛应用的一些随机梯度型算法。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.286,
                0.828,
                0.468
            ],
            "angle": 0,
            "content": "为了方便理解本节主要考虑的问题形式，首先介绍机器学习的一个基本模型——监督学习模型。假定 \\((a, b)\\) 服从概率分布 \\(P\\)，其中 \\(a\\) 为输入，\\(b\\) 为标签。我们的任务是要给定输入 \\(a\\) 预测标签 \\(b\\)，即要决定一个最优的函数 \\(\\phi\\) 使得期望风险 \\(\\mathbb{E}[L(\\phi(a), b)]\\) 最小，其中 \\(L(\\cdot, \\cdot)\\) 表示损失函数，用来衡量预测的准确度，函数 \\(\\phi\\) 为某个函数空间中的预测函数。在实际问题中我们并不知道真实的概率分布 \\(P\\)，而是随机采样得到的一个数据集 \\(\\mathcal{D} = \\{(a_1, b_1), (a_2, b_2), \\dots, (a_N, b_N)\\}\\)。然后我们用经验风险来近似期望风险，并将预测函数 \\(\\phi(\\cdot)\\) 参数化为 \\(\\phi(\\cdot; x)\\) 以缩小要找的预测函数的范围，即要求解下面的极小化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.442,
                0.482,
                0.826,
                0.521
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x} \\quad \\frac {1}{N} \\sum_ {i = 1} ^ {N} L \\left(\\phi \\left(a _ {i}; x\\right), b _ {i}\\right). \\tag {8.7.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.543,
                0.627,
                0.56
            ],
            "angle": 0,
            "content": "对应于随机优化问题 (4.4.1), 有 \\(\\xi_{i} = (a_{i}, b_{i})\\) 以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.414,
                0.586,
                0.669,
                0.604
            ],
            "angle": 0,
            "content": "\\[\nf _ {i} (x) = L \\left(\\phi \\left(a _ {i}; x\\right), b _ {i}\\right), \\quad h (x) = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.63,
                0.825,
                0.709
            ],
            "angle": 0,
            "content": "在机器学习中, 大量的问题都可以表示为问题 (8.7.1) 的形式。由于数据规模巨大, 计算目标函数的梯度变得非常困难, 但是可以通过采样的方式只计算部分样本的梯度来进行梯度下降, 往往也能达到非常好的数值表现, 而每步的运算量却得到了极大的减小。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.718,
                0.562,
                0.735
            ],
            "angle": 0,
            "content": "我们将主要考虑如下随机优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.44,
                0.755,
                0.826,
                0.794
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x) \\stackrel {\\text {d e f}} {=} \\frac {1}{N} \\sum_ {i = 1} ^ {N} f _ {i} (x), \\tag {8.7.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(f_{i}(x)\\) 对应第 \\(i\\) 个样本的损失函数．问题(8.7.2)也称为随机优化问题的有限和形式."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "463"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.156,
                0.393,
                0.175
            ],
            "angle": 0,
            "content": "8.7.1 随机梯度下降算法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.187,
                0.739,
                0.224
            ],
            "angle": 0,
            "content": "下面为了讨论方便，先假设(8.7.2)中每一个 \\(f_{i}(x)\\) 是凸的、可微的．因此可以运用梯度下降算法"
        },
        {
            "type": "equation",
            "bbox": [
                0.368,
                0.239,
                0.737,
                0.261
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\alpha_ {k} \\nabla f (x ^ {k}), \\tag {8.7.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.274,
                0.52,
                0.291
            ],
            "angle": 0,
            "content": "来求解原始的优化问题. 在迭代格式(8.7.3)中,"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.302,
                0.548,
                0.341
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (x ^ {k}) = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\nabla f _ {i} (x ^ {k}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.353,
                0.739,
                0.432
            ],
            "angle": 0,
            "content": "在绝大多数情况下，我们不能通过化简的方式得到 \\(\\nabla f(x^{k})\\) 的表达式，要计算这个梯度必须计算出所有的 \\(\\nabla f_{i}(x^{k}), i = 1,2,\\dots ,N\\) 然后将它们相加．然而在机器学习中，采集到的样本量是巨大的，因此计算 \\(\\nabla f(x^{k})\\) 需要非常大的计算量．使用传统的梯度法求解机器学习问题并不是一个很好的做法."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.436,
                0.738,
                0.473
            ],
            "angle": 0,
            "content": "既然梯度的计算很复杂，有没有减少计算量的方法呢？这就是下面要介绍的随机梯度下降算法 (SGD)。它的基本迭代格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.363,
                0.488,
                0.737,
                0.508
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\alpha_ {k} \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right), \\tag {8.7.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.522,
                0.741,
                0.646
            ],
            "angle": 0,
            "content": "其中 \\(s_k\\) 是从 \\(\\{1,2,\\dots ,N\\}\\) 中随机等可能地抽取的一个样本， \\(\\alpha_{k}\\) 称为步长<sup>1</sup>。通过对比(8.7.3)式和(8.7.4)式可知，随机梯度算法不去计算全梯度 \\(\\nabla f(x^{k})\\) 而是从众多样本中随机抽出一个样本 \\(s_i\\) ，然后仅仅计算这个样本处的梯度 \\(\\nabla f_{s_k}(x^k)\\) ，以此作为 \\(\\nabla f(x^{k})\\) 的近似。注意，在全梯度 \\(\\nabla f(x^{k})\\) 的表达式中含系数 \\(\\frac{1}{N}\\) ，而迭代格式(8.7.4)中不含 \\(\\frac{1}{N}\\) 。这是因为我们要保证随机梯度的条件期望恰好是全梯度，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.35,
                0.661,
                0.559,
                0.681
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} _ {s _ {k}} \\left[ \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) \\mid x ^ {k} \\right] = \\nabla f \\left(x ^ {k}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.695,
                0.739,
                0.775
            ],
            "angle": 0,
            "content": "这里使用条件期望的符号 \\(\\mathbb{E}[\\cdot |x^k ]\\) 的原因是迭代点 \\(x^{k}\\) 本身也是一个随机变量．实际计算中每次只抽取一个样本 \\(s_k\\) 的做法比较极端，常用的形式是小批量（mini-batch）随机梯度法，即随机选择一个元素个数很少的集合\\(\\mathcal{I}_k\\subset \\{1,2,\\dots ,N\\}\\) ，然后执行迭代格式"
        },
        {
            "type": "equation",
            "bbox": [
                0.344,
                0.786,
                0.564,
                0.82
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\frac {\\alpha_ {k}}{\\left| \\mathcal {I} _ {k} \\right|} \\sum_ {s \\in \\mathcal {I} _ {k}} \\nabla f _ {s} (x ^ {k}),\n\\]"
        },
        {
            "type": "page_footnote",
            "bbox": [
                0.195,
                0.839,
                0.595,
                0.853
            ],
            "angle": 0,
            "content": "1在机器学习和深度学习领域中，更多的时候被称为学习率(learning rate)"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "464"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.214
            ],
            "angle": 0,
            "content": "其中 \\(|\\mathcal{I}_k|\\) 表示 \\(\\mathcal{I}_k\\) 中的元素个数。本节后面的阐述中虽然只考虑最简单形式的随机梯度下降算法 (8.7.4)，但很多变形和分析都可以推广到小批量随机梯度法。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.219,
                0.825,
                0.32
            ],
            "angle": 0,
            "content": "随机梯度下降法使用一个样本点的梯度代替了全梯度，并且每次迭代选取的样本点是随机的，这使得每次迭代时计算梯度的复杂度变为了原先的 \\(\\frac{1}{N}\\)，在样本量 \\(N\\) 很大的时候无疑是一个巨大的改进。但正因为如此，算法中也引入了随机性，一个自然的问题是这样的算法还会有收敛性吗？如果收敛，是什么意义下的收敛？这些问题将在第8.7.3小节中给出具体的回答。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.324,
                0.825,
                0.361
            ],
            "angle": 0,
            "content": "当 \\( f_{i}(x) \\) 是凸函数但不一定可微时，我们可以用 \\( f_{i}(x) \\) 的次梯度代替梯度进行迭代。这就是随机次梯度算法，它的迭代格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.477,
                0.375,
                0.824,
                0.394
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\alpha_ {k} g ^ {k}, \\tag {8.7.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.408,
                0.783,
                0.426
            ],
            "angle": 0,
            "content": "其中 \\(\\alpha_{k}\\) 为步长， \\(g^{k}\\in \\partial f_{s_{k}}(x^{k})\\) 为随机次梯度，其期望为真实的次梯度"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.43,
                0.825,
                0.466
            ],
            "angle": 0,
            "content": "随机梯度算法在深度学习中得到了广泛的应用，下面介绍其在深度学习中的一些变形."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.491,
                0.362,
                0.506
            ],
            "angle": 0,
            "content": "1. 动量方法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.52,
                0.826,
                0.681
            ],
            "angle": 0,
            "content": "传统的梯度法在问题比较病态时收敛速度非常慢，随机梯度下降法也有类似的问题。为了克服这一缺陷，人们提出了动量方法（momentum），旨在加速学习。该方法在处理高曲率或是带噪声的梯度上非常有效，其思想是在算法迭代时一定程度上保留之前更新的方向，同时利用当前计算的梯度调整最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习得更快，并且还有一定摆脱局部最优解的能力。从形式上来看，动量方法引入了一个速度变量 \\( v \\)，它代表参数移动的方向和大小。动量方法的具体迭代格式如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.447,
                0.696,
                0.824,
                0.715
            ],
            "angle": 0,
            "content": "\\[\nv ^ {k + 1} = \\mu_ {k} v ^ {k} - \\alpha_ {k} \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right), \\tag {8.7.6}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.447,
                0.72,
                0.824,
                0.739
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} + v ^ {k + 1}. \\tag {8.7.7}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.753,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "在计算当前点的随机梯度 \\(\\nabla f_{s_i}(x^k)\\) 后，我们并不是直接将其更新到变量 \\(x^k\\) 上，即不完全相信这个全新的更新方向，而是将其和上一步更新方向 \\(v^k\\) 做线性组合来得到新的更新方向 \\(v^{k+1}\\)。由动量方法迭代格式立即得出当 \\(\\mu_k = 0\\) 时该方法退化成随机梯度下降法。在动量方法中，参数 \\(\\mu_k\\) 的范围是 \\([0,1)\\)，通常取 \\(\\mu_k \\geqslant 0.5\\)，其含义为迭代点带有较大惯性，每次迭代会在原始迭代方"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "465"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.215
            ],
            "angle": 0,
            "content": "向的基础上做一个小的修正。在普通的梯度法中，每一步迭代只用到了当前点的梯度估计，动量方法的更新方向还使用了之前的梯度信息。当许多连续的梯度指向相同的方向时，步长就会很大，这从直观上看也是非常合理的。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.219,
                0.744,
                0.279
            ],
            "angle": 0,
            "content": "图8.11比较了梯度法和动量方法在例6.2中的表现。可以看到普通梯度法生成的点列会在椭圆的短轴方向上来回移动，而动量方法生成的点列更快收敛到了最小值点。"
        },
        {
            "type": "image",
            "bbox": [
                0.254,
                0.305,
                0.667,
                0.439
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.27,
                0.468,
                0.638,
                0.485
            ],
            "angle": 0,
            "content": "图8.11 动量方法在海瑟矩阵病态条件下的表现"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.526,
                0.351,
                0.544
            ],
            "angle": 0,
            "content": "2. Nesterov 加速算法"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.556,
                0.615,
                0.574
            ],
            "angle": 0,
            "content": "针对光滑问题的Nesterov加速算法迭代的随机版本为"
        },
        {
            "type": "equation",
            "bbox": [
                0.348,
                0.586,
                0.737,
                0.606
            ],
            "angle": 0,
            "content": "\\[\ny ^ {k + 1} = x ^ {k} + \\mu_ {k} \\left(x ^ {k} - x ^ {k - 1}\\right), \\tag {8.7.8}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.35,
                0.611,
                0.737,
                0.631
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = y ^ {k + 1} - \\alpha_ {k} \\nabla f _ {s _ {k}} \\left(y ^ {k + 1}\\right), \\tag {8.7.9}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.642,
                0.741,
                0.709
            ],
            "angle": 0,
            "content": "其中 \\(\\mu_{k} = \\frac{k - 1}{k + 2}\\)，步长 \\(\\alpha_{k}\\) 是一个固定值或者由线搜索确定。现在我们通过一些等价变形来说明Nesterov加速算法可以看成是某种动量方法。首先在第 \\(k\\) 步迭代引入速度变量"
        },
        {
            "type": "equation",
            "bbox": [
                0.396,
                0.712,
                0.512,
                0.73
            ],
            "angle": 0,
            "content": "\\[\nv ^ {k} = x ^ {k} - x ^ {k - 1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.741,
                0.564,
                0.758
            ],
            "angle": 0,
            "content": "再合并原始Nesterov加速算法的两步迭代可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.245,
                0.77,
                0.663,
                0.791
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} + \\mu_ {k} (x ^ {k} - x ^ {k - 1}) - \\alpha_ {k} \\nabla f _ {k} (x ^ {k} + \\mu_ {k} (x ^ {k} - x ^ {k - 1})).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.804,
                0.389,
                0.822
            ],
            "angle": 0,
            "content": "如果定义有关 \\(v^{k + 1}\\) 的迭代式"
        },
        {
            "type": "equation",
            "bbox": [
                0.331,
                0.834,
                0.577,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\boldsymbol {v} ^ {k + 1} = \\mu_ {k} \\boldsymbol {v} ^ {k} - \\alpha_ {k} \\nabla f _ {k} (\\boldsymbol {x} ^ {k} + \\mu_ {k} \\boldsymbol {v} ^ {k}),\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "466"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.156,
                0.507,
                0.174
            ],
            "angle": 0,
            "content": "则得到关于 \\(x^{k}\\) 和 \\(v^{k}\\) 的等价迭代："
        },
        {
            "type": "equation",
            "bbox": [
                0.417,
                0.188,
                0.825,
                0.208
            ],
            "angle": 0,
            "content": "\\[\nv ^ {k + 1} = \\mu_ {k} v ^ {k} - \\alpha_ {k} \\nabla f _ {s _ {k}} \\left(x ^ {k} + \\mu_ {k} v ^ {k}\\right), \\tag {8.7.10}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.418,
                0.213,
                0.825,
                0.232
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} + v ^ {k + 1}. \\tag {8.7.11}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.248,
                0.825,
                0.305
            ],
            "angle": 0,
            "content": "与动量方法相比，二者的主要差别在梯度的计算上。Nesterov 加速算法先对点施加速度的作用，再求梯度，这可以理解为对标准动量方法做了一个校正。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.331,
                0.367,
                0.345
            ],
            "angle": 0,
            "content": "3. AdaGrad"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.36,
                0.825,
                0.542
            ],
            "angle": 0,
            "content": "在一般的随机梯度法中，调参是一个很大的难点，参数设置的好坏对算法的性能有显著的影响。所以我们希望算法能在运行的过程中，根据当前情况自发地调整参数，这就是 AdaGrad(adaptive subgradient methods) 的出发点。对无约束光滑凸优化问题，点 \\(x\\) 是问题的解等价于该点处梯度为零向量。但对大部分问题而言，梯度的每个分量收敛到零的速度是不同的。传统梯度算法只有一个统一的步长 \\(\\alpha_{k}\\) 来调节每一步迭代，它没有针对每一个分量考虑。当梯度的某个分量较大时，可以推断出在该方向上函数变化比较剧烈，此时应该用小步长；当梯度的某个分量较小时，在该方向上函数比较平缓，此时应该用大步长。AdaGrad 就是根据这个思想设计的。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.545,
                0.825,
                0.583
            ],
            "angle": 0,
            "content": "令 \\( g^{k} = \\nabla f_{s_{k}}(x^{k}) \\)，为了记录整个迭代过程中梯度各个分量的累积情况，引入向量"
        },
        {
            "type": "equation",
            "bbox": [
                0.481,
                0.582,
                0.602,
                0.62
            ],
            "angle": 0,
            "content": "\\[\nG ^ {k} = \\sum_ {i = 1} ^ {k} g ^ {i} \\odot g ^ {i}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.627,
                0.825,
                0.686
            ],
            "angle": 0,
            "content": "从 \\(G^{k}\\) 的定义可知 \\(G^{k}\\) 的每个分量表示在迭代过程中, 梯度在该分量处的累积平方和. 当 \\(G^{k}\\) 的某分量较大时, 我们认为该分量变化比较剧烈, 因此应采用小步长, 反之亦然. 因此 AdaGrad 的迭代格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.433,
                0.697,
                0.825,
                0.73
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\frac {\\alpha}{\\sqrt {G ^ {k} + \\varepsilon \\mathbf {1} _ {n}}} \\odot g ^ {k}, \\tag {8.7.12}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.431,
                0.733,
                0.825,
                0.753
            ],
            "angle": 0,
            "content": "\\[\nG ^ {k + 1} = G ^ {k} + g ^ {k + 1} \\odot g ^ {k + 1}, \\tag {8.7.13}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.766,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "这里 \\(\\frac{\\alpha}{\\sqrt{G^k + \\varepsilon\\mathbf{1}_n}}\\) 中的除法和求根运算都是对向量每个分量分别操作的（下同), \\(\\alpha\\) 为初始步长, 引入 \\(\\varepsilon \\mathbf{1}_n\\) 这一项是为了防止除零运算. 可以看到 AdaGrad 的步长大致反比于历史梯度累计值的算术平方根, 所以梯度较大时步长下降很快, 反之则下降较慢, 这样做的效果就是在参数空间更平缓的方向上,"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "467"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.215
            ],
            "angle": 0,
            "content": "前后两次迭代的距离较大。在凸优化问题中 AdaGrad 有比较好的理论性质，但实际应用中也发现在训练深度神经网络模型时，从训练开始就积累梯度平方会导致步长过早或过多减小。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.22,
                0.741,
                0.259
            ],
            "angle": 0,
            "content": "如果在 AdaGrad 中使用真实梯度 \\(\\nabla f(x^{k})\\)，那么 AdaGrad 也可以看成是一种介于一阶和二阶的优化算法。考虑 \\(f(x)\\) 在点 \\(x^{k}\\) 处的二阶泰勒展开："
        },
        {
            "type": "equation",
            "bbox": [
                0.236,
                0.269,
                0.672,
                0.302
            ],
            "angle": 0,
            "content": "\\[\nf (x) \\approx f (x ^ {k}) + \\nabla f (x ^ {k}) ^ {\\mathrm {T}} (x - x ^ {k}) + \\frac {1}{2} (x - x ^ {k}) ^ {\\mathrm {T}} B ^ {k} (x - x ^ {k}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.313,
                0.741,
                0.372
            ],
            "angle": 0,
            "content": "我们知道选取不同的 \\(B^{k}\\) 可以导出不同的优化算法．例如使用常数倍单位矩阵近似 \\(B^{k}\\) 时可得到梯度法；利用海瑟矩阵作为 \\(B^{k}\\) 时可得到牛顿法．而AdaGrad则是使用一个对角矩阵来作为 \\(B^{k}\\) ．具体地，取"
        },
        {
            "type": "equation",
            "bbox": [
                0.358,
                0.382,
                0.55,
                0.413
            ],
            "angle": 0,
            "content": "\\[\nB ^ {k} = \\frac {1}{\\alpha} \\operatorname {D i a g} \\left(\\sqrt {G ^ {k} + \\varepsilon \\mathbf {1} _ {n}}\\right)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.424,
                0.531,
                0.44
            ],
            "angle": 0,
            "content": "时导出的算法就是AdaGrad. 读者可自行验证"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.469,
                0.284,
                0.485
            ],
            "angle": 0,
            "content": "4. RMSProp"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.499,
                0.741,
                0.6
            ],
            "angle": 0,
            "content": "RMSProp（root mean square propagation）是对AdaGrad的一个改进，该方法在非凸问题上可能表现更好。AdaGrad会累加之前所有的梯度分量平方，这就导致步长是单调递减的，因此在训练后期步长会非常小，同时这样做也加大了计算的开销。所以RMSProp提出只需使用离当前迭代点比较近的项，同时引入衰减参数 \\(\\rho\\) 。具体地，令"
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.614,
                0.586,
                0.634
            ],
            "angle": 0,
            "content": "\\[\nM ^ {k + 1} = \\rho M ^ {k} + (1 - \\rho) g ^ {k + 1} \\odot g ^ {k + 1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.651,
                0.634,
                0.668
            ],
            "angle": 0,
            "content": "再对其每个分量分别求根，就得到均方根(root mean square)"
        },
        {
            "type": "equation",
            "bbox": [
                0.385,
                0.682,
                0.738,
                0.703
            ],
            "angle": 0,
            "content": "\\[\nR ^ {k} = \\sqrt {M ^ {k} + \\varepsilon \\mathbf {1} _ {n}}, \\tag {8.7.14}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.718,
                0.729,
                0.736
            ],
            "angle": 0,
            "content": "最后将均方根的倒数作为每个分量步长的修正，得到 RMSProp 迭代格式："
        },
        {
            "type": "equation",
            "bbox": [
                0.33,
                0.747,
                0.738,
                0.776
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\frac {\\alpha}{R ^ {k}} \\odot g ^ {k}, \\tag {8.7.15}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.78,
                0.738,
                0.8
            ],
            "angle": 0,
            "content": "\\[\nM ^ {k + 1} = \\rho M ^ {k} + (1 - \\rho) g ^ {k + 1} \\odot g ^ {k + 1}. \\tag {8.7.16}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.816,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "引入参数 \\(\\varepsilon\\) 同样是为了防止分母为0的情况发生．一般取 \\(\\rho = 0.9,\\alpha = 0.001\\) 可以看到RMSProp和AdaGrad的唯一区别是将 \\(G^{k}\\) 替换成了 \\(M^k\\)"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "468"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.157,
                0.37,
                0.172
            ],
            "angle": 0,
            "content": "5. AdaDelta"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.188,
                0.825,
                0.226
            ],
            "angle": 0,
            "content": "AdaDelta 在 RMSProp 的基础上，对历史的 \\(\\Delta x^{k}\\) 也同样累积平方并求均方根："
        },
        {
            "type": "equation",
            "bbox": [
                0.417,
                0.242,
                0.826,
                0.262
            ],
            "angle": 0,
            "content": "\\[\nD ^ {k} = \\rho D ^ {k - 1} + (1 - \\rho) \\Delta x ^ {k} \\odot \\Delta x ^ {k}, \\tag {8.7.17}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.422,
                0.265,
                0.826,
                0.286
            ],
            "angle": 0,
            "content": "\\[\nT ^ {k} = \\sqrt {D ^ {k} + \\varepsilon \\mathbf {1} _ {n}}, \\tag {8.7.18}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.303,
                0.826,
                0.342
            ],
            "angle": 0,
            "content": "然后使用 \\(T^{k-1}\\) 和 \\(R^k\\) 的商对梯度进行校正，完整的过程如算法 8.13 所示，其中 \\(T^k\\) 和 \\(R^k\\) 的定义分别为 (8.7.18) 式和 (8.7.14) 式。"
        },
        {
            "type": "title",
            "bbox": [
                0.261,
                0.358,
                0.409,
                0.374
            ],
            "angle": 0,
            "content": "算法8.13AdaDelta"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.38,
                0.386,
                0.396
            ],
            "angle": 0,
            "content": "1. 输入 \\(x^{1}, \\rho, \\varepsilon\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.4,
                0.468,
                0.417
            ],
            "angle": 0,
            "content": "2. 置初值 \\(M^0 = 0, D^0 = 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.422,
                0.446,
                0.437
            ],
            "angle": 0,
            "content": "3. for \\( k = 1,2,\\dots ,K \\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.441,
                0.691,
                0.459
            ],
            "angle": 0,
            "content": "4. 随机选取 \\(i \\in \\{1, 2, \\dots, N\\}\\)，计算梯度 \\(g^{k} = \\nabla f_{i}(x^{k})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.461,
                0.572,
                0.48
            ],
            "angle": 0,
            "content": "5. 计算 \\(M^{k} = \\rho M^{k - 1} + (1 - \\rho)g^{k}\\odot g^{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.48,
                0.49,
                0.51
            ],
            "angle": 0,
            "content": "6. 计算 \\(\\Delta x^{k} = -\\frac{T^{k - 1}}{R^{k}}\\odot g^{k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.509,
                0.59,
                0.528
            ],
            "angle": 0,
            "content": "7. 计算 \\(D^{k} = \\rho D^{\\hat{k - 1}} + (1 - \\rho)\\Delta x^{k}\\odot \\Delta x^{k}.\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.529,
                0.435,
                0.546
            ],
            "angle": 0,
            "content": "8. \\(x^{k + 1}\\gets x^k +\\Delta x^k\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.553,
                0.35,
                0.566
            ],
            "angle": 0,
            "content": "9. end for"
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.38,
                0.691,
                0.566
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.592,
                0.825,
                0.651
            ],
            "angle": 0,
            "content": "注意，计算步长时 \\(T\\) 和 \\(R\\) 的下标相差1，这是因为我们还没有计算出 \\(\\Delta x^{k}\\) 的值，无法使用 \\(T^k\\) 进行计算．AdaDelta的特点是步长选择较为保守，同时也改善了AdaGrad步长单调下降的缺陷."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.681,
                0.342,
                0.695
            ],
            "angle": 0,
            "content": "6. Adam"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.711,
                0.842,
                0.811
            ],
            "angle": 0,
            "content": "Adam（adaptive moment estimation）本质上是带动量项的 RMSProp，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数步长。虽然 RMSProp 也采用了二阶矩估计，但是缺少修正因子，所以它在训练初期可能有比较大的偏差。Adam 的优点主要在于经过偏差修正后，每一次迭代的步长都有一个确定范围，使得参数比较平稳。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.816,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "与 RMSProp 和动量方法相比，Adam 可以看成是带修正的二者的结合。在 Adam 中不直接使用随机梯度作为基础的更新方向，而是选择了一个动"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.317,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "469"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.157,
                0.284,
                0.173
            ],
            "angle": 0,
            "content": "量项进行更新："
        },
        {
            "type": "equation",
            "bbox": [
                0.357,
                0.175,
                0.549,
                0.196
            ],
            "angle": 0,
            "content": "\\[\nS ^ {k} = \\rho_ {1} S ^ {k - 1} + (1 - \\rho_ {1}) g ^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.202,
                0.677,
                0.22
            ],
            "angle": 0,
            "content": "于此同时它和 RMSProp 类似，也会记录迭代过程中梯度的二阶矩："
        },
        {
            "type": "equation",
            "bbox": [
                0.331,
                0.228,
                0.576,
                0.249
            ],
            "angle": 0,
            "content": "\\[\nM ^ {k} = \\rho_ {2} M ^ {k - 1} + (1 - \\rho_ {2}) g ^ {k} \\odot g ^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.258,
                0.737,
                0.295
            ],
            "angle": 0,
            "content": "与原始动量方法和 RMSProp 的区别是, 由于 \\(S^{k}\\) 和 \\(M^{k}\\) 本身带有偏差, Adam 不直接使用这两个量更新, 而是在更新前先对其进行修正:"
        },
        {
            "type": "equation",
            "bbox": [
                0.345,
                0.3,
                0.562,
                0.338
            ],
            "angle": 0,
            "content": "\\[\n\\hat {S} ^ {k} = \\frac {S ^ {k}}{1 - \\rho_ {1} ^ {k}}, \\quad \\hat {M} ^ {k} = \\frac {M ^ {k}}{1 - \\rho_ {2} ^ {k}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.343,
                0.737,
                0.381
            ],
            "angle": 0,
            "content": "这里 \\(\\rho_1^k,\\rho_2^k\\) 分别表示 \\(\\rho_{1},\\rho_{2}\\) 的 \\(k\\) 次方．Adam最终使用修正后的一阶矩和二阶矩进行迭代点的更新."
        },
        {
            "type": "equation",
            "bbox": [
                0.34,
                0.386,
                0.567,
                0.421
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\frac {\\alpha}{\\sqrt {\\hat {M} ^ {k} + \\varepsilon \\mathbf {1} _ {n}}} \\odot \\hat {S} ^ {k}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.426,
                0.741,
                0.443
            ],
            "angle": 0,
            "content": "我们将完整的迭代过程整理到算法8.14中．这里参数 \\(\\rho_{1}\\) 通常选为0.9，"
        },
        {
            "type": "title",
            "bbox": [
                0.172,
                0.456,
                0.296,
                0.472
            ],
            "angle": 0,
            "content": "算法8.14Adam"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.477,
                0.555,
                0.496
            ],
            "angle": 0,
            "content": "1. 给定步长 \\(\\alpha\\) ，矩估计的指数衰减速率 \\(\\rho_{1},\\rho_{2}\\) ， \\(x^{1}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.498,
                0.384,
                0.515
            ],
            "angle": 0,
            "content": "2. 置初值 \\(S^0 = 0\\) ， \\(M^0 = 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.52,
                0.358,
                0.535
            ],
            "angle": 0,
            "content": "3. for \\( k = 1,2,\\dots ,K \\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.539,
                0.602,
                0.557
            ],
            "angle": 0,
            "content": "4. 随机选取 \\(i \\in \\{1,2,\\dots,N\\}\\)，计算梯度 \\(g^{k} = \\nabla f_{i}(x^{k})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.56,
                0.545,
                0.578
            ],
            "angle": 0,
            "content": "5. 更新一阶矩估计： \\(S^k = \\rho_1 S^{k-1} + (1 - \\rho_1) g^k\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.58,
                0.595,
                0.599
            ],
            "angle": 0,
            "content": "6. 更新二阶矩估计： \\(M^{k} = \\rho_{2}M^{k - 1} + (1 - \\rho_{2})g^{k}\\odot g^{k}.\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.599,
                0.467,
                0.634
            ],
            "angle": 0,
            "content": "7. 修正一阶矩的偏差: \\(\\hat{S}^k = \\frac{S^k}{1 - \\rho_1^k}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.634,
                0.474,
                0.668
            ],
            "angle": 0,
            "content": "8. 修正二阶矩的偏差： \\(\\hat{M}^k = \\frac{M^k}{1 - \\rho_2^k}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.183,
                0.668,
                0.442,
                0.699
            ],
            "angle": 0,
            "content": "9. \\(x^{k + 1} = x^k -\\frac{\\alpha}{\\sqrt{\\hat{M}^k + \\varepsilon\\mathbf{1}_n}}\\odot \\hat{S}^k.\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.183,
                0.477,
                0.602,
                0.699
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.177,
                0.7,
                0.262,
                0.712
            ],
            "angle": 0,
            "content": "10. end for"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.733,
                0.436,
                0.749
            ],
            "angle": 0,
            "content": "\\(\\rho_{2}\\) 选为0.999，全局步长 \\(\\alpha = 0.001\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.753,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "上面的很多算法已经被实现在主流的深度学习框架中，可以非常方便地用于训练神经网络：Pytorch里实现的算法有AdaDelta，AdaGrad，Adam，Nesterov，RMSProp等；Tensorflow里实现的算法则有AdaDelta，AdaGradDA，AdaGrad，ProximalAdagrad，Ftrl，Momentum，Adam和CenteredRMSProp等."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "470"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.402,
                0.174
            ],
            "angle": 0,
            "content": "8.7.2 应用举例"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.187,
                0.825,
                0.224
            ],
            "angle": 0,
            "content": "随机优化在机器学习的监督模型中有非常多的应用，本小节介绍两个例子——逻辑回归和神经网络。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.249,
                0.362,
                0.265
            ],
            "angle": 0,
            "content": "1. 逻辑回归"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.278,
                0.825,
                0.336
            ],
            "angle": 0,
            "content": "逻辑回归是最基本的线性分类模型，它经常用来作为各种分类模型的比较标准。给定数据集 \\(\\{(a_i, b_i)\\}_{i=1}^N\\)，带 \\(\\ell_2\\) 范数平方正则项的逻辑回归对应的优化问题 (8.7.2) 可以写成如下形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.346,
                0.787,
                0.385
            ],
            "angle": 0,
            "content": "\\[\n\\min  _ {x \\in \\mathbb {R} ^ {n}} f (x) = \\frac {1}{N} \\sum_ {i = 1} ^ {N} f _ {i} (x) = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\ln \\left(1 + \\exp \\left(- b _ {i} \\cdot a _ {i} ^ {\\mathrm {T}} x\\right)\\right) + \\lambda \\| x \\| _ {2} ^ {2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.396,
                0.298,
                0.411
            ],
            "angle": 0,
            "content": "其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.415,
                0.692,
                0.434
            ],
            "angle": 0,
            "content": "\\[\nf _ {i} (x) = \\ln \\left(1 + \\exp \\left(- b _ {i} \\cdot a _ {i} ^ {\\mathrm {T}} x\\right)\\right) + \\lambda \\| x \\| _ {2} ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.444,
                0.825,
                0.481
            ],
            "angle": 0,
            "content": "每步我们随机取一个下标 \\(i_{k}\\) 对应的梯度 \\(\\nabla f_{i_k}(x^k)\\) 做随机梯度下降，其迭代格式可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.285,
                0.49,
                0.799,
                0.534
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\alpha_ {k} \\nabla f _ {i _ {k}} (x ^ {k}) = x ^ {k} - \\alpha_ {k} \\left(\\frac {- \\exp (- b _ {i _ {k}} \\cdot a _ {i _ {k}} ^ {\\mathrm {T}} x ^ {k}) b _ {i _ {k}} a _ {i _ {k}}}{1 + \\exp (- b _ {i _ {k}} \\cdot a _ {i _ {k}} ^ {\\mathrm {T}} x ^ {k})} + 2 \\lambda x ^ {k}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.543,
                0.719,
                0.56
            ],
            "angle": 0,
            "content": "其中 \\(i_k\\) 为从 \\(\\{1,2,\\dots ,N\\}\\) 中随机抽取的一个样本， \\(\\alpha_{k}\\) 为步长"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.56,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "这里和第6.4节一样，采用LIBSVM[42]网站的数据集并令 \\(\\lambda = \\frac{10^{-2}}{N}\\) 分别测试不同随机算法在数据集CINA和a9a上的表现．我们采用网格搜索方法来确定随机算法中的参数值，对每个参数重复5次数值实验并取其平均表现．对比发现：对随机梯度算法，步长 \\(\\alpha_{k} = 10^{-3}\\) 表现最好．动量方法中取 \\(\\mu_{k} = 0.8,\\alpha_{k} = 10^{-3}\\) ；Adagrad,RMSProp和Adam中的 \\(\\alpha\\) 分别取为\\(0.4,10^{-3},5\\cdot 10^{-3}\\) ，数值稳定参数均设置为 \\(\\varepsilon = 10^{-7}\\) ．数值结果见图8.12．随机算法的批量大小（batch size）表示每次迭代所采用的样本数（即随机生成的下标 \\(i\\) 的个数).在横坐标中，每个时期（epoch）表示计算了 \\(N\\) 次分量函数 \\(f_{i}\\) 的梯度．可以看到，加入了动量之后，随机梯度法的收敛加快，但没有自适应类梯度方法快．值得注意的是，当批量大小从1变成10时，随机梯度法和动量方法达到相同精度需要的时期数变多，但大的批量大小对应的算法效率更高（计算梯度时矩阵乘法的并行效率以及内存利用率会更高).在自适应梯度类方法中，AdaGrad对该凸问题收敛最快，Adam次之．在a9a数据集上，RMSProp和AdaDelta在批量大小为1时尾部出现函数值上"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "471"
        },
        {
            "type": "image",
            "bbox": [
                0.177,
                0.161,
                0.435,
                0.32
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.221,
                0.33,
                0.386,
                0.343
            ],
            "angle": 0,
            "content": "(a) CINA数据集，批量大小1"
        },
        {
            "type": "image",
            "bbox": [
                0.476,
                0.162,
                0.734,
                0.32
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.528,
                0.33,
                0.681,
                0.343
            ],
            "angle": 0,
            "content": "(b) a9a 数据集，批量大小 1"
        },
        {
            "type": "image",
            "bbox": [
                0.177,
                0.353,
                0.435,
                0.511
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.218,
                0.522,
                0.391,
                0.534
            ],
            "angle": 0,
            "content": "(c) CINA数据集，批量大小10"
        },
        {
            "type": "image",
            "bbox": [
                0.476,
                0.353,
                0.734,
                0.511
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.525,
                0.522,
                0.685,
                0.534
            ],
            "angle": 0,
            "content": "(d) a9a 数据集，批量大小 10"
        },
        {
            "type": "image_caption",
            "bbox": [
                0.251,
                0.554,
                0.657,
                0.57
            ],
            "angle": 0,
            "content": "图8.12 使用次梯度法和罚函数法求解逻辑回归问题"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.6,
                0.737,
                0.639
            ],
            "angle": 0,
            "content": "升（尽管设置更小的 \\(\\alpha\\) 可以避免出现上升，但会大大降低目标函数值的下降速度），批量大小为10时算法产生的 \\(x\\) 对应的目标函数值更小."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.67,
                0.36,
                0.687
            ],
            "angle": 0,
            "content": "2. 多层感知机神经网络"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.703,
                0.739,
                0.762
            ],
            "angle": 0,
            "content": "多层感知机也叫全连接神经网络，是一种基本的网络结构，第1.4节已经简要地介绍了这一模型．考虑有 \\(L\\) 个隐藏层的多层感知机，给定输入\\(a\\in \\mathbb{R}^p\\) ，则多层感知机的输出可用如下迭代过程表示："
        },
        {
            "type": "equation",
            "bbox": [
                0.283,
                0.779,
                0.623,
                0.799
            ],
            "angle": 0,
            "content": "\\[\ny ^ {(l)} = t \\big (x ^ {(l)} y ^ {(l - 1)} + w ^ {(l)} \\big), \\quad l = 1, 2, \\dots , L + 1,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.815,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "其中 \\(x^{(l)} \\in \\mathbb{R}^{m_{l-1} \\times m_l}\\) 为系数矩阵，\\(w^{(l)} \\in \\mathbb{R}^{m_l}\\) 为非齐次项，\\(t(\\cdot)\\) 为非线性激活函数。该感知机的输出为 \\(y^{(L+1)}\\)。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "472"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.157,
                0.696,
                0.174
            ],
            "angle": 0,
            "content": "现在用非线性函数 \\(h(a;x)\\) 来表示该多层感知机，其中"
        },
        {
            "type": "equation",
            "bbox": [
                0.386,
                0.19,
                0.696,
                0.211
            ],
            "angle": 0,
            "content": "\\[\nx = \\left(x ^ {(1)}, x ^ {(2)}, \\dots , x ^ {(L)}, w ^ {(1)}, w ^ {(2)}, \\dots , w ^ {(L)}\\right)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.229,
                0.824,
                0.265
            ],
            "angle": 0,
            "content": "表示所有网络参数的集合，则学习问题可以表示成经验损失函数求极小问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.442,
                0.268,
                0.64,
                0.307
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad \\frac {1}{N} \\sum_ {i = 1} ^ {N} L \\left(h \\left(a _ {i}; x\\right), b _ {i}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.317,
                0.825,
                0.354
            ],
            "angle": 0,
            "content": "同样地，由于目标函数表示成了样本平均的形式，我们可以用随机梯度算法："
        },
        {
            "type": "equation",
            "bbox": [
                0.416,
                0.361,
                0.667,
                0.38
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\tau_ {k} \\nabla_ {x} L (h \\left(a _ {s _ {k}}; x ^ {k}\\right), b _ {s _ {k}}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.394,
                0.825,
                0.478
            ],
            "angle": 0,
            "content": "其中 \\(s_k\\) 为从 \\(\\{1,2,\\dots ,N\\}\\) 中随机抽取的一个样本．算法最核心的部分为求梯度，由于函数具有复合结构，因此可以采用后传算法．假定已经得到关于第 \\(l\\) 隐藏层的导数 \\(\\frac{\\partial L}{\\partial y^{(l)}}\\) ，然后可以通过下面递推公式得到关于第 \\(l\\) 隐藏层参数的导数以及关于前一个隐藏层的导数："
        },
        {
            "type": "equation",
            "bbox": [
                0.433,
                0.492,
                0.576,
                0.525
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\partial L}{\\partial w ^ {(l)}} = \\frac {\\partial L}{\\partial y ^ {(l)}} \\odot \\frac {\\partial t}{\\partial z},\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.436,
                0.529,
                0.823,
                0.563
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\partial L}{\\partial x ^ {(l)}} = \\left(\\frac {\\partial L}{\\partial y ^ {(l)}} \\odot \\frac {\\partial t}{\\partial z}\\right) \\left(y ^ {(l - 1)}\\right) ^ {\\mathrm {T}}, \\tag {8.7.19}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.421,
                0.566,
                0.652,
                0.601
            ],
            "angle": 0,
            "content": "\\[\n\\frac {\\partial L}{\\partial y ^ {(l - 1)}} = \\left(x ^ {(l)}\\right) ^ {\\mathrm {T}} \\left(\\frac {\\partial L}{\\partial y ^ {(l)}} \\odot \\frac {\\partial t}{\\partial z}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.614,
                0.651,
                0.631
            ],
            "angle": 0,
            "content": "其中 \\(\\odot\\) 为逐元素相乘. 完整的后传算法见算法 8.15."
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.649,
                0.405,
                0.664
            ],
            "angle": 0,
            "content": "算法8.15后传算法"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.67,
                0.412,
                0.687
            ],
            "angle": 0,
            "content": "1. \\(g\\gets \\nabla_{\\hat{y}}L(\\hat{y},b_{s_k})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.691,
                0.477,
                0.706
            ],
            "angle": 0,
            "content": "2. for \\( l = L + 1, L, \\dots, 1 \\). do"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.706,
                0.399,
                0.733
            ],
            "angle": 0,
            "content": "3. \\(g\\gets g\\odot \\frac{\\partial t}{\\partial z}.\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.733,
                0.384,
                0.762
            ],
            "angle": 0,
            "content": "4. \\(\\frac{\\partial L}{\\partial w^{(l)}} = g.\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.763,
                0.442,
                0.792
            ],
            "angle": 0,
            "content": "5. \\(\\frac{\\partial L}{\\partial x^{(l)}} = g(y^{(l - 1)})^{\\mathrm{T}}.\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.792,
                0.403,
                0.809
            ],
            "angle": 0,
            "content": "6. \\(g\\gets (x^{(l)})^{\\mathrm{T}}g.\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.272,
                0.814,
                0.349,
                0.827
            ],
            "angle": 0,
            "content": "7. end for"
        },
        {
            "type": "list",
            "bbox": [
                0.272,
                0.67,
                0.477,
                0.827
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "473"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.156,
                0.334,
                0.175
            ],
            "angle": 0,
            "content": "8.7.3 收敛性分析"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.187,
                0.741,
                0.287
            ],
            "angle": 0,
            "content": "随机梯度算法具有不确定性，这样的算法会有收敛性吗？本小节将比较细致地回答这一问题。概括来说，随机梯度下降法的收敛性依赖于步长的选取以及函数 \\( f \\) 本身的性质，在不同条件下会有不同结果。这一点和梯度法是类似的。我们将对目标函数分别为一般凸函数和可微强凸函数的情况进行讨论。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.311,
                0.482,
                0.328
            ],
            "angle": 0,
            "content": "1. 一般凸函数下随机梯度算法的收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.341,
                0.739,
                0.4
            ],
            "angle": 0,
            "content": "首先考虑每个 \\(f_{i}(x)\\) 是凸函数的情况，此时只假设 \\(f_{i}(x)\\) 存在次梯度。相应地，随机梯度下降法在这种情况下实际上是随机次梯度算法。在介绍具体定理之前，我们先列出算法和函数所满足的基本假设。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.412,
                0.547,
                0.429
            ],
            "angle": 0,
            "content": "假设8.4 对问题(8.7.2)使用迭代算法(8.7.4)时"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.442,
                0.492,
                0.459
            ],
            "angle": 0,
            "content": "(1) 每个 \\(f_{i}(x)\\) 是闭凸函数，存在次梯度；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.472,
                0.737,
                0.51
            ],
            "angle": 0,
            "content": "(2) 随机次梯度二阶矩是一致有界的, 即存在 \\(M\\), 对任意的 \\(x \\in \\mathbb{R}^n\\) 以及随机下标 \\(s_k\\), 有"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.442,
                0.737,
                0.51
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.523,
                0.625,
                0.544
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} _ {s _ {k}} \\left[ \\| g ^ {k} \\| ^ {2} \\right] \\leqslant M ^ {2} <   + \\infty , \\quad g ^ {k} \\in \\partial f _ {s _ {k}} \\left(x ^ {k}\\right);\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.561,
                0.737,
                0.6
            ],
            "angle": 0,
            "content": "(3) 迭代的随机点列 \\(\\{x^{k}\\}\\) 处处有界, 即 \\(\\| x^{k} - x^{*}\\| \\leqslant R, \\forall K\\), 其中 \\(x^{*}\\) 是问题 (8.7.2) 的最优解."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.613,
                0.559,
                0.63
            ],
            "angle": 0,
            "content": "在假设8.4的条件下，我们有如下重要的引理："
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.642,
                0.738,
                0.68
            ],
            "angle": 0,
            "content": "引理8.8在假设8.4的条件下，令 \\(\\{\\alpha_{k}\\}\\) 是任一正步长序列， \\(\\{x^k\\}\\) 是由随机次梯度法产生的序列，那么对所有的 \\(K\\geqslant 1\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.221,
                0.689,
                0.738,
                0.73
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {k = 1} ^ {K} \\alpha_ {k} \\mathbb {E} [ f (x ^ {k}) - f (x ^ {*}) ] \\leqslant \\frac {1}{2} \\mathbb {E} [ \\| x ^ {1} - x ^ {*} \\| ^ {2} ] + \\frac {1}{2} \\sum_ {k = 1} ^ {K} \\alpha_ {k} ^ {2} M ^ {2}. \\tag {8.7.20}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.739,
                0.655,
                0.759
            ],
            "angle": 0,
            "content": "证明．令 \\(\\bar{g}^k = \\mathbb{E}[g^k |x^k ]\\) ， \\(\\xi^{k} = g^{k} - \\bar{g}^{k}\\) ．由随机次梯度法的性质，"
        },
        {
            "type": "equation",
            "bbox": [
                0.364,
                0.77,
                0.543,
                0.791
            ],
            "angle": 0,
            "content": "\\[\n\\bar {g} ^ {k} = \\mathbb {E} \\left[ g ^ {k} \\mid x ^ {k} \\right] \\in \\partial f (x ^ {k}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.804,
                0.502,
                0.822
            ],
            "angle": 0,
            "content": "也就是说 \\(\\bar{g}^k\\) 就是次梯度．由次梯度的性质，"
        },
        {
            "type": "equation",
            "bbox": [
                0.343,
                0.835,
                0.567,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\langle \\bar {g} ^ {k}, x ^ {*} - x ^ {k} \\rangle \\leqslant f (x ^ {*}) - f (x ^ {k}).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "474"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.157,
                0.351,
                0.174
            ],
            "angle": 0,
            "content": "可以推导得"
        },
        {
            "type": "equation",
            "bbox": [
                0.279,
                0.181,
                0.825,
                0.346
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\frac {1}{2} \\left\\| x ^ {k + 1} - x ^ {*} \\right\\| ^ {2} \\\\ = \\frac {1}{2} \\left\\| x ^ {k} - \\alpha_ {k} g ^ {k} - x ^ {*} \\right\\| ^ {2} \\\\ = \\frac {1}{2} \\| x ^ {k} - x ^ {*} \\| ^ {2} + \\alpha_ {k} \\left\\langle g ^ {k}, x ^ {*} - x ^ {k} \\right\\rangle + \\frac {\\alpha_ {k} ^ {2}}{2} \\| g ^ {k} \\| ^ {2} \\tag {8.7.21} \\\\ = \\frac {1}{2} \\| x ^ {k} - x ^ {*} \\| ^ {2} + \\alpha_ {k} \\langle \\bar {g} ^ {k}, x ^ {*} - x ^ {k} \\rangle + \\frac {\\alpha_ {k} ^ {2}}{2} \\| g ^ {k} \\| ^ {2} + \\alpha_ {k} \\langle \\xi^ {k}, x ^ {*} - x ^ {k} \\rangle \\\\ \\leqslant \\frac {1}{2} \\| x ^ {k} - x ^ {*} \\| ^ {2} + \\alpha_ {k} (f (x ^ {*}) - f (x ^ {k})) + \\frac {\\alpha_ {k} ^ {2}}{2} \\| g ^ {k} \\| ^ {2} + \\alpha_ {k} \\langle \\xi^ {k}, x ^ {*} - x ^ {k} \\rangle . \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.352,
                0.752,
                0.371
            ],
            "angle": 0,
            "content": "注意到 \\(\\mathbb{E}[\\xi^k |x^k ] = \\mathbb{E}[g^k |x^k ] - \\bar{g}^k = 0\\) ，再利用条件期望的性质就有"
        },
        {
            "type": "equation",
            "bbox": [
                0.375,
                0.38,
                0.708,
                0.4
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ \\langle \\xi^ {k}, x ^ {*} - x ^ {k} \\rangle \\right] = \\mathbb {E} \\left[ \\mathbb {E} \\left[ \\langle \\xi^ {k}, x ^ {*} - x ^ {k} \\rangle | x _ {k} \\right] \\right] = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.411,
                0.525,
                0.428
            ],
            "angle": 0,
            "content": "对不等式(8.7.21)两端求期望就得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.438,
                0.825,
                0.493
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\alpha_ {k} \\mathbb {E} [ f (x ^ {k}) - f (x ^ {*}) ] \\\\ \\leqslant \\frac {1}{2} \\mathbb {E} \\left[ \\| x ^ {k} - x ^ {*} \\| ^ {2} \\right] - \\frac {1}{2} \\mathbb {E} \\left[ \\| x ^ {k + 1} - x ^ {*} \\| ^ {2} \\right] + \\frac {\\alpha_ {k} ^ {2}}{2} M ^ {2}. \\tag {8.7.22} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.499,
                0.405,
                0.515
            ],
            "angle": 0,
            "content": "两边对 \\(k\\) 求和即证"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.5,
                0.826,
                0.513
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.528,
                0.825,
                0.567
            ],
            "angle": 0,
            "content": "引理8.8没有直接说明收敛性，因为此时步长 \\(\\alpha_{k}\\) 的选取还是未知的．根据该引理，我们很容易得到随机次梯度算法在收缩步长下的收敛性."
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.576,
                0.825,
                0.631
            ],
            "angle": 0,
            "content": "定理8.17（随机次梯度算法的收敛性1）在假设8.4的条件下，令 \\(A_{K} = \\sum_{i=1}^{K} \\alpha_{i}\\)，定义 \\(\\bar{x}_{K} = \\frac{1}{A_{K}} \\sum_{k=1}^{K} \\alpha_{k} x^{k}\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.638,
                0.825,
                0.709
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ f \\left(\\bar {x} _ {K}\\right) - f \\left(x ^ {*}\\right) \\right] \\leqslant \\frac {R ^ {2} + \\sum_ {k = 1} ^ {K} \\alpha_ {k} ^ {2} M ^ {2}}{2 \\sum_ {k = 1} ^ {K} \\alpha_ {k}}. \\tag {8.7.23}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.715,
                0.607,
                0.733
            ],
            "angle": 0,
            "content": "证明. 由 \\(f(x)\\) 的凸性以及引理8.8立即得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.348,
                0.74,
                0.735,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} A _ {k} \\mathbb {E} [ f (\\bar {x} _ {K}) - f (x ^ {*}) ] \\leqslant \\sum_ {k = 1} ^ {K} \\alpha_ {k} \\mathbb {E} [ f (x ^ {k}) - f (x ^ {*}) ] \\\\ \\leqslant \\frac {1}{2} \\mathbb {E} [ \\| x ^ {1} - x ^ {*} \\| ^ {2} ] + \\frac {1}{2} \\sum_ {k = 1} ^ {K} \\alpha_ {k} ^ {2} M ^ {2} \\\\ = \\frac {R ^ {2} + \\sum_ {k = 1} ^ {K} \\alpha_ {k} ^ {2} M ^ {2}}{2}. \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "475"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.38,
                0.174
            ],
            "angle": 0,
            "content": "不等式两边同除以 \\(A_{K}\\) 得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.32,
                0.186,
                0.589,
                0.24
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ f (\\bar {x} _ {K}) - f (x ^ {*}) \\right] \\leqslant \\frac {R ^ {2} + \\sum_ {k = 1} ^ {K} \\alpha_ {k} ^ {2} M ^ {2}}{2 A _ {K}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.253,
                0.405,
                0.269
            ],
            "angle": 0,
            "content": "从定理8.17可以看到，当"
        },
        {
            "type": "equation",
            "bbox": [
                0.348,
                0.282,
                0.56,
                0.322
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {k = 1} ^ {\\infty} \\alpha_ {k} = + \\infty , \\quad \\frac {\\sum_ {k = 1} ^ {K} \\alpha_ {k} ^ {2}}{\\sum_ {k = 1} ^ {K} \\alpha_ {k}} \\rightarrow 0\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.335,
                0.739,
                0.395
            ],
            "angle": 0,
            "content": "时，随机次梯度算法收敛。对一个固定的步长 \\(\\alpha\\) ，不等式(8.7.23)右侧有一个不随 \\(K\\) 递减的常数，因此固定步长随机次梯度算法在函数值取期望意义下是不收敛的，它仅仅能找到一个次优解："
        },
        {
            "type": "equation",
            "bbox": [
                0.327,
                0.406,
                0.582,
                0.44
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\big [ f (\\bar {x} _ {K}) - f (x ^ {*}) \\big ] \\leqslant \\frac {R ^ {2}}{2 K \\alpha} + \\frac {\\alpha M ^ {2}}{2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.451,
                0.739,
                0.52
            ],
            "angle": 0,
            "content": "特别地，对于给定的迭代次数 \\(K\\) ，选取固定步长 \\(\\alpha = \\frac{R}{M\\sqrt{K}}\\) ，可以达到\\(\\mathcal{O}\\left(\\frac{1}{\\sqrt{K}}\\right)\\) 的精度，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.356,
                0.532,
                0.553,
                0.567
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ f \\left(\\bar {x} _ {K}\\right) - f \\left(x ^ {*}\\right) \\right] \\leqslant \\frac {R M}{\\sqrt {K}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.582,
                0.738,
                0.62
            ],
            "angle": 0,
            "content": "定理8.17的收敛性是在步长加权平均意义下的收敛性，在步长不增的情况下，我们可以得到直接平均意义下的收敛性。"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.637,
                0.738,
                0.692
            ],
            "angle": 0,
            "content": "定理8.18 (随机次梯度算法的收敛性2) 在假设8.4的条件下，令 \\(\\{\\alpha_{k}\\}\\) 是一个不增的正步长序列，\\(\\bar{x}_{K} = \\frac{1}{K}\\sum_{k=1}^{K}x^{k}\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.706,
                0.738,
                0.746
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ f \\left(\\bar {x} _ {K}\\right) - f \\left(x ^ {*}\\right) \\right] \\leqslant \\frac {R ^ {2}}{2 K \\alpha_ {K}} + \\frac {1}{2 K} \\sum_ {k = 1} ^ {K} \\alpha_ {k} M ^ {2}. \\tag {8.7.24}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.761,
                0.573,
                0.777
            ],
            "angle": 0,
            "content": "证明. 对定理 8.8 中的 (8.7.22) 式两边同除 \\(\\alpha_{k}\\), 就有"
        },
        {
            "type": "equation",
            "bbox": [
                0.193,
                0.79,
                0.714,
                0.823
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ f \\left(x ^ {k}\\right) - f \\left(x ^ {*}\\right) \\right] \\leqslant \\frac {1}{2 \\alpha_ {k}} \\mathbb {E} \\left[ \\| x ^ {k} - x ^ {*} \\| _ {2} ^ {2} \\right] - \\frac {1}{2 \\alpha_ {k}} \\mathbb {E} \\left[ \\| x ^ {k + 1} - x ^ {*} \\| _ {2} ^ {2} \\right] + \\frac {\\alpha_ {k}}{2} M ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.836,
                0.569,
                0.853
            ],
            "angle": 0,
            "content": "再对 \\(k\\) 求和，并且利用 \\(f(x)\\) 的凸性和 \\(\\alpha_{k}\\) 的单调性得"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "476"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "equation",
            "bbox": [
                0.334,
                0.183,
                0.746,
                0.348
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathbb {E} \\left[ f \\left(\\bar {x} _ {K}\\right) - f \\left(x ^ {*}\\right) \\right] \\leqslant \\frac {1}{K} \\sum_ {k = 1} ^ {K} \\mathbb {E} \\left[ f \\left(x ^ {k}\\right) - f \\left(x ^ {*}\\right) \\right] \\\\ \\leqslant \\frac {1}{2 K} \\left(\\frac {1}{\\alpha_ {1}} \\mathbb {E} [ \\| x ^ {1} - x ^ {*} \\| ^ {2} ] + \\sum_ {k = 1} ^ {K} \\alpha_ {k} M ^ {2} + \\right. \\\\ \\sum_ {k = 2} ^ {K} \\left(\\frac {1}{\\alpha_ {k}} - \\frac {1}{\\alpha_ {k - 1}}\\right) \\mathbb {E} [ \\| x ^ {k} - x ^ {*} \\| ^ {2} ] \\\\ \\leqslant \\frac {R}{2 K \\alpha_ {k}} + \\frac {1}{2 K} \\sum_ {k = 1} ^ {K} \\alpha_ {k} M ^ {2}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.36,
                0.676,
                0.376
            ],
            "angle": 0,
            "content": "注意该定理和定理8.17的不同之处在于 \\(\\bar{x}_K\\) 的定义"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.377,
                0.825,
                0.443
            ],
            "angle": 0,
            "content": "通过选取 \\(\\mathcal{O}\\left(\\frac{1}{\\sqrt{k}}\\right)\\) 阶数的步长，我们可以得到目标函数的收敛速度为\\(\\mathcal{O}\\left(\\frac{1}{\\sqrt{K}}\\right)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.45,
                0.674,
                0.486
            ],
            "angle": 0,
            "content": "推论8.4 在假设8.4的条件下，令 \\(\\alpha_{k} = \\frac{R}{M\\sqrt{k}}\\) ，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.44,
                0.495,
                0.824,
                0.531
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ f \\left(\\bar {x} _ {K}\\right) - f \\left(x ^ {*}\\right) \\right] \\leqslant \\frac {3 R M}{2 \\sqrt {K}}. \\tag {8.7.25}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.541,
                0.506,
                0.557
            ],
            "angle": 0,
            "content": "其中 \\(\\bar{x}_K\\) 的定义和定理8.18相同，"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.572,
                0.378,
                0.587
            ],
            "angle": 0,
            "content": "证明. 注意到"
        },
        {
            "type": "equation",
            "bbox": [
                0.437,
                0.586,
                0.645,
                0.625
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {k = 1} ^ {K} \\frac {1}{\\sqrt {k}} \\leqslant \\int_ {0} ^ {K} \\frac {1}{\\sqrt {t}} d t = 2 \\sqrt {K}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.63,
                0.538,
                0.665
            ],
            "angle": 0,
            "content": "将 \\(\\alpha_{k} = \\frac{R}{M\\sqrt{k}}\\) 代入式 (8.7.24) 就得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.675,
                0.728,
                0.716
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ f \\left(\\bar {x} _ {K}\\right) - f \\left(x ^ {*}\\right) \\right] \\leqslant \\frac {R ^ {2}}{2 K \\frac {R}{M \\sqrt {K}}} + \\frac {R M}{2 K} 2 \\sqrt {K} = \\frac {3 R M}{2 \\sqrt {K}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.727,
                0.825,
                0.811
            ],
            "angle": 0,
            "content": "前面的定理分析了随机次梯度算法在期望意义下的收敛性和收敛速度.我们可以发现它和非随机次梯度算法具有相同的收敛速度——\\(\\mathcal{O}\\left(\\frac{1}{\\sqrt{K}}\\right)\\).而随机次梯度算法每步的计算代价远小于非随机次梯度，这一定程度上解释了为什么随机算法在一些问题中的表现要远远好于非随机算法."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "很多情况下仅有期望的分析是不够的，我们需要更细致的刻画，下面主要讨论随机次梯度算法在依概率意义下的收敛性和收敛速度。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "477"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.157,
                0.739,
                0.195
            ],
            "angle": 0,
            "content": "定理8.19 选择推论8.4中的步长 \\(\\alpha_{k}\\)，使得 \\(\\mathbb{E}[f(\\bar{x}_K) - f(x^*)] \\to 0\\)，那么我们有依概率收敛 \\(f(\\bar{x}_K) - f(x^*) \\stackrel{P}{\\to} 0 (K \\to \\infty)\\)，即对任意的 \\(\\varepsilon > 0\\)，都有"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.207,
                0.738,
                0.231
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {K \\rightarrow \\infty} P \\left(f \\left(\\bar {x} _ {K}\\right) - f \\left(x ^ {*}\\right) \\geqslant \\varepsilon\\right) = 0. \\tag {8.7.26}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.239,
                0.447,
                0.255
            ],
            "angle": 0,
            "content": "证明．由马尔可夫不等式立即得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.27,
                0.262,
                0.639,
                0.293
            ],
            "angle": 0,
            "content": "\\[\nP \\left(f \\left(\\bar {x} _ {K}\\right) - f \\left(x ^ {*}\\right) \\geqslant \\varepsilon\\right) \\leqslant \\frac {1}{\\varepsilon} \\mathbb {E} \\left[ f \\left(\\bar {x} _ {K}\\right) - f \\left(x ^ {*}\\right)\\right]\\rightarrow 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.299,
                0.739,
                0.338
            ],
            "angle": 0,
            "content": "上面的定理只保证目标函数值是依概率收敛的，下面将进行更加细致的分析，给出其收敛速度."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.349,
                0.737,
                0.388
            ],
            "angle": 0,
            "content": "定理8.20 (随机次梯度算法的收敛性3) 在假设8.4的条件下，进一步假设对于所有的随机次梯度 \\(g\\) ，有 \\(\\| g\\| \\leqslant M\\) 。那么对任意的 \\(\\varepsilon >0\\)"
        },
        {
            "type": "equation",
            "bbox": [
                0.279,
                0.395,
                0.738,
                0.434
            ],
            "angle": 0,
            "content": "\\[\nf \\left(\\bar {x} _ {K}\\right) - f \\left(x ^ {*}\\right) \\leqslant \\frac {R ^ {2}}{2 K \\alpha_ {K}} + \\frac {1}{2 K} \\sum_ {k = 1} ^ {K} \\alpha_ {k} M ^ {2} + \\frac {R M}{\\sqrt {K}} \\varepsilon \\tag {8.7.27}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.444,
                0.737,
                0.482
            ],
            "angle": 0,
            "content": "以大于等于 \\(1 - e^{-\\frac{1}{2}\\varepsilon^2}\\) 的概率成立，其中步长列 \\(\\{\\alpha_k\\}\\) 是单调不增序列，\\(\\bar{x}_K\\) 的定义和定理8.18中的定义相同."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.493,
                0.737,
                0.532
            ],
            "angle": 0,
            "content": "证明. 令 \\(\\overline{g}^k = \\mathbb{E}[g^k |x^k], \\xi^k = g^k -\\overline{g}^k\\) ，在引理8.8中，由(8.7.21)式的推导过程我们已经得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.265,
                0.538,
                0.643,
                0.601
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (x ^ {k}) - f (x ^ {*}) \\leqslant \\frac {1}{2 \\alpha_ {k}} \\| x ^ {k} - x ^ {*} \\| ^ {2} - \\frac {1}{2 \\alpha_ {k}} \\| x ^ {k + 1} - x ^ {*} \\| ^ {2} \\\\ + \\frac {\\alpha_ {k}}{2} \\| g ^ {k} \\| ^ {2} + \\alpha_ {k} \\langle \\xi^ {k}, x ^ {*} - x ^ {k} \\rangle . \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.609,
                0.586,
                0.626
            ],
            "angle": 0,
            "content": "两边对 \\(k\\) 求和，并利用 \\(f(x)\\) 的凸性与 \\(\\alpha_{k}\\) 的单调性就有"
        },
        {
            "type": "equation",
            "bbox": [
                0.226,
                0.635,
                0.677,
                0.757
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} f (\\bar {x} _ {K}) - f (x ^ {*}) \\leqslant \\frac {1}{K} \\sum_ {k = 1} ^ {K} f (x ^ {k}) - f (x ^ {*}) \\\\ \\leqslant \\frac {R ^ {2}}{2 K \\alpha_ {K}} + \\frac {1}{2 K} \\sum_ {k = 1} ^ {K} \\alpha_ {k} \\| g ^ {k} \\| ^ {2} + \\frac {1}{K} \\sum_ {k = 1} ^ {K} \\langle \\xi^ {k}, x ^ {*} - x ^ {k} \\rangle \\\\ \\leqslant \\frac {R ^ {2}}{2 K \\alpha_ {K}} + \\frac {1}{2 K} \\sum_ {k = 1} ^ {K} \\alpha_ {k} M ^ {2} + \\frac {1}{K} \\sum_ {k = 1} ^ {K} \\langle \\zeta^ {k}, x ^ {*} - x ^ {k} \\rangle . \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.767,
                0.439,
                0.805
            ],
            "angle": 0,
            "content": "令 \\(\\omega = \\frac{R^2}{2K\\alpha_K} +\\frac{1}{2K}\\sum_{k = 1}^{K}\\alpha_kM^2\\) ，得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.214,
                0.813,
                0.738,
                0.857
            ],
            "angle": 0,
            "content": "\\[\nP \\left(f \\left(\\bar {x} _ {K}\\right) - f \\left(x ^ {*}\\right) - \\omega \\geqslant t\\right) \\leqslant P \\left(\\frac {1}{K} \\sum_ {k = 1} ^ {K} \\langle \\xi^ {k}, x ^ {*} - x ^ {k} \\rangle \\geqslant t\\right). \\tag {8.7.28}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "478"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.155,
                0.531,
                0.174
            ],
            "angle": 0,
            "content": "设 \\(Z^{k} = (x^{1},x^{2},\\dots ,x^{k + 1})\\) 因为"
        },
        {
            "type": "equation",
            "bbox": [
                0.371,
                0.187,
                0.711,
                0.206
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ \\xi^ {k} \\mid Z ^ {k - 1} \\right] = \\mathbb {E} \\left[ \\xi^ {k} \\mid x ^ {k} \\right] = 0, \\quad \\mathbb {E} \\left[ x ^ {k} \\mid Z ^ {k - 1} \\right] = x ^ {k},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.22,
                0.731,
                0.238
            ],
            "angle": 0,
            "content": "我们知道 \\(\\langle \\xi^k, x^* - x^k \\rangle\\) 是一个鞅差序列（见定义B.10）。同时由"
        },
        {
            "type": "equation",
            "bbox": [
                0.444,
                0.251,
                0.64,
                0.271
            ],
            "angle": 0,
            "content": "\\[\n\\| \\xi^ {k} \\| _ {2} = \\| g ^ {k} - \\overline {{g}} ^ {k} \\| _ {2} \\leqslant 2 M,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.285,
                0.298,
                0.301
            ],
            "angle": 0,
            "content": "推出"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.304,
                0.694,
                0.323
            ],
            "angle": 0,
            "content": "\\[\n\\left| \\langle \\xi^ {k}, x ^ {*} - x ^ {k} \\rangle \\right| \\leqslant \\| \\xi^ {k} \\| \\| x ^ {*} - x ^ {k} \\| _ {2} \\leqslant 2 M R,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.333,
                0.802,
                0.35
            ],
            "angle": 0,
            "content": "即 \\(\\langle \\xi_{k},x^{*} - x_{k}\\rangle\\) 有界．由Azuma-Hoeffding不等式（定理B.6），就得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.364,
                0.36,
                0.718,
                0.402
            ],
            "angle": 0,
            "content": "\\[\nP \\left(\\frac {1}{K} \\sum_ {k = 1} ^ {K} \\langle \\xi^ {k}, x ^ {*} - x ^ {k} \\rangle \\geqslant t\\right) \\leqslant \\exp \\left(- \\frac {K t ^ {2}}{2 M ^ {2} R ^ {2}}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.411,
                0.427,
                0.446
            ],
            "angle": 0,
            "content": "将 \\(t = \\frac{MR\\varepsilon}{\\sqrt{K}}\\) 代入，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.456,
                0.717,
                0.499
            ],
            "angle": 0,
            "content": "\\[\nP \\left(\\frac {1}{K} \\sum_ {k = 1} ^ {K} \\langle \\zeta^ {k}, x ^ {*} - x ^ {k} \\rangle \\geqslant \\frac {M R \\varepsilon}{\\sqrt {K}}\\right) \\leqslant \\exp \\left(- \\frac {\\varepsilon^ {2}}{2}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.507,
                0.468,
                0.525
            ],
            "angle": 0,
            "content": "结合(8.7.28)式，定理得证"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.538,
                0.825,
                0.586
            ],
            "angle": 0,
            "content": "定理8.20给出随机次梯度算法的收敛性更加细致的刻画．如果取 \\(\\alpha_{k} = \\frac{R}{\\sqrt{k}M}\\) 并令 \\(\\delta = e^{-\\frac{1}{2}\\varepsilon^2}\\) ，就有"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.595,
                0.737,
                0.648
            ],
            "angle": 0,
            "content": "\\[\nP \\left(f (\\bar {x} _ {K}) - f (x ^ {*}) \\leqslant \\frac {3 R M}{2 \\sqrt {K}} + \\frac {R M \\sqrt {2 \\ln \\frac {1}{\\delta}}}{\\sqrt {K}}\\right) \\geqslant 1 - \\delta .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.658,
                0.747,
                0.694
            ],
            "angle": 0,
            "content": "可以看到除一个很小的概率外，函数值以 \\(\\mathcal{O}\\left(\\frac{1}{\\sqrt{K}}\\right)\\) 的速度收敛."
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.71,
                0.585,
                0.727
            ],
            "angle": 0,
            "content": "2. 可微强凸函数下随机梯度算法的收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.74,
                0.825,
                0.838
            ],
            "angle": 0,
            "content": "在前面的讨论中, 我们知道对一般凸优化问题而言, 随机梯度下降法的收敛速度是 \\(\\mathcal{O}\\left(\\frac{1}{\\sqrt{K}}\\right)\\). 如果 \\(f(x)\\) 有更好的性质, 例如 \\(f(x)\\) 是可微强凸函数, 随机梯度下降法的收敛速度会有改善吗? 在本小节我们将指出: 若 \\(f(x)\\) 是可微强凸函数, 则随机梯度下降法的收敛速度可以提升到 \\(\\mathcal{O}\\left(\\frac{1}{K}\\right)\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.837,
                0.56,
                0.853
            ],
            "angle": 0,
            "content": "首先我们列出这一部分的统一假设"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "479"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.157,
                0.551,
                0.174
            ],
            "angle": 0,
            "content": "假设8.5 对问题(8.7.2)使用迭代算法(8.7.4)时，"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.186,
                0.513,
                0.202
            ],
            "angle": 0,
            "content": "(1) \\(f(x)\\) 是可微函数，每个 \\(f_{i}(x)\\) 梯度存在；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.215,
                0.551,
                0.232
            ],
            "angle": 0,
            "content": "(2) \\(f(x)\\) 是梯度利普希茨连续的，相应常数为 \\(L\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.245,
                0.465,
                0.262
            ],
            "angle": 0,
            "content": "(3) \\(f(x)\\) 是强凸函数, 强凸参数为 \\(\\mu\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.274,
                0.736,
                0.311
            ],
            "angle": 0,
            "content": "(4) 随机梯度二阶矩是一致有界的, 即存在 \\(M\\), 对任意的 \\(x \\in \\mathbb{R}^{n}\\) 以及随机下标 \\(s^{k}\\), 有"
        },
        {
            "type": "list",
            "bbox": [
                0.181,
                0.186,
                0.736,
                0.311
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.361,
                0.314,
                0.585,
                0.334
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} _ {s _ {k}} \\left[ \\| \\nabla f _ {s _ {k}} (x) \\| ^ {2} \\right] \\leqslant M ^ {2} <   + \\infty .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.347,
                0.661,
                0.364
            ],
            "angle": 0,
            "content": "下面的定理将给出随机梯度算法在固定步长下的收敛性分析."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.375,
                0.737,
                0.425
            ],
            "angle": 0,
            "content": "定理8.21（随机梯度算法的收敛性）在假设8.5的条件下，定义 \\(\\Delta_k = \\| x^k - x^* \\|\\)。对固定的步长 \\(\\alpha_k = \\alpha, 0 < \\alpha < \\frac{1}{2\\mu}\\)，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.192,
                0.432,
                0.737,
                0.468
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ f \\left(x ^ {K + 1}\\right) - f \\left(x ^ {*}\\right) \\right] \\leqslant \\frac {L}{2} \\mathbb {E} \\left[ \\Delta_ {K + 1} ^ {2} \\right] \\leqslant \\frac {L}{2} \\left[ (1 - 2 \\alpha \\mu) ^ {K} \\Delta_ {1} ^ {2} + \\frac {\\alpha M ^ {2}}{2 \\mu} \\right]. \\tag {8.7.29}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.475,
                0.469,
                0.492
            ],
            "angle": 0,
            "content": "证明．根据随机梯度算法的更新公式，"
        },
        {
            "type": "equation",
            "bbox": [
                0.23,
                0.503,
                0.677,
                0.573
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\Delta_ {k + 1} ^ {2} = \\left\\| x ^ {k + 1} - x ^ {*} \\right\\| ^ {2} = \\left\\| x ^ {k} - \\alpha_ {k} \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) - x ^ {*} \\right\\| ^ {2} \\\\ = \\| x ^ {k} - x ^ {*} \\| ^ {2} - 2 \\alpha_ {k} \\left\\langle \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right), x ^ {k} - x ^ {*} \\right\\rangle + \\alpha_ {k} ^ {2} \\| \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) \\| ^ {2} \\\\ = \\Delta_ {k} ^ {2} - 2 \\alpha_ {k} \\left\\langle \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right), x ^ {k} - x ^ {*} \\right\\rangle + \\alpha_ {k} ^ {2} \\| \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) \\| ^ {2}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.584,
                0.736,
                0.623
            ],
            "angle": 0,
            "content": "其中比较难处理的一项是 \\(\\left\\langle \\nabla f_{s_k}(x^k), x^k - x^*\\right\\rangle\\)，原因是 \\(s_k\\) 和 \\(x^k\\) 都具有随机性。由条件期望的性质 \\(\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X|Y]]\\)，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.265,
                0.635,
                0.643,
                0.752
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathbb {E} _ {s _ {1}, s _ {2}, \\dots , s _ {k}} [ \\langle \\nabla f _ {s _ {k}} (x ^ {k}), x ^ {k} - x ^ {*} \\rangle ] \\\\ = \\mathbb {E} _ {s _ {1}, s _ {2}, \\dots , s _ {k - 1}} \\left[ \\mathbb {E} _ {s _ {k}} \\left[ \\left\\langle \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right), x ^ {k} - x ^ {*} \\right\\rangle \\mid s _ {1}, \\dots , s _ {k - 1} \\right] \\right] \\\\ = \\mathbb {E} _ {s _ {1}, s _ {2}, \\dots , s _ {k - 1}} [ \\langle \\mathbb {E} _ {s _ {k}} [ \\nabla f _ {s _ {k}} (x _ {k}) | s _ {1}, s _ {2}, \\dots , s _ {k - 1} ], x ^ {k} - x ^ {*} \\rangle ] \\\\ = \\mathbb {E} _ {s _ {1}, s _ {2}, \\dots , s _ {k - 1}} [ \\langle \\nabla f (x ^ {k}), x ^ {k} - x ^ {*} \\rangle ] \\\\ = \\mathbb {E} _ {s _ {1}, s _ {2}, \\dots , s _ {k}} [ \\langle \\nabla f (x ^ {k}), x ^ {k} - x ^ {*} \\rangle ]. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.764,
                0.737,
                0.822
            ],
            "angle": 0,
            "content": "推导过程利用了 \\(x^{k}\\) 仅仅和 \\(s_{1}, s_{2}, \\cdots, s_{k-1}\\) 有关，因此固定 \\(s_{1}, s_{2}, \\cdots, s_{k-1}\\) 后 \\(x^{k}\\) 是一个常数。我们通过取期望的方式，将 \\(\\nabla f_{s_{k}}(x^{k})\\) 替换成了 \\(\\nabla f(x^{k})\\)。根据强凸函数的单调性，"
        },
        {
            "type": "equation",
            "bbox": [
                0.212,
                0.835,
                0.695,
                0.855
            ],
            "angle": 0,
            "content": "\\[\n\\left\\langle \\nabla f (x ^ {k}), x ^ {k} - x ^ {*} \\right\\rangle = \\left\\langle \\nabla f (x ^ {k}) - \\nabla f (x ^ {*}), x ^ {k} - x ^ {*} \\right\\rangle \\geqslant \\mu \\| x ^ {k} - x ^ {*} \\| ^ {2}.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "480"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.156,
                0.825,
                0.194
            ],
            "angle": 0,
            "content": "因此利用随机梯度二阶矩的一致有界性以及对 \\(\\langle \\nabla f(x^k),x^k -x^*\\rangle\\) 进行放缩可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.331,
                0.206,
                0.826,
                0.226
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} _ {s _ {1}, s _ {2}, \\dots , s _ {k}} \\left[ \\Delta_ {k + 1} ^ {2} \\right] \\leqslant (1 - 2 \\alpha \\mu) \\mathbb {E} _ {s _ {1}, s _ {2}, \\dots , s _ {k}} \\left[ \\Delta_ {k} ^ {2} \\right] + \\alpha^ {2} M ^ {2}. \\tag {8.7.30}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.238,
                0.417,
                0.253
            ],
            "angle": 0,
            "content": "对 \\(k\\) 做归纳，就得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.311,
                0.262,
                0.826,
                0.3
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} _ {s _ {1}, s _ {2}, \\dots , s _ {K}} \\left[ \\Delta_ {K + 1} ^ {2} \\right] \\leqslant (1 - 2 \\alpha \\mu) ^ {K} \\Delta_ {1} ^ {2} + \\sum_ {i = 0} ^ {K - 1} (1 - 2 \\alpha \\mu) ^ {i} \\alpha^ {2} M ^ {2}. \\tag {8.7.31}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.308,
                0.484,
                0.326
            ],
            "angle": 0,
            "content": "由定理条件， \\(0 <   2\\alpha \\mu <  1\\) 则，"
        },
        {
            "type": "equation",
            "bbox": [
                0.399,
                0.333,
                0.684,
                0.372
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i = 0} ^ {K - 1} (1 - 2 \\alpha \\mu) ^ {i} <   \\sum_ {i = 0} ^ {\\infty} (1 - 2 \\alpha \\mu) ^ {i} = \\frac {1}{2 \\alpha \\mu},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.38,
                0.298,
                0.396
            ],
            "angle": 0,
            "content": "所以"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.394,
                0.826,
                0.429
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} _ {s _ {1}, s _ {2}, \\dots , s _ {K}} \\left[ \\Delta_ {K + 1} ^ {2} \\right] \\leqslant (1 - 2 \\alpha \\mu) ^ {K} \\Delta_ {1} ^ {2} + \\frac {\\alpha M ^ {2}}{2 \\mu}. \\tag {8.7.32}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.433,
                0.825,
                0.492
            ],
            "angle": 0,
            "content": "注意，证明进行到这一步并没有利用 \\( f(x) \\) 梯度 \\( L \\)-利普希茨连续这一条件。我们已经能够得到迭代点列在期望意义下的一些收敛结果。如果再利用梯度 \\( L \\)-利普希茨连续函数的二次上界(2.2.3)，可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.323,
                0.499,
                0.758,
                0.528
            ],
            "angle": 0,
            "content": "\\[\nf \\left(x ^ {K + 1}\\right) - f \\left(x ^ {*}\\right) \\leqslant \\left\\langle \\nabla f \\left(x ^ {*}\\right), x ^ {K + 1} - x ^ {*} \\right\\rangle + \\frac {L}{2} \\| x ^ {K + 1} - x ^ {*} \\| ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.536,
                0.617,
                0.552
            ],
            "angle": 0,
            "content": "利用 \\(\\nabla f(x^{*}) = 0\\) 并对上式左右两边取期望可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.307,
                0.56,
                0.776,
                0.597
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ f \\left(x ^ {K + 1}\\right) - f \\left(x ^ {*}\\right) \\right] \\leqslant \\frac {L}{2} \\mathbb {E} \\left[ \\Delta_ {K + 1} ^ {2} \\right] \\leqslant \\frac {L}{2} \\left[ (1 - 2 \\alpha \\mu) ^ {K} \\Delta_ {1} ^ {2} + \\frac {\\alpha M ^ {2}}{2 \\mu} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.604,
                0.592,
                0.62
            ],
            "angle": 0,
            "content": "此即目标函数值在期望的意义下的收敛结果"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.633,
                0.825,
                0.698
            ],
            "angle": 0,
            "content": "可以看到，对于固定的步长，算法不能保证收敛，这是因为(8.7.32)的右端有不随 \\(K\\) 变化的常数。如果设置递减的步长，收敛速度可以达到 \\(\\mathcal{O}\\left(\\frac{1}{K}\\right)\\)，这个结论将在下面的定理中给出。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.709,
                0.825,
                0.745
            ],
            "angle": 0,
            "content": "定理8.22（随机梯度算法的收敛速度 \\([28]^{\\text{定理4.7}}\\)）在定理8.21的结果中，取递减的步长"
        },
        {
            "type": "equation",
            "bbox": [
                0.496,
                0.744,
                0.588,
                0.778
            ],
            "angle": 0,
            "content": "\\[\n\\alpha_ {k} = \\frac {\\beta}{k + \\gamma},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.781,
                0.757,
                0.816
            ],
            "angle": 0,
            "content": "其中 \\(\\beta >\\frac{1}{2\\mu},\\gamma >0\\) ，使得 \\(\\alpha_{1}\\leqslant \\frac{1}{2\\mu}\\) 那么对于任意的 \\(k\\geqslant 1\\) ，都有"
        },
        {
            "type": "equation",
            "bbox": [
                0.394,
                0.822,
                0.826,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ f \\left(x ^ {k}\\right) - f \\left(x ^ {*}\\right) \\right] \\leqslant \\frac {L}{2} \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] \\leqslant \\frac {L}{2} \\frac {v}{\\gamma + k}, \\tag {8.7.33}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.317,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "481"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.155,
                0.452,
                0.191
            ],
            "angle": 0,
            "content": "这里 \\(v = \\max \\left\\{\\frac{\\beta^2M^2}{2\\beta\\mu - 1},(\\gamma +1)\\Delta_1^2\\right\\} .\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.204,
                0.398,
                0.22
            ],
            "angle": 0,
            "content": "证明. 定理8.21已经证明了"
        },
        {
            "type": "equation",
            "bbox": [
                0.268,
                0.238,
                0.638,
                0.258
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} _ {s _ {1}, s _ {2}, \\dots , s _ {k}} \\left[ \\Delta_ {k + 1} ^ {2} \\right] \\leqslant \\left(1 - 2 \\alpha_ {k} \\mu\\right) \\mathbb {E} _ {s _ {1}, s _ {2}, \\dots , s _ {k}} \\left[ \\Delta_ {k} ^ {2} \\right] + \\alpha_ {k} ^ {2} M ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.276,
                0.737,
                0.338
            ],
            "angle": 0,
            "content": "我们采用数学归纳法证明(8.7.33)式．当 \\(k = 1\\) 时，由 \\(\\mathcal{V}\\) 的定义知(8.7.33)式成立．现假设该式对 \\(k\\) 成立，为了记号简便定义 \\(\\hat{k} = \\gamma + k\\) ，则 \\(\\alpha_{k} = \\frac{\\beta}{\\hat{k}}\\) 由归纳假设，"
        },
        {
            "type": "equation",
            "bbox": [
                0.306,
                0.351,
                0.599,
                0.494
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathbb {E} \\left[ \\Delta_ {k + 1} ^ {2} \\right] \\leqslant \\left(1 - \\frac {2 \\beta \\mu}{\\hat {k}}\\right) \\frac {v}{\\hat {k}} + \\frac {\\beta^ {2} M ^ {2}}{\\hat {k} ^ {2}} \\\\ = \\frac {\\hat {k} - 2 \\beta \\mu}{\\hat {k} ^ {2}} v + \\frac {\\beta^ {2} M ^ {2}}{\\hat {k} ^ {2}} \\\\ = \\frac {\\hat {k} - 1}{\\hat {k} ^ {2}} v - \\frac {2 \\beta \\mu - 1}{\\hat {k} ^ {2}} v + \\frac {\\beta^ {2} M ^ {2}}{\\hat {k} ^ {2}} \\\\ \\leqslant \\frac {v}{\\hat {k} + 1}, \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.507,
                0.658,
                0.525
            ],
            "angle": 0,
            "content": "最后一个不等式用到了 \\(v\\) 的定义．所以(8.7.33)式对 \\(k + 1\\) 也成立"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.551,
                0.739,
                0.713
            ],
            "angle": 0,
            "content": "本小节分析了次梯度算法和梯度下降法的普通版本和随机版本的收敛速度，它们的算法复杂度如表8.1所示．这里复杂度是指计算梯度的次数，\\(\\varepsilon\\) 代表希望达到的精度，\\(N\\) 表示样本数量．对于普通梯度下降方法，每一次迭代都需要计算 \\(N\\) 次梯度，而随机算法只需要计算一次．我们可以看到次梯度算法的普通版本和随机版本的收敛速度并没有差别，而每步的计算量能大大降低．但是在可微光滑和强凸情形下，随机梯度算法的收敛速度要慢于梯度算法．下一小节将讨论产生这一差别的原因，并介绍方差减小技术来改进它."
        },
        {
            "type": "table_caption",
            "bbox": [
                0.327,
                0.731,
                0.58,
                0.748
            ],
            "angle": 0,
            "content": "表 8.1 梯度下降法的算法复杂度"
        },
        {
            "type": "table",
            "bbox": [
                0.192,
                0.76,
                0.719,
                0.851
            ],
            "angle": 0,
            "content": "<table><tr><td></td><td>f凸(次梯度算法)</td><td>f可微强凸</td><td>f可微强凸且L-光滑</td></tr><tr><td>随机算法</td><td>O(1/ε2)</td><td>O(1/ε)</td><td>O(1/ε)</td></tr><tr><td>普通算法</td><td>O(N/ε2)</td><td>O(N/ε)</td><td>O(Nln(1/ε))</td></tr></table>"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "482"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.156,
                0.44,
                0.174
            ],
            "angle": 0,
            "content": "8.7.4 方差减小技术"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.187,
                0.825,
                0.293
            ],
            "angle": 0,
            "content": "上一小节证明了在次梯度情形以及 \\(f(x)\\) 可微强凸的条件下, 随机梯度方法的算法复杂度要小于普通的梯度法. 但是我们也发现随机方法容易受到梯度估计噪声的影响, 这使得它在固定步长下不能收敛, 并且在递减步长下也只能达到 R-次线性收敛 \\(\\mathcal{O}\\left(\\frac{1}{k}\\right)\\), 而普通梯度法在强凸且梯度 \\(L\\)-利普希茨连续的条件下可以达到 Q-线性收敛速度. 下面分析这两种算法的主要区别."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.297,
                0.586,
                0.313
            ],
            "angle": 0,
            "content": "在假设8.5的条件下，对梯度下降法有"
        },
        {
            "type": "equation",
            "bbox": [
                0.334,
                0.324,
                0.824,
                0.416
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\Delta_ {k + 1} ^ {2} = \\left\\| x ^ {k + 1} - x ^ {*} \\right\\| ^ {2} = \\left\\| x ^ {k} - \\alpha \\nabla f (x ^ {k}) - x ^ {*} \\right\\| ^ {2} \\\\ = \\Delta_ {k} ^ {2} - 2 \\alpha \\left\\langle \\nabla f \\left(x ^ {k}\\right), x ^ {k} - x ^ {*} \\right\\rangle + \\alpha^ {2} \\| \\nabla f \\left(x ^ {k}\\right) \\| ^ {2} \\tag {8.7.34} \\\\ \\leqslant (1 - 2 \\alpha \\mu) \\Delta_ {k} ^ {2} + \\alpha^ {2} \\| \\nabla f (x ^ {k}) \\| _ {2} ^ {2} \\quad (\\mu - \\text {强 凸}) \\\\ \\leqslant \\left(1 - 2 \\alpha \\mu + \\alpha^ {2} L ^ {2}\\right) \\Delta_ {k} ^ {2}. \\quad (L - \\text {光 滑}) \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.427,
                0.812,
                0.443
            ],
            "angle": 0,
            "content": "对随机梯度下降法，利用条件期望的性质（具体细节读者可自行验证）有"
        },
        {
            "type": "equation",
            "bbox": [
                0.293,
                0.454,
                0.791,
                0.595
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathbb {E} \\left[ \\Delta_ {k + 1} ^ {2} \\right] = \\mathbb {E} \\left[ \\| x ^ {k + 1} - x ^ {*} \\| _ {2} ^ {2} \\right] = \\mathbb {E} \\left[ \\| x ^ {k} - \\alpha \\nabla f _ {s _ {k}} (x ^ {k}) - x ^ {*} \\| \\right] ^ {2} \\\\ = \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] - 2 \\alpha \\mathbb {E} \\left[ \\langle \\nabla f _ {s _ {k}} (x ^ {k}), x ^ {k} - x ^ {*} \\rangle \\right] + \\alpha^ {2} \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} (x ^ {k}) \\| ^ {2} \\right] \\\\ = \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] - 2 \\alpha \\mathbb {E} \\left[ \\left\\langle \\nabla f (x ^ {k}), x ^ {k} - x ^ {*} \\right\\rangle \\right] + \\alpha^ {2} \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} (x ^ {k}) \\| ^ {2} \\right] \\\\ \\leqslant (1 - 2 \\alpha \\mu) \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] + \\alpha^ {2} \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) \\| ^ {2} \\right] \\quad (\\mu - \\text {强 凸}) \\\\ = (1 - 2 \\alpha \\mu) \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] + \\alpha^ {2} \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) - \\nabla f \\left(x ^ {k}\\right) + \\nabla f \\left(x ^ {k}\\right) \\| ^ {2} \\right] \\\\ \\leqslant \\left(1 - 2 \\alpha \\mu + \\alpha^ {2} L ^ {2}\\right) \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] + \\alpha^ {2} \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} (x ^ {k}) - \\nabla f (x ^ {k}) \\| ^ {2} \\right], \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.606,
                0.298,
                0.621
            ],
            "angle": 0,
            "content": "也即"
        },
        {
            "type": "equation",
            "bbox": [
                0.273,
                0.634,
                0.825,
                0.671
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ \\Delta_ {k + 1} ^ {2} \\right] \\leqslant \\underbrace {(1 - 2 \\alpha \\mu + \\alpha^ {2} L ^ {2}) \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right]} _ {A} + \\underbrace {\\alpha^ {2} \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) - \\nabla f \\left(x ^ {k}\\right) \\| ^ {2} \\right]} _ {B}. \\tag {8.7.35}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.679,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "从上面的分析中可以看到两种算法的主要差别就在 \\(B\\) 项上，也就是梯度估计的某种方差。它导致了随机梯度算法只能有 \\(\\mathcal{O}\\left(\\frac{1}{k}\\right)\\) 的收敛速度。但是在许多机器学习的应用中，随机梯度算法的真实的收敛速度可能会更快一些。这主要是因为许多应用对解的精度要求并没有太高，而在开始部分方差较小，即有 \\(B \\ll A\\)，那么我们会观察到近似 Q-线性收敛速度；而随着迭代步数增多，方差逐渐增大，算法最终的收敛速度为 \\(\\mathcal{O}\\left(\\frac{1}{k}\\right)\\)。所以为了能获得比较快的渐进收敛速度，我们的主要目标就是减少方差项 \\(B\\) 来加快随机梯度算法的收敛速度。下面将要介绍三种减小方差的算法："
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "483"
        },
        {
            "type": "text",
            "bbox": [
                0.194,
                0.158,
                0.474,
                0.174
            ],
            "angle": 0,
            "content": "SAG (stochastic average gradient)"
        },
        {
            "type": "text",
            "bbox": [
                0.194,
                0.186,
                0.273,
                0.201
            ],
            "angle": 0,
            "content": "SAGA"
        },
        {
            "type": "text",
            "bbox": [
                0.194,
                0.217,
                0.555,
                0.233
            ],
            "angle": 0,
            "content": "- SVRG (stochastic variance reduced gradient)"
        },
        {
            "type": "list",
            "bbox": [
                0.194,
                0.158,
                0.555,
                0.233
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.244,
                0.737,
                0.282
            ],
            "angle": 0,
            "content": "相对于普通梯度算法使用更多的样本点，这些算法的基本思想都是通过利用之前计算得到的信息来减小方差，最终获得Q-线性收敛速度."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.305,
                0.395,
                0.323
            ],
            "angle": 0,
            "content": "1. SAG算法和SAGA算法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.335,
                0.739,
                0.457
            ],
            "angle": 0,
            "content": "在随机梯度下降法中，每一步迭代仅仅使用了当前点的随机梯度，而迭代计算的历史随机梯度则直接丢弃不再使用。当迭代接近收敛时，上一步的随机梯度同样也是当前迭代点处梯度的一个很好的估计。随机平均梯度法(SAG)就是基于这一想法构造的。在迭代中，SAG算法记录所有之前计算过的随机梯度，再与当前新计算的随机梯度求平均，最终作为下一步的梯度估计。具体来说，SAG算法在内存中开辟了存储 \\(N\\) 个随机梯度的空间"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.467,
                0.512,
                0.487
            ],
            "angle": 0,
            "content": "\\[\n\\left[ g _ {1} ^ {k}, g _ {2} ^ {k}, \\dots , g _ {N} ^ {k} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.499,
                0.738,
                0.579
            ],
            "angle": 0,
            "content": "分别用于记录和第 \\(i\\) 个样本相关的最新的随机梯度。在第 \\(k\\) 步更新时，若抽取的样本点下标为 \\(s_k\\)，则计算随机梯度后将 \\(g_{s_k}^k\\) 的值更新为当前的随机梯度值，而其他未抽取到的下标对应的 \\(g_i^k\\) 保持不变。每次SAG算法更新使用的梯度方向是所有 \\(g_i^k\\) 的平均值。它的数学迭代格式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.374,
                0.587,
                0.738,
                0.626
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\frac {\\alpha_ {k}}{N} \\sum_ {i = 1} ^ {N} g _ {i} ^ {k}, \\tag {8.7.36}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.634,
                0.338,
                0.653
            ],
            "angle": 0,
            "content": "其中 \\(g_{i}^{k}\\) 的更新方式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.356,
                0.661,
                0.737,
                0.711
            ],
            "angle": 0,
            "content": "\\[\ng _ {i} ^ {k} = \\left\\{ \\begin{array}{l l} \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right), & i = s _ {k}, \\\\ g _ {i} ^ {k - 1}, & \\text {其 他}, \\end{array} \\right. \\tag {8.7.37}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.721,
                0.739,
                0.78
            ],
            "angle": 0,
            "content": "这里 \\(s_k\\) 是第 \\(k\\) 次迭代随机抽取的样本。由 \\(g_i^k\\) 的更新方式不难发现，每次迭代时只有一个 \\(g_i^k\\) 的内容发生了改变，而其他的 \\(g_i^k\\) 是直接沿用上一步的值。因此SAG迭代公式还可以写成"
        },
        {
            "type": "equation",
            "bbox": [
                0.231,
                0.786,
                0.738,
                0.83
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\alpha_ {k} \\left(\\frac {1}{N} \\left(\\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) - g _ {s _ {k}} ^ {k - 1}\\right) + \\frac {1}{N} \\sum_ {i = 1} ^ {N} g _ {i} ^ {k - 1}\\right), \\tag {8.7.38}\n\\]"
        },
        {
            "type": "page_footnote",
            "bbox": [
                0.195,
                0.839,
                0.574,
                0.852
            ],
            "angle": 0,
            "content": "2和SAG算法不同，SAGA算法名字本身并不是四个英文单词的缩写"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.119,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "484"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.156,
                0.825,
                0.235
            ],
            "angle": 0,
            "content": "其中 \\(g_{i}^{k}\\) 的更新方式如(8.7.37)式。在SAG算法中 \\(\\{g_{i}^{k}\\}\\) 的初值可简单地取为零向量或中心化的随机梯度向量，我们不加证明地指出，SAG算法每次使用的随机梯度的条件期望并不是真实梯度 \\(\\nabla f(x^{k})\\) ，但随着迭代进行，随机梯度的期望和真实梯度的偏差会越来越小。"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.24,
                0.604,
                0.256
            ],
            "angle": 0,
            "content": "接下来直接给出SAG算法的收敛性分析"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.267,
                0.823,
                0.313
            ],
            "angle": 0,
            "content": "定理8.23 (SAG算法的收敛性[168]) 在假设8.5的条件下，取固定步长 \\(\\alpha_{k} = \\frac{1}{16L}\\)，\\(g_{i}^{k}\\) 的初值取为零向量，则对任意的 \\(k\\)，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.361,
                0.318,
                0.721,
                0.354
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ f \\left(x ^ {k}\\right) \\right] - f \\left(x ^ {*}\\right) \\leqslant \\left(1 - \\min  \\left\\{\\frac {\\mu}{1 6 L}, \\frac {1}{8 N} \\right\\}\\right) ^ {k} C _ {0},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.36,
                0.501,
                0.376
            ],
            "angle": 0,
            "content": "其中常数 \\(C_0\\) 为与 \\(k\\) 无关的常数"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.386,
                0.825,
                0.464
            ],
            "angle": 0,
            "content": "上述定理表明SAG算法确实有Q-线性收敛速度．但是SAG算法的缺点在于需要存储 \\(N\\) 个梯度向量，当样本量 \\(N\\) 很大时，这无疑是一个很大的开销．因此SAG算法在实际中很少使用，它的主要价值在于算法的思想．很多其他实用算法都是根据SAG算法变形而来的."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.469,
                0.825,
                0.527
            ],
            "angle": 0,
            "content": "SAGA算法是SAG算法的一个修正。我们知道SAG算法每一步的随机梯度的条件期望并不是真实梯度，这其实是一个缺陷。SAGA算法则使用无偏的梯度向量作为更新方向，它的迭代方式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.335,
                0.533,
                0.825,
                0.577
            ],
            "angle": 0,
            "content": "\\[\nx ^ {k + 1} = x ^ {k} - \\alpha_ {k} \\left(\\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) - g _ {s _ {k}} ^ {k - 1} + \\frac {1}{N} \\sum_ {i = 1} ^ {N} g _ {i} ^ {k - 1}\\right). \\tag {8.7.39}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.584,
                0.825,
                0.627
            ],
            "angle": 0,
            "content": "对比(8.7.38)式可以发现，SAGA算法去掉了 \\(\\nabla f_{s_k}(x^k) - g_{s_k}^{k-1}\\) 前面的系数 \\(\\frac{1}{N}\\). 可以证明每次迭代使用的梯度方向都是无偏的，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.633,
                0.722,
                0.676
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ \\nabla f _ {s _ {k}} (x ^ {k}) - g _ {s _ {k}} ^ {k - 1} + \\frac {1}{N} \\sum_ {i = 1} ^ {N} g _ {i} ^ {k - 1} \\mid x ^ {k} \\right] = \\nabla f (x ^ {k}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.291,
                0.682,
                0.759,
                0.699
            ],
            "angle": 0,
            "content": "SAGA算法同样有Q-线性收敛速度，实际上我们有如下结果："
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.709,
                0.824,
                0.758
            ],
            "angle": 0,
            "content": "定理8.24（SAGA算法的收敛性[55]）在假设8.5的条件下，取固定步长 \\(\\alpha_{k} = \\frac{1}{2(\\mu N + L)}\\) 定义 \\(\\Delta_k = \\| x^k -x^*\\|\\) ，则对任意的 \\(k\\geqslant 1\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.3,
                0.764,
                0.825,
                0.803
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] \\leqslant \\left(1 - \\frac {\\mu}{2 (\\mu N + L)}\\right) ^ {k} \\left(\\Delta_ {1} ^ {2} + \\frac {N \\left(f \\left(x ^ {1}\\right) - f \\left(x ^ {*}\\right)\\right)}{\\mu N + L}\\right). \\tag {8.7.40}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.809,
                0.825,
                0.852
            ],
            "angle": 0,
            "content": "如果强凸的参数 \\(\\mu\\) 是未知的，也可以取 \\(\\alpha = \\frac{1}{3L}\\)，此时有类似的收敛结果."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "485"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.157,
                0.296,
                0.174
            ],
            "angle": 0,
            "content": "2. SVRG算法"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.187,
                0.744,
                0.286
            ],
            "angle": 0,
            "content": "与SAG算法和SAGA算法不同，SVRG算法通过周期性缓存全梯度的方法来减小方差．具体做法是在随机梯度下降方法中，每经过 \\(m\\) 次迭代就设置一个检查点，计算一次全梯度，在之后的 \\(m\\) 次迭代中，将这个全梯度作为参考点来达到减小方差的目的．令 \\(\\tilde{x}^j\\) 是第 \\(j\\) 个检查点，则我们需要计算点 \\(\\tilde{x}^j\\) 处的全梯度"
        },
        {
            "type": "equation",
            "bbox": [
                0.361,
                0.286,
                0.547,
                0.323
            ],
            "angle": 0,
            "content": "\\[\n\\nabla f (\\tilde {x} ^ {j}) = \\frac {1}{N} \\sum_ {i = 1} ^ {N} \\nabla f _ {i} (\\tilde {x} ^ {j}),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.33,
                0.501,
                0.347
            ],
            "angle": 0,
            "content": "在之后的迭代中使用方向 \\(v^k\\) 作为更新方向："
        },
        {
            "type": "equation",
            "bbox": [
                0.31,
                0.361,
                0.737,
                0.38
            ],
            "angle": 0,
            "content": "\\[\nv ^ {k} = \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) - \\left(\\nabla f _ {s _ {k}} \\left(\\tilde {x} ^ {j}\\right) - \\nabla f \\left(\\tilde {x} ^ {j}\\right)\\right), \\tag {8.7.41}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.395,
                0.741,
                0.434
            ],
            "angle": 0,
            "content": "其中 \\(s_k \\in \\{1, 2, \\dots, N\\}\\) 是随机选取的一个样本. 注意到给定 \\(s_1, s_2, \\dots, s_{k-1}\\) 时 \\(x^k, \\tilde{x}^j\\) 均为定值，由 \\(v^k\\) 的表达式可知"
        },
        {
            "type": "equation",
            "bbox": [
                0.171,
                0.444,
                0.735,
                0.489
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathbb {E} \\big [ v ^ {k} | s _ {1}, s _ {2}, \\dots , s _ {k - 1} \\big ] = \\mathbb {E} \\big [ \\nabla f _ {s _ {k}} (x ^ {k}) | x ^ {k} \\big ] - \\mathbb {E} \\big [ \\nabla f _ {s _ {k}} (\\tilde {x} ^ {j}) - \\nabla f (\\tilde {x} ^ {j}) | s _ {1}, s _ {2}, \\dots , s _ {k - 1} \\big ] \\\\ \\mathbf {\\nabla} = \\nabla f (\\boldsymbol {x} ^ {k}) - 0 = \\nabla f (\\boldsymbol {x} ^ {k}), \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.501,
                0.738,
                0.581
            ],
            "angle": 0,
            "content": "即 \\(v^{k}\\) 在条件期望的意义下是 \\(\\nabla f(x^{k})\\) 的一个无偏估计．公式(8.7.41)有简单的直观理解.事实上，我们希望用 \\(\\nabla f_{s_k}(\\tilde{x}^j)\\) 去估计 \\(\\nabla f(\\tilde{x}^j)\\) ，那么 \\(\\nabla f_{s_k}(\\tilde{x}^j) - \\nabla f(\\tilde{x}^j)\\) 就可以看作梯度估计的误差，所以在每一步随机梯度迭代用该项来对 \\(\\nabla f_{s_k}(x^k)\\) 做一个校正."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.584,
                0.741,
                0.622
            ],
            "angle": 0,
            "content": "接下来分析方差，并通过数学的方法说明为什么选取参考点会使得估计的方差变小。这里需要作一个额外的假设"
        },
        {
            "type": "equation",
            "bbox": [
                0.27,
                0.637,
                0.638,
                0.656
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| \\nabla f _ {i} (x) - \\nabla f _ {i} (y) \\right\\| \\leqslant L \\| x - y \\|, \\quad i = 1, 2, \\dots , N,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.67,
                0.739,
                0.709
            ],
            "angle": 0,
            "content": "也即 \\(f(x)\\) 的每一个子函数都是梯度 \\(L\\)-利普希茨连续的．为记号方便，令\\(y = \\tilde{x}^j\\) ， \\(x^{*}\\) 为 \\(f(x)\\) 的最小值点， \\(\\Delta_{k} = \\| x^{k} - x^{*}\\|\\) 为 \\(x^{k}\\) 与 \\(x^{*}\\) 的距离，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.19,
                0.721,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathbb {E} \\left[ \\| v ^ {k} \\| ^ {2} \\right] = \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} (x ^ {k}) - (\\nabla f _ {s _ {k}} (y) - \\nabla f (y)) \\| ^ {2} \\right] \\\\ = \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} (x ^ {k}) - \\nabla f _ {s _ {k}} (y) + \\nabla f (y) + \\nabla f _ {s _ {k}} (x ^ {*}) - \\nabla f _ {s _ {k}} (x ^ {*}) \\| ^ {2} \\right] \\\\ \\leqslant 2 \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} (x ^ {k}) - \\nabla f _ {s _ {k}} (x ^ {*}) \\| ^ {2} \\right] + 2 \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} (y) - \\nabla f (y) - \\nabla f _ {s _ {k}} (x ^ {*}) \\| ^ {2} \\right] \\\\ \\leqslant 2 L ^ {2} \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] + 2 \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} (y) - \\nabla f _ {s _ {k}} (x ^ {*}) \\| ^ {2} \\right] \\\\ \\leqslant 2 L ^ {2} \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] + 2 L ^ {2} \\mathbb {E} \\left[ \\| y - x ^ {*} \\| ^ {2} \\right]. \\tag {8.7.42} \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "486"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.156,
                0.823,
                0.194
            ],
            "angle": 0,
            "content": "其中第一个不等式是因为 \\(\\| a + b\\| ^2\\leqslant 2\\| a\\| ^2 +2\\| b\\| ^2\\) ，第二个不等式使用了有关二阶矩的不等式"
        },
        {
            "type": "equation",
            "bbox": [
                0.439,
                0.213,
                0.644,
                0.234
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ \\| \\xi - \\mathbb {E} \\xi \\| ^ {2} \\right] \\leqslant \\mathbb {E} \\left[ \\| \\xi \\| ^ {2} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.252,
                0.825,
                0.311
            ],
            "angle": 0,
            "content": "从上面的不等式可以看出，如果 \\(x^{k}\\) 和 \\(y\\) 非常接近 \\(x^{*}\\)，梯度估计的方差就很小。显然频繁地更新 \\(y\\) 可以使得方差更小，但是这样同时也增加了计算全梯度的次数。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.317,
                0.826,
                0.356
            ],
            "angle": 0,
            "content": "算法 8.16 中给出了完整的 SVRG 算法，在这里注意为了之后推导收敛性的方便，我们将 SVRG 算法写成了二重循环的形式。在 SVRG 方法中，只"
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.374,
                0.421,
                0.39
            ],
            "angle": 0,
            "content": "算法8.16 SVRG算法"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.396,
                0.521,
                0.413
            ],
            "angle": 0,
            "content": "1. 给定 \\(\\tilde{x}^0\\) ，步长 \\(\\alpha\\) ，更新次数 \\(m\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.416,
                0.442,
                0.434
            ],
            "angle": 0,
            "content": "2. 计算全梯度 \\(\\nabla f(x^0)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.438,
                0.44,
                0.455
            ],
            "angle": 0,
            "content": "3. for \\(j = 1,2,\\dots ,J\\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.457,
                0.488,
                0.475
            ],
            "angle": 0,
            "content": "4. 赋值 \\( y = \\tilde{x}^{j - 1}, x^1 = \\tilde{x}^{j - 1} \\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.479,
                0.453,
                0.497
            ],
            "angle": 0,
            "content": "5. 计算全梯度 \\(\\nabla f(y)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.5,
                0.465,
                0.516
            ],
            "angle": 0,
            "content": "6. for \\(k = 1,2,\\dots ,m\\) do"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.521,
                0.53,
                0.538
            ],
            "angle": 0,
            "content": "7. 随机选取 \\(s_k \\in \\{1, 2, \\dots, N\\}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.54,
                0.632,
                0.559
            ],
            "angle": 0,
            "content": "8. 计算 \\(v^{k} = \\nabla f_{s_{k}}(x^{k}) - (\\nabla f_{s_{k}}(y) - \\nabla f(y))\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.271,
                0.561,
                0.483,
                0.578
            ],
            "angle": 0,
            "content": "9. 更新 \\(x^{k + 1} = x^k -\\alpha v^k\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.266,
                0.584,
                0.368,
                0.597
            ],
            "angle": 0,
            "content": "10. end for"
        },
        {
            "type": "text",
            "bbox": [
                0.266,
                0.597,
                0.493,
                0.63
            ],
            "angle": 0,
            "content": "11. 计算参考点 \\(\\tilde{x}^j = \\frac{1}{m}\\sum_{i=1}^{m}x^i\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.266,
                0.631,
                0.35,
                0.644
            ],
            "angle": 0,
            "content": "12. end for"
        },
        {
            "type": "list",
            "bbox": [
                0.266,
                0.396,
                0.632,
                0.644
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.672,
                0.825,
                0.752
            ],
            "angle": 0,
            "content": "给出了 \\(\\tilde{x}^j\\) 的一种更新方法, 实际上 \\(\\tilde{x}^j\\) 可以有其他的选法, 例如取随机选取前 \\(m\\) 步的 \\(x^k\\) 中的一个. 与 SAG 算法和 SAGA 算法不同, SVRG 算法不需要分配存储空间来记录 \\(N\\) 个梯度向量, 但它的代价在于每 \\(m\\) 步都要计算一次全梯度, 在每次迭代的时候也需要多计算一个梯度 \\(\\nabla f_{s_k}(\\tilde{x}^j)\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.757,
                0.825,
                0.795
            ],
            "angle": 0,
            "content": "下面分析SVRG算法的收敛性．这里的收敛性是针对参考点序列 \\(\\{\\tilde{x}^j\\}\\) 而言的."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.815,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "定理8.25 (SVRG算法的收敛性[110]) 设每个 \\(f_{i}(x)\\) 是可微的，且梯度为 \\(L\\)-利普希茨连续的，函数 \\(f(x)\\) 是强凸的，强凸参数为 \\(\\mu\\)。在算法8.16中"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.318,
                0.133
            ],
            "angle": 0,
            "content": "8.7 随机优化算法"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "487"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.154,
                0.483,
                0.19
            ],
            "angle": 0,
            "content": "取步长 \\(\\alpha \\in \\left(0, \\frac{1}{2L}\\right]\\)，并且 \\(m\\) 充分大使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.32,
                0.202,
                0.738,
                0.238
            ],
            "angle": 0,
            "content": "\\[\n\\rho = \\frac {1}{\\mu \\alpha (1 - 2 L \\alpha) m} + \\frac {2 L \\alpha}{1 - 2 L \\alpha} <   1, \\tag {8.7.43}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.252,
                0.729,
                0.269
            ],
            "angle": 0,
            "content": "那么SVRG算法对于参考点 \\(\\tilde{x}^j\\) 在函数值期望的意义下有 \\(Q\\)-线性收敛速度："
        },
        {
            "type": "equation",
            "bbox": [
                0.306,
                0.286,
                0.739,
                0.307
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} f \\left(\\tilde {x} ^ {j}\\right) - f \\left(x ^ {*}\\right) \\leqslant \\rho \\mathbb {E} \\left[ f \\left(\\tilde {x} ^ {j - 1}\\right) - f \\left(x ^ {*}\\right) \\right]. \\tag {8.7.44}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.325,
                0.739,
                0.364
            ],
            "angle": 0,
            "content": "证明. 定义 \\(\\Delta_{k} = \\| x^{k} - x^{*}\\|\\) 为 \\(x^{k}\\) 与最优解 \\(x^{*}\\) 的距离, 与之前的误差分析类似, 对算法 8.16 的内层循环 (固定 \\(j\\)) 进行分析可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.23,
                0.38,
                0.674,
                0.475
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathbb {E} \\left[ \\Delta_ {k + 1} ^ {2} \\right] = \\mathbb {E} \\left[ \\| x ^ {k + 1} - x ^ {*} \\| ^ {2} \\right] = \\mathbb {E} \\left[ \\| x ^ {k} - \\alpha v ^ {k} - x ^ {*} \\| ^ {2} \\right] \\\\ = \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] - 2 \\alpha \\mathbb {E} \\left[ \\langle v ^ {k}, x ^ {k} - x ^ {*} \\rangle \\right] + \\alpha^ {2} \\mathbb {E} \\left[ \\| v ^ {k} \\| ^ {2} \\right] \\\\ = \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] - 2 \\alpha \\mathbb {E} \\left[ \\langle \\nabla f (x ^ {k}), x ^ {k} - x ^ {*} \\rangle \\right] + \\alpha^ {2} \\mathbb {E} \\left[ \\| v ^ {k} \\| ^ {2} \\right] \\\\ \\leqslant \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] - 2 \\alpha \\mathbb {E} \\left[ \\left(f \\left(x ^ {k}\\right) - f \\left(x ^ {*}\\right)\\right) \\right] + \\alpha^ {2} \\mathbb {E} \\left[ \\| v ^ {k} \\| ^ {2} \\right]. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.492,
                0.331,
                0.508
            ],
            "angle": 0,
            "content": "接下来构造辅助函数"
        },
        {
            "type": "equation",
            "bbox": [
                0.297,
                0.527,
                0.61,
                0.546
            ],
            "angle": 0,
            "content": "\\[\n\\phi_ {i} (x) = f _ {i} (x) - f _ {i} \\left(x ^ {*}\\right) - \\nabla f _ {i} \\left(x ^ {*}\\right) \\left(x - x ^ {*}\\right),\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.564,
                0.738,
                0.601
            ],
            "angle": 0,
            "content": "注意到 \\(\\phi_{i}(x)\\) 也是凸函数且梯度 \\(L\\)-利普希茨连续，根据推论2.1的结论，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.335,
                0.602,
                0.573,
                0.633
            ],
            "angle": 0,
            "content": "\\[\n\\frac {1}{2 L} \\| \\nabla \\phi_ {i} (x) \\| ^ {2} \\leqslant \\phi_ {i} (x) - \\phi_ {i} \\left(x ^ {*}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.641,
                0.441,
                0.659
            ],
            "angle": 0,
            "content": "展开 \\(\\phi_i(x)\\) 与 \\(\\nabla \\phi_i(x)\\) 的表达式可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.22,
                0.676,
                0.688,
                0.696
            ],
            "angle": 0,
            "content": "\\[\n\\left\\| \\nabla f _ {i} (x) - \\nabla f _ {i} \\left(x ^ {*}\\right) \\right\\| ^ {2} \\leqslant 2 L \\left[ f _ {i} (x) - f _ {i} \\left(x ^ {*}\\right) - \\nabla f _ {i} \\left(x ^ {*}\\right) ^ {\\mathrm {T}} \\left(x - x ^ {*}\\right) \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.713,
                0.564,
                0.731
            ],
            "angle": 0,
            "content": "对 \\(i\\) 从1到 \\(N\\) 进行求和，注意 \\(\\nabla f(x^{*}) = 0\\) ，就得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.226,
                0.745,
                0.737,
                0.783
            ],
            "angle": 0,
            "content": "\\[\n\\frac {1}{N} \\sum_ {i = 1} ^ {N} \\| \\nabla f _ {i} (x) - \\nabla f _ {i} \\left(x ^ {*}\\right) \\| ^ {2} \\leqslant 2 L [ f (x) - f \\left(x ^ {*}\\right) ], \\quad \\forall x. \\tag {8.7.45}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.8,
                0.65,
                0.817
            ],
            "angle": 0,
            "content": "利用(8.7.42)式的推导过程容易推出 \\(v^{k}\\) 二阶矩的上界表达式"
        },
        {
            "type": "equation",
            "bbox": [
                0.17,
                0.835,
                0.735,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ \\| v ^ {k} \\| ^ {2} \\right] \\leqslant 2 \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) - \\nabla f _ {s _ {k}} \\left(x ^ {*}\\right) \\| ^ {2} \\right] + 2 \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} \\left(\\tilde {x} ^ {j - 1}\\right) - \\nabla f _ {s _ {k}} \\left(x ^ {*}\\right) \\| ^ {2} \\right].\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "488"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.47,
                0.174
            ],
            "angle": 0,
            "content": "对上式右侧第一项，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.373,
                0.183,
                0.712,
                0.295
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) - \\nabla f _ {s _ {k}} \\left(x ^ {*}\\right) \\| ^ {2} \\right] \\\\ = \\mathbb {E} \\left[ \\left\\| \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) - \\nabla f _ {s _ {k}} \\left(x ^ {*}\\right) \\right\\| ^ {2} \\mid s _ {1}, s _ {2}, \\dots , s _ {k - 1} \\right] \\\\ = \\mathbb {E} \\left[ \\frac {1}{N} \\cdot \\sum_ {i = 1} ^ {N} \\| \\nabla f _ {i} \\left(x ^ {k}\\right) - \\nabla f _ {i} \\left(x ^ {*}\\right) \\| ^ {2} \\right] \\\\ \\leqslant 2 L \\mathbb {E} [ f (x ^ {k}) - f (x ^ {*}) ], \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.304,
                0.828,
                0.341
            ],
            "angle": 0,
            "content": "这里第二个等式直接利用求期望的公式，最后的不等式利用了不等式(8.7.45).类似地，对右侧第二项，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.343,
                0.352,
                0.742,
                0.371
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ \\left\\| \\nabla f _ {s _ {k}} \\left(\\tilde {x} ^ {j - 1}\\right) - \\nabla f _ {s _ {k}} \\left(x ^ {*}\\right) \\right\\| ^ {2} \\right] \\leqslant 2 L \\mathbb {E} \\left[ f \\left(\\tilde {x} ^ {j - 1}\\right) - f \\left(x ^ {*}\\right) \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.38,
                0.48,
                0.397
            ],
            "angle": 0,
            "content": "最终可得对 \\(\\mathbb{E}[\\| v^k\\|^2]\\) 的估计："
        },
        {
            "type": "equation",
            "bbox": [
                0.337,
                0.409,
                0.748,
                0.428
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ \\| v ^ {k} \\| ^ {2} \\right] \\leqslant 4 L \\left(\\mathbb {E} \\left[ f \\left(x ^ {k}\\right) - f \\left(x ^ {*}\\right) \\right] + \\mathbb {E} \\left[ f \\left(\\tilde {x} ^ {j - 1}\\right) - f \\left(x ^ {*}\\right) \\right]\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.437,
                0.66,
                0.455
            ],
            "angle": 0,
            "content": "将 \\(\\mathbb{E}[||v^k ||^2 ]\\) 的上界代入对 \\(\\mathbb{E}[\\Delta_{k + 1}^2 ]\\) 的估计，就有"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.466,
                0.744,
                0.532
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathbb {E} \\left[ \\Delta_ {k + 1} ^ {2} \\right] \\leqslant \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] - 2 \\alpha \\mathbb {E} \\left[ f (x ^ {k}) - f (x ^ {*}) \\right] + \\alpha^ {2} \\mathbb {E} \\left[ \\| v ^ {k} \\| ^ {2} \\right] \\\\ \\leqslant \\mathbb {E} \\left[ \\Delta_ {k} ^ {2} \\right] - 2 \\alpha (1 - 2 \\alpha L) \\mathbb {E} \\left[ f \\left(x ^ {k}\\right) - f \\left(x ^ {*}\\right) \\right] \\\\ + 4 L \\alpha^ {2} \\mathbb {E} [ f (\\tilde {x} ^ {j - 1}) - f (x ^ {*}) ]. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.543,
                0.67,
                0.56
            ],
            "angle": 0,
            "content": "对 \\(k\\) 从1到 \\(m\\) 求和，并且注意到 \\(x^{1} = \\tilde{x}^{j - 1}\\) 就可以得到"
        },
        {
            "type": "equation",
            "bbox": [
                0.353,
                0.568,
                0.733,
                0.664
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathbb {E} \\left[ \\Delta_ {m + 1} ^ {2} \\right] + 2 \\alpha (1 - 2 \\alpha L) \\sum_ {k = 1} ^ {m} \\mathbb {E} \\left[ f \\left(x ^ {k}\\right) - f \\left(x ^ {*}\\right) \\right] \\\\ \\leqslant \\mathbb {E} \\left[ \\| \\tilde {x} ^ {j - 1} - x ^ {*} \\| ^ {2} \\right] + 4 L \\alpha^ {2} m \\mathbb {E} \\left[ f \\left(\\tilde {x} ^ {j - 1}\\right) - f \\left(x ^ {*}\\right) \\right] \\\\ \\leqslant \\frac {2}{\\mu} \\mathbb {E} \\left[ f \\left(\\tilde {x} ^ {j - 1}\\right) - f \\left(x ^ {*}\\right) \\right] + 4 L \\alpha^ {2} m \\mathbb {E} \\left[ f \\left(\\tilde {x} ^ {j - 1}\\right) - f \\left(x ^ {*}\\right) \\right], \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.671,
                0.601,
                0.688
            ],
            "angle": 0,
            "content": "其中最后一个不等式利用了 \\(f(x)\\) 强凸的性质"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.689,
                0.499,
                0.724
            ],
            "angle": 0,
            "content": "注意到 \\(\\tilde{x}^j = \\frac{1}{m}\\sum_{k = 1}^{m}x^k\\) ，所以"
        },
        {
            "type": "equation",
            "bbox": [
                0.36,
                0.735,
                0.726,
                0.853
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathbb {E} \\left[ f \\left(\\tilde {x} ^ {j}\\right) - f \\left(x ^ {*}\\right) \\right] \\\\ \\leqslant \\frac {1}{m} \\sum_ {k = 1} ^ {m} \\mathbb {E} \\left[ f \\left(x ^ {k}\\right) - f \\left(x ^ {*}\\right) \\right] \\\\ \\leqslant \\frac {1}{2 \\alpha (1 - 2 \\alpha L) m} \\left(\\frac {2}{\\mu} + 4 m L \\alpha^ {2}\\right) \\mathbb {E} \\left[ f \\left(\\tilde {x} ^ {j - 1}\\right) - f \\left(x ^ {*}\\right) \\right] \\\\ = \\rho \\mathbb {E} [ f (\\tilde {x} ^ {j - 1}) - f (x ^ {*}) ]. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.809,
                0.838,
                0.824,
                0.85
            ],
            "angle": 0,
            "content": "□"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.249,
                0.133
            ],
            "angle": 0,
            "content": "8.8 总结"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.119,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "489"
        },
        {
            "type": "title",
            "bbox": [
                0.399,
                0.155,
                0.51,
                0.177
            ],
            "angle": 0,
            "content": "8.8 总结"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.199,
                0.744,
                0.342
            ],
            "angle": 0,
            "content": "本章介绍了众多求解复合优化问题的算法，这些算法可以应用到常见的大部分凸优化问题上．针对非凸问题，适用的算法有近似点梯度法、分块坐标下降法、交替方向乘子法以及随机优化算法．Nesterov加速算法和近似点算法在经过合适变形后也可推广到一些非凸问题上．由于在非凸情形下原始问题和对偶问题之间可能缺乏明显的关系，对偶算法应用在此类问题上比较困难．读者需要在应用算法之前判断优化问题的种类，之后选择合适的算法求解."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.347,
                0.744,
                0.509
            ],
            "angle": 0,
            "content": "近似点梯度法是解决非光滑、无约束、规模较大问题的常用算法。通常情况下它能利用问题的结构，有着比次梯度法更好的表现。有关近似点梯度法更进一步的讨论我们推荐读者阅读文献[149]。近似点梯度算法还有一些变形，比如镜像下降算法[15]，条件梯度算法[72,108]，惯性近似点梯度法[146]，有兴趣的读者可以自行了解。近似点算法可以理解成一种特殊的近似点梯度算法，也可以理解成次梯度算法的隐式格式。同时，近似点算法与增广拉格朗日函数法有着非常密切的关系，由于其等价性，增广拉格朗日函数法可以视作（次）梯度算法隐式格式的一种实现方式。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.515,
                0.744,
                0.768
            ],
            "angle": 0,
            "content": "Nesterov加速算法能够对近似点梯度算法进行加速,对于凸复合优化问题目标函数的收敛速度能够从 \\(\\mathcal{O}\\left(\\frac{1}{k}\\right)\\) 的速度提高到 \\(\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\\). 但Nesterov算法最初是如何构造出来的并没有一个很直观的解释,人们只能将这一贡献归功于Nesterov本人强大的数学推导能力.直观理解Nesterov加速算法的本质是一个很有意思的课题,例如,文献[173]的作者提出使用微分方程的观点来解释Nesterov加速算法.通过理解Nesterov加速算法的背后原理,人们或许能够构造出其他非平凡的加速算法.本章主要给出了常用的一些Nesterov加速算法,有关FISTA算法的部分参考了[16-17],第二类Nesterov加速算法在[141]中提出,第三类Nesterov加速算法可参考[19,140].值得注意的是,这些加速算法框架同样可以应用到非凸的复合优化问题中,[76-78]给出了非凸函数情形下的收敛性分析.更多有关加速算法的结果我们推荐读者阅读[142-143,184]."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.772,
                0.741,
                0.831
            ],
            "angle": 0,
            "content": "除Nesterov加速算法外，实际上还有很多其他类型的加速算法．例如深度学习中的动量算法[155,176]以及在很多领域广泛应用的安德森加速算法[5,189]，有兴趣的读者可以自己了解相关内容."
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.836,
                0.741,
                0.854
            ],
            "angle": 0,
            "content": "分块坐标下降法的历史可以追溯到1960年之前，它由于算法结构简单"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "490"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.156,
                0.827,
                0.402
            ],
            "angle": 0,
            "content": "且易于实现而受到一些应用数学家的关注。然而当时分块坐标下降法并不是一个很流行的算法，由于其过于简单、收敛速度缓慢，人们的研究重点大多在其他有较快收敛速度的算法上。分块坐标下降法的复兴主要是由统计学习和机器学习推动的，这些问题的特点是自变量维数高，而且不需要求解太精确，因此分块坐标下降法非常适用于处理大规模问题。有关收敛性方面的结论也是非常多，早在1963年，文章[193]就给出了格式(8.4.3)对凸函数的收敛结果，之后[129, 183]对凸函数的收敛性做了进一步改善。对一类特殊函数，文章[10, 90]给出了格式(8.4.4)的收敛性结果。在分块坐标下降算法中，每个变量块更新的顺序对算法的表现有很大影响，常用的做法是循环更新、随机更新以及选取梯度模长最大的分量更新等，随机更新的分块坐标下降法还可衍生出异步并行算法，详见[18, 124-125, 161]。其他的一些有关分块坐标下降法的综述可参考[198]。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.411,
                0.825,
                0.511
            ],
            "angle": 0,
            "content": "当原始问题求解过于困难时，其对偶问题可能比较容易求解。对偶算法就是利用这种思想设计的算法。在算法的构造当中，共轭函数起到了非常关键的作用。有关共轭函数的性质读者可参考[102]的第十章，[26]的第七章以及[164]。我们没有介绍原始-对偶混合梯度法(PDHG)的收敛性，实际上PDHG收敛需要更强的条件，更详细的讨论推荐读者阅读[97-98]。"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.52,
                0.827,
                0.744
            ],
            "angle": 0,
            "content": "早期的交替方向乘子法可以参考 1975 年左右针对变分问题提出的算法 [74, 81], 其在 21 世纪初产生了大量的相关应用. 因为其结构简单、易于实现, 交替方向乘子法很快受到人们的青睐. 但该方法本身也具有一定的缺陷, 例如正文给出了多块变量的交替方向乘子法的反例, 该反例来自 [43]. 为了解决这一问题, 人们提出 RP-ADMM[174], 即如果每一轮迭代的乘子更新前随机独立地改变多块变量的更新顺序, 那么这种算法在求解上述问题时在期望意义下收敛. 此后 RP-ADMM 被拓展到不可分的多块凸优化问题上 [44]. 正文还提到交替方向乘子法在一般情况下每一步可能不是良定义的, 因此人们提出了近似点交替方向乘子法 [66], 即在更新 \\(x_{1}\\) 与 \\(x_{2}\\) 子问题目标函数中再添加正定二次项. 近几十年也有许多文献对交替方向乘子法的收敛性有进一步讨论, 读者可参考文献 [57, 99, 104-105]."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.753,
                0.828,
                0.854
            ],
            "angle": 0,
            "content": "随机优化的早期研究可参考1951年的Monro-Robbins算法[162].最近一些年，随着大数据和深度学习等相关领域的发展，随机优化算法受到了人们的大量关注和研究.我们这里介绍了无约束光滑优化问题的随机梯度法以及相应的方差减少和加速技术．除了梯度方法外，人们也研究了随机拟牛顿法[25,36,137,200]和带有子采样的牛顿法[20,34-35,132,152,201].对"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.224,
                0.133
            ],
            "angle": 0,
            "content": "习题8"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "491"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.739,
                0.215
            ],
            "angle": 0,
            "content": "于目标函数中带有非光滑项的情形，也有相应的随机近似点梯度类算法，见[169, 199]. 对于约束优化问题（例如随机主成分分析）的随机梯度法，感兴趣的读者可以参考[109, 210]."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.219,
                0.744,
                0.343
            ],
            "angle": 0,
            "content": "本章的部分内容基于 Lieven Vandenberghe 教授的课件编写，包含近似点梯度法、Nesterov 加速算法、近似点算法、对偶分解算法、交替方向乘子法。分块坐标下降法结构叙述主要参考了 [202]，相关的收敛性分析主要参考了 [24]。Chambolle-Pock 算法的收敛性编写参考了 [41]。交替方向乘子法方面的内容同时参考了印卧涛教授的课件，相关的收敛性分析主要参考了 [45]."
        },
        {
            "type": "title",
            "bbox": [
                0.419,
                0.372,
                0.492,
                0.394
            ],
            "angle": 0,
            "content": "习题8"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.412,
                0.515,
                0.43
            ],
            "angle": 0,
            "content": "8.1 证明例8.1中的(3)(4)的邻近算子的形式"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.444,
                0.433,
                0.46
            ],
            "angle": 0,
            "content": "8.2 证明例8.2中的运算法则成立"
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.476,
                0.398,
                0.493
            ],
            "angle": 0,
            "content": "8.3 求下列函数的邻近算子："
        },
        {
            "type": "list",
            "bbox": [
                0.18,
                0.412,
                0.515,
                0.493
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.507,
                0.623,
                0.525
            ],
            "angle": 0,
            "content": "(a) \\(f(x) = I_C(x)\\) ，其中 \\(C = \\{(x,t)\\in \\mathbb{R}^{n + 1}||x||_2\\leqslant t\\}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.533,
                0.538,
                0.559
            ],
            "angle": 0,
            "content": "(b) \\(f(x) = \\inf_{y\\in C}\\| x - y\\|\\) ，其中 \\(C\\) 是闭凸集；"
        },
        {
            "type": "text",
            "bbox": [
                0.219,
                0.565,
                0.569,
                0.597
            ],
            "angle": 0,
            "content": "(c) \\(f(x) = \\frac{1}{2} (\\inf_{y\\in C}\\| x - y\\|)^2\\) ，其中 \\(C\\) 是闭凸集"
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.507,
                0.623,
                0.597
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.18,
                0.607,
                0.741,
                0.644
            ],
            "angle": 0,
            "content": "8.4 对矩阵函数我们也可类似地定义邻近算子，只需将向量版本中的 \\(\\ell_2\\) 范数替换为 \\(F\\) 范数，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.319,
                0.655,
                0.63,
                0.693
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname{prox}_{f}(X) = \\operatorname *{arg  min}_{U\\in \\mathbf{dom}f}f(U) + \\frac{1}{2}\\| U - X\\|_{F}^{2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.212,
                0.704,
                0.482,
                0.722
            ],
            "angle": 0,
            "content": "试求出如下函数的邻近算子表达式："
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.736,
                0.525,
                0.754
            ],
            "angle": 0,
            "content": "(a) \\(f(U) = \\| U\\| _1\\) ，其中 \\(\\mathbf{dom}f = \\mathbb{R}^{m\\times n}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.762,
                0.736,
                0.8
            ],
            "angle": 0,
            "content": "(b) \\(f(U) = -\\ln \\operatorname{det}(U)\\)，其中 \\(\\mathbf{dom} f = \\{U \\mid U \\succ 0\\}\\)，这里邻近算子的自变量 \\(X\\) 为对称矩阵（不一定正定）；"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.809,
                0.585,
                0.828
            ],
            "angle": 0,
            "content": "(c) \\(f(U) = I_{C}(U)\\), 其中 \\(C = \\{U \\in \\mathcal{S}^{n} \\mid U \\succeq 0\\}\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.836,
                0.523,
                0.854
            ],
            "angle": 0,
            "content": "(d) \\(f(U) = \\| U\\|_{*}\\) ，其中 \\(\\mathbf{dom}f = \\mathbb{R}^{m\\times n}\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.736,
                0.736,
                0.854
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "492"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.157,
                0.706,
                0.174
            ],
            "angle": 0,
            "content": "8.5 对一般复合优化问题的加速算法（算法8.9），试证明："
        },
        {
            "type": "text",
            "bbox": [
                0.304,
                0.187,
                0.823,
                0.224
            ],
            "angle": 0,
            "content": "(a) 当 \\(t_k = \\gamma_k \\lambda_k\\) 且 \\(h(x) = 0\\) 时, 算法 8.9 等价于第二类 Nesterov 加速算法;"
        },
        {
            "type": "text",
            "bbox": [
                0.304,
                0.233,
                0.674,
                0.249
            ],
            "angle": 0,
            "content": "(b) 当 \\(t_{k} = \\lambda_{k}\\) 时, 算法 8.9 等价于近似点梯度法."
        },
        {
            "type": "list",
            "bbox": [
                0.304,
                0.187,
                0.823,
                0.249
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.263,
                0.682,
                0.28
            ],
            "angle": 0,
            "content": "8.6 假设 \\(f\\) 是闭凸函数，证明 Moreau 分解的成立，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.442,
                0.294,
                0.68,
                0.314
            ],
            "angle": 0,
            "content": "\\[\nx = \\operatorname {p r o x} _ {f} (x) + \\operatorname {p r o x} _ {f ^ {*}} (x) \\quad \\forall x.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.33,
                0.825,
                0.366
            ],
            "angle": 0,
            "content": "8.7 假设 \\(f\\) 是闭凸函数, 证明 Moreau 分解的推广成立, 即对任意的 \\(\\lambda > 0\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.415,
                0.366,
                0.707,
                0.394
            ],
            "angle": 0,
            "content": "\\[\nx = \\operatorname {p r o x} _ {\\lambda f} (x) + \\lambda \\operatorname {p r o x} _ {\\lambda^ {- 1} f ^ {*}} \\left(\\frac {x}{\\lambda}\\right) \\quad \\forall x.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.399,
                0.542,
                0.414
            ],
            "angle": 0,
            "content": "提示：利用Moreau分解的结论."
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.428,
                0.589,
                0.445
            ],
            "angle": 0,
            "content": "8.8 根据不等式(8.5.23)推导不等式(8.5.24)."
        },
        {
            "type": "text",
            "bbox": [
                0.268,
                0.458,
                0.825,
                0.495
            ],
            "angle": 0,
            "content": "8.9 写出关于 LASSO 问题的鞍点问题形式，并写出原始 - 对偶混合梯度算法和 Chambolle-Pock 算法。"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.508,
                0.825,
                0.567
            ],
            "angle": 0,
            "content": "8.10 设函数 \\( f(x_{1},x_{2}) = |x_{1} - x_{2}| - \\min \\{x_{1},x_{2}\\} \\)，其定义域为 \\([0,1]\\times [0,1]\\)。试推导基于格式(8.4.3)的分块坐标下降法（\\( x_{1} \\) 和 \\( x_{2} \\) 分别看做一个变量块），此算法是否收敛？"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.58,
                0.825,
                0.617
            ],
            "angle": 0,
            "content": "8.11 试对分组 LASSO 问题（即例8.7）推导出基于格式(8.4.4)的分块坐标下降法."
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.631,
                0.51,
                0.647
            ],
            "angle": 0,
            "content": "8.12 考虑最大割问题的非凸松弛"
        },
        {
            "type": "list",
            "bbox": [
                0.26,
                0.428,
                0.825,
                0.647
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.437,
                0.658,
                0.685,
                0.717
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\quad \\left\\langle C, V ^ {T} V \\right\\rangle , \\\\ \\begin{array}{l l} \\text {s . t .} & \\| v _ {i} \\| = 1, i = 1, 2, \\dots , n, \\end{array} \\\\ V = \\left[ v _ {1}, v _ {2}, \\dots , v _ {n} \\right] \\in \\mathbb {R} ^ {p \\times n}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.729,
                0.812,
                0.746
            ],
            "angle": 0,
            "content": "仿照算法8.12的构造过程，推导出使用格式(8.4.4)的分块坐标下降法。"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.759,
                0.442,
                0.775
            ],
            "angle": 0,
            "content": "8.13 考虑约束优化问题"
        },
        {
            "type": "equation",
            "bbox": [
                0.47,
                0.786,
                0.653,
                0.825
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\max  \\left\\{e ^ {- x} + y, y ^ {2} \\right\\}, \\\\ \\begin{array}{l l} \\text {s . t .} & y \\geqslant 2, \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.837,
                0.478,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(x,y\\in \\mathbb{R}\\) 为自变量"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.224,
                0.133
            ],
            "angle": 0,
            "content": "习题8"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "493"
        },
        {
            "type": "text",
            "bbox": [
                0.218,
                0.157,
                0.574,
                0.174
            ],
            "angle": 0,
            "content": "(a) 通过引入松弛变量 \\(z\\)，试说明该问题等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.187,
                0.618,
                0.226
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\max  \\left\\{e ^ {- x} + y, y ^ {2} \\right\\} + I _ {\\mathbb {R} _ {+}} (z), \\\\ \\begin{array}{l l} \\text {s . t .} & y - z = 2; \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.241,
                0.666,
                0.258
            ],
            "angle": 0,
            "content": "(b) 推导 (a) 中问题的对偶问题，并求出原始问题的最优解；"
        },
        {
            "type": "text",
            "bbox": [
                0.218,
                0.267,
                0.734,
                0.284
            ],
            "angle": 0,
            "content": "(c) 对 (a) 中的问题形式, 使用 ADMM 求解时可能会遇到什么问题?"
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.241,
                0.734,
                0.284
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.297,
                0.739,
                0.356
            ],
            "angle": 0,
            "content": "8.14 写出对于线性规划对偶问题运用ADMM的迭代格式，以及与之等价的对于原始问题的DRS格式，并指出ADMM和DRS算法更新变量之间的关系。"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.37,
                0.457,
                0.386
            ],
            "angle": 0,
            "content": "8.15 相关系数矩阵逼近问题的定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.359,
                0.395,
                0.588,
                0.464
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\frac {1}{2} \\| X - G \\| _ {F} ^ {2}, \\\\ \\begin{array}{l l} \\text {s . t .} & X _ {i i} = 1, \\quad i = 1, 2, \\dots , n, \\end{array} \\\\ X \\succeq 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.209,
                0.477,
                0.739,
                0.577
            ],
            "angle": 0,
            "content": "其中自变量 \\(X\\) 取值于对称矩阵空间 \\(\\mathcal{S}^n\\), \\(G\\) 为给定的实对称矩阵. 这个问题在金融领域中有重要的应用. 由于误差等因素, 根据实际观测得到的相关系数矩阵的估计 \\(G\\) 往往不具有相关系数矩阵的性质 (如对角线为 1 , 正定性), 我们的最终目标是找到一个和 \\(G\\) 最接近的相关系数矩阵 \\(X\\). 试给出满足如下要求的算法:"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.591,
                0.603,
                0.607
            ],
            "angle": 0,
            "content": "(a) 对偶近似点梯度法，并给出化简后的迭代公式；"
        },
        {
            "type": "text",
            "bbox": [
                0.217,
                0.617,
                0.667,
                0.633
            ],
            "angle": 0,
            "content": "(b) 针对原始问题的 ADMM，并给出每个子问题的显式解."
        },
        {
            "type": "list",
            "bbox": [
                0.217,
                0.591,
                0.667,
                0.633
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.647,
                0.738,
                0.684
            ],
            "angle": 0,
            "content": "8.16 鲁棒主成分分析问题是将一个已知矩阵 \\(M\\) 分解成一个低秩部分 \\(L\\) 和一个稀疏部分 \\(S\\) 的和，即求解如下优化问题："
        },
        {
            "type": "equation",
            "bbox": [
                0.393,
                0.698,
                0.553,
                0.735
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\quad \\| L \\| _ {*} + \\lambda \\| S \\| _ {1}, \\\\ \\begin{array}{l l} \\text {s . t .} & L + S = M, \\end{array} \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.209,
                0.749,
                0.739,
                0.786
            ],
            "angle": 0,
            "content": "其中 \\(L, S\\) 均为自变量。写出求解鲁棒主成分分析问题的 ADMM 格式，并说明如何求解每个子问题。提示：可以利用习题 8.4 的结论。"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.801,
                0.489,
                0.817
            ],
            "angle": 0,
            "content": "8.17 考虑 \\(\\ell_0\\) 范数优化问题的罚函数形式："
        },
        {
            "type": "equation",
            "bbox": [
                0.365,
                0.826,
                0.582,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\min  \\quad \\lambda \\| x \\| _ {0} + \\frac {1}{2} \\| A x - b \\| ^ {2},\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "494"
        },
        {
            "type": "header",
            "bbox": [
                0.65,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "第八章 复合优化算法"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.157,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "其中 \\(A \\in \\mathbb{R}^{m \\times n}, m < n\\) 为实矩阵，\\(\\|\\cdot\\|_0\\) 为 \\(\\ell_0\\) 范数，即非零元素的个数。试针对 \\(\\ell_0\\) 范数优化问题形式化推导具有两个变量块的ADMM格式。在算法中每个子问题是如何求解的？"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.229,
                0.714,
                0.246
            ],
            "angle": 0,
            "content": "8.18 试说明 LASSO 对偶问题中，若在问题(8.6.27)中对约束"
        },
        {
            "type": "equation",
            "bbox": [
                0.514,
                0.261,
                0.611,
                0.28
            ],
            "angle": 0,
            "content": "\\[\nA ^ {T} y + z = 0\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.296,
                0.784,
                0.312
            ],
            "angle": 0,
            "content": "引入乘子 \\(x\\) ，则 \\(x\\) 恰好对应LASSO原始问题(8.6.26)中的自变量"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.326,
                0.78,
                0.343
            ],
            "angle": 0,
            "content": "8.19 实现关于 LASSO 问题使用以下算法的程序，并比较它们的效率"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.357,
                0.467,
                0.372
            ],
            "angle": 0,
            "content": "(a) 近似点梯度算法；"
        },
        {
            "type": "text",
            "bbox": [
                0.305,
                0.383,
                0.488,
                0.398
            ],
            "angle": 0,
            "content": "(b) Nesterov 加速算法;"
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.408,
                0.467,
                0.424
            ],
            "angle": 0,
            "content": "(c) 交替方向乘子法;"
        },
        {
            "type": "text",
            "bbox": [
                0.304,
                0.434,
                0.51,
                0.45
            ],
            "angle": 0,
            "content": "(d) Chambolle-Pock 算法;"
        },
        {
            "type": "text",
            "bbox": [
                0.306,
                0.46,
                0.467,
                0.476
            ],
            "angle": 0,
            "content": "(e) 分块坐标下降法；"
        },
        {
            "type": "text",
            "bbox": [
                0.309,
                0.486,
                0.498,
                0.502
            ],
            "angle": 0,
            "content": "(f) 随机近似点梯度算法."
        },
        {
            "type": "list",
            "bbox": [
                0.304,
                0.357,
                0.51,
                0.502
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.512,
                0.825,
                0.587
            ],
            "angle": 0,
            "content": "8.20 设 \\( f(x) = \\frac{1}{N} \\sum_{i=1}^{N} f_i(x) \\)，其中每个 \\( f_i(x) \\) 是可微函数，且 \\( f(x) \\) 为梯度 \\( L \\)-利普希茨连续的．\\(\\{x^k\\}\\) 是由随机梯度下降法产生的迭代序列，\\( s_k \\) 为第 \\( k \\) 步随机抽取的下标．证明："
        },
        {
            "type": "equation",
            "bbox": [
                0.325,
                0.602,
                0.798,
                0.622
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) \\| ^ {2} \\right] \\leqslant \\mathbb {E} \\left[ \\| x ^ {k} - x ^ {*} \\| ^ {2} \\right] + \\alpha_ {k} \\mathbb {E} \\left[ \\| \\nabla f _ {s _ {k}} \\left(x ^ {k}\\right) - \\nabla f \\left(x ^ {k}\\right) \\| ^ {2} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.637,
                0.704,
                0.654
            ],
            "angle": 0,
            "content": "其中 \\(x^{*}\\) 是 \\(f(x)\\) 的一个最小值点， \\(\\alpha_{k}\\) 为第 \\(k\\) 步的步长"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.668,
                0.619,
                0.684
            ],
            "angle": 0,
            "content": "8.21 在SAGA算法中，每一步的下降方向取为"
        },
        {
            "type": "equation",
            "bbox": [
                0.434,
                0.696,
                0.69,
                0.734
            ],
            "angle": 0,
            "content": "\\[\nv ^ {k} = \\nabla f _ {s _ {k}} (x ^ {k}) - g _ {s _ {k}} ^ {k - 1} + \\frac {1}{N} \\sum_ {i = 1} ^ {N} g _ {i} ^ {k - 1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.299,
                0.746,
                0.59,
                0.764
            ],
            "angle": 0,
            "content": "假设初值 \\(g_{i}^{0} = 0,i = 1,2,\\dots ,N\\) ，证明："
        },
        {
            "type": "equation",
            "bbox": [
                0.447,
                0.778,
                0.677,
                0.797
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ v ^ {k} \\mid s _ {1}, s _ {2}, \\dots , s _ {k - 1} \\right] = \\nabla f (x ^ {k}).\n\\]"
        }
    ],
    [
        {
            "type": "title",
            "bbox": [
                0.325,
                0.251,
                0.583,
                0.283
            ],
            "angle": 0,
            "content": "附录A 符号表"
        },
        {
            "type": "table",
            "bbox": [
                0.179,
                0.344,
                0.729,
                0.848
            ],
            "angle": 0,
            "content": "<table><tr><td>符号</td><td>含义</td></tr><tr><td>IR</td><td>实数集合</td></tr><tr><td>IR</td><td>广义实数集合</td></tr><tr><td>Rn</td><td>n维欧几里得空间</td></tr><tr><td>Rm×n</td><td>m×n矩阵空间</td></tr><tr><td>Sn</td><td>n阶对称矩阵空间</td></tr><tr><td>Sn+</td><td>n阶对称半正定矩阵空间</td></tr><tr><td>Sn++</td><td>n阶对称正定矩阵空间</td></tr><tr><td>intS</td><td>集合S的内点</td></tr><tr><td>affineS</td><td>集合S的仿射包</td></tr><tr><td>convS</td><td>集合S的凸包</td></tr><tr><td>coneS</td><td>集合S的锥包</td></tr><tr><td>A⊆B</td><td>集合A是B的子集</td></tr><tr><td>A⊂B</td><td>集合A是B的真子集</td></tr><tr><td>A+B</td><td>集合A与B的加法（分别从A与B中选取一个元素相加构成的集合）</td></tr><tr><td>A-B</td><td>集合A与B的减法（分别从A与B中选取一个元素相减构成的集合）</td></tr><tr><td>Nδ(x)</td><td>点x处半径为δ的邻域</td></tr><tr><td>dist(x,S)</td><td>点x到集合S的欧几里得距离</td></tr><tr><td>a def b</td><td>表达式a的定义为表达式b</td></tr><tr><td>x</td><td>最优化问题的变量</td></tr><tr><td>xk</td><td>最优化算法在第k步时自变量x的值</td></tr><tr><td>x*</td><td>最优化问题的最优解</td></tr></table>"
        },
        {
            "type": "page_number",
            "bbox": [
                0.44,
                0.87,
                0.469,
                0.882
            ],
            "angle": 0,
            "content": "495"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "496"
        },
        {
            "type": "header",
            "bbox": [
                0.7,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "附录A 符号表"
        },
        {
            "type": "table",
            "bbox": [
                0.269,
                0.154,
                0.816,
                0.838
            ],
            "angle": 0,
            "content": "<table><tr><td>符号</td><td>含义</td></tr><tr><td>xT</td><td>向量或矩阵的转置</td></tr><tr><td>xT</td><td>向量或矩阵的共轭转置</td></tr><tr><td>domf</td><td>函数f的定义域</td></tr><tr><td>epif</td><td>函数f的上方图</td></tr><tr><td>∇f(x)</td><td>函数f在点x处的梯度</td></tr><tr><td>∇2f(x)</td><td>函数f在点x处的海瑟矩阵</td></tr><tr><td>∂f(x;d)</td><td>函数f在点x处关于方向d的方向导数</td></tr><tr><td>∂f(x)</td><td>函数f在点x处的次微分</td></tr><tr><td>argminf(x)</td><td>函数f在可行域X中的一个最小值点</td></tr><tr><td>argmaxf(x)</td><td>函数f在可行域X中的一个最大值点</td></tr><tr><td>sign(x)</td><td>符号函数</td></tr><tr><td>ln(x)</td><td>自然对数（以e为底）</td></tr><tr><td>dk</td><td>最优化算法在第k步时的下降方向</td></tr><tr><td>αk</td><td>最优化算法在第k步时的步长</td></tr><tr><td>x≥0</td><td>向量x每一个分量都大于或等于0，即xi≥0,i=1,2,...,n</td></tr><tr><td>x ⊙ y</td><td>向量或矩阵x和y的Hadamard积(逐分量相乘)</td></tr><tr><td>{x,y}</td><td>向量或矩阵x和y的内积</td></tr><tr><td>R(X)</td><td>矩阵X的像空间</td></tr><tr><td>N(X)</td><td>矩阵X的零空间</td></tr><tr><td>Tr(X)</td><td>矩阵X的迹</td></tr><tr><td>det(X)</td><td>矩阵X的行列式</td></tr><tr><td>X≥0</td><td>矩阵X半正定</td></tr><tr><td>X≥Y</td><td>矩阵X-Y半正定</td></tr><tr><td>X&gt;0</td><td>矩阵X严格正定</td></tr><tr><td>X&gt;Y</td><td>矩阵X-Y严格正定</td></tr><tr><td>||x||,||x||2</td><td>向量x的l2范数</td></tr><tr><td>||x||1</td><td>向量x的l1范数（所有分量绝对值的和）</td></tr><tr><td>||X||2</td><td>矩阵X的谱范数（最大奇异值）</td></tr><tr><td>||X||1</td><td>矩阵X的l1范数（所有分量绝对值的和）</td></tr><tr><td>||X||F</td><td>矩阵X的F范数</td></tr></table>"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "497"
        },
        {
            "type": "table",
            "bbox": [
                0.182,
                0.156,
                0.729,
                0.616
            ],
            "angle": 0,
            "content": "<table><tr><td>符号</td><td>含义</td></tr><tr><td>||X||*</td><td>矩阵X的核范数（所有奇异值的和）</td></tr><tr><td>diag(X)</td><td>方阵X所有对角线元素组成的向量</td></tr><tr><td>Diag(x)</td><td>以向量x元素生成的对角矩阵</td></tr><tr><td>sk</td><td>在拟牛顿算法中，相邻两次迭代点的差x^{k+1} - x^k</td></tr><tr><td>yk</td><td>在拟牛顿算法中，相邻两次迭代点处梯度的差，即 ∇f(x^{k+1}) - ∇f(x^k)</td></tr><tr><td>B^k</td><td>二阶算法中第k步海瑟矩阵的近似矩阵</td></tr><tr><td>H^k</td><td>二阶算法中第k步海瑟矩阵逆的近似矩阵</td></tr><tr><td>ε</td><td>约束优化问题中所有等式约束指标集合</td></tr><tr><td>I</td><td>约束优化问题中所有不等式约束指标集合</td></tr><tr><td>A(x)</td><td>约束优化问题中点x处的积极集</td></tr><tr><td>PE(x,σ)</td><td>等式约束罚函数</td></tr><tr><td>PI(x,σ)</td><td>不等式约束罚函数</td></tr><tr><td>L(x,λ)</td><td>拉格朗日函数</td></tr><tr><td>Lσ(x,λ)</td><td>增广拉格朗日函数</td></tr><tr><td>prox_f(x)</td><td>函数f的邻近算子</td></tr><tr><td>IS(x)</td><td>集合S的示性函数</td></tr><tr><td>PS(x)</td><td>点x到闭集S的欧几里得投影</td></tr><tr><td>P(A)</td><td>随机事件A的概率</td></tr><tr><td>IE[X]</td><td>随机变量X的数学期望</td></tr><tr><td>IE[X|Y]</td><td>随机变量X关于Y的条件期望</td></tr></table>"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.119,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "498"
        },
        {
            "type": "header",
            "bbox": [
                0.7,
                0.118,
                0.826,
                0.134
            ],
            "angle": 0,
            "content": "附录A 符号表"
        }
    ],
    [
        {
            "type": "title",
            "bbox": [
                0.31,
                0.252,
                0.6,
                0.283
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "title",
            "bbox": [
                0.348,
                0.339,
                0.56,
                0.361
            ],
            "angle": 0,
            "content": "B.1 线性代数基础"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.38,
                0.357,
                0.398
            ],
            "angle": 0,
            "content": "B.1.1 矩阵内积与迹"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.411,
                0.737,
                0.448
            ],
            "angle": 0,
            "content": "类似于向量的内积, 我们可以定义矩阵的内积. 具体地, 对于两个 \\(m \\times n\\) 矩阵 \\(A, B\\), 其内积定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.389,
                0.462,
                0.519,
                0.494
            ],
            "angle": 0,
            "content": "\\[\n\\langle A, B \\rangle = \\sum_ {i, j} a _ {i j} b _ {i j}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.505,
                0.738,
                0.573
            ],
            "angle": 0,
            "content": "从定义可以看出 \\(\\langle A,B\\rangle = \\langle B,A\\rangle\\) ，对于方阵 \\(A\\in \\mathbb{R}^{m\\times m}\\) ，我们可以定义 \\(A\\) 的迹，记为 \\(\\operatorname {Tr}(A) = \\sum_{i = 1}^{m}a_{ii}\\) 容易看出 \\(\\operatorname {Tr}(A) = \\operatorname {Tr}(A^{\\mathrm{T}})\\) .那么对于 \\(m\\times n\\) 矩阵\\(A,B\\) ，以下关系式成立："
        },
        {
            "type": "equation",
            "bbox": [
                0.341,
                0.588,
                0.567,
                0.607
            ],
            "angle": 0,
            "content": "\\[\n\\langle A, B \\rangle = \\operatorname {T r} (A B ^ {\\mathrm {T}}) = \\operatorname {T r} (B ^ {\\mathrm {T}} A).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.622,
                0.737,
                0.659
            ],
            "angle": 0,
            "content": "更一般地，假设 \\(A_{1}, A_{2}, \\dots, A_{m}\\) 的维数是相容的（\\(A_{i}\\) 的列数等于 \\(A_{i+1}, i = 1, 2, \\dots, m-1\\) 的行数，且 \\(A_{m}\\) 的列数等于 \\(A_{1}\\) 的行数），则"
        },
        {
            "type": "equation",
            "bbox": [
                0.208,
                0.675,
                0.698,
                0.693
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {T r} \\left(A _ {1} A _ {2} \\dots A _ {m}\\right) = \\operatorname {T r} \\left(A _ {2} A _ {3} \\dots A _ {m} A _ {1}\\right) = \\dots = \\operatorname {T r} \\left(A _ {m} A _ {1} \\dots A _ {m - 1}\\right).\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.717,
                0.473,
                0.736
            ],
            "angle": 0,
            "content": "B.1.2 正交矩阵与（半）正定矩阵"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.748,
                0.738,
                0.807
            ],
            "angle": 0,
            "content": "对于方阵 \\(A\\) 和同阶单位矩阵 \\(I\\)，若 \\(A^{\\mathrm{T}}A = AA^{\\mathrm{T}} = I\\)，则称 \\(A\\) 为正交矩阵。对于矩阵 \\(A \\in \\mathbb{R}^{m \\times n}\\)，其中 \\(m > n\\)，若 \\(A^{\\mathrm{T}}A = I\\)，则称其为列正交矩阵。对于对称矩阵 \\(A \\in S^{m}\\)，定义如下二次型"
        },
        {
            "type": "equation",
            "bbox": [
                0.352,
                0.817,
                0.553,
                0.855
            ],
            "angle": 0,
            "content": "\\[\nf (x) = x ^ {\\mathrm {T}} A x = \\sum_ {i, j = 1} ^ {m} a _ {i j} x _ {i} x _ {j}.\n\\]"
        },
        {
            "type": "page_number",
            "bbox": [
                0.44,
                0.87,
                0.469,
                0.882
            ],
            "angle": 0,
            "content": "499"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.291,
                0.13
            ],
            "angle": 0,
            "content": "500"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "如果对于任意的向量 \\(x \\in \\mathbb{R}^m\\) 都有 \\(f(x) \\geqslant 0\\) 成立，则称 \\(A\\) 为半正定矩阵，记为 \\(A \\succeq 0\\)；进一步地，如果对于任意的非零向量 \\(x\\)，都有 \\(f(x) > 0\\) 成立，则称 \\(A\\) 为正定矩阵，记为 \\(A \\succ 0\\)."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.245,
                0.404,
                0.263
            ],
            "angle": 0,
            "content": "B.1.3 矩阵的秩"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.278,
                0.825,
                0.356
            ],
            "angle": 0,
            "content": "定义B.1(秩）给定一个 \\(m \\times n\\) 矩阵 \\(A\\) ，其 \\(m\\) 个行向量的极大线性无关组对应的向量个数称为矩阵的行秩；其 \\(n\\) 个列向量的极大线性无关组对应的向量个数称为矩阵的列秩．矩阵的行秩等于列秩，称为矩阵的秩，记为\\(\\operatorname{rank}(A)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.372,
                0.825,
                0.43
            ],
            "angle": 0,
            "content": "有了秩的概念，我们讨论线性方程组 \\(Ax = b\\) 解的存在性问题会容易很多．定义增广矩阵 \\(\\widehat{A} = (A,b)\\) ．如果方程组的解存在，即 \\(b\\) 可以由 \\(A\\) 的列向量线性表出，因此 \\(\\mathrm{rank}(A) = \\mathrm{rank}(\\widehat{A})\\) ；反之也成立．具体地，有如下定理："
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.446,
                0.825,
                0.484
            ],
            "angle": 0,
            "content": "定理B.1对于线性方程组 \\(Ax = b\\) ，其中 \\(A\\in \\mathbb{R}^{m\\times n}\\) ， \\(x\\in \\mathbb{R}^n\\) ， \\(b\\in \\mathbb{R}^m\\) ，如下结论成立："
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.499,
                0.586,
                0.516
            ],
            "angle": 0,
            "content": "(1) 方程组有解 \\(\\Longleftrightarrow \\operatorname{rank}(A) = \\operatorname{rank}(\\widehat{A})\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.533,
                0.822,
                0.569
            ],
            "angle": 0,
            "content": "(2) 在方程组有解的情况下，方程组有唯一解 \\(\\Longleftrightarrow \\operatorname{rank}(A) = n\\) ，否则有无穷多组解."
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.499,
                0.822,
                0.569
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.601,
                0.463,
                0.619
            ],
            "angle": 0,
            "content": "B.1.4 像空间和零空间"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.633,
                0.825,
                0.67
            ],
            "angle": 0,
            "content": "为了进一步理解矩阵秩的概念以及线性方程组解的存在性，我们定义与矩阵本身相关的两个线性空间."
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.686,
                0.825,
                0.725
            ],
            "angle": 0,
            "content": "定义B.2（像空间和零空间）对于矩阵 \\(A \\in \\mathbb{R}^{m \\times n}\\)，定义像空间 \\(\\mathcal{R}(A) = \\{y \\mid y = Ax, x \\in \\mathbb{R}^n\\}\\)；零空间 \\(\\mathcal{N}(A) = \\{x \\mid Ax = 0, x \\in \\mathbb{R}^n\\}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.74,
                0.825,
                0.818
            ],
            "angle": 0,
            "content": "从定义容易看出，像空间 \\(\\mathcal{R}(A)\\) 的维数，记为 \\(\\dim (\\mathcal{R}(A))\\) ，等于矩阵的秩 \\(\\operatorname {rank}(A)\\) ．此外，线性方程组 \\(Ax = b\\) 有解，当且仅当 \\(b\\in \\mathcal{R}(A)\\) ，并且方程的解都可以表示为 \\(x^{*} + v\\) ，其中 \\(Ax^{*} = b\\) ， \\(v\\in \\mathcal{N}(A)\\) ．对于像空间与零空间的维数关系，我们不加证明地给出如下结论："
        },
        {
            "type": "equation",
            "bbox": [
                0.419,
                0.836,
                0.663,
                0.853
            ],
            "angle": 0,
            "content": "\\[\n\\dim (\\mathcal {R} (A)) + \\dim (\\mathcal {N} (A)) = n.\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.32,
                0.133
            ],
            "angle": 0,
            "content": "B.1 线性代数基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "501"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.174
            ],
            "angle": 0,
            "content": "进一步地，给定任意矩阵 \\(A \\in \\mathbb{R}^{m \\times n}\\)，全空间 \\(\\mathbb{R}^n\\) 都可以写成如下正交分解："
        },
        {
            "type": "equation",
            "bbox": [
                0.238,
                0.189,
                0.668,
                0.209
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {R} ^ {n} = \\mathcal {N} (A) \\oplus \\mathcal {R} \\left(A ^ {\\mathrm {T}}\\right), \\quad x \\perp y \\quad \\forall x \\in \\mathcal {N} (A), y \\in \\mathcal {R} \\left(A ^ {\\mathrm {T}}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.225,
                0.576,
                0.242
            ],
            "angle": 0,
            "content": "这个结论在推导线性空间的一些基本性质时尤其有用."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.27,
                0.297,
                0.288
            ],
            "angle": 0,
            "content": "B.1.5 行列式"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.302,
                0.632,
                0.319
            ],
            "angle": 0,
            "content": "借助变换的概念，我们定义方阵的行列式，记为 \\(\\operatorname{det}(A)\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.334,
                0.632,
                0.35
            ],
            "angle": 0,
            "content": "定义B.3(行列式）给定方阵 \\(A\\in \\mathbb{R}^{n\\times n}\\) ，其行列式定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.337,
                0.362,
                0.572,
                0.4
            ],
            "angle": 0,
            "content": "\\[\n\\det  (A) = \\sum_ {\\sigma \\in S _ {n}} (- 1) ^ {\\tau (\\sigma)} \\prod_ {i = 1} ^ {n} a _ {i \\sigma (i)},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.412,
                0.686,
                0.43
            ],
            "angle": 0,
            "content": "其中 \\(S_{n}\\) 是 \\(1,2,\\dots ,n\\) 的所有全排列的集合， \\(\\tau (\\sigma)\\) 为排列 \\(\\sigma\\) 的逆序数."
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.444,
                0.579,
                0.46
            ],
            "angle": 0,
            "content": "对于两个方阵 \\(A, B \\in \\mathbb{R}^{n \\times n}\\) 和常数 \\(c \\in \\mathbb{R}\\), 我们有"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.475,
                0.359,
                0.493
            ],
            "angle": 0,
            "content": "(1) \\(\\operatorname{det}(A) = \\operatorname{det}(A^{\\mathrm{T}})\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.508,
                0.414,
                0.525
            ],
            "angle": 0,
            "content": "(2) \\(\\operatorname{det}(AB) = \\operatorname{det}(A)\\operatorname{det}(B)\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.54,
                0.375,
                0.557
            ],
            "angle": 0,
            "content": "(3) \\(\\operatorname{det}(cA) = c^n\\operatorname{det}(A)\\);"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.475,
                0.414,
                0.557
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.572,
                0.57,
                0.589
            ],
            "angle": 0,
            "content": "根据行列式是否为0，我们可以将方阵分为两类"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.604,
                0.737,
                0.641
            ],
            "angle": 0,
            "content": "定义B.4(奇异与非奇异）对于方阵 \\(A\\) ，若 \\(\\operatorname{det}(A) = 0\\) ，则称其为奇异的，否则为非奇异的."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.656,
                0.737,
                0.714
            ],
            "angle": 0,
            "content": "对于非奇异矩阵 \\(A\\) ，可以证明 \\(\\operatorname{rank}(A) = n\\) 。如果 \\(A\\) 奇异，那么有 \\(\\operatorname{rank}(A) \\leqslant n - 1\\) 。由于非奇异矩阵的满秩性，其对应的线性方程组的解总是存在并且唯一的。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.742,
                0.396,
                0.76
            ],
            "angle": 0,
            "content": "B.1.6 特征值与特征向量"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.737,
                0.853
            ],
            "angle": 0,
            "content": "定义B.5(特征值与特征向量）对于矩阵 \\(A\\in \\mathbb{R}^{n\\times n}\\) ，若存在某个非零向量 \\(v\\in \\mathbb{R}^n\\) 和 \\(\\lambda \\in \\mathbb{R}\\) 使得 \\(Av = \\lambda v\\) ，则称 \\(\\lambda\\) 为该矩阵的特征值， \\(v\\) 为 \\(A\\) 的（对应于特征值 \\(\\lambda\\) 的）特征向量．矩阵 \\(A - \\lambda I\\) 的零空间称为特征值 \\(\\lambda\\) 的特征子空间."
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "502"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.297
            ],
            "angle": 0,
            "content": "容易看出，\\(\\lambda\\) 是 \\(A\\) 的特征值的充要条件是 \\(\\operatorname{det}(\\lambda I - A) = 0\\)。由行列式的定义可知，\\(\\operatorname{det}(\\lambda I - A)\\) 是一个关于 \\(\\lambda\\) 的 \\(n\\) 次多项式，我们称其为矩阵 \\(A\\) 的特征多项式，一般用 \\(p_A(\\lambda)\\) 表示。因为 \\(p_A(\\lambda)\\) 是 \\(n\\) 次多项式，故其有 \\(n\\) 个复根。特别地，对于一个对角矩阵，我们可知其特征值全体就是其对角线元素构成的集合。对于特征值 \\(\\lambda\\)，如果它是特征多项式的单根，我们称之为单特征值；如果它是特征多项式的重根，我们称之为重特征值。\\(n\\) 阶方阵有 \\(n\\) 个特征值（计重数）。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.303,
                0.825,
                0.339
            ],
            "angle": 0,
            "content": "在求解和分析矩阵的特征值与特征向量时，一个重要的概念是相似矩阵."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.355,
                0.825,
                0.392
            ],
            "angle": 0,
            "content": "定义B.6（相似矩阵）设 \\(A,B\\in \\mathbb{R}^{n\\times n}\\) ，若存在非奇异矩阵 \\(X\\in \\mathbb{R}^{n\\times n}\\) ，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.492,
                0.398,
                0.591,
                0.415
            ],
            "angle": 0,
            "content": "\\[\nB = X A X ^ {- 1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.429,
                0.431,
                0.443
            ],
            "angle": 0,
            "content": "则称 \\(A\\) 与 \\(B\\) 是相似的."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.46,
                0.825,
                0.539
            ],
            "angle": 0,
            "content": "相似关系是矩阵之间的一种等价关系。根据行列式的性质，我们知道相似矩阵有相同的特征多项式，因而有相同的特征值。因此，如果能找到非奇异矩阵 \\(X\\) 使得矩阵 \\(XAX^{-1}\\) 是对角的，那么我们就找到了 \\(A\\) 的全体特征值 \\((XAX^{-1}\\) 的对角线元素）。"
        },
        {
            "type": "text",
            "bbox": [
                0.291,
                0.544,
                0.785,
                0.56
            ],
            "angle": 0,
            "content": "对于矩阵的特征值和特征子空间，我们不加证明地给出以下性质："
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.577,
                0.599,
                0.593
            ],
            "angle": 0,
            "content": "(1) 单特征值对应的特征子空间的维数为 1;"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.61,
                0.634,
                0.626
            ],
            "angle": 0,
            "content": "(2) 特征子空间的维数不大于对应特征值的重数"
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.577,
                0.634,
                0.626
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.641,
                0.825,
                0.679
            ],
            "angle": 0,
            "content": "通过展开特征多项式 \\(p_A(\\lambda)\\) 并利用韦达定理，我们可以得到行列式、迹与特征值的关系：给定 \\(A \\in \\mathbb{R}^{n \\times n}\\)，及其特征值 \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_n\\)，那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.416,
                0.69,
                0.665,
                0.728
            ],
            "angle": 0,
            "content": "\\[\n\\det  (A) = \\prod_ {i = 1} ^ {n} \\lambda_ {i}, \\quad \\operatorname {T r} (A) = \\sum_ {i = 1} ^ {n} \\lambda_ {i}.\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.749,
                0.385,
                0.767
            ],
            "angle": 0,
            "content": "B.1.7 广义逆"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.782,
                0.825,
                0.818
            ],
            "angle": 0,
            "content": "一般问题中，矩阵 \\(A\\) 不是方阵，即使是方阵，也不一定可逆。因此，我们需要定义矩阵的广义逆。对于矩阵 \\(A \\in \\mathbb{R}^{m \\times n}\\)，其广义逆是指使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.501,
                0.837,
                0.824,
                0.853
            ],
            "angle": 0,
            "content": "\\[\nA G A = A \\tag {B.1.1}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.32,
                0.133
            ],
            "angle": 0,
            "content": "B.1 线性代数基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "503"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.194
            ],
            "angle": 0,
            "content": "成立的 \\(G \\in \\mathbb{R}^{n \\times m}\\) 。一般来说广义逆是不唯一的。可以看到当 \\(m = n\\) 且 \\(A\\) 可逆时，其广义逆唯一，即 \\(A^{-1}\\) 。"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.198,
                0.737,
                0.216
            ],
            "angle": 0,
            "content": "我们介绍一类特殊的广义逆，即Moore-Penrose逆，记为 \\(A^{\\dagger}\\)，其满足"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.225,
                0.308,
                0.242
            ],
            "angle": 0,
            "content": "(1) \\(AA^{\\dagger}A = A\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.253,
                0.32,
                0.27
            ],
            "angle": 0,
            "content": "(2) \\(A^{\\dagger}AA^{\\dagger} = A^{\\dagger}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.283,
                0.347,
                0.3
            ],
            "angle": 0,
            "content": "(3) \\(AA^{\\dagger}\\) 为对称矩阵；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.311,
                0.344,
                0.329
            ],
            "angle": 0,
            "content": "(4) \\(A^{\\dagger}A\\) 为对称矩阵"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.225,
                0.347,
                0.329
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.339,
                0.739,
                0.419
            ],
            "angle": 0,
            "content": "可以证明这种广义逆矩阵总是存在且是唯一的. 我们给出一种Moore-Penrose逆矩阵的构造方法．记 \\(r = \\operatorname{rank}(A)\\) ，那么 \\(A\\) 可以做一个满秩分解，即 \\(A = BC\\) ，其中 \\(B \\in \\mathbb{R}^{m \\times r}\\) ，\\(C \\in \\mathbb{R}^{r \\times n}\\) ，且 \\(\\operatorname{rank}(A) = \\operatorname{rank}(B) = \\operatorname{rank}(C) = r\\) 那么Moore-Penrose逆矩阵可以表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.346,
                0.428,
                0.561,
                0.448
            ],
            "angle": 0,
            "content": "\\[\nA ^ {\\dagger} = C ^ {T} \\left(C C ^ {T}\\right) ^ {- 1} \\left(B ^ {T} B\\right) ^ {- 1} B ^ {T}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.459,
                0.573,
                0.476
            ],
            "angle": 0,
            "content": "进一步地，我们给出Moore-Penrose逆的一些性质"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.485,
                0.632,
                0.502
            ],
            "angle": 0,
            "content": "引理 B.1 设 \\(A \\in \\mathbb{R}^{m \\times n}\\), 其 Moore-Penrose 逆为 \\(A^{\\dagger}\\), 则"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.512,
                0.3,
                0.53
            ],
            "angle": 0,
            "content": "(1) \\((A^{\\dagger})^{\\dagger} = A\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.54,
                0.331,
                0.56
            ],
            "angle": 0,
            "content": "(2) \\((A^{\\mathrm{T}})^{\\dagger} = (A^{\\dagger})^{\\mathrm{T}}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.569,
                0.32,
                0.587
            ],
            "angle": 0,
            "content": "(3) \\(A^{\\dagger}AA^{\\mathrm{T}} = A^{\\mathrm{T}}\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.512,
                0.331,
                0.587
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.598,
                0.739,
                0.657
            ],
            "angle": 0,
            "content": "有了Moore-Penrose广义逆矩阵之后，对于线性方程组 \\(Ax = b\\) （假设其解存在，即 \\(b\\in \\mathcal{R}(A)\\) ），其任意解可以表示为 \\(x = A^{\\dagger}b + (I - A^{\\dagger}A)w\\) 其中 \\(w\\in \\mathbb{R}^n\\) 为任意向量."
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.681,
                0.555,
                0.7
            ],
            "angle": 0,
            "content": "B.1.8 Sherman-Morrison-Woodbury 公式"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.712,
                0.74,
                0.854
            ],
            "angle": 0,
            "content": "一般来说，求解矩阵的逆矩阵是比较困难的，我们只能对低阶矩阵和一些带有特殊结构的矩阵来快速求逆。这里介绍一类带特殊结构的矩阵的逆矩阵，其在实际中被广泛应用。具体地，假设已知某个矩阵的逆矩阵（或者其逆矩阵可以很容易地算出），我们对该矩阵做了一些改动得到一个新矩阵，那么新矩阵的逆矩阵与原矩阵的逆矩阵之间存在着某种联系。Sherman-Morrison-Woodbury（SMW）公式就给出了原矩阵和其在秩 \\(k\\) 更新后的矩阵的逆矩阵之间的关系。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "504"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.156,
                0.827,
                0.215
            ],
            "angle": 0,
            "content": "定理B.2(SMW公式）设 \\(A\\in \\mathbb{R}^{n\\times n}\\) 为可逆矩阵，给定矩阵 \\(U\\in \\mathbb{R}^{n\\times k}\\) \\(C\\in \\mathbb{R}^{k\\times k}\\) ， \\(V\\in \\mathbb{R}^{k\\times n}\\) 且 \\(C\\) 可逆．那么 \\(A + UCV\\) 可逆当且仅当 \\(C^{-1} + VA^{-1}U\\) 可逆，且此时 \\(A + UCV\\) 的逆矩阵可以表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.316,
                0.23,
                0.825,
                0.249
            ],
            "angle": 0,
            "content": "\\[\n(A + U C V) ^ {- 1} = A ^ {- 1} - A ^ {- 1} U \\left(C ^ {- 1} + V A ^ {- 1} U\\right) ^ {- 1} V A ^ {- 1}. \\tag {B.1.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.265,
                0.71,
                0.282
            ],
            "angle": 0,
            "content": "当 \\(k = 1\\) 且 \\(C = 1\\) 时，可以得到一个更为简单而实用的推论"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.295,
                0.825,
                0.334
            ],
            "angle": 0,
            "content": "推论B.1设 \\(A\\in \\mathbb{R}^{n\\times n}\\) 为可逆矩阵，给定向量 \\(u\\) ， \\(v\\in \\mathbb{R}^n\\) .那么 \\(A + uv^{\\mathrm{T}}\\) 可逆当且仅当 \\(1 + v^{\\mathrm{T}}A^{-1}u\\neq 0\\) ，且此时 \\(A + uv^{\\mathrm{T}}\\) 的逆矩阵可以表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.409,
                0.343,
                0.677,
                0.378
            ],
            "angle": 0,
            "content": "\\[\n\\left(A + u v ^ {\\mathrm {T}}\\right) ^ {- 1} = A ^ {- 1} - \\frac {A ^ {- 1} u v ^ {\\mathrm {T}} A ^ {- 1}}{1 + v ^ {\\mathrm {T}} A ^ {- 1} u}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.387,
                0.493,
                0.404
            ],
            "angle": 0,
            "content": "当 \\(C = I\\) 时，我们有如下结论："
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.417,
                0.825,
                0.475
            ],
            "angle": 0,
            "content": "推论B.2设 \\(A\\in \\mathbb{R}^{n\\times n}\\) 为可逆矩阵，给定矩阵 \\(U\\in \\mathbb{R}^{n\\times k}\\) ， \\(V\\in \\mathbb{R}^{k\\times n}\\) .那么 \\(A + UV\\) 可逆当且仅当 \\(I + VA^{-1}U\\) 可逆，且此时 \\(A + UV\\) 的逆矩阵可以表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.355,
                0.478,
                0.728,
                0.498
            ],
            "angle": 0,
            "content": "\\[\n(A + U V) ^ {- 1} = A ^ {- 1} - A ^ {- 1} U (I + V A ^ {- 1} U) ^ {- 1} V A ^ {- 1}.\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.523,
                0.403,
                0.541
            ],
            "angle": 0,
            "content": "B.1.9 Schur补"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.555,
                0.825,
                0.592
            ],
            "angle": 0,
            "content": "Schur补是线性代数中一个重要的概念，是处理分块矩阵的常用工具.设分块矩阵 \\(M\\in \\mathbb{R}^{(m + n)\\times (m + n)}\\) ，且有结构"
        },
        {
            "type": "equation",
            "bbox": [
                0.483,
                0.602,
                0.602,
                0.647
            ],
            "angle": 0,
            "content": "\\[\nM = \\left[ \\begin{array}{c c} A & B \\\\ C & D \\end{array} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.658,
                0.825,
                0.695
            ],
            "angle": 0,
            "content": "其中 \\(A \\in \\mathbb{R}^{m \\times m}, D \\in \\mathbb{R}^{n \\times n}\\). 若 \\(D\\) 可逆, 则 \\(D\\) 在 \\(M\\) 中的 Schur 补为 \\(A - BD^{-1}C\\); 若 \\(A\\) 可逆, 则 \\(A\\) 在 \\(M\\) 中的 Schur 补为 \\(D - CA^{-1}B\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.699,
                0.827,
                0.736
            ],
            "angle": 0,
            "content": "Schur补的来源是矩阵的初等行变换（即矩阵分块消元）．假设 \\(D\\) 可逆，定义初等行变换"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.735,
                0.617,
                0.78
            ],
            "angle": 0,
            "content": "\\[\nL = \\left[ \\begin{array}{c c} I & - B D ^ {- 1} \\\\ 0 & I \\end{array} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.785,
                0.825,
                0.802
            ],
            "angle": 0,
            "content": "矩阵 \\(L\\) 左乘 \\(M\\) 的效果就是使用 \\(M\\) 的第二行的块消去第一行第二列的块，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.345,
                0.811,
                0.737,
                0.857
            ],
            "angle": 0,
            "content": "\\[\nL M = \\left[ \\begin{array}{c c} I & - B D ^ {- 1} \\\\ 0 & I \\end{array} \\right] \\left[ \\begin{array}{c c} A & B \\\\ C & D \\end{array} \\right] = \\left[ \\begin{array}{c c} A - B D ^ {- 1} C & 0 \\\\ C & D \\end{array} \\right].\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "B.2 数值代数基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "505"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.314,
                0.174
            ],
            "angle": 0,
            "content": "再定义初等列变换"
        },
        {
            "type": "equation",
            "bbox": [
                0.379,
                0.171,
                0.528,
                0.215
            ],
            "angle": 0,
            "content": "\\[\nR = \\left[ \\begin{array}{c c} I & 0 \\\\ - D ^ {- 1} C & I \\end{array} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.218,
                0.737,
                0.255
            ],
            "angle": 0,
            "content": "则 \\(R\\) 右乘 \\(M\\) 的效果就是使用 \\(M\\) 的第二列的块消去第一列第二行的块，结合 \\(L\\) 和 \\(R\\) 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.254,
                0.562,
                0.297
            ],
            "angle": 0,
            "content": "\\[\nL M R = \\left[ \\begin{array}{c c} A - B D ^ {- 1} C & 0 \\\\ 0 & D \\end{array} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.3,
                0.51,
                0.316
            ],
            "angle": 0,
            "content": "和 \\(D\\) 相对的块正好是 \\(D\\) 在 \\(M\\) 中的Schur补"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.321,
                0.627,
                0.337
            ],
            "angle": 0,
            "content": "利用Schur补我们可得到一个关于正定矩阵的判定方法"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.348,
                0.451,
                0.363
            ],
            "angle": 0,
            "content": "定理B.3 设分块矩阵 \\(M\\) 有形式"
        },
        {
            "type": "equation",
            "bbox": [
                0.393,
                0.369,
                0.513,
                0.413
            ],
            "angle": 0,
            "content": "\\[\nM = \\left[ \\begin{array}{c c} A & B \\\\ B ^ {\\mathrm {T}} & C \\end{array} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.42,
                0.737,
                0.457
            ],
            "angle": 0,
            "content": "其中矩阵 \\(A\\) 正定，则矩阵 \\(M\\) 正定当且仅当 \\(A\\) 的Schur补 \\(C - B^{\\mathrm{T}}A^{-1}B\\) 正定."
        },
        {
            "type": "title",
            "bbox": [
                0.348,
                0.487,
                0.56,
                0.509
            ],
            "angle": 0,
            "content": "B.2 数值代数基础"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.528,
                0.356,
                0.546
            ],
            "angle": 0,
            "content": "B.2.1 解线性方程组"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.559,
                0.739,
                0.638
            ],
            "angle": 0,
            "content": "数值代数中的一个基本问题是：给定矩阵 \\(A \\in \\mathbb{R}^{m \\times n}\\) 和向量 \\(b \\in \\mathbb{R}^m\\)，求解 \\(x \\in \\mathbb{R}^n\\) 使得 \\(Ax = b\\) 成立，即求解线性方程组 \\(Ax = b\\)。若 \\(A\\) 可逆，则 \\(Ax = b\\) 的解可以写为 \\(x = A^{-1}b\\)。通常来说，矩阵求逆的代价比直接解对应的线性方程组的代价要高，因此在计算机中我们应该尽量避免求逆运算。"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.642,
                0.618,
                0.658
            ],
            "angle": 0,
            "content": "一般的求解稠密矩阵 \\(A\\) 对应的线性方程组的流程如下："
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.668,
                0.707,
                0.686
            ],
            "angle": 0,
            "content": "(1) 将矩阵 \\(A \\in \\mathbb{R}^{n \\times n}\\) 分解为若干个简单矩阵的乘积（一般 2 到 3 个）："
        },
        {
            "type": "equation",
            "bbox": [
                0.409,
                0.697,
                0.54,
                0.713
            ],
            "angle": 0,
            "content": "\\[\nA = A _ {1} A _ {2} \\dots A _ {k},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.725,
                0.621,
                0.742
            ],
            "angle": 0,
            "content": "\\(A_{i}\\) 一般为对角矩阵、下（上）三角矩阵、正交矩阵等."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.754,
                0.613,
                0.77
            ],
            "angle": 0,
            "content": "(2) 通过求解 \\(k\\) 个更简单的方程组来求出原方程组的解 \\(x\\):"
        },
        {
            "type": "equation",
            "bbox": [
                0.31,
                0.782,
                0.637,
                0.798
            ],
            "angle": 0,
            "content": "\\[\nA _ {1} x _ {1} = b, \\quad A _ {2} x _ {2} = x _ {1}, \\quad \\dots , \\quad A _ {k} x = x _ {k - 1}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.21,
                0.81,
                0.531,
                0.827
            ],
            "angle": 0,
            "content": "通常矩阵分解这一步的计算量占主导地位"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.836,
                0.592,
                0.853
            ],
            "angle": 0,
            "content": "这里列举一些在求解线性方程组中常用的矩阵分解"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "506"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.157,
                0.643,
                0.174
            ],
            "angle": 0,
            "content": "1. LU分解，Cholesky分解，对称不定矩阵分解"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.195,
                0.687,
                0.212
            ],
            "angle": 0,
            "content": "考虑非奇异矩阵 \\(A \\in \\mathbb{R}^{n \\times n}\\), 其 LU 分解可以表示为:"
        },
        {
            "type": "equation",
            "bbox": [
                0.502,
                0.238,
                0.584,
                0.253
            ],
            "angle": 0,
            "content": "\\[\nP A = L U,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.279,
                0.828,
                0.635
            ],
            "angle": 0,
            "content": "其中 \\(L\\) 为对角元均为1的下三角矩阵，\\(U\\) 为上三角矩阵，\\(P\\) 是排列矩阵. LU分解实际上就是对矩阵 \\(A\\) 进行高斯消元的过程，由于消元中可能会出现主元为零的情况，因此需要引入排列矩阵 \\(P\\) 来交换系数矩阵 \\(A\\) 的行的顺序. 特别地，当矩阵 \\(A\\) 对称正定时，存在对角元均为正数的下三角矩阵 \\(L \\in \\mathbb{R}^{n \\times n}\\)，使得 \\(A = LL^{\\mathrm{T}}\\). 我们称之为正定矩阵的Cholesky分解. 该分解还有一个等价的形式：\\(A = LDL^{\\mathrm{T}}\\)，其中 \\(L\\) 是对角元均为1的下三角矩阵，\\(D\\) 是对角元为正数的对角矩阵．和Cholesky分解相比，该形式的优点是计算过程中不需要进行开方运算. 当 \\(A\\) 对称不定时，我们有对称不定矩阵分解，即存在排列矩阵 \\(P\\)，对角元均为1的下三角矩阵 \\(L\\) 和块对角矩阵 \\(D\\)，使得 \\(PAP^{\\mathrm{T}} = LDL^{\\mathrm{T}}\\)，其中 \\(D\\) 的每一个对角块为 \\(1 \\times 1\\) 或 \\(2 \\times 2\\) 的矩阵. LU分解，Cholesky分解，对称不定矩阵分解是矩阵计算中最基本的分解方式，它们被用于解决一般的稠密线性方程组问题. 有了这些分解后，我们接下来需要求解一系列系数矩阵为三角矩阵或（块）对角矩阵的方程组问题，后面会介绍这些有特殊结构的矩阵对应的方程组的求法．通常来讲对一般矩阵的LU分解所需运算量（复杂度）为 \\(\\mathcal{O}\\left(\\frac{2n^3}{3}\\right)\\)，而Cholesky分解的复杂度为 \\(\\mathcal{O}\\left(\\frac{n^3}{3}\\right)\\)，为LU分解的一半．原因是Cholesky分解利用了对称正定矩阵的结构．所以在实际计算中我们需要针对不同类型的矩阵调用不同的分解方式来提高效率."
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.682,
                0.36,
                0.699
            ],
            "angle": 0,
            "content": "2. QR分解"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.721,
                0.825,
                0.779
            ],
            "angle": 0,
            "content": "QR分解（又称正交分解）是数值代数中的另一种分解，它可作用于一般的长方形矩阵上．对于“瘦高”形状的矩阵 \\(A\\in \\mathbb{R}^{m\\times n}\\) ，其中 \\(m\\geqslant n\\) ，它的QR分解可以表示为"
        },
        {
            "type": "equation",
            "bbox": [
                0.506,
                0.796,
                0.579,
                0.812
            ],
            "angle": 0,
            "content": "\\[\nA = Q R,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.836,
                0.78,
                0.853
            ],
            "angle": 0,
            "content": "其中 \\(Q \\in \\mathbb{R}^{m \\times m}\\) 为正交矩阵, \\(R \\in \\mathbb{R}^{m \\times n}\\) 是上三角矩阵, 如图B.1所示."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "B.2 数值代数基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "507"
        },
        {
            "type": "image",
            "bbox": [
                0.316,
                0.168,
                0.592,
                0.246
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.388,
                0.283,
                0.52,
                0.3
            ],
            "angle": 0,
            "content": "图B.1 QR分解"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.325,
                0.737,
                0.385
            ],
            "angle": 0,
            "content": "我们注意到 \\(R\\) 的下半部分都是零，而当 \\(m \\gg n\\) 时，这种分解方式会极大地浪费存储空间，因为 \\(Q\\) 的后 \\((m - n)\\) 列是多余的。我们将这部分列向量去除可以得到约化的 QR 分解（又称经济的 QR 分解）："
        },
        {
            "type": "equation",
            "bbox": [
                0.418,
                0.399,
                0.491,
                0.416
            ],
            "angle": 0,
            "content": "\\[\nA = Q R,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.431,
                0.704,
                0.449
            ],
            "angle": 0,
            "content": "其中 \\(Q \\in \\mathbb{R}^{m \\times n}\\) 满足 \\(Q^{\\mathrm{T}} Q = I\\), \\(R \\in \\mathbb{R}^{n \\times n}\\) 为上三角矩阵, 如图B.2所示."
        },
        {
            "type": "image",
            "bbox": [
                0.345,
                0.474,
                0.563,
                0.552
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "image_caption",
            "bbox": [
                0.36,
                0.589,
                0.547,
                0.606
            ],
            "angle": 0,
            "content": "图B.2 约化的QR分解"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.631,
                0.737,
                0.669
            ],
            "angle": 0,
            "content": "对于列满秩矩阵 \\(A\\) ，其满足 \\(R\\) 对角元为正数的约化的QR分解是唯一的，即有如下定理："
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.682,
                0.737,
                0.741
            ],
            "angle": 0,
            "content": "定理B.4(约化的QR分解的唯一性）对于矩阵 \\(A\\in \\mathbb{R}^{m\\times n}(m\\geqslant n)\\) ，假设其列满秩，即 \\(\\mathrm{rank}(A) = n\\) ，那么 \\(A\\) 有唯一的使得上三角矩阵 \\(R\\) 的对角元为正数（即 \\(r_{ii} > 0\\) ）的约化的QR分解，"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.754,
                0.737,
                0.791
            ],
            "angle": 0,
            "content": "我们有多种途径来得到QR分解,例如Gram-Schmidt正交化或Householder三角化，这里不详细展开具体实现过程."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.795,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "和 LU 分解不同, QR 分解除了可以求解线性方程组之外, 还可以用于估计矩阵的秩和求解线性最小二乘问题. 若 \\(A\\) 是方阵, 则求解 \\(Ax = b\\) 等价于求解 \\(Rx = Q^{\\mathrm{T}}b\\); 而对于线性最小二乘问题, 利用约化的 QR 分解可以求"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.289,
                0.13
            ],
            "angle": 0,
            "content": "508"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.194
            ],
            "angle": 0,
            "content": "出其最小二乘解；还可以根据QR分解中 \\(R\\) 对角元的零元素个数大致判断\\(A\\) 的秩1."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.198,
                0.826,
                0.284
            ],
            "angle": 0,
            "content": "QR分解的计算量通常要比LU分解等的计算量更大，例如进行 \\(m \\times n\\) 矩阵的约化的QR分解至少需要 \\(\\mathcal{O}\\left(\\frac{4mn^2}{3}\\right)\\) 的计算量。但QR分解的优点是数值稳定性较好，可以用于求解欠定方程组或超定方程组（即最小二乘问题）。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.31,
                0.621,
                0.329
            ],
            "angle": 0,
            "content": "B.2.2 系数矩阵为特殊矩阵的方程组解法"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.341,
                0.825,
                0.379
            ],
            "angle": 0,
            "content": "利用矩阵分解可将一般的矩阵分解成结构简单的矩阵，接下来介绍如何求解这些带特殊结构的矩阵对应的方程组."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.403,
                0.414,
                0.42
            ],
            "angle": 0,
            "content": "1. (块) 对角矩阵"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.433,
                0.768,
                0.451
            ],
            "angle": 0,
            "content": "对应于 \\(n\\) 阶对角矩阵 \\(A\\) 的线性方程组 \\(Ax = b\\) 的求解是简单的，"
        },
        {
            "type": "equation",
            "bbox": [
                0.413,
                0.459,
                0.671,
                0.498
            ],
            "angle": 0,
            "content": "\\[\nx = A ^ {- 1} b = \\left(\\frac {b _ {1}}{a _ {1 1}}, \\frac {b _ {2}}{a _ {2 2}}, \\dots , \\frac {b _ {n}}{a _ {n n}}\\right) ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.508,
                0.707,
                0.525
            ],
            "angle": 0,
            "content": "其计算量为 \\(\\mathcal{O}(n)\\). 类似地, 当 \\(A\\) 为块对角矩阵时, 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.455,
                0.535,
                0.627,
                0.627
            ],
            "angle": 0,
            "content": "\\[\nx = A ^ {- 1} b = \\left[ \\begin{array}{c} A _ {1 1} ^ {- 1} b _ {1} \\\\ A _ {2 2} ^ {- 1} b _ {2} \\\\ \\vdots \\\\ A _ {k k} ^ {- 1} b _ {k} \\end{array} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.637,
                0.651,
                0.654
            ],
            "angle": 0,
            "content": "其中 \\(k\\) 为 \\(A\\) 对角线的块数，\\(b\\) 的分块方式和 \\(A\\) 相同"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.678,
                0.362,
                0.695
            ],
            "angle": 0,
            "content": "2. 三角矩阵"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.708,
                0.826,
                0.746
            ],
            "angle": 0,
            "content": "另一类常见的矩阵是上三角矩阵和下三角矩阵，我们以 \\(n\\) 阶下三角矩阵 \\(A\\) 为例来介绍对应的线性方程组 \\(Ax = b\\) 的求解方法．具体地，我们可以"
        },
        {
            "type": "page_footnote",
            "bbox": [
                0.257,
                0.824,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "这种做法不稳定，一般是利用选主元的QR分解对矩阵的秩进行估计，这种QR分解一般又称RRQR（rankrevealing QR）分解."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "B.2 数值代数基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "509"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.512,
                0.174
            ],
            "angle": 0,
            "content": "利用前向替代法来依次求解 \\(x_{1}, x_{2}, \\dots, x_{n}\\)，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.282,
                0.184,
                0.359,
                0.215
            ],
            "angle": 0,
            "content": "\\[\nx _ {1} = \\frac {1}{a _ {1 1}} b _ {1}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.282,
                0.219,
                0.43,
                0.248
            ],
            "angle": 0,
            "content": "\\[\nx _ {2} = \\frac {1}{a _ {2 2}} \\left(b _ {2} - a _ {2 1} x _ {1}\\right)\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.282,
                0.252,
                0.487,
                0.283
            ],
            "angle": 0,
            "content": "\\[\nx _ {3} = \\frac {1}{a _ {3 3}} \\left(b _ {3} - a _ {3 1} x _ {1} - a _ {3 2} x _ {2}\\right)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.28,
                0.295,
                0.364,
                0.301
            ],
            "angle": 0,
            "content": "··"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.313,
                0.632,
                0.343
            ],
            "angle": 0,
            "content": "\\[\nx _ {n} = \\frac {1}{a _ {n n}} \\left(b _ {n} - a _ {n 1} x _ {1} - a _ {n 2} x _ {2} - \\dots - a _ {n, n - 1} x _ {n - 1}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.354,
                0.738,
                0.392
            ],
            "angle": 0,
            "content": "其计算量为 \\(\\mathcal{O}(n^2)\\). 而对于上三角矩阵也有类似的方法, 称之为后向替代法, 即按照 \\(x_{n}, x_{n-1}, \\cdots, x_{1}\\) 的顺序依次进行求解."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.416,
                0.275,
                0.433
            ],
            "angle": 0,
            "content": "3. 正交矩阵"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.445,
                0.737,
                0.484
            ],
            "angle": 0,
            "content": "对于一般的正交矩阵 \\(A\\) ，因为 \\(A^{\\mathrm{T}}A = I\\) ，故线性方程组 \\(Ax = b\\) 的解可以表示为 \\(x = A^{\\mathrm{T}}b\\) 计算量为 \\(\\mathcal{O}(2n^{2})\\)"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.507,
                0.343,
                0.523
            ],
            "angle": 0,
            "content": "4. 有结构子块的矩阵"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.537,
                0.738,
                0.594
            ],
            "angle": 0,
            "content": "若矩阵中某个子块带有一定的结构，如为对角阵、正交矩阵等，那么如何有效利用该结构来更快地求解线性方程组也是实际中常常遇到的问题。考虑线性方程组"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.594,
                0.736,
                0.636
            ],
            "angle": 0,
            "content": "\\[\n\\left[ \\begin{array}{l l} A _ {1 1} & A _ {1 2} \\\\ A _ {2 1} & A _ {2 2} \\end{array} \\right] \\left[ \\begin{array}{l} x _ {1} \\\\ x _ {2} \\end{array} \\right] = \\left[ \\begin{array}{l} b _ {1} \\\\ b _ {2} \\end{array} \\right], \\tag {B.2.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.641,
                0.738,
                0.7
            ],
            "angle": 0,
            "content": "其中 \\(x_{1} \\in \\mathbb{R}^{n_{1}}\\) ，\\(x_{2} \\in \\mathbb{R}^{n_{2}}\\) ，子块 \\(A_{ij} \\in \\mathbb{R}^{n_{i} \\times n_{j}}\\) ，\\(A_{11}\\) 非奇异且带有一定的结构.对于该线性方程组的求解，利用前 \\(n_{1}\\) 个等式，我们有 \\(x_{1} = A_{11}^{-1}(b_{1} - A_{12}x_{2})\\) 因此只需要求出 \\(x_{2}\\) ：将 \\(x_{1}\\) 的表达式代入后 \\(n_{2}\\) 个等式，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.306,
                0.714,
                0.602,
                0.733
            ],
            "angle": 0,
            "content": "\\[\n\\left(A _ {2 2} - A _ {2 1} A _ {1 1} ^ {- 1} A _ {1 2}\\right) x _ {2} = b _ {2} - A _ {2 1} A _ {1 1} ^ {- 1} b _ {1},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.747,
                0.732,
                0.764
            ],
            "angle": 0,
            "content": "从而将一个高维的问题转化为较低维的问题。我们给出该方法的具体步骤："
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.775,
                0.388,
                0.793
            ],
            "angle": 0,
            "content": "(1) 计算 \\(A_{11}^{-1}A_{12}\\) 和 \\(A_{11}^{-1}b_{1}\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.805,
                0.613,
                0.824
            ],
            "angle": 0,
            "content": "(2) 计算 \\(S = A_{22} - A_{21}\\left(A_{11}^{-1}A_{12}\\right)\\) 和 \\(\\tilde{b} = b_{2} - A_{21}\\left(A_{11}^{-1}b_{1}\\right)\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.835,
                0.403,
                0.852
            ],
            "angle": 0,
            "content": "(3) 求解线性方程组 \\(Sx_{2} = \\tilde{b}\\);"
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.775,
                0.613,
                0.852
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "510"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.157,
                0.574,
                0.174
            ],
            "angle": 0,
            "content": "(4) 求解线性方程组 \\(A_{11}x_{1} = b_{1} - A_{12}x_{2}\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.193,
                0.545,
                0.21
            ],
            "angle": 0,
            "content": "下面分析各个步骤主要的计算量："
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.215,
                0.825,
                0.253
            ],
            "angle": 0,
            "content": "步骤(1)需要对 \\(A_{11}\\) 进行分解，用 \\(f\\) 表示分解的计算量，之后求解 \\((n_2 + 1)\\) 个方程， \\(s\\) 是解方程的计算量，故计算量约为 \\(f + n_{2}s\\) ："
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.256,
                0.777,
                0.275
            ],
            "angle": 0,
            "content": "步骤 (2) 的计算量主要来自求 \\(A_{21}\\left(A_{11}^{-1} A_{12}\\right)\\), 计算量约为 \\(2 n_{2}^{2} n_{1}\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.275,
                0.694,
                0.306
            ],
            "angle": 0,
            "content": "步骤 (3) 主要计算量是对 \\(S\\) 的分解, 其计算量为 \\(\\frac{2}{3} n_{2}^{3}\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.306,
                0.617,
                0.322
            ],
            "angle": 0,
            "content": "步骤 (4) 的计算量来自计算 \\(A_{12}x_{2}\\)，可忽略"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.322,
                0.825,
                0.367
            ],
            "angle": 0,
            "content": "故其总的计算量约为 \\( f + n_{2} s + 2 n_{2}^{2} n_{1} + \\frac{2}{3} n_{2}^{3} \\); 而用一般的方法来求解方程组 (B.2.1) 所需的计算量大约为"
        },
        {
            "type": "equation",
            "bbox": [
                0.379,
                0.379,
                0.705,
                0.412
            ],
            "angle": 0,
            "content": "\\[\n\\frac {2}{3} \\left(n _ {1} + n _ {2}\\right) ^ {3} = \\frac {2}{3} n _ {1} ^ {3} + 2 n _ {1} ^ {2} n _ {2} + 2 n _ {1} n _ {2} ^ {2} + \\frac {2}{3} n _ {2} ^ {3}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.425,
                0.826,
                0.485
            ],
            "angle": 0,
            "content": "若 \\(A_{11}\\) 带有一定的结构，如上三角矩阵，那么计算量约为 \\(n_1^2 n_2 + 2n_2^2 n_1 + \\frac{2}{3} n_2^3\\) 若 \\(A_{11}\\) 为对角矩阵，则计算量降低为约 \\(2n_2^2 n_1 + \\frac{2}{3} n_2^3.\\)"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.512,
                0.449,
                0.528
            ],
            "angle": 0,
            "content": "5. 结构矩阵加上低秩项"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.545,
                0.419,
                0.561
            ],
            "angle": 0,
            "content": "考虑线性方程组"
        },
        {
            "type": "equation",
            "bbox": [
                0.479,
                0.57,
                0.825,
                0.588
            ],
            "angle": 0,
            "content": "\\[\n(A + U V) x = b, \\tag {B.2.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.604,
                0.827,
                0.641
            ],
            "angle": 0,
            "content": "其中可逆矩阵 \\(A \\in \\mathbb{R}^{n \\times n}\\) 具有一定结构，即假设求解 \\(Au = c\\) 是非常容易的，\\(U \\in \\mathbb{R}^{n \\times k}\\)，\\(V \\in \\mathbb{R}^{k \\times n}\\)．实际上，这等价于求解"
        },
        {
            "type": "equation",
            "bbox": [
                0.46,
                0.654,
                0.825,
                0.7
            ],
            "angle": 0,
            "content": "\\[\n\\left[ \\begin{array}{l l} A & U \\\\ V & - I \\end{array} \\right] \\left[ \\begin{array}{l} x \\\\ y \\end{array} \\right] = \\left[ \\begin{array}{l} b \\\\ 0 \\end{array} \\right], \\tag {B.2.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.713,
                0.825,
                0.75
            ],
            "angle": 0,
            "content": "这就转化为了上一节所讨论的矩阵。利用前面的做法，我们先求解线性方程组"
        },
        {
            "type": "equation",
            "bbox": [
                0.449,
                0.758,
                0.637,
                0.777
            ],
            "angle": 0,
            "content": "\\[\n(I + V A ^ {- 1} U) y = V A ^ {- 1} b,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.792,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "然后求解线性方程组 \\(Ax = b - Uy\\)。这实际上这给出了推论B.2的一个证明。另外讨论可知该方法的计算量为 \\(f + ks + 2k^2 n + \\frac{2}{3} k^3\\)，其中 \\(f\\) 为分解矩阵 \\(A\\) 的计算量，\\(s\\) 为求解的计算量。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.32,
                0.133
            ],
            "angle": 0,
            "content": "B.2 数值代数基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.736,
                0.131
            ],
            "angle": 0,
            "content": "511"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.156,
                0.455,
                0.175
            ],
            "angle": 0,
            "content": "B.2.3 特征值分解与奇异值分解"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.188,
                0.741,
                0.309
            ],
            "angle": 0,
            "content": "矩阵的特征值与特征向量求解是线性代数的另一个基本问题，在数据降维、聚类分析等各种实际问题中有重要的应用。对于具体的优化问题而言，其海瑟矩阵的特征值与特征向量也在算法设计和理论分析中扮演着重要角色。在半定规划和低秩矩阵优化等问题中，很多一阶算法的每一次迭代都需要用到特征值分解或奇异值分解。因此，如何在计算机中求解一个矩阵的特征值分解或奇异值分解也是数值代数研究的问题之一。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.335,
                0.292,
                0.351
            ],
            "angle": 0,
            "content": "1. 特征值分解"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.365,
                0.739,
                0.402
            ],
            "angle": 0,
            "content": "对于矩阵 \\(A \\in \\mathbb{R}^{n \\times n}\\), 若存在非奇异矩阵 \\(X \\in \\mathbb{R}^{n \\times n}\\) 和对角矩阵 \\(\\Sigma \\in \\mathbb{R}^{n \\times n}\\), 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.404,
                0.407,
                0.504,
                0.422
            ],
            "angle": 0,
            "content": "\\[\nA = X \\Sigma X ^ {- 1}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.437,
                0.738,
                0.474
            ],
            "angle": 0,
            "content": "则称 \\(A\\) 可对角化，并称上式为 \\(A\\) 的特征值分解。矩阵 \\(\\Sigma\\) 的对角元为 \\(A\\) 的特征值，矩阵 \\(X\\) 的每一列为其对应的特征向量。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.479,
                0.739,
                0.537
            ],
            "angle": 0,
            "content": "计算一个任意方阵的特征值分解通常来说是比较难的，其最大的困难是对任意的方阵 \\(A\\) ，上述特征值分解未必存在．然而在很多优化问题中，我们研究的往往是实对称矩阵．而实对称矩阵的特征值分解有非常好的性质："
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.552,
                0.482,
                0.569
            ],
            "angle": 0,
            "content": "(1) 实对称矩阵的所有特征值均为实数；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.584,
                0.637,
                0.601
            ],
            "angle": 0,
            "content": "(2) 实对称矩阵对应于不同特征值的特征向量是相互正交的；"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.616,
                0.735,
                0.653
            ],
            "angle": 0,
            "content": "(3) 实对称矩阵可以正交对角化, 即存在正交矩阵 \\(U\\), 使得 \\(U^{\\mathrm{T}} A U\\) 为对角矩阵."
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.552,
                0.735,
                0.653
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.669,
                0.738,
                0.706
            ],
            "angle": 0,
            "content": "从上面的结果可以看出，实对称矩阵的特征值分解总是存在的，即给定 \\(A \\in S^n\\)，其有分解"
        },
        {
            "type": "equation",
            "bbox": [
                0.407,
                0.71,
                0.737,
                0.728
            ],
            "angle": 0,
            "content": "\\[\nA = U \\Lambda U ^ {\\mathrm {T}}, \\tag {B.2.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.741,
                0.739,
                0.819
            ],
            "angle": 0,
            "content": "其中 \\(U \\in \\mathbb{R}^{n \\times n}\\) 为正交矩阵, \\(\\Lambda = \\operatorname{Diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)\\) 为对角矩阵并且 \\(\\lambda_i, i = 1, 2, \\dots, n\\) 对应于 \\(A\\) 的特征值. 记 \\(U = [u_1, u_2, \\dots, u_n]\\), 那么 \\(u_i\\) 为特征值 \\(\\lambda_i\\) 对应的特征向量, 我们还可以将 (B.2.4) 式写成如下秩 1 矩阵的和的形式 (又称为外积形式):"
        },
        {
            "type": "equation",
            "bbox": [
                0.397,
                0.82,
                0.51,
                0.856
            ],
            "angle": 0,
            "content": "\\[\nA = \\sum_ {i = 1} ^ {n} \\lambda_ {i} u _ {i} u _ {i} ^ {\\mathrm {T}}.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "512"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.256
            ],
            "angle": 0,
            "content": "对于矩阵特征值分解的计算，非常自然的想法是计算矩阵 \\(A\\) 的特征多项式 \\(p_A(\\lambda)\\)。但是由于计算行列式是困难的，并且多项式的求根同样在数值上是困难的，所以这一方法通常行不通。在数值代数中，矩阵特征值分解的计算也分为直接法和迭代法，直接法如QR方法，迭代法如幂法、Krylov子空间法等，我们这里不详细展开。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.283,
                0.38,
                0.299
            ],
            "angle": 0,
            "content": "2. 奇异值分解"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.313,
                0.825,
                0.391
            ],
            "angle": 0,
            "content": "特征值分解关注的对象一般是实对称矩阵。当矩阵不是方阵时，我们可以考虑它的奇异值分解。该分解在信息检索、统计学、信号与图像处理中有大量的应用，并经常出现在各种算法中，也是一种非常具有实用意义和理论意义的矩阵分解。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.407,
                0.825,
                0.441
            ],
            "angle": 0,
            "content": "定理B.5设 \\(A\\in \\mathbb{R}^{m\\times n}(m\\geqslant n)\\) ，则存在正交矩阵 \\(U\\in \\mathbb{R}^{m\\times m}\\) 和 \\(V\\in \\mathbb{R}^{n\\times n}\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.483,
                0.443,
                0.826,
                0.486
            ],
            "angle": 0,
            "content": "\\[\nA = U \\left[ \\begin{array}{l} \\Sigma \\\\ 0 \\end{array} \\right] V ^ {\\mathrm {T}}, \\tag {B.2.5}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.493,
                0.734,
                0.51
            ],
            "angle": 0,
            "content": "其中 \\(\\Sigma = \\mathrm{Diag}(\\sigma_1,\\sigma_2,\\dots ,\\sigma_n)\\) ， \\(\\sigma_{i}\\in \\mathbb{R}\\) 且 \\(\\sigma_{1}\\geqslant \\sigma_{2}\\geqslant \\dots \\geqslant \\sigma_{n}\\geqslant 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.525,
                0.826,
                0.603
            ],
            "angle": 0,
            "content": "我们称 (B.2.5) 式为矩阵 \\(A\\) 的奇异值分解, \\(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{n}\\) 为 \\(A\\) 的奇异值, \\(U\\) 和 \\(V\\) 的每一列分别称为左、右奇异向量. 设 \\(r = \\operatorname{rank}(A) \\leqslant n\\) 为矩阵 \\(A\\) 的秩, 当 \\(r < n\\) 时, \\(\\Sigma\\) 中只有 \\(r\\) 个元素不为 0 , 因此可以得到约化的奇异值分解:"
        },
        {
            "type": "equation",
            "bbox": [
                0.49,
                0.607,
                0.594,
                0.626
            ],
            "angle": 0,
            "content": "\\[\nA = U _ {r} \\Sigma_ {r} V _ {r} ^ {\\mathrm {T}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.637,
                0.825,
                0.696
            ],
            "angle": 0,
            "content": "其中 \\(U_{r} \\in \\mathbb{R}^{m \\times r}\\) 为 (B.2.5) 式中 \\(U\\) 的前 \\(r\\) 列, \\(\\Sigma_{r} \\in \\mathbb{R}^{r \\times r}\\) 为 \\(\\Sigma\\) 的左上方 \\(r \\times r\\) 子块, \\(V_{r}^{\\mathrm{T}} \\in \\mathbb{R}^{r \\times n}\\) 为 \\(V^{\\mathrm{T}}\\) 的前 \\(r\\) 行. 从 \\(U_{r}\\) 和 \\(V_{r}\\) 的取法我们有 \\(U_{r}^{\\mathrm{T}} U_{r} = V_{r}^{\\mathrm{T}} V_{r} = I_{r}\\). 注意, 此时 \\(U_{r} U_{r}^{\\mathrm{T}}\\) 和 \\(V_{r} V_{r}^{\\mathrm{T}}\\) 均不等于单位矩阵."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.7,
                0.825,
                0.758
            ],
            "angle": 0,
            "content": "计算一个矩阵的奇异值分解可转化为对称矩阵特征值分解问题. 从定义式(B.2.5)我们知道 \\(A^{\\mathrm{T}}A = V\\Sigma^{\\mathrm{T}}\\Sigma V^{\\mathrm{T}}\\) ，即 \\(A^{\\mathrm{T}}A\\) 与 \\(\\Sigma^{\\mathrm{T}}\\Sigma\\) 相似，所以要求 \\(A\\) 的奇异值，只需要求 \\(A^{\\mathrm{T}}A\\) 的特征值．其流程可以表示如下："
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.772,
                0.384,
                0.788
            ],
            "angle": 0,
            "content": "(1) 计算 \\(A^{\\mathrm{T}}A\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.803,
                0.613,
                0.82
            ],
            "angle": 0,
            "content": "(2) 计算 \\(A^{\\mathrm{T}} A\\) 的特征值分解: \\(A^{\\mathrm{T}} A = V \\Lambda V^{\\mathrm{T}}\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.835,
                0.583,
                0.852
            ],
            "angle": 0,
            "content": "(3) 对 \\(\\Lambda\\) 所有对角元素开根号: \\(\\Sigma = \\sqrt{\\Lambda}\\);"
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.772,
                0.613,
                0.852
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.319,
                0.133
            ],
            "angle": 0,
            "content": "B.2 数值代数基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.132
            ],
            "angle": 0,
            "content": "513"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.155,
                0.65,
                0.201
            ],
            "angle": 0,
            "content": "(4) 求解正交矩阵 \\(U\\), 使得 \\(U \\begin{bmatrix} \\Sigma \\\\ 0 \\end{bmatrix} = A V\\) (可以利用 QR 分解)."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.217,
                0.744,
                0.337
            ],
            "angle": 0,
            "content": "需要指出的是，因为 \\(A^{\\mathrm{T}}A\\) 损失了原矩阵 \\(A\\) 的部分信息，可能会带来数值不稳定，所以在一些应用中并不会直接对矩阵相乘．但其本身也有一个优点，就是当 \\(n \\ll m\\) 时，上述流程中涉及的是 \\(n\\) 阶小方阵的特征值分解，因而求解起来很快并且只需要存储 \\(n\\) 维的特征向量．另一种方法是将这一问题转化为另一个矩阵的特征值分解．具体地，对于矩阵 \\(A\\) 及其奇异值分解\\(A = U\\Sigma V^{\\mathrm{T}}\\) ，我们构造矩阵"
        },
        {
            "type": "equation",
            "bbox": [
                0.322,
                0.352,
                0.586,
                0.398
            ],
            "angle": 0,
            "content": "\\[\nH = \\left[ \\begin{array}{c c} 0 & A ^ {\\mathrm {T}} \\\\ A & 0 \\end{array} \\right], \\quad X = \\left[ \\begin{array}{c c} V & V \\\\ U & - U \\end{array} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.413,
                0.212,
                0.429
            ],
            "angle": 0,
            "content": "那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.378,
                0.435,
                0.531,
                0.48
            ],
            "angle": 0,
            "content": "\\[\nH X = X \\left[ \\begin{array}{c c} \\Sigma & 0 \\\\ 0 & - \\Sigma \\end{array} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.493,
                0.741,
                0.553
            ],
            "angle": 0,
            "content": "故而可以将 \\(A\\) 的奇异值分解问题转化为 \\(H\\) 的特征值分解问题。这一做法是数值稳定的，但会使得问题的维数变高，并且有需要更大的存储量和迭代次数。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.593,
                0.357,
                0.612
            ],
            "angle": 0,
            "content": "B.2.4 数值代数软件"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.629,
                0.74,
                0.854
            ],
            "angle": 0,
            "content": "线性代数中向量和矩阵之间的运算作为基本操作（子程序），被广泛应用于各种数值编程中。因此，这些基本操作的有效实现备受关注。本书最后三章大量涉及优化算法的编写。在实际应用中，一个算法的性能除了和算法本身的性质（如收敛速度、参数选取）有关，跟算法的软件实现也有密切的关系。同样算法的不同实现，性能可能会相差很多，而导致性能差别的原因之一是数值代数软件的使用方式。正确地选取和使用数值代数软件是编写出高效算法的必备条件。本小节我们简单介绍一些常用的数值代数软件。需要注意的是，后面介绍的很多软件包（如BLAS和LAPACK）已经集成到MATLAB等平台，使用时一般不需要额外的处理（但仍然需要注意代码本身的写法）。如果需要基于C, C++, FORTRAN等语言实现算法，这时需要特别注意调用合适的程序。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "514"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.157,
                0.619,
                0.174
            ],
            "angle": 0,
            "content": "1. Basic Linear Algebra Subroutines (BLAS)"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.187,
                0.825,
                0.287
            ],
            "angle": 0,
            "content": "基本线性代数子程序 BLAS 提供了一种计算上非常有效的应用程序接口。它最初是用 FORTRAN 语言编写，同时也有着标准的接口。通过使用接口，我们可以调用进而编写高级的线性代数程序。BLAS 的实现通常会针对特定的机器来优化加速，因而带来显著的性能优势。同时，它还可以利用特殊的硬件指令集来实现向量化，这些指令集含 avx 指令集，sse 指令集等。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.29,
                0.825,
                0.328
            ],
            "angle": 0,
            "content": "BLAS实现了数值代数的基本操作。这些操作进一步按照运算量划分为三个层次："
        },
        {
            "type": "text",
            "bbox": [
                0.28,
                0.34,
                0.729,
                0.357
            ],
            "angle": 0,
            "content": "- 层次 1 \\(\\mathcal{O}(n)\\) 向量操作：向量加法、数乘、点乘、范数；"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.37,
                0.825,
                0.429
            ],
            "angle": 0,
            "content": "- 层次 2 \\(\\mathcal{O}(n^{2})\\) 矩阵 - 向量操作: 矩阵 - 向量乘积, 三角矩阵 - 向量求解, 矩阵的秩一更新 \\((A \\leftarrow A + uv^{\\mathrm{T}})\\) 及对称矩阵的秩二更新 \\((A \\leftarrow A + uv^{\\mathrm{T}} + vu^{\\mathrm{T}}\\), 其中 \\(A\\) 为对称矩阵);"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.442,
                0.825,
                0.48
            ],
            "angle": 0,
            "content": "- 层次3 \\(\\mathcal{O}(n^3)\\) 矩阵-矩阵操作：矩阵－矩阵乘积，三角矩阵－矩阵求解，低秩更新."
        },
        {
            "type": "list",
            "bbox": [
                0.28,
                0.34,
                0.825,
                0.48
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.492,
                0.825,
                0.53
            ],
            "angle": 0,
            "content": "我们总是选用更高层次的BLAS程序而不是多次利用低层次的BLAS程序来达到同样的效果。举例来说，为了完成如下操作："
        },
        {
            "type": "equation",
            "bbox": [
                0.362,
                0.539,
                0.721,
                0.577
            ],
            "angle": 0,
            "content": "\\[\nA \\leftarrow A + \\sum_ {i = 1} ^ {k} x _ {i} y _ {i} ^ {\\mathrm {T}}, \\quad A \\in \\mathbb {R} ^ {m \\times n}, x _ {i} \\in \\mathbb {R} ^ {m}, y _ {i} \\in \\mathbb {R} ^ {n},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.587,
                0.579,
                0.604
            ],
            "angle": 0,
            "content": "通常有两个选择：调用 \\(k\\) 次层次2的程序，"
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.616,
                0.729,
                0.637
            ],
            "angle": 0,
            "content": "\\[\nA \\leftarrow A + x _ {1} y _ {1} ^ {\\mathrm {T}}, A \\leftarrow A + x _ {2} y _ {2} ^ {\\mathrm {T}}, \\dots , A \\leftarrow A + x _ {k} y _ {k} ^ {\\mathrm {T}};\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.65,
                0.493,
                0.667
            ],
            "angle": 0,
            "content": "或者仅调用一次层次3的程序，"
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.68,
                0.757,
                0.7
            ],
            "angle": 0,
            "content": "\\[\nA \\leftarrow A + X Y ^ {\\mathrm {T}}, \\quad X = \\left[ x _ {1}, x _ {2}, \\dots , x _ {k} \\right], \\quad Y = \\left[ y _ {1}, y _ {2}, \\dots , y _ {k} \\right].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.712,
                0.405,
                0.729
            ],
            "angle": 0,
            "content": "而后者表现得更好"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.733,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "使用 BLAS 是因为它通常会比我们自己实现的矩阵操作要快，经过优化的 BLAS 库一般快 10 倍或是更多。其原因是 BLAS 库在实现时考虑了硬件特性，通过选取适合机器处理器和高速缓存的块大小来达到最佳的表现。严格来说，BLAS 是一种规范，它规定了线性代数操作子程序的接口。在保证接口形式相同的条件下，它可以有很多种不同的实现。比较常用的有如下的软件包："
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.32,
                0.133
            ],
            "angle": 0,
            "content": "B.2 数值代数基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "515"
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.157,
                0.737,
                0.195
            ],
            "angle": 0,
            "content": "- 官方的 BLAS [23]: 由 netlib 提供, 是历史最悠久的版本, 最初是用 FORTRAN 语言编写, 之后有了 C, C++ 语言接口 (CBLAS, BLAS++.)."
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.208,
                0.738,
                0.267
            ],
            "angle": 0,
            "content": "- ATLAS [197]: 全名为 Automatically Tuned Linear Algebra Software, 它是利用自动编码生成器和检测方法来生成一个对于特定计算机的优化 BLAS 库, 比传统的 BLAS 更高效."
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.279,
                0.738,
                0.338
            ],
            "angle": 0,
            "content": "- Intel MKL [190]: 全名为 Intel Math Kernel Library, 它是 Intel 公司开发的数学库, 针对 Intel CPU 进行优化. 其中包含完整的 BLAS 运算, 且支持串行和并行计算."
        },
        {
            "type": "list",
            "bbox": [
                0.193,
                0.157,
                0.738,
                0.338
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.35,
                0.739,
                0.409
            ],
            "angle": 0,
            "content": "上面介绍的BLAS库仅仅针对稠密矩阵进行设计。由于稀疏矩阵的数据结构与一般的矩阵不同，所以也需要相应的BLAS库，但是至今为止并没有一个标准的稀疏BLAS库，我们列出一些常用的库："
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.421,
                0.472,
                0.437
            ],
            "angle": 0,
            "content": "- 官方的稀疏 BLAS [22-23, 61, 117]."
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.451,
                0.649,
                0.469
            ],
            "angle": 0,
            "content": "- \\(C++\\): Boost uBlas, Matrix Template Library, SparseLib++."
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.481,
                0.737,
                0.519
            ],
            "angle": 0,
            "content": "- Intel MKL [190]: 在 Intel MKL 中也集成了稀疏 BLAS 的运算, 并且支持串行和并行计算."
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.531,
                0.737,
                0.569
            ],
            "angle": 0,
            "content": "- SciPy [187]: 有 Python 语言接口的科学计算软件包，支持稀疏矩阵的操作。"
        },
        {
            "type": "list",
            "bbox": [
                0.193,
                0.421,
                0.737,
                0.569
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.593,
                0.492,
                0.611
            ],
            "angle": 0,
            "content": "2. Linear Algebra PACKage (LAPACK)"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.623,
                0.739,
                0.744
            ],
            "angle": 0,
            "content": "LAPACK则是基于BLAS来实现线性系统的求解和矩阵的分解等更为高级的操作，故其与BLAS支持相同的数据类型与矩阵结构。该程序包的最初版本（1.0版本）发行于1992年，3.0版本发行于2000年，成功替代了之前的EISPACK与LINPACK。LAPACK的结构要比BLAS更复杂，子程序可以被分为三类：辅助子程序、计算子程序和驱动子程序，其中计算子程序和驱动子程序与求解线性代数基本问题直接相关。"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.747,
                0.579,
                0.765
            ],
            "angle": 0,
            "content": "辅助子程序主要完成的是一些底层的操作，例如："
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.776,
                0.516,
                0.793
            ],
            "angle": 0,
            "content": "- 获取当前机器的机器精度和寄存器大小；"
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.806,
                0.482,
                0.822
            ],
            "angle": 0,
            "content": "- 生成均匀分布或高斯分布的随机数；"
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.836,
                0.496,
                0.853
            ],
            "angle": 0,
            "content": "- 使用低层次 BLAS 运算完成矩阵分解"
        },
        {
            "type": "list",
            "bbox": [
                0.193,
                0.776,
                0.516,
                0.853
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "516"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.768,
                0.174
            ],
            "angle": 0,
            "content": "计算子程序主要利用BLAS层次3运算完成某些单一、特定的任务："
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.19,
                0.827,
                0.228
            ],
            "angle": 0,
            "content": "- 矩阵分解，包括 LU 分解，Cholesky 分解，对称不定矩阵分解和 QR 分解等；"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.245,
                0.825,
                0.283
            ],
            "angle": 0,
            "content": "- 将对称（埃尔米特（Hermite））矩阵化为三对角矩阵，以便进行特征值分解的后续步骤；"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.301,
                0.703,
                0.317
            ],
            "angle": 0,
            "content": "- 三对角矩阵的特征值分解和二对角矩阵的奇异值分解."
        },
        {
            "type": "list",
            "bbox": [
                0.281,
                0.19,
                0.827,
                0.317
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.334,
                0.812,
                0.35
            ],
            "angle": 0,
            "content": "驱动子程序则通过调用一系列计算子程序来解决标准的线性代数问题，如"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.367,
                0.468,
                0.383
            ],
            "angle": 0,
            "content": "- 线性方程组： \\(Ax = b\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.401,
                0.592,
                0.422
            ],
            "angle": 0,
            "content": "- 线性最小二乘问题: \\(\\min_{x} \\|b - Ax\\|_2\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.435,
                0.448,
                0.452
            ],
            "angle": 0,
            "content": "- 线性最小范数问题："
        },
        {
            "type": "list",
            "bbox": [
                0.281,
                0.367,
                0.592,
                0.452
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.41,
                0.47,
                0.713,
                0.513
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\min  \\quad \\| c - A x \\| _ {2} \\quad \\text {s . t .} \\quad B x = d, \\\\ \\min  \\quad \\| y \\| _ {2} \\quad \\text {s . t .} \\quad A x + B y = d; \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.537,
                0.497,
                0.552
            ],
            "angle": 0,
            "content": "- 实对称矩阵的特征值分解"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.57,
                0.825,
                0.628
            ],
            "angle": 0,
            "content": "概括来说，数值代数中介绍的算法大都可以在 LAPACK 中找到实现。通过自行组合计算子程序里的函数，我们也可利用 LAPACK 来构造特定功能的子程序。常见的 LAPACK 实现有如下两种："
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.645,
                0.825,
                0.703
            ],
            "angle": 0,
            "content": "- 官方 LAPACK [6]: 由 netlib 提供, 是历史最悠久的版本, 最初用 FORTRAN 语言编写, 之后有了 C, C++ 语言接口 (LAPACKE, LAPACK++.)."
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.72,
                0.825,
                0.758
            ],
            "angle": 0,
            "content": "- Intel MKL [190]: 在 Intel MKL 中也实现了完整的 LAPACK 功能, 且支持串行和并行计算."
        },
        {
            "type": "list",
            "bbox": [
                0.281,
                0.645,
                0.825,
                0.758
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.774,
                0.827,
                0.854
            ],
            "angle": 0,
            "content": "和 BLAS 类似，LAPACK 是针对稠密矩阵设计的数学库，它不能处理稀疏矩阵的分解。现有的对稀疏矩阵实现线性系统求解和矩阵分解操作的库中，较全面的是 SuiteSparse [53]，其他的还有 PARDISO [54]，MUMPS [4]，SuperLU [118]，SPOOLES [8] 等。"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.32,
                0.133
            ],
            "angle": 0,
            "content": "B.2 数值代数基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "517"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.157,
                0.44,
                0.174
            ],
            "angle": 0,
            "content": "3. 其他版本的BLAS和LAPACK"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.187,
                0.737,
                0.224
            ],
            "angle": 0,
            "content": "在这里我们给出其他的BLAS, LAPACK实现，这些软件包不具有官方BLAS, LAPACK的标准接口，但也是常用的软件."
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.237,
                0.737,
                0.275
            ],
            "angle": 0,
            "content": "- Portable, Extensible Toolkit for Scientific Computation (PETSc), 用 C 语言实现的高效稀疏矩阵、常微分线性方程数学库 [13]."
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.287,
                0.737,
                0.326
            ],
            "angle": 0,
            "content": "- Hypre，在并行机上使用的C程序库，其中实现了大量的稀疏矩阵运算，主要用于求解有限元的多重网格问题。更多内容可参考[65]。"
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.338,
                0.737,
                0.396
            ],
            "angle": 0,
            "content": "- Elemental, 一个在并行机上使用的 C++ 程序库, 主要用于解决稠密线性系统问题和对稀疏线性系统利用直接法求解. 感兴趣的读者可以参考 [156]."
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.41,
                0.737,
                0.447
            ],
            "angle": 0,
            "content": "- Eigen [92], 一个使用 C++ 语言实现的模板库, 有完整的矩阵、向量封装并且实现了部分 BLAS, LAPACK 操作, 可以结合 Intel MKL 使用."
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.46,
                0.737,
                0.498
            ],
            "angle": 0,
            "content": "- ScaLAPACK，在并行机上使用的高性能线性系统子程序库，主要解决稠密的和带状的线性系统问题。更多相关内容可以参考[46]。"
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.511,
                0.737,
                0.548
            ],
            "angle": 0,
            "content": "- cuBLAS, cuSolver，由 NVIDIA 实现的在 GPU 上运行的 BLAS, LAPACK 库，有非常高的性能."
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.562,
                0.737,
                0.62
            ],
            "angle": 0,
            "content": "- MAGMA，在GPU上使用的高性能线性代数程序库，和LAPACK的功能类似，对于异构和混合系统使用“多核 \\(+\\mathrm{GPU}\\)”系统。使用手册可以参考[27]。"
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.633,
                0.737,
                0.671
            ],
            "angle": 0,
            "content": "- PLASMA，一个利用多核处理器和Xeon Phi协处理器解决稠密线性代数问题的软件包．详细内容可以参考[33]."
        },
        {
            "type": "list",
            "bbox": [
                0.193,
                0.237,
                0.737,
                0.671
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.694,
                0.48,
                0.712
            ],
            "angle": 0,
            "content": "4. 线性特征值（奇异值）问题求解软件"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.724,
                0.737,
                0.804
            ],
            "angle": 0,
            "content": "线性特征值（奇异值）问题主要分为稠密矩阵特征值（奇异值）问题和稀疏矩阵特征值（奇异值）问题。求解稠密矩阵特征值（奇异值）问题的算法已经集成在 LAPACK 软件包中，而处理稀疏矩阵特征值（奇异值）问题的软件包种类非常多，以下给出一些常见的实现："
        },
        {
            "type": "text",
            "bbox": [
                0.193,
                0.816,
                0.737,
                0.854
            ],
            "angle": 0,
            "content": "- ARPACK，一个求解大规模对称、非对称稀疏矩阵特征值分解的软件包，最早用FORTRAN语言编写，是早期MATLAB内置函数eigs"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.289,
                0.13
            ],
            "angle": 0,
            "content": "518"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.298,
                0.157,
                0.827,
                0.174
            ],
            "angle": 0,
            "content": "的后台实现. 目前已经有单机版本和并行版本, 详细内容可参考 [115]."
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.21,
                0.826,
                0.268
            ],
            "angle": 0,
            "content": "- FEAST，一个求解复数特征值问题的软件包，主要用于求解给定区间、复平面给定区域内所有特征值，目前已经被集成在Intel MKL中。详情可参考[154]。"
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.284,
                0.825,
                0.363
            ],
            "angle": 0,
            "content": "- SLEPc (the Scalable Library for Eigenvalue Problem Computations), 一个基于 PETSc 开发的求解大规模稀疏矩阵特征值问题和广义特征值问题的软件包, 用 C 语言编写, 支持在并行机上运行, 详情可参考 [100]."
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.379,
                0.825,
                0.416
            ],
            "angle": 0,
            "content": "- PROPACK [114], 一个求解稠密、稀疏矩阵SVD的软件包, 主要实现了Lanczos算法及其变形算法."
        },
        {
            "type": "text",
            "bbox": [
                0.281,
                0.432,
                0.825,
                0.47
            ],
            "angle": 0,
            "content": "- LMSVD (Limited Memory SVD)，基于块 Krylov 子空间迭代法的有限内存 SVD 求解程序，详情可参考 [126]."
        },
        {
            "type": "list",
            "bbox": [
                0.281,
                0.21,
                0.826,
                0.47
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.46,
                0.504,
                0.624,
                0.525
            ],
            "angle": 0,
            "content": "B.3 概率基础"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.543,
                0.825,
                0.602
            ],
            "angle": 0,
            "content": "本节中涉及的概率论知识主要用于第三章中概率统计建模部分以及第八章中随机算法部分。我们只给出一些必要的定义以及常用的结论，更加细致全面的讨论我们推荐读者阅读[63]。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.631,
                0.404,
                0.649
            ],
            "angle": 0,
            "content": "B.3.1 概率空间"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.662,
                0.825,
                0.742
            ],
            "angle": 0,
            "content": "概率论中的一个最基本概念是概率空间, 它是一个三元组 \\((\\Omega, \\mathcal{F}, P)\\), 其中 \\(\\Omega\\) 是样本空间, \\(\\mathcal{F}\\) 是事件域, 以及 \\(P: \\mathcal{F} \\to [0,1]\\) 是概率函数. 对于事件域 \\(\\mathcal{F}\\), 其为样本空间 \\(\\Omega\\) 幂集 (即所有子集构成的集合) 的一个非空子集并且满足"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.757,
                0.399,
                0.774
            ],
            "angle": 0,
            "content": "(1) 空集 \\(\\varnothing \\in \\mathcal{F}\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.789,
                0.528,
                0.807
            ],
            "angle": 0,
            "content": "(2) 如果 \\(A \\in \\mathcal{F}\\), 那么有 \\(A^c \\in \\mathcal{F}\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.817,
                0.821,
                0.856
            ],
            "angle": 0,
            "content": "(3) 如果可列（包含有限的情形）个 \\(A_{n} \\in \\mathcal{F}, n = 1,2,\\dots\\)，则 \\(\\bigcup_{n=1}^{\\infty} A_{n} \\in \\mathcal{F}\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.757,
                0.821,
                0.856
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.118,
                0.284,
                0.132
            ],
            "angle": 0,
            "content": "B.3 概率基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.737,
                0.13
            ],
            "angle": 0,
            "content": "519"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.737,
                0.214
            ],
            "angle": 0,
            "content": "二元组 \\((\\Omega, \\mathcal{F})\\) 也被称为可测空间，即我们可以在上面定义概率函数（概率测度）。具体地，概率是定义在事件域 \\(\\mathcal{F}\\) 上的一个实值函数，记为 \\(P: \\mathcal{F} \\to \\mathbb{R}\\)，并且满足"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.229,
                0.536,
                0.245
            ],
            "angle": 0,
            "content": "(1) 非负性: 对任一事件 \\(A \\in \\mathcal{F}\\), 有 \\(P(A) \\geqslant 0\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.26,
                0.361,
                0.277
            ],
            "angle": 0,
            "content": "(2) 正则性: \\(P(\\Omega) = 1\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.291,
                0.737,
                0.327
            ],
            "angle": 0,
            "content": "(3) 可列可加性: 如果 \\(A_{1}, A_{2}, \\cdots\\) 互不相交, 这里指 \\(A_{i} \\cap A_{j} = \\emptyset, \\forall i, j\\), 那么"
        },
        {
            "type": "list",
            "bbox": [
                0.181,
                0.229,
                0.737,
                0.327
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.386,
                0.327,
                0.561,
                0.369
            ],
            "angle": 0,
            "content": "\\[\nP \\left(\\bigcup_ {i = 1} ^ {\\infty} A _ {i}\\right) = \\sum_ {i = 1} ^ {\\infty} P (A _ {i}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.38,
                0.737,
                0.417
            ],
            "angle": 0,
            "content": "这样的 \\(P(A), A \\in \\mathcal{F}\\) 表示事件 \\(A\\) 发生的概率．由定义容易推出，\\(P(A) \\leqslant P(B), \\forall A \\subseteq B\\) 以及 \\(P(\\emptyset) = 0\\)."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.444,
                0.316,
                0.46
            ],
            "angle": 0,
            "content": "B.3.2 随机变量"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.475,
                0.309,
                0.492
            ],
            "angle": 0,
            "content": "1. 定义与可测性"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.505,
                0.738,
                0.666
            ],
            "angle": 0,
            "content": "实际中许多随机现象的样本空间和事件域都是比较复杂的，而且我们并非需要关心事件域中的所有事件。为了更好地描述随机事件及其发生的概率，我们引入随机变量的概念。它相当于将一个随机现象量化的过程。随机变量 \\(X\\) 是定义在样本空间 \\(\\Omega\\) 上的实值函数，即 \\(X: \\Omega \\to \\mathbb{R}\\)。也就是说，对于样本空间 \\(\\Omega\\) 的一个样本点 \\(\\omega\\)，随机变量将其映射为一个实数 \\(X(\\omega)\\)。类似地，我们可以引入随机向量的概念。随机向量 \\(X\\) 将样本点 \\(\\omega\\) 映射为 \\(\\mathbb{R}^n\\) 空间的向量，即 \\(X: \\Omega \\to \\mathbb{R}^n\\)。直观来看，随机向量的每一个分量都是随机变量。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.671,
                0.738,
                0.853
            ],
            "angle": 0,
            "content": "通过引入随机变量，我们可以将关注的重点转移到随机变量本身的取值，而无需太多关注这个取值到底是对应了哪一个样本点。为了使得我们仍旧能在原概率空间 \\((\\Omega, \\mathcal{F}, P)\\) 上研究所定义的随机变量 \\(X\\)，必须假设 \\(X\\) 满足一定的要求。一个最基本的要求就是可测性，即对任意 \\(t \\in \\mathbb{R}\\)，集合 \\(\\{\\omega \\mid X(\\omega) > t\\}\\) 总是属于 \\(\\mathcal{F}\\)，此时也称 \\(X\\) 关于事件域 \\(\\mathcal{F}\\) 可测。有了可测性我们就可以将概率作用在随机变量上，给定样本空间中的概率函数 \\(P\\)，随机变量 \\(X\\) 大于某个值 \\(x\\) 的概率为 \\(P(\\{\\omega \\mid X(\\omega) > x\\})\\)。为了方便，一般简记为 \\(P(X > x)\\)。类似地，我们可以定义 \\(P(X = x)\\) 或更一般的 \\(P(X \\in A)\\)，其中 \\(A\\) 是 Borel 集，即 \\(\\mathbb{R}\\) 中开区间经过可列次交、并、补运算得到的集合全体。"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "520"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "需要注意的是，随机变量的可测性是一个很重要的概念，它实际上刻画了随机变量至多包含的信息量．接下来用一个例子来说明．设样本空间\\(\\Omega = \\{1,2,\\dots ,8\\}\\) ，随机变量 \\(X\\) 的定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.448,
                0.225,
                0.635,
                0.275
            ],
            "angle": 0,
            "content": "\\[\nX (\\omega) = \\left\\{ \\begin{array}{l l} 0, & \\omega \\text {是 偶 数}, \\\\ 1, & \\omega \\text {是 奇 数}. \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.285,
                0.825,
                0.384
            ],
            "angle": 0,
            "content": "现在我们讨论 \\(X\\) 可测性的问题。显然，\\(X\\) 关于 \\(\\Omega\\) 的幂集是可测的，但 \\(X\\) 本身并没有提供与 \\(\\Omega\\) 的幂集等量的信息。即对 \\(X\\) 观测只能得到 0 或 1（只能知道 \\(\\omega\\) 的奇偶性），并不能知道 \\(\\omega\\) 的其他性质（例如 \\(\\omega\\) 是否比 2 大，是否是 3 的倍数等）。所以我们感兴趣的是使得 \\(X\\) 具有可测性的事件域中最小的那个，在这个例子中为"
        },
        {
            "type": "equation",
            "bbox": [
                0.413,
                0.398,
                0.669,
                0.416
            ],
            "angle": 0,
            "content": "\\[\n\\mathcal {F} = \\{\\varnothing , \\{1, 3, 5, 7 \\}, \\{2, 4, 6, 8 \\}, \\Omega \\}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.429,
                0.825,
                0.467
            ],
            "angle": 0,
            "content": "容易看出 \\(X\\) 提供的信息和 \\(\\mathcal{F}\\) 提供的信息是等量的．为了强调这一关系，我们引入一个新的定义：\\(X\\) 诱导的事件域."
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.479,
                0.71,
                0.496
            ],
            "angle": 0,
            "content": "定义B.7(诱导的事件域）设 \\(X\\) 为 \\(\\Omega\\) 上的随机变量，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.422,
                0.508,
                0.66,
                0.528
            ],
            "angle": 0,
            "content": "\\[\n\\sigma (X) = \\bigcap \\{\\mathcal {F} \\mid X \\text {关 于} \\mathcal {F} \\text {可 测} \\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.541,
                0.496,
                0.557
            ],
            "angle": 0,
            "content": "称为随机变量 \\(X\\) 诱导的事件域"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.57,
                0.42,
                0.586
            ],
            "angle": 0,
            "content": "实际上容易验证"
        },
        {
            "type": "equation",
            "bbox": [
                0.407,
                0.6,
                0.675,
                0.619
            ],
            "angle": 0,
            "content": "\\[\n\\sigma (X) = \\{\\{\\omega \\mid X (\\omega) \\in B \\} \\mid B \\in \\mathcal {B} \\},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.632,
                0.692,
                0.648
            ],
            "angle": 0,
            "content": "其中 \\(\\mathcal{B}\\) 表示Borel集．这给出了 \\(\\sigma (X)\\) 的一个显式表达式"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.672,
                0.551,
                0.689
            ],
            "angle": 0,
            "content": "2. 离散型随机变量与连续型随机变量"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.702,
                0.825,
                0.822
            ],
            "angle": 0,
            "content": "常见的随机变量分为离散型随机变量与连续型随机变量。离散型随机变量的值域是 \\(\\mathbb{R}\\) 中的有限个或者可列个离散点，而连续型随机变量的值域是 \\(\\mathbb{R}\\) 中的一个区间。对于离散型随机变量 \\(X\\) ，可以通过定义在所有可列个取值上的概率来确定 \\(X\\) 取任意值的概率。而对于连续型随机变量 \\(X\\) ，则是通过定义 \\(X\\) 取值于所有左开右闭区间 \\((a,b]\\) 的概率来确定 \\(X\\) 取任意值的概率。根据概率的定义，"
        },
        {
            "type": "equation",
            "bbox": [
                0.396,
                0.836,
                0.687,
                0.853
            ],
            "angle": 0,
            "content": "\\[\nP (a <   X \\leqslant b) = P (X \\leqslant b) - P (X \\leqslant a).\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.132
            ],
            "angle": 0,
            "content": "B.3 概率基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.735,
                0.131
            ],
            "angle": 0,
            "content": "521"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.736,
                0.194
            ],
            "angle": 0,
            "content": "因此，我们只需要定义 \\(P(X \\leqslant a)\\)，就可以知道该连续型随机变量的取值概率。这对离散型随机变量也成立。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.199,
                0.736,
                0.235
            ],
            "angle": 0,
            "content": "因为随机变量 \\(X\\) 的取值概率由 \\(P(X \\leqslant x)\\) 决定，我们定义随机变量的分布函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.351,
                0.24,
                0.555,
                0.257
            ],
            "angle": 0,
            "content": "\\[\nF (x) = P (X \\leqslant x), \\quad x \\in \\mathbb {R}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.267,
                0.737,
                0.325
            ],
            "angle": 0,
            "content": "此时，称 \\(X\\) 服从分布 \\(F(x)\\)，记为 \\(X \\sim F(x)\\)。对于一个随机变量，如果其分布函数已知，那么其取任意值的概率都可以计算得到。由概率函数的性质，我们知道分布函数 \\(F(x)\\) 一定是存在的，并且满足"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.337,
                0.737,
                0.375
            ],
            "angle": 0,
            "content": "(1) 单调性: \\(F(x)\\) 是单调不减函数, 即对于任意的 \\(x_{1}, x_{2} \\in \\mathbb{R}\\), 如果 \\(x_{1} < x_{2}\\), 那么 \\(F(x_{1}) \\leqslant F(x_{2})\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.387,
                0.586,
                0.404
            ],
            "angle": 0,
            "content": "(2) 有界性: 对任意的 \\(x \\in \\mathbb{R}\\), 有 \\(0 \\leqslant F(x) \\leqslant 1\\), 并且"
        },
        {
            "type": "list",
            "bbox": [
                0.181,
                0.337,
                0.737,
                0.404
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.356,
                0.418,
                0.591,
                0.44
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {x \\rightarrow + \\infty} F (x) = 1, \\lim  _ {x \\rightarrow - \\infty} F (x) = 0;\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.453,
                0.6,
                0.47
            ],
            "angle": 0,
            "content": "(3) 右连续性: \\(F(x)\\) 是右连续函数, 即对任意的 \\(x_0\\), 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.401,
                0.484,
                0.546,
                0.508
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {x \\rightarrow x _ {0 +}} F (x) = F (x _ {0}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.522,
                0.506,
                0.538
            ],
            "angle": 0,
            "content": "关于证明细节，感兴趣的读者可以自行推导"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.542,
                0.737,
                0.6
            ],
            "angle": 0,
            "content": "对于连续型随机变量，另一个常用的函数是概率密度函数。它描述了随机变量取值在某个点附近的可能性，这有助于我们分析随机变量的期望、方差等特征。对于连续型变量 \\(X\\)，如果存在函数 \\(f(x)\\) 使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.381,
                0.608,
                0.527,
                0.64
            ],
            "angle": 0,
            "content": "\\[\nF (x) = \\int_ {- \\infty} ^ {x} f (t) d t,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.648,
                0.736,
                0.687
            ],
            "angle": 0,
            "content": "那么称 \\( f(x) \\) 为 \\( X \\) 的概率密度函数。根据分布函数 \\( F(x) \\) 的性质，概率密度函数 \\( f(x) \\) 满足"
        },
        {
            "type": "text",
            "bbox": [
                0.181,
                0.698,
                0.511,
                0.715
            ],
            "angle": 0,
            "content": "(1) 非负性：对任意的 \\(x \\in \\mathbb{R}\\)，有 \\(f(x) \\geqslant 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.728,
                0.542,
                0.745
            ],
            "angle": 0,
            "content": "(2) 归一性: \\(f(x)\\) 在整个实数轴上积分为 1 , 即"
        },
        {
            "type": "list",
            "bbox": [
                0.181,
                0.698,
                0.542,
                0.745
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.408,
                0.753,
                0.54,
                0.786
            ],
            "angle": 0,
            "content": "\\[\n\\int_ {- \\infty} ^ {+ \\infty} f (x) \\mathrm {d} x = 1;\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.798,
                0.512,
                0.815
            ],
            "angle": 0,
            "content": "(3) 相容性: 对任意的 \\(a, b \\in \\mathbb{R}\\) 且 \\(a < b\\), 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.315,
                0.824,
                0.632,
                0.857
            ],
            "angle": 0,
            "content": "\\[\nP (a <   x \\leqslant b) = F (b) - F (a) = \\int_ {a} ^ {b} f (x) \\mathrm {d} x.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "522"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.714,
                0.174
            ],
            "angle": 0,
            "content": "根据概率密度函数的定义，我们知道对连续型随机变量 \\(X\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.445,
                0.192,
                0.639,
                0.21
            ],
            "angle": 0,
            "content": "\\[\nP (X = x) = 0, \\quad \\forall x \\in \\mathbb {R},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.227,
                0.539,
                0.244
            ],
            "angle": 0,
            "content": "即连续型变量取任意单点的概率为0."
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.248,
                0.825,
                0.286
            ],
            "angle": 0,
            "content": "类似地，对离散型随机变量 \\(X\\) 常用的函数为分布律（概率质量函数）它的定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.439,
                0.292,
                0.644,
                0.31
            ],
            "angle": 0,
            "content": "\\[\np (x) = P (X = x), \\quad x \\in \\mathbb {R}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.323,
                0.825,
                0.361
            ],
            "angle": 0,
            "content": "因为离散型随机变量的值域仅有可列个点，所以其分布律 \\( p(\\cdot) \\) 仅在可列个点处取值非零."
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.39,
                0.362,
                0.407
            ],
            "angle": 0,
            "content": "3. 数学期望"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.421,
                0.826,
                0.508
            ],
            "angle": 0,
            "content": "给定一个随机变量 \\(X\\) ，其数学期望记为 \\(\\mathbb{E}[X]\\) ，用来反映 \\(X\\) 在概率意义下的平均值.具体地，对于离散型随机变量 \\(X\\) ，假设其取值集合为 \\(\\{x_{1},x_{2},\\dots \\}\\) 分布律为 \\(P(x)\\) .若级数 \\(\\sum_{i = 1}^{\\infty}x_iP_i(x_i)\\) 是绝对收敛的，则称其为 \\(X\\) 的数学期望，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.468,
                0.51,
                0.614,
                0.547
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} [ X ] = \\sum_ {i = 1} ^ {\\infty} x _ {i} P _ {i} (x _ {i}).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.555,
                0.826,
                0.617
            ],
            "angle": 0,
            "content": "这可以看成是用 \\(P_{i}(x_{i})\\) 作为权重的加权和。对于连续型随机变量，设其概率密度函数为 \\(f(x)\\)，如果积分 \\(\\int_{-\\infty}^{+\\infty}|xf(x)|\\mathrm{d}x\\) 收敛，则称 \\(\\int_{-\\infty}^{+\\infty}xf(x)\\mathrm{d}x\\) 为 \\(X\\) 的数学期望，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.455,
                0.619,
                0.627,
                0.65
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} [ X ] = \\int_ {- \\infty} ^ {+ \\infty} x f (x) d x.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.66,
                0.748,
                0.677
            ],
            "angle": 0,
            "content": "根据随机变量的数学期望的定义，我们易知其满足以下性质："
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.693,
                0.825,
                0.732
            ],
            "angle": 0,
            "content": "命题B.1(数学期望的基本性质）设 \\(X,Y\\) 是定义在概率空间 \\((\\Omega ,\\mathcal{F},P)\\) 上的随机变量．且 \\(\\mathbb{E}[X]\\) ， \\(\\mathbb{E}[Y]\\) 均存在，则"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.747,
                0.784,
                0.765
            ],
            "angle": 0,
            "content": "(1) 线性性: \\(\\mathbb{E}[aX + bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]\\), 在这里 \\(a, b\\) 为任意实数;"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.781,
                0.597,
                0.799
            ],
            "angle": 0,
            "content": "(2）保号性：如果 \\(X\\geqslant Y\\) ，则 \\(E[X]\\geqslant E[Y]\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.815,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": "(3) 常数随机变量的数学期望是其本身，即若 \\(P(X = a) = 1\\) （\\(a \\in \\mathbb{R}\\) 为常数），那么 \\(\\mathbb{E}[X] = a\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.747,
                0.825,
                0.854
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.132
            ],
            "angle": 0,
            "content": "B.3 概率基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "523"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.157,
                0.241,
                0.174
            ],
            "angle": 0,
            "content": "4. 方差"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.187,
                0.737,
                0.245
            ],
            "angle": 0,
            "content": "为了衡量随机变量的取值与其数学期望的偏离程度，我们引入方差的概念。给定一个随机变量 \\(X\\)，其数学期望为 \\(\\mathbb{E}[X]\\)。假设 \\(\\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right]\\) 存在，我们定义其为随机变量 \\(X\\) 的方差，记为 \\(\\operatorname{Var}(X)\\)，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.258,
                0.559,
                0.279
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {V a r} (X) = \\mathbb {E} \\left[ (X - \\mathbb {E} [ X ]) ^ {2} \\right],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.291,
                0.736,
                0.328
            ],
            "angle": 0,
            "content": "这可以看做 \\(X\\) 的取值与其数学期望 \\(\\mathbb{E}[X]\\) 的距离平方加权和。此外容易利用数学期望的性质证明"
        },
        {
            "type": "equation",
            "bbox": [
                0.347,
                0.341,
                0.559,
                0.361
            ],
            "angle": 0,
            "content": "\\[\n\\operatorname {V a r} (X) = \\mathbb {E} [ X ^ {2} ] - (\\mathbb {E} [ X ]) ^ {2}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.374,
                0.71,
                0.391
            ],
            "angle": 0,
            "content": "对于随机变量 \\(X\\)，假设其方差 \\(\\operatorname{Var}(X)\\) 存在，易知其满足以下性质："
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.403,
                0.622,
                0.42
            ],
            "angle": 0,
            "content": "命题B.2(方差的基本性质) (1)非负性： \\(\\operatorname {Var}(X)\\geqslant 0\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.433,
                0.734,
                0.47
            ],
            "angle": 0,
            "content": "(2) 常数随机变量的方差是 0 , 即若 \\(P(X = a) = 1 (a \\in \\mathbb{R}\\) 为常数), 那么 \\(\\operatorname{Var}(X) = 0\\);"
        },
        {
            "type": "text",
            "bbox": [
                0.182,
                0.483,
                0.657,
                0.5
            ],
            "angle": 0,
            "content": "(3) 对于任意给定的常数 \\(a, b \\in \\mathbb{R}\\), 有 \\(\\operatorname{Var}(aX + b) = a^2\\operatorname{Var}(X)\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.433,
                0.734,
                0.5
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.524,
                0.275,
                0.54
            ],
            "angle": 0,
            "content": "5. 联合分布"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.554,
                0.737,
                0.591
            ],
            "angle": 0,
            "content": "上文讨论了单个随机变量的概率分布及其数字特征。在实际中，我们经常感兴趣的是两个或者多个随机变量同时取值的概率，即联合分布。"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.596,
                0.576,
                0.612
            ],
            "angle": 0,
            "content": "对于两个随机变量 \\(X\\) 和 \\(Y\\), 定义联合分布函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.28,
                0.626,
                0.626,
                0.644
            ],
            "angle": 0,
            "content": "\\[\nF (a, b) = P (X \\leqslant a, Y \\leqslant b), \\quad - \\infty <   a, b <   + \\infty .\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.658,
                0.566,
                0.674
            ],
            "angle": 0,
            "content": "通过联合分布函数可以得到随机变量 \\(X\\) 的分布函数："
        },
        {
            "type": "equation",
            "bbox": [
                0.246,
                0.688,
                0.66,
                0.711
            ],
            "angle": 0,
            "content": "\\[\nF _ {X} (a) = P (X \\leqslant a) = P (X \\leqslant a, Y <   + \\infty) = \\lim  _ {b \\rightarrow + \\infty} F (a, b).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.722,
                0.436,
                0.738
            ],
            "angle": 0,
            "content": "同样地，随机变量 \\(Y\\) 的分布函数为"
        },
        {
            "type": "equation",
            "bbox": [
                0.248,
                0.753,
                0.658,
                0.776
            ],
            "angle": 0,
            "content": "\\[\nF _ {Y} (b) = P (Y \\leqslant b) = P (X <   + \\infty , Y \\leqslant b) = \\lim  _ {a \\rightarrow + \\infty} F (a, b).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.785,
                0.591,
                0.801
            ],
            "angle": 0,
            "content": "我们也称这种只含部分分量的分布函数为边缘分布函数"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.806,
                0.698,
                0.822
            ],
            "angle": 0,
            "content": "对于 \\(X\\) 和 \\(Y\\) 都是离散型随机变量的情形, 联合分布律可以定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.352,
                0.836,
                0.554,
                0.854
            ],
            "angle": 0,
            "content": "\\[\np (x, y) = P (X = x, Y = y).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "524"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.157,
                0.674,
                0.174
            ],
            "angle": 0,
            "content": "假设 \\(p(x,y)\\) 已知，我们可以计算 \\(X\\) 和 \\(Y\\) 的边缘分布律"
        },
        {
            "type": "equation",
            "bbox": [
                0.351,
                0.188,
                0.731,
                0.222
            ],
            "angle": 0,
            "content": "\\[\np _ {X} (x) = \\sum_ {y: p (x, y) > 0} p (x, y), \\quad p _ {Y} (y) = \\sum_ {x: p (x, y) > 0} p (x, y).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.234,
                0.825,
                0.272
            ],
            "angle": 0,
            "content": "对于 \\(X\\) 和 \\(Y\\) 都是连续型随机变量的情形，如果存在函数 \\(f(x,y)\\)，使得对任意实数集合 \\(A,B\\)，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.283,
                0.682,
                0.314
            ],
            "angle": 0,
            "content": "\\[\nP (X \\in A, Y \\in B) = \\int_ {B} \\int_ {A} f (x, y) d x d y\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.324,
                0.825,
                0.362
            ],
            "angle": 0,
            "content": "成立，那么我们称 \\( f(x, y) \\) 为 \\( X \\) 和 \\( Y \\) 的联合密度函数。类似地，针对部分分量的密度函数也被称为边缘密度函数："
        },
        {
            "type": "equation",
            "bbox": [
                0.353,
                0.373,
                0.728,
                0.406
            ],
            "angle": 0,
            "content": "\\[\nf _ {X} (x) = \\int_ {- \\infty} ^ {+ \\infty} f (x, y) d y, \\quad f _ {Y} (y) = \\int_ {- \\infty} ^ {+ \\infty} f (x, y) d x.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.417,
                0.825,
                0.517
            ],
            "angle": 0,
            "content": "注意，联合分布函数给出的信息要多于所有边缘分布函数给出的信息的和，即我们可以通过联合分布函数来计算任意的边缘分布函数，但通过边缘分布函数一般是无法反推联合分布函数的。出现这种情况的主要原因是多个随机变量之间有相互的耦合作用，一个随机变量的取值可以影响其他的变量，而这在边缘分布中是体现不出来的。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.522,
                0.825,
                0.56
            ],
            "angle": 0,
            "content": "为了进一步讨论随机变量之间的相互关系，我们引入独立性的概念．如果对任意的实数 \\(a,b\\) ，事件 \\(\\{X\\leqslant a\\}\\) 与 \\(\\{Y\\leqslant b\\}\\) 是相互独立的，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.401,
                0.576,
                0.682,
                0.594
            ],
            "angle": 0,
            "content": "\\[\nP (X \\leqslant a, Y \\leqslant b) = P (X \\leqslant a) P (Y \\leqslant b)\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.61,
                0.634,
                0.627
            ],
            "angle": 0,
            "content": "成立，称随机变量 \\(X\\) 和 \\(Y\\) 是相互独立的．此时有"
        },
        {
            "type": "equation",
            "bbox": [
                0.42,
                0.644,
                0.664,
                0.661
            ],
            "angle": 0,
            "content": "\\[\nF (a, b) = F _ {X} (a) F _ {Y} (b), \\forall a, b \\in \\mathbb {R};\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.679,
                0.462,
                0.696
            ],
            "angle": 0,
            "content": "如果 \\(X\\) 和 \\(Y\\) 是离散的，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.458,
                0.713,
                0.626,
                0.732
            ],
            "angle": 0,
            "content": "\\[\np (x, y) = p _ {X} (x) p _ {Y} (y);\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.747,
                0.462,
                0.763
            ],
            "angle": 0,
            "content": "如果 \\(X\\) 和 \\(Y\\) 是连续的，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.459,
                0.781,
                0.623,
                0.8
            ],
            "angle": 0,
            "content": "\\[\nf (x, y) = f _ {X} (x) f _ {Y} (y).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.816,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "当两个随机变量不相互独立时，我们引入随机变量协方差的概念来衡量两个随机变量之间的相关性。具体地，对于随机变量 \\(X\\) 和 \\(Y\\) ，协方差"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.285,
                0.132
            ],
            "angle": 0,
            "content": "B.3 概率基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "525"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.157,
                0.308,
                0.174
            ],
            "angle": 0,
            "content": "\\(\\operatorname{Cov}(X,Y)\\) 定义为"
        },
        {
            "type": "equation",
            "bbox": [
                0.262,
                0.187,
                0.645,
                0.252
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\operatorname {C o v} (X, Y) = \\mathbb {E} \\left[ \\left(X - \\mathbb {E} [ X ]\\right) \\left(Y - \\mathbb {E} [ Y ]\\right) \\right] \\\\ = \\mathbb {E} [ X Y - Y \\mathbb {E} [ X ] - X \\mathbb {E} [ Y ] + \\mathbb {E} [ X ] \\mathbb {E} [ Y ] ] \\\\ = \\mathbb {E} [ X Y ] - \\mathbb {E} [ X ] \\mathbb {E} [ Y ]. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.263,
                0.737,
                0.321
            ],
            "angle": 0,
            "content": "如果 \\(X\\) 和 \\(Y\\) 是相互独立的, 容易验证 \\(\\operatorname{Cov}(X, Y) = 0\\). 随机变量 \\(X\\) 和 \\(Y\\) 协方差为零的情况又被称为 \\(X\\) 和 \\(Y\\) 不相关. 上面的论述说明独立性是不相关性的一个充分条件, 但一般情况下不相关性无法推出独立性."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.347,
                0.317,
                0.365
            ],
            "angle": 0,
            "content": "B.3.3 条件期望"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.378,
                0.739,
                0.519
            ],
            "angle": 0,
            "content": "条件期望是概率论中一个重要的概念，其在实际问题中有着广泛应用。我们知道随机变量之间往往都有一定的相关性，假设现在有随机向量 \\((X, Y_1, Y_2, \\dots, Y_n)\\)，并且我们已经观测到了 \\((Y_1, Y_2, \\dots, Y_n)\\)。一个自然的问题是：如何根据 \\((Y_1, Y_2, \\dots, Y_n)\\) 来对 \\(X\\) 进行预测？根据预测的实际含义，进行预测时应当仅仅使用 \\((Y_1, Y_2, \\dots, Y_n)\\) 的信息，且使得某种“误差”最小。因为预测时使用的也是随机变量，所以我们实际上需要度量两个随机变量之间的距离。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.542,
                0.262,
                0.559
            ],
            "angle": 0,
            "content": "1. \\(L^2\\) 空间"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.572,
                0.668,
                0.59
            ],
            "angle": 0,
            "content": "为了在特定概率空间中引入度量，我们先引入 \\(L^2\\) 空间的定义"
        },
        {
            "type": "text",
            "bbox": [
                0.203,
                0.601,
                0.644,
                0.619
            ],
            "angle": 0,
            "content": "定义B.8 \\((L^2\\) 空间）设概率空间 \\((\\Omega ,\\mathcal{F},P)\\) ，定义 \\(L^2\\) 空间为"
        },
        {
            "type": "equation",
            "bbox": [
                0.324,
                0.631,
                0.737,
                0.65
            ],
            "angle": 0,
            "content": "\\[\nL ^ {2} (\\Omega , \\mathcal {F}, P) = \\{X \\mid \\mathbb {E} [ X ^ {2} ] <   + \\infty \\}, \\tag {B.3.1}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.663,
                0.534,
                0.68
            ],
            "angle": 0,
            "content": "即 \\(L^2\\) 空间为二阶矩有限的随机变量构成的集合."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.692,
                0.737,
                0.73
            ],
            "angle": 0,
            "content": "考虑 \\(L^2\\) 空间的好处是我们可以在其上引入内积的概念．设随机变量\\(X,Y\\in L^{2}(\\Omega ,\\mathcal{F},P)\\) ，定义 \\(X\\) 与 \\(Y\\) 的内积"
        },
        {
            "type": "equation",
            "bbox": [
                0.391,
                0.743,
                0.518,
                0.762
            ],
            "angle": 0,
            "content": "\\[\n\\langle X, Y \\rangle = \\mathbb {E} [ X Y ].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.774,
                0.534,
                0.792
            ],
            "angle": 0,
            "content": "有了内积我们就可在 \\(L^2\\) 空间上引入范数和距离："
        },
        {
            "type": "equation",
            "bbox": [
                0.282,
                0.801,
                0.627,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\left\\| X \\right\\| _ {L ^ {2}} = \\sqrt {\\mathbb {E} \\left[ X ^ {2} \\right]}, \\\\ d (X, Y) = \\sqrt {\\langle X - Y , X - Y \\rangle} = \\sqrt {\\mathbb {E} [ (X - Y) ^ {2} ]}. \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "526"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.156,
                0.826,
                0.215
            ],
            "angle": 0,
            "content": "\\(L^2\\) 空间的严格定义需要以测度论和泛函分析为基础，在这里我们不进行深入讨论．读者可以将 \\(L^2\\) 空间和欧几里得空间 \\(\\mathbb{R}^n\\) 进行类比来理解内积和范数等概念."
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.24,
                0.521,
                0.258
            ],
            "angle": 0,
            "content": "2. \\(L^2\\) 空间上随机变量的条件期望"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.27,
                0.825,
                0.351
            ],
            "angle": 0,
            "content": "介绍了 \\(L^2\\) 空间之后，我们可以引入随机变量的条件期望了．给定随机向量 \\((Y_{1},Y_{2},\\dots ,Y_{n})\\) ，现在我们要预测随机变量X．也就是要找一个仅用\\((Y_{1},Y_{2},\\dots ,Y_{n})\\) 表示的 \\(L^2\\) 空间上的随机变量，使得其与 \\(X\\) 的距离最小．写成数学语言即为寻找一个 \\(n\\) 元函数 \\(f\\) ，使得"
        },
        {
            "type": "equation",
            "bbox": [
                0.38,
                0.366,
                0.825,
                0.391
            ],
            "angle": 0,
            "content": "\\[\nd \\left(f \\left(Y _ {1}, Y _ {2}, \\dots , Y _ {n}\\right), X\\right) = \\inf  _ {Z \\in L ^ {2} (\\Omega , \\mathcal {F}, P)} d (Z, X), \\tag {B.3.2}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.405,
                0.694,
                0.422
            ],
            "angle": 0,
            "content": "这里我们要求 \\(f(Y_{1},Y_{2},\\dots ,Y_{n})\\) 也为 \\(L^2\\) 空间上的随机变量"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.426,
                0.825,
                0.464
            ],
            "angle": 0,
            "content": "根据几何直观， \\( f(Y_{1},Y_{2},\\dots ,Y_{n}) \\) 应该为 \\( X \\) 到由 \\( Y_{1},Y_{2},\\dots ,Y_{n} \\) 张成空间上的投影，即对任意随机变量 \\( g(Y_1,Y_2,\\dots ,Y_n) \\)，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.377,
                0.48,
                0.706,
                0.499
            ],
            "angle": 0,
            "content": "\\[\n\\langle X - f (Y _ {1}, Y _ {2}, \\dots , Y _ {n}), g (Y _ {1}, Y _ {2}, \\dots , Y _ {n}) \\rangle = 0.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.514,
                0.525,
                0.53
            ],
            "angle": 0,
            "content": "化简上式可以得到条件期望的定义"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.545,
                0.835,
                0.584
            ],
            "angle": 0,
            "content": "定义B.9 \\((L^2\\) 空间上随机变量的条件期望）设 \\(X,Y_{1},Y_{2},\\dots ,Y_{n}\\) 为 \\((\\Omega ,\\mathcal{F},P)\\) 上的随机变量且 \\(\\mathbb{E}[X^2 ] < + \\infty\\) ， \\(f\\) 为 \\(n\\) 元函数，若"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.596,
                0.534,
                0.615
            ],
            "angle": 0,
            "content": "(1) \\(f(Y_{1},Y_{2},\\dots ,Y_{n})\\in L^{2}(\\Omega ,\\mathcal{F},P);\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.628,
                0.616,
                0.647
            ],
            "angle": 0,
            "content": "(2) 对任意 \\(g(Y_{1},Y_{2},\\dots ,Y_{n})\\in L^{2}(\\Omega ,\\mathcal{F},P)\\) ，有"
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.596,
                0.616,
                0.647
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "equation",
            "bbox": [
                0.315,
                0.662,
                0.825,
                0.681
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ X g \\left(Y _ {1}, Y _ {2}, \\dots , Y _ {n}\\right) \\right] = \\mathbb {E} \\left[ f \\left(Y _ {1}, Y _ {2}, \\dots , Y _ {n}\\right) g \\left(Y _ {1}, Y _ {2}, \\dots , Y _ {n}\\right) \\right], \\tag {B.3.3}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.702,
                0.825,
                0.74
            ],
            "angle": 0,
            "content": "则称随机变量 \\(f(Y_{1},Y_{2},\\dots ,Y_{n})\\) 为 \\(X\\) 关于 \\((Y_{1},Y_{2},\\dots ,Y_{n})\\) 的条件期望，并记为 \\(\\mathbb{E}[X|Y_1,Y_2,\\dots ,Y_n]\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.256,
                0.753,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "从定义可以看出，条件期望是随机变量而非具体数值．这与数学期望有本质的区别．另一个重要的观察是，条件期望 \\(\\mathbb{E}[X|Y_1,Y_2,\\dots ,Y_n]\\) 关于\\(\\sigma (Y_{1},Y_{2},\\dots ,Y_{n})\\) 可测，可用 \\((Y_{1},Y_{2},\\dots ,Y_{n})\\) 表示出来．但随机变量 \\(X\\) 不一定关于 \\(\\sigma (Y_1,Y_2,\\dots ,Y_n)\\) 可测，用 \\((Y_{1},Y_{2},\\dots ,Y_{n})\\) 一般不能表示 \\(X\\) ，和 \\(X\\) 最接近的表示是条件期望 \\(\\mathbb{E}[X|Y_1,Y_2,\\dots ,Y_n]\\)"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.285,
                0.133
            ],
            "angle": 0,
            "content": "B.3 概率基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "527"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.157,
                0.275,
                0.174
            ],
            "angle": 0,
            "content": "3. 一些例子"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.188,
                0.737,
                0.226
            ],
            "angle": 0,
            "content": "上述条件期望的定义比较抽象，我们用两个具体的例子来说明条件数学期望如何计算."
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.24,
                0.737,
                0.278
            ],
            "angle": 0,
            "content": "例B.1（离散型随机变量的条件期望）设 \\(X\\) 和 \\(Y\\) 是两个离散型随机变量，有联合分布律"
        },
        {
            "type": "equation",
            "bbox": [
                0.305,
                0.295,
                0.603,
                0.314
            ],
            "angle": 0,
            "content": "\\[\np _ {i j} = P (X = x _ {i}, Y = y _ {j}), \\quad i, j = 1, 2, \\dots ,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.329,
                0.737,
                0.388
            ],
            "angle": 0,
            "content": "接下来计算 \\(\\mathbb{E}[X|Y]\\)。根据定义，我们需要寻找一个仅仅和 \\(Y\\) 有关的随机变量 \\(f(Y)\\) 使得(B.3.3)式成立。注意到 \\(\\sigma(Y)\\) 中随机变量结构是已知的，若 \\(f(Y) \\in \\sigma(Y)\\)，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.338,
                0.389,
                0.569,
                0.464
            ],
            "angle": 0,
            "content": "\\[\nf (Y) (\\omega) = \\left\\{ \\begin{array}{l l} f _ {1}, & Y (\\omega) = y _ {1}, \\\\ f _ {2}, & Y (\\omega) = y _ {2}, \\\\ \\ldots & \\ldots \\ldots \\ldots . \\end{array} \\right.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.472,
                0.519,
                0.489
            ],
            "angle": 0,
            "content": "我们需要确定的就是 \\(f_{i}\\) 的值．(B.3.3)式等价于"
        },
        {
            "type": "equation",
            "bbox": [
                0.371,
                0.503,
                0.537,
                0.536
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {i, j} p _ {i j} x _ {i} g _ {j} = \\sum_ {i, j} p _ {i j} f _ {j} g _ {j},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.548,
                0.329,
                0.565
            ],
            "angle": 0,
            "content": "整理等号左右两边有"
        },
        {
            "type": "equation",
            "bbox": [
                0.327,
                0.577,
                0.58,
                0.612
            ],
            "angle": 0,
            "content": "\\[\n\\sum_ {j} \\left(\\sum_ {i} x _ {i} p _ {i j}\\right) g _ {j} = \\sum_ {j} \\left(f _ {j} \\sum_ {i} p _ {i j}\\right) g _ {j}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.624,
                0.421,
                0.642
            ],
            "angle": 0,
            "content": "注意到 \\(g_{j}\\) 是任意的，因此我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.402,
                0.653,
                0.506,
                0.69
            ],
            "angle": 0,
            "content": "\\[\nf _ {j} = \\frac {\\sum_ {i} x _ {i} p _ {i j}}{\\sum_ {i} p _ {i j}}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.701,
                0.737,
                0.761
            ],
            "angle": 0,
            "content": "例 B.2 (连续型随机变量的条件期望) 设 \\(X\\) 和 \\(Y\\) 是两个连续型随机变量, 有联合密度函数 \\(p(x, y) > 0\\), 接下来计算 \\(\\mathbb{E}[X | Y]\\). 这同样化为求一个函数 \\(f\\) 使得(B.3.3)式成立. 对任意的 \\(g\\), 我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.77,
                0.587,
                0.802
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} [ X g (Y) ] = \\int_ {\\mathbb {R} ^ {2}} x g (y) p (x, y) \\mathrm {d} x \\mathrm {d} y,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.811,
                0.211,
                0.825
            ],
            "angle": 0,
            "content": "以及"
        },
        {
            "type": "equation",
            "bbox": [
                0.299,
                0.827,
                0.61,
                0.857
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} [ f (Y) g (Y) ] = \\int_ {\\mathbb {R} ^ {2}} f (y) g (y) p (x, y) \\mathrm {d} x \\mathrm {d} y.\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "528"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.157,
                0.825,
                0.174
            ],
            "angle": 0,
            "content": "交换积分顺序，先关于 \\(x\\) 积分再关于 \\(y\\) 积分，并使得上述两积分相等，则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.316,
                0.181,
                0.766,
                0.213
            ],
            "angle": 0,
            "content": "\\[\n\\int_ {\\mathbb {R}} \\left(\\int_ {\\mathbb {R}} x p (x, y) \\mathrm {d} x\\right) g (y) \\mathrm {d} y = \\int_ {\\mathbb {R}} \\left(\\int_ {\\mathbb {R}} p (x, y) \\mathrm {d} x\\right) f (y) g (y) \\mathrm {d} y.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.218,
                0.443,
                0.235
            ],
            "angle": 0,
            "content": "由 \\(g(y)\\) 的任意性我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.455,
                0.243,
                0.626,
                0.282
            ],
            "angle": 0,
            "content": "\\[\nf (y) = \\frac {\\int_ {\\mathbb {R}} x p (x , y) \\mathrm {d} x}{\\int_ {\\mathbb {R}} p (x , y) \\mathrm {d} x}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.29,
                0.495,
                0.306
            ],
            "angle": 0,
            "content": "注意，分母即是 \\(Y\\) 的边缘密度."
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.329,
                0.414,
                0.346
            ],
            "angle": 0,
            "content": "4. 条件期望的性质"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.359,
                0.825,
                0.398
            ],
            "angle": 0,
            "content": "最后列出一些重要的条件期望的性质。这些性质在推导随机算法的时候非常有用。为了方便，下面用黑体的 \\(\\mathbf{Y}\\) 来表示随机向量 \\((Y_{1}, Y_{2}, \\dots, Y_{n})\\)。"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.408,
                0.462,
                0.425
            ],
            "angle": 0,
            "content": "命题B.3(全期望公式)"
        },
        {
            "type": "equation",
            "bbox": [
                0.467,
                0.438,
                0.615,
                0.456
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} [ X ] = \\mathbb {E} [ \\mathbb {E} [ X | Y ] ].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.468,
                0.627,
                0.486
            ],
            "angle": 0,
            "content": "证明. 在(B.3.3)式中取 \\(g(\\mathbf{Y}) = 1\\) 即可得到结论"
        },
        {
            "type": "image",
            "bbox": [
                0.808,
                0.469,
                0.824,
                0.482
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.498,
                0.825,
                0.557
            ],
            "angle": 0,
            "content": "全期望公式表明对条件期望再取一次数学期望就得到 \\(X\\) 本身的数学期望。这个性质尤其有用，在很多情况下我们为了减少要分析的随机变量的个数经常需要逆用这个公式。"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.568,
                0.718,
                0.585
            ],
            "angle": 0,
            "content": "命题B.4(线性性和单调性) (1) 若 \\(a, b\\) 为任意实数，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.406,
                0.597,
                0.718,
                0.616
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ a X _ {1} + b X _ {2} | \\boldsymbol {Y} \\right] = a \\mathbb {E} \\left[ X _ {1} | \\boldsymbol {Y} \\right] + b \\mathbb {E} \\left[ X _ {2} | \\boldsymbol {Y} \\right];\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.269,
                0.632,
                0.573,
                0.65
            ],
            "angle": 0,
            "content": "(2) 若 \\(X_{1} \\geqslant X_{2}\\), 则 \\(\\mathbb{E}[X_1|\\mathbf{Y}] \\geqslant \\mathbb{E}[X_2|\\mathbf{Y}]\\)."
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.66,
                0.694,
                0.677
            ],
            "angle": 0,
            "content": "命题B.4说明条件期望也具有和数学期望类似的性质"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.688,
                0.56,
                0.706
            ],
            "angle": 0,
            "content": "命题B.5 设 \\(Z\\) 关于 \\(\\sigma(Y)\\) 可测，则"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.716,
                0.4,
                0.734
            ],
            "angle": 0,
            "content": "(1) \\(\\mathbb{E}[Z|\\boldsymbol {Y}] = Z\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.27,
                0.745,
                0.466,
                0.763
            ],
            "angle": 0,
            "content": "(2) \\(\\mathbb{E}[XZ|\\mathbf{Y}] = Z\\mathbb{E}[X|\\mathbf{Y}]\\)."
        },
        {
            "type": "list",
            "bbox": [
                0.27,
                0.716,
                0.466,
                0.763
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.774,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "命题B.5的证明是显然的，我们可通过如下方式来理解该命题的直观含义：(1)表明，若Z本身就可以用Y来表示，那么其条件期望显然就是它自己；(2)表明，当固定Y时，由于Z关于 \\(\\sigma (\\mathbf{Y})\\) 可测，因此求条件期望时可以看成一个常数，从而利用“线性性”得到结论."
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.132
            ],
            "angle": 0,
            "content": "B.3 概率基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "529"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.157,
                0.736,
                0.195
            ],
            "angle": 0,
            "content": "命题B.6（相互独立变量的条件期望）设随机向量 \\(\\mathbf{X} = (X_{1},X_{2},\\dots ,X_{m})\\) 与随机向量Y独立， \\(h\\) 是 \\(m\\) 元函数且 \\(\\mathbb{E}[|h(\\mathbf{X})|]\\) 有限，则"
        },
        {
            "type": "equation",
            "bbox": [
                0.366,
                0.205,
                0.54,
                0.223
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} [ h (\\boldsymbol {X}) | \\boldsymbol {Y} ] = \\mathbb {E} [ h (\\boldsymbol {X}) ].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.234,
                0.737,
                0.292
            ],
            "angle": 0,
            "content": "命题B.6说明, 当 \\(h(\\mathbf{X})\\) 和 \\(\\mathbf{Y}\\) 相互独立时, \\(h(\\mathbf{X})\\) 关于 \\(\\mathbf{Y}\\) 的条件期望是恒为常值 \\(\\mathbb{E}[h(\\mathbf{X})]\\) 的随机变量. 这从相互独立本身的含义也可以看出: \\(h(\\mathbf{X})\\) 的数学期望与是否给定 \\(\\mathbf{Y}\\) 是没有关系的."
        },
        {
            "type": "title",
            "bbox": [
                0.171,
                0.317,
                0.395,
                0.334
            ],
            "angle": 0,
            "content": "B.3.4 随机变量的收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.171,
                0.348,
                0.737,
                0.406
            ],
            "angle": 0,
            "content": "随机变量本质上是定义在样本空间上的函数。和函数列一样，我们可以定义一列随机变量的极限。给定概率空间 \\((\\Omega, \\mathcal{F}, P)\\) 中的一列随机变量 \\(\\{X_n\\}_{n=1}^{\\infty}\\)，若存在随机变量 \\(X\\)，使得事件"
        },
        {
            "type": "equation",
            "bbox": [
                0.355,
                0.416,
                0.551,
                0.438
            ],
            "angle": 0,
            "content": "\\[\n\\left\\{\\omega \\mid \\lim  _ {n \\rightarrow \\infty} X _ {n} (\\omega) = X (\\omega) \\right\\}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.446,
                0.737,
                0.463
            ],
            "angle": 0,
            "content": "发生的概率为 1 , 那么就称随机变量序列 \\(\\left\\{X_{n}\\right\\}_{n = 1}^{\\infty}\\) 几乎必然收敛到 \\(X\\), 记作"
        },
        {
            "type": "equation",
            "bbox": [
                0.318,
                0.469,
                0.59,
                0.496
            ],
            "angle": 0,
            "content": "\\[\nX _ {n} \\xrightarrow {\\mathrm {a . s .}} X \\quad {\\text {或}} \\quad P \\left(\\lim  _ {n \\to \\infty} X _ {n} = X\\right) = 1.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.502,
                0.736,
                0.539
            ],
            "angle": 0,
            "content": "因此，假设一列随机变量 \\(\\{X_{n}\\}_{n = 1}^{\\infty}\\) 几乎必然收敛到 \\(X\\) ，那么不收敛到 \\(X\\) 的样本点组成的事件发生的概率为0."
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.543,
                0.737,
                0.579
            ],
            "angle": 0,
            "content": "相较于几乎必然收敛，一个弱化的版本是依概率收敛。如果对于任意的 \\(\\varepsilon > 0\\)，都有"
        },
        {
            "type": "equation",
            "bbox": [
                0.354,
                0.584,
                0.552,
                0.606
            ],
            "angle": 0,
            "content": "\\[\n\\lim  _ {n \\rightarrow \\infty} P (| X _ {n} - X | \\geqslant \\varepsilon) = 0,\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.609,
                0.738,
                0.731
            ],
            "angle": 0,
            "content": "那么称随机变量序列 \\(\\{X_{n}\\}_{n = 1}^{\\infty}\\) 依概率收敛到 \\(X\\) 。依概率收敛的直观含义是当任意给定 \\(\\varepsilon >0\\) 且 \\(n\\) 趋于无穷时，事件“\\(X_{n}\\) 与 \\(X\\) 的距离大于 \\(\\varepsilon\\)”发生的概率趋于零。依概率收敛并没有要求 \\(X_{n}\\) 具体到每一个样本点 \\(\\omega\\) 上的极限行为，因此它比几乎必然收敛要弱。实际上几乎必然收敛可以推出依概率收敛，但反过来结论不成立。读者可尝试构造出依概率收敛的序列 \\(\\{X_{n}\\}_{n = 1}^{\\infty}\\)，但 \\(\\{X_{n}\\}_{n = 1}^{\\infty}\\) 在所有样本点上不收敛（即几乎必然不收敛）。"
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.755,
                0.317,
                0.772
            ],
            "angle": 0,
            "content": "B.3.5 随机过程"
        },
        {
            "type": "title",
            "bbox": [
                0.171,
                0.786,
                0.274,
                0.802
            ],
            "angle": 0,
            "content": "1. 基本定义"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.816,
                0.739,
                0.854
            ],
            "angle": 0,
            "content": "一个随机过程 \\(\\{X(t), t \\in T\\}\\) 是一串随机变量的集合，即对于每个 \\(t \\in T\\)，\\(X(t)\\) 为一个随机变量。\\(t\\) 一般用来指代不同的时刻，我们称 \\(X(t)\\) 为随机过"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "530"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.157,
                0.825,
                0.235
            ],
            "angle": 0,
            "content": "程在时刻 \\(t\\) 的状态。随机过程的状态空间定义为所有随机变量 \\(X(t)\\) 的取值集合。从定义来看，一个随机过程是一族随机变量，其描述一个物理过程随时间的演化。例如 \\(X(t)\\) 可以为某商场在时刻 \\(t\\) 的顾客数，或者为某粒子在时刻 \\(t\\) 的空间坐标等。"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.24,
                0.825,
                0.318
            ],
            "angle": 0,
            "content": "集合 \\(T\\) 称为随机过程的指标集. 如果 \\(T\\) 是一个可数集合, 对应的随机过程称为离散随机过程; 如果 \\(T\\) 是实数轴上的一个区间, 对应的随机过程称为连续随机过程. 比如 \\(\\{X_{n}, n = 0,1,\\dots\\}\\) 为一个离散随机过程, \\(\\{X(t), t > 0\\}\\) 为一个连续随机过程."
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.342,
                0.398,
                0.359
            ],
            "angle": 0,
            "content": "2. 马尔可夫过程"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.372,
                0.825,
                0.472
            ],
            "angle": 0,
            "content": "在一个随机过程中，\\(X(t)\\) 与 \\(t\\) 的关系往往是很复杂的。但是在很多实际问题中，当给定当前状态以及过去所有状态时，未来状态的条件分布仅依赖于当前状态，而与过去的状态无关。这种性质称为马尔可夫性质，具有马尔可夫性质的随机过程称为马尔可夫过程。具体地，假设 \\(\\{X(t), t > 0\\}\\) 为一个连续随机过程，马尔可夫性质是指"
        },
        {
            "type": "equation",
            "bbox": [
                0.39,
                0.483,
                0.695,
                0.522
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} P (X (t + h) = y \\mid X (s) = x (s), s \\leqslant t) \\\\ = P (X (t + h) = y \\mid X (t) = x (t)), \\quad \\forall h > 0. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.533,
                0.679,
                0.549
            ],
            "angle": 0,
            "content": "对于离散随机过程，我们也可类似地定义马尔可夫性质。"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.573,
                0.311,
                0.589
            ],
            "angle": 0,
            "content": "3. 鞅"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.603,
                0.825,
                0.641
            ],
            "angle": 0,
            "content": "鞅是随机过程论中一个基本概念，其在随机过程的理论分析中扮演着重要角色。本书利用鞅的一些知识来分析随机算法的收敛性。"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.651,
                0.833,
                0.688
            ],
            "angle": 0,
            "content": "定义B.10(离散鞅）设 \\(\\{X_{n},n = 1,2,\\dots \\}\\) 和 \\(\\{Z_{n},n = 1,2,\\dots \\}\\) 为 \\((\\Omega ,\\mathcal{F},P)\\) 上的随机过程，如果对于任意正整数 \\(n\\) ，"
        },
        {
            "type": "text",
            "bbox": [
                0.273,
                0.7,
                0.416,
                0.716
            ],
            "angle": 0,
            "content": "(1) \\(\\mathbb{E}[|X_n|] < +\\infty\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.273,
                0.73,
                0.473,
                0.745
            ],
            "angle": 0,
            "content": "(2) \\(X_{n}\\in \\sigma (Z_{1},Z_{2},\\dots ,Z_{n})\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.273,
                0.759,
                0.604,
                0.774
            ],
            "angle": 0,
            "content": "(3) \\(\\mathbb{E}[X_{n + 1}|Z_1,Z_2,\\dots ,Z_n] = X_n,n = 1,2,\\dots ,\\)"
        },
        {
            "type": "list",
            "bbox": [
                0.273,
                0.7,
                0.604,
                0.774
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.786,
                0.823,
                0.823
            ],
            "angle": 0,
            "content": "则称 \\(\\{X_{n}, n = 1,2,\\dots\\}\\) 为（关于 \\(\\{Z_{n}, n = 1,2,\\dots\\}\\) 的）离散鞅。若在上面定义中，(3) 改为"
        },
        {
            "type": "equation",
            "bbox": [
                0.384,
                0.836,
                0.698,
                0.854
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} \\left[ X _ {n + 1} \\mid Z _ {1}, Z _ {2}, \\dots , Z _ {n} \\right] = 0, \\quad n = 1, 2, \\dots ,\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.284,
                0.132
            ],
            "angle": 0,
            "content": "B.3 概率基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.735,
                0.131
            ],
            "angle": 0,
            "content": "531"
        },
        {
            "type": "text",
            "bbox": [
                0.17,
                0.157,
                0.658,
                0.174
            ],
            "angle": 0,
            "content": "则称 \\(\\{X_{n}, n = 1,2,\\dots\\}\\) 为（关于 \\(\\{Z_{n}, n = 1,2,\\dots\\}\\) 的）鞅差序列"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.195,
                0.736,
                0.233
            ],
            "angle": 0,
            "content": "类似地，对于连续时间随机过程 \\(\\{X(t), t \\geqslant 0\\}\\) 也可定义连续鞅，由于我们没有用到这部分知识，所以略去说明。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.239,
                0.737,
                0.297
            ],
            "angle": 0,
            "content": "可以通过这样的方式来理解鞅：令 \\(\\{X_{n}\\} = \\{Z_{n}\\}\\)，在给定 \\(X_{1}, X_{2}, \\dots, X_{n}\\) 后，对 \\(X_{n+1}\\) 的最佳预测就是 \\(X_{n}\\)。换句话说，我们没有办法通过所有历史信息来预测一个鞅序列到底是上升还是下降，最好的预测就是当前 \\(X_{n}\\) 的值。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.303,
                0.737,
                0.382
            ],
            "angle": 0,
            "content": "我们利用下面的例子来进一步说明鞅的含义。假设进行一次多轮投资，每一轮有 \\(\\frac{1}{2}\\) 的概率获利，其利润等于投入的本金，还有 \\(\\frac{1}{2}\\) 的概率亏损全部已经投入的本金。在每轮投资进行之前可以根据先前盈亏情况来决定这一轮的本金。设在第 \\(n\\) 轮投资结束后总资产为 \\(X_{n}\\)，\\(\\eta_{n}\\) 为第 \\(n\\) 轮获利情况，即"
        },
        {
            "type": "equation",
            "bbox": [
                0.387,
                0.398,
                0.519,
                0.43
            ],
            "angle": 0,
            "content": "\\[\nP (\\eta_ {n} = \\pm 1) = \\frac {1}{2},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.445,
                0.737,
                0.503
            ],
            "angle": 0,
            "content": "且 \\(\\eta_{n}\\) 之间相互独立．为了方便，令 \\(\\mathbf{X}_{n} = (X_{0}, X_{1}, \\dots, X_{n})\\) ，设 \\(a_{n}(\\mathbf{X}_{n-1})\\) 为第 \\(n\\) 轮投资的本金，注意这里 \\(a_{n}\\) 只与时刻 \\(n-1\\) 之前的历史资产有关．显然，在 \\(n\\) 轮投资结束后，总资产 \\(X_{n}\\) 的表达式为"
        },
        {
            "type": "equation",
            "bbox": [
                0.359,
                0.518,
                0.737,
                0.555
            ],
            "angle": 0,
            "content": "\\[\nX _ {n} = X _ {0} + \\sum_ {i = 1} ^ {n} a _ {i} \\left(\\mathbf {X} _ {i - 1}\\right) \\eta_ {i}. \\tag {B.3.4}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.571,
                0.538,
                0.587
            ],
            "angle": 0,
            "content": "容易计算出，当给定 \\(\\mathbf{X}_{n - 1}\\) 时， \\(X_{n}\\) 的条件期望为"
        },
        {
            "type": "equation",
            "bbox": [
                0.248,
                0.604,
                0.659,
                0.706
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} \\mathbb {E} \\left[ X _ {n} \\mid \\mathbf {X} _ {n - 1} \\right] = \\mathbb {E} \\left[ X _ {0} + \\sum_ {i = 1} ^ {n} a _ {i} \\left(\\mathbf {X} _ {i - 1}\\right) \\eta_ {i} \\mid \\mathbf {X} _ {n - 1} \\right] \\\\ = X _ {0} + \\sum_ {i = 1} ^ {n - 1} a _ {i} (\\mathbf {X} _ {i - 1}) + \\mathbb {E} \\left[ a _ {n} \\left(\\mathbf {X} _ {n - 1}\\right) \\eta_ {n} \\mid \\mathbf {X} _ {n - 1} \\right] \\\\ = X _ {n - 1} + a _ {n} \\left(\\mathbf {X} _ {n - 1}\\right) \\mathbb {E} \\left[ \\eta_ {n} \\mid \\mathbf {X} _ {n - 1} \\right] = X _ {n - 1}. \\\\ \\end{array}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.723,
                0.736,
                0.761
            ],
            "angle": 0,
            "content": "这表明 \\(\\{X_{n}\\}\\) 是一个鞅. 上面推导的另一个解释为: 无论使用何种策略 \\(a_{n}\\) 来决定本金, 我们对下一轮预测的结果总是 “不盈不亏”."
        },
        {
            "type": "title",
            "bbox": [
                0.17,
                0.801,
                0.336,
                0.818
            ],
            "angle": 0,
            "content": "B.3.6 概率不等式"
        },
        {
            "type": "text",
            "bbox": [
                0.204,
                0.836,
                0.454,
                0.853
            ],
            "angle": 0,
            "content": "这里介绍一些常用的概率不等式"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.259,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "532"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        },
        {
            "type": "title",
            "bbox": [
                0.258,
                0.157,
                0.403,
                0.174
            ],
            "angle": 0,
            "content": "1. Jensen 不等式"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.187,
                0.825,
                0.223
            ],
            "angle": 0,
            "content": "给定一个凸函数 \\(\\phi\\)，假设随机变量 \\(X\\) 的期望 \\(\\mathbb{E}[X]\\) 存在以及 \\(\\mathbb{E}[\\phi(X)]\\) 存在，那么"
        },
        {
            "type": "equation",
            "bbox": [
                0.463,
                0.227,
                0.621,
                0.247
            ],
            "angle": 0,
            "content": "\\[\n\\phi (\\mathbb {E} [ X ]) \\leqslant \\mathbb {E} [ \\phi (X) ],\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.253,
                0.825,
                0.29
            ],
            "angle": 0,
            "content": "这个不等式称为概率Jensen不等式．当 \\(X\\) 为离散型随机变量时，根据凸函数的性质，"
        },
        {
            "type": "equation",
            "bbox": [
                0.367,
                0.29,
                0.715,
                0.326
            ],
            "angle": 0,
            "content": "\\[\n\\phi (\\mathbb {E} [ X ]) = \\phi (\\sum_ {i = 1} ^ {\\infty} p _ {i} x _ {i}) \\leqslant \\sum_ {i = 1} ^ {\\infty} p _ {i} \\phi (x _ {i}) = \\mathbb {E} [ \\phi (X) ].\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.328,
                0.825,
                0.366
            ],
            "angle": 0,
            "content": "当 \\(X\\) 为连续型随机变量时，根据积分定义可以用类似的方式证出Jensen不等式．通过取 \\(\\phi (x) = |x|\\) 和 \\(\\phi (x) = x^{2}\\) ，我们有"
        },
        {
            "type": "equation",
            "bbox": [
                0.398,
                0.375,
                0.684,
                0.395
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{r} {\\left| \\mathbb {E} [ X ] \\right| \\leqslant \\mathbb {E} [ | X | ] \\quad \\text {和} \\quad \\mathbb {E} [ X ] ^ {2} \\leqslant \\mathbb {E} [ X ^ {2} ].} \\end{array}\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.417,
                0.414,
                0.433
            ],
            "angle": 0,
            "content": "2. 马尔可夫不等式"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.447,
                0.706,
                0.463
            ],
            "angle": 0,
            "content": "设 \\(X\\) 为一个非负随机变量，则对任意的实数 \\(a > 0\\) ，有"
        },
        {
            "type": "equation",
            "bbox": [
                0.47,
                0.467,
                0.612,
                0.5
            ],
            "angle": 0,
            "content": "\\[\nP (X \\geqslant a) \\leqslant \\frac {\\mathbb {E} [ X ]}{a}.\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.504,
                0.75,
                0.521
            ],
            "angle": 0,
            "content": "我们针对连续随机变量的情形给出其证明。根据随机变量的定义，"
        },
        {
            "type": "equation",
            "bbox": [
                0.283,
                0.525,
                0.799,
                0.558
            ],
            "angle": 0,
            "content": "\\[\n\\mathbb {E} [ X ] = \\int_ {- \\infty} ^ {+ \\infty} x f (x) \\mathrm {d} x \\geqslant \\int_ {a} ^ {+ \\infty} x f (x) \\mathrm {d} x \\geqslant \\int_ {a} ^ {+ \\infty} a f (x) \\mathrm {d} x = a P (X \\geqslant a).\n\\]"
        },
        {
            "type": "title",
            "bbox": [
                0.257,
                0.575,
                0.414,
                0.592
            ],
            "angle": 0,
            "content": "3. 切比雪夫不等式"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.606,
                0.825,
                0.642
            ],
            "angle": 0,
            "content": "方差是用来刻画随机变量偏离其平均值的程度大小。利用方差的定义，可以得到切比雪夫（Chebyshev）不等式"
        },
        {
            "type": "equation",
            "bbox": [
                0.428,
                0.648,
                0.657,
                0.681
            ],
            "angle": 0,
            "content": "\\[\nP (| X - \\mathbb {E} [ X ] | \\geqslant a) \\leqslant \\frac {\\mathrm {V a r} (X)}{a ^ {2}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.257,
                0.686,
                0.825,
                0.724
            ],
            "angle": 0,
            "content": "其中实数 \\(a > 0\\) ，不妨设 \\(\\mathbb{E}[X] = 0\\) （如果 \\(\\mathbb{E}[X]\\neq 0\\) ，可令 \\(\\hat{X} = X - \\mathbb{E}[X]\\) 并对 \\(\\hat{X}\\) 进行分析)，那么不等式变为"
        },
        {
            "type": "equation",
            "bbox": [
                0.456,
                0.728,
                0.627,
                0.761
            ],
            "angle": 0,
            "content": "\\[\nP (| X | \\geqslant a) \\leqslant \\frac {\\operatorname {V a r} (X)}{a ^ {2}},\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.258,
                0.765,
                0.391,
                0.781
            ],
            "angle": 0,
            "content": "其证明过程如下："
        },
        {
            "type": "equation",
            "bbox": [
                0.31,
                0.786,
                0.773,
                0.856
            ],
            "angle": 0,
            "content": "\\[\n\\begin{array}{l} P (| X | \\geqslant a) = \\int_ {(- \\infty , - a ] \\cup [ a, + \\infty)} f (x) d x \\leqslant \\int_ {(- \\infty , - a ] \\cup [ a, + \\infty)} \\frac {x ^ {2}}{a ^ {2}} f (x) d x \\\\ \\leqslant \\frac {1}{a ^ {2}} \\int_ {- \\infty} ^ {+ \\infty} x ^ {2} f (x) d x = \\frac {1}{a ^ {2}} \\operatorname {V a r} (X). \\\\ \\end{array}\n\\]"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.171,
                0.117,
                0.285,
                0.133
            ],
            "angle": 0,
            "content": "B.3 概率基础"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "533"
        },
        {
            "type": "title",
            "bbox": [
                0.169,
                0.157,
                0.406,
                0.174
            ],
            "angle": 0,
            "content": "4. Azuma-Hoeffding 不等式"
        },
        {
            "type": "text",
            "bbox": [
                0.168,
                0.187,
                0.737,
                0.224
            ],
            "angle": 0,
            "content": "下面给出非常有用的 Azuma-Hoeffding 不等式，它是鞅版本的 Ho-effding 不等式。"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.239,
                0.737,
                0.276
            ],
            "angle": 0,
            "content": "定理B.6（Azuma-Hoeffding不等式）设 \\(X_{1}, X_{2}, \\cdots\\) 是鞅差序列，且对所有的 \\(i\\) ，有 \\(|X_{i}| \\leqslant B\\) ，则对任意的 \\(t > 0\\) 有"
        },
        {
            "type": "equation",
            "bbox": [
                0.345,
                0.288,
                0.736,
                0.33
            ],
            "angle": 0,
            "content": "\\[\nP \\left(\\sum_ {i = 1} ^ {n} X _ {i} \\geqslant t\\right) \\leqslant \\exp \\left(- \\frac {2 t ^ {2}}{n B ^ {2}}\\right), \\tag {B.3.5}\n\\]"
        },
        {
            "type": "equation",
            "bbox": [
                0.329,
                0.332,
                0.736,
                0.374
            ],
            "angle": 0,
            "content": "\\[\nP \\left(\\sum_ {i = 1} ^ {n} X _ {i} \\leqslant - t\\right) \\leqslant \\exp \\left(- \\frac {2 t ^ {2}}{n B ^ {2}}\\right). \\tag {B.3.6}\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.385,
                0.737,
                0.429
            ],
            "angle": 0,
            "content": "特别地，我们经常会将上面定理中的不等式写成关于样本均值的形式，如果令 \\(\\delta = \\frac{t}{n}\\)，则有"
        },
        {
            "type": "equation",
            "bbox": [
                0.321,
                0.439,
                0.586,
                0.482
            ],
            "angle": 0,
            "content": "\\[\nP \\left(\\frac {1}{n} \\sum_ {i = 1} ^ {n} X _ {i} \\geqslant \\delta\\right) \\leqslant \\exp \\left(- \\frac {2 n \\delta^ {2}}{B ^ {2}}\\right).\n\\]"
        },
        {
            "type": "text",
            "bbox": [
                0.169,
                0.492,
                0.737,
                0.53
            ],
            "angle": 0,
            "content": "如果 \\(\\{X_i\\}\\) 是独立同分布的，且 \\(\\mathbb{E}[X_i] = \\mu\\) ，则 \\(\\{X_i - \\mu\\}\\) 是鞅差序列，所以根据定理B.6可得"
        },
        {
            "type": "equation",
            "bbox": [
                0.294,
                0.54,
                0.613,
                0.584
            ],
            "angle": 0,
            "content": "\\[\nP \\left(\\left| \\frac {1}{n} \\sum_ {i = 1} ^ {n} X _ {i} - \\mu \\right| \\geqslant \\delta\\right) \\leqslant 2 \\exp \\left(- \\frac {2 n \\delta^ {2}}{B ^ {2}}\\right).\n\\]"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "534"
        },
        {
            "type": "header",
            "bbox": [
                0.686,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "附录B 数学基础"
        }
    ],
    [
        {
            "type": "title",
            "bbox": [
                0.381,
                0.252,
                0.53,
                0.283
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.188,
                0.333,
                0.74,
                0.392
            ],
            "angle": 0,
            "content": "[1] ABADI M, AGARWAL A, BARHAM P, et al. TensorFlow: large-scale machine learning on heterogeneous systems[Z]. TensorFlow. 2015."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.188,
                0.4,
                0.742,
                0.44
            ],
            "angle": 0,
            "content": "[2] ABSIL P A, MAHONY R, SEPULCHRE R. Optimization algorithms on matrix manifolds[M]. Princeton: Princeton University Press, 2009."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.188,
                0.447,
                0.74,
                0.486
            ],
            "angle": 0,
            "content": "[3] ADBY P. Introduction to optimization methods[M]. Berlin: Springer Science & Business Media, 2013."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.188,
                0.493,
                0.74,
                0.554
            ],
            "angle": 0,
            "content": "[4] AMESTOY P R, DUFF I S, KOSTER J, et al. A fully asynchronous multifrontal solver using distributed dynamic scheduling[J]. SIAM Journal on Matrix Analysis and Applications, 2001, 23(1): 15-41."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.188,
                0.56,
                0.74,
                0.601
            ],
            "angle": 0,
            "content": "[5] ANDERSON D G. Iterative procedures for nonlinear integral equations[J]. Journal of the ACM (JACM), 1965, 12(4): 547-560."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.188,
                0.606,
                0.74,
                0.647
            ],
            "angle": 0,
            "content": "[6] ANDERSON E, BAI Z, BISCHOF C, et al. LAPACK users' guide[M]. 3rd ed. Philadelphia: Society for Industrial, 1999."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.188,
                0.653,
                0.74,
                0.714
            ],
            "angle": 0,
            "content": "[7] ARORA R, COTTER A, SREBRO N. Stochastic optimization of PCA with capped MSG[C]// Advances in neural information processing systems. Cambridge, Massachusetts: MIT Press, 2013: 1815-1823."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.188,
                0.72,
                0.74,
                0.802
            ],
            "angle": 0,
            "content": "[8] ASHCRAFT C, PIERCE D, WAH D K, et al. The reference manual for SPOOLES, release 2.2: An object oriented software library for solving sparse linear systems of equations[J]. Boeing Shared Service Group, 1999."
        },
        {
            "type": "list",
            "bbox": [
                0.188,
                0.333,
                0.742,
                0.802
            ],
            "angle": 0,
            "content": null
        },
        {
            "type": "page_number",
            "bbox": [
                0.44,
                0.869,
                0.469,
                0.882
            ],
            "angle": 0,
            "content": "535"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "536"
        },
        {
            "type": "header",
            "bbox": [
                0.753,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.276,
                0.156,
                0.847,
                0.215
            ],
            "angle": 0,
            "content": "[9] AUBERT G, KORNPROBST P. Mathematical problems in image processing: partial differential equations and the calculus of variations[M]. Berlin: Springer Science & Business Media, 2006."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.224,
                0.828,
                0.283
            ],
            "angle": 0,
            "content": "[10] AUSLENDER A. Asymptotic properties of the Fenchel dual functional and applications to decomposition problems[J]. Journal of Optimization Theory and Applications, 1992, 73(3): 427-449."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.29,
                0.827,
                0.33
            ],
            "angle": 0,
            "content": "[11] AVERICK B M, CARTER R G, XUE G L, et al. The MINPACK-2 test problem collection[R]. Chicago: Argonne National Lab, 1992."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.338,
                0.827,
                0.375
            ],
            "angle": 0,
            "content": "[12] BAIRE R, DENJOY A. Leçons sur les fonctions discontinues: professionnelles au collège de france[M]. Paris: Gauthier-Villars, 1905."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.383,
                0.827,
                0.464
            ],
            "angle": 0,
            "content": "[13] BALAY S, GROPP W D, MCINNES L C, et al. Efficient management of parallelism in object oriented numerical software libraries[C]// ARGE E, BRUASET A M, LANGTANGEN H P. Modern software tools in scientific computing. Berlin: Birkhäuser Press, 1997: 163-202."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.471,
                0.831,
                0.528
            ],
            "angle": 0,
            "content": "[14] BAUSCHKE H H, COMBETTES P L, et al. Convex analysis and monotone operator theory in Hilbert spaces[M]. New York: Springer, 2011."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.538,
                0.842,
                0.597
            ],
            "angle": 0,
            "content": "[15] BECK A, TEBOULLE M. Mirror descent and nonlinear projected subgradient methods for convex optimization[J]. Operations Research Letters, 2003, 31(3): 167-175."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.605,
                0.827,
                0.664
            ],
            "angle": 0,
            "content": "[16] BECK A, TEBOULLE M. A fast iterative shrinkage-thresholding algorithm for linear inverse problems[J]. SIAM Journal on Imaging Sciences, 2009, 2(1): 183-202."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.672,
                0.827,
                0.753
            ],
            "angle": 0,
            "content": "[17] BECK A, TEBOULLE M. Gradient-based algorithms with applications to signal-recovery problems[M]//Convex optimization in signal processing and communications. Cambridge, UK: Cambridge University Press, 2009: 42-88."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.76,
                0.827,
                0.819
            ],
            "angle": 0,
            "content": "[18] BECK A, TETRUASHVILI L. On the convergence of block coordinate descent type methods[J]. SIAM Journal on Optimization, 2013, 23(4): 2037-2060."
        },
        {
            "type": "list",
            "bbox": [
                0.269,
                0.156,
                0.847,
                0.819
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.173,
                0.117,
                0.244,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "537"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.18,
                0.156,
                0.739,
                0.215
            ],
            "angle": 0,
            "content": "[19] BECKER S, BOBIN J, CANDÉS E J. NESTA: a fast and accurate first-order method for sparse recovery[J]. SIAM Journal on Imaging Sciences, 2011, 4(1): 1-39."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.181,
                0.224,
                0.737,
                0.283
            ],
            "angle": 0,
            "content": "[20] BERAHAS A S, BOLLAPRAGADA R, NOCEDAL J. An investigation of Newton-sketch and subsampled Newton methods[J]. Optimization Methods and Software, 2020: 1-20."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.291,
                0.737,
                0.33
            ],
            "angle": 0,
            "content": "[21] BERTSEKAS D P. Constrained optimization and Lagrange multiplier methods[M]. Salt Lake: Academic press, 2014."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.338,
                0.737,
                0.396
            ],
            "angle": 0,
            "content": "[22] BINDEL D, DEMMEL J, KAHAN W, et al. On computing Givens rotations reliably and efficiently[J]. ACM Transactions on Mathematical Software (TOMS), 2002, 28(2): 206-238."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.405,
                0.739,
                0.463
            ],
            "angle": 0,
            "content": "[23] BLACKFORD L S, PETITET A, POZO R, et al. An updated set of basic linear algebra subprograms (BLAS)[J]. ACM Transactions on Mathematical Software, 2002, 28(2): 135-151."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.471,
                0.742,
                0.53
            ],
            "angle": 0,
            "content": "[24] BOLTE J, SABACH S, TEBOULLE M. Proximal alternating linearized minimization for nonconvex and nonsmooth problems[J]. Mathematical Programming, 2014, 146(1): 459-494."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.538,
                0.739,
                0.597
            ],
            "angle": 0,
            "content": "[25] BORDES A, BOTTOU L, GALLINARI P. SGD-QN: careful quasiNewton stochastic gradient descent[J]. Journal of Machine Learning Research, 2009, 10(59): 1737-1754."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.605,
                0.739,
                0.663
            ],
            "angle": 0,
            "content": "[26] BORWEIN J, LEWIS A S. Convex analysis and nonlinear optimization: theory and examples[M]. Berlin: Springer Science & Business Media, 2010."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.673,
                0.739,
                0.731
            ],
            "angle": 0,
            "content": "[27] BOSMA W, CANNON J, PLAYOUST C. The Magma algebra system i: the user language[J]. Journal of Symbolic Computation, 1997, 24(3-4): 235-265."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.74,
                0.739,
                0.779
            ],
            "angle": 0,
            "content": "[28] BOTTOU L, CURTIS F E, NOCEDAL J. Optimization methods for large-scale machine learning[J]. SIAM Review, 2018, 60(2): 223-311."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.786,
                0.739,
                0.845
            ],
            "angle": 0,
            "content": "[29] BOUMAL N, MISHRA B, ABSIL P A, et al. Manopt, a MATLAB toolbox for optimization on manifolds[J]. Journal of Machine Learning Research, 2014, 15(42): 1455-1459."
        },
        {
            "type": "list",
            "bbox": [
                0.18,
                0.156,
                0.742,
                0.845
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "538"
        },
        {
            "type": "header",
            "bbox": [
                0.754,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.156,
                0.826,
                0.235
            ],
            "angle": 0,
            "content": "[30] BOYD S, PARIKH N, CHU E, et al. Distributed optimization and statistical learning via the alternating direction method of multipliers[J]. Foundations and Trendsö in Machine learning, 2011, 3(1): 1-122."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.267,
                0.244,
                0.836,
                0.283
            ],
            "angle": 0,
            "content": "[31] BOYDS, VANDENBERGHE L. Convex optimization[M]. Cambridge, UK: Cambridge University Press, 2004."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.29,
                0.828,
                0.35
            ],
            "angle": 0,
            "content": "[32] BUNCH J R, PARLETT B N. Direct methods for solving symmetric indefinite systems of linear equations[J]. SIAM Journal on Numerical Analysis, 1971, 8(4):639-655."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.357,
                0.835,
                0.417
            ],
            "angle": 0,
            "content": "[33] BUTTARI A, LANGOU J, KURZAK J, et al. A class of parallel tiled linear algebra algorithms for multicore architectures[J]. Parallel Computing, 2009, 35(1): 38-53."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.424,
                0.826,
                0.484
            ],
            "angle": 0,
            "content": "[34] BYRD R H, CHIN G M, NEVEITT W, et al. On the use of stochastic Hessian information in optimization methods for machine learning[J]. SIAM Journal on Optimization, 2011, 21(3): 977-995."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.491,
                0.826,
                0.552
            ],
            "angle": 0,
            "content": "[35] BYRD R H, CHIN G M, NOCEDAL J, et al. Sample size selection in optimization methods for machine learning[J]. Mathematical Programming, 2012, 134(1): 127-155."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.558,
                0.826,
                0.618
            ],
            "angle": 0,
            "content": "[36] BYRD R H, HANSEN S L, NOCEDAL J, et al. A stochastic quasiNewton method for large-scale optimization[J]. SIAM Journal on Optimization, 2016, 26(2): 1008-1031."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.625,
                0.829,
                0.685
            ],
            "angle": 0,
            "content": "[37] BYRD R H, NOCEDAL J, SCHNABEL R B. Representations of quasiNewton matrices and their use in limited memory methods[J]. Mathematical Programming, 1994, 63(1-3): 129-156."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.693,
                0.826,
                0.753
            ],
            "angle": 0,
            "content": "[38] BYRD R H, SCHNABEL R B, SHULTZ G A. Approximate solution of the trust region problem by minimization over two-dimensional subspaces[J]. Mathematical Programming, 1988, 40(1-3): 247-263."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.759,
                0.857,
                0.819
            ],
            "angle": 0,
            "content": "[39] CANDÉS E J, LI X, SOLTANOLKOTABI M. Phase retrieval via Wirtinger flow: theory and algorithms[J]. IEEE Transactions on Information Theory, 2015, 61(4): 1985-2007."
        },
        {
            "type": "list",
            "bbox": [
                0.267,
                0.156,
                0.857,
                0.819
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.173,
                0.117,
                0.244,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "539"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.181,
                0.156,
                0.739,
                0.236
            ],
            "angle": 0,
            "content": "[40] CANDÉS E J, STROHMER T, VORONINSKI V. Phaselift: exact and stable signal recovery from magnitude measurements via convex programming[J]. Communications on Pure and Applied Mathematics, 2013, 66(8): 1241-1274."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.181,
                0.244,
                0.739,
                0.305
            ],
            "angle": 0,
            "content": "[41] CHAMBOLLE A, POCK T. A first-order primal-dual algorithm for convex problems with applications to imaging[J]. Journal of Mathematical Imaging and Vision, 2011, 40(1): 120-145."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.181,
                0.311,
                0.74,
                0.372
            ],
            "angle": 0,
            "content": "[42] CHANG C C, LIN C J. Libsvm: a library for support vector machines[J]. ACM Transactions on Intelligent Systems and Technology (TIST), 2011, 2(3): 1-27."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.181,
                0.378,
                0.74,
                0.44
            ],
            "angle": 0,
            "content": "[43] CHEN C, HE B, YE Y, et al. The direct extension of ADMM for multi-block convex minimization problems is not necessarily convergent[J]. Mathematical Programming, 2016, 155(1-2): 57-79."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.181,
                0.446,
                0.74,
                0.506
            ],
            "angle": 0,
            "content": "[44] CHEN C, LI M, LIU X, et al. On the convergence of multi-block alternating direction method of multipliers and block coordinate descent method[J]. ArXiv:1508.00193, 2015."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.181,
                0.512,
                0.74,
                0.573
            ],
            "angle": 0,
            "content": "[45] CHEN L, SUN D, TOH K C. A note on the convergence of ADMM for linearly constrained convex optimization problems[J]. Computational Optimization and Applications, 2017, 66(2): 327-343."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.181,
                0.579,
                0.74,
                0.68
            ],
            "angle": 0,
            "content": "[46] CHOI J, DONGARRA J J, POZO R, et al. ScaLAPACK: a scalable linear algebra library for distributed memory concurrent computers[C]//The fourth symposium on the frontiers of massively parallel computation. Washington: IEEE Computer Society Press, 1992: 120-127."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.181,
                0.687,
                0.74,
                0.747
            ],
            "angle": 0,
            "content": "[47] COLLOBERT R, KAVUKCUOGLU K, FARABET C. Torch7: a MATLAB-like environment for machine learning[C]//Nips 2011. [S.l.]: NIPS Foundation, 2011."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.181,
                0.754,
                0.74,
                0.813
            ],
            "angle": 0,
            "content": "[48] COMBETTES P L, WAJS V R. Signal recovery by proximal forward-backward splitting[J]. Multiscale Modeling & Simulation, 2005, 4(4): 1168-1200."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.181,
                0.822,
                0.556,
                0.841
            ],
            "angle": 0,
            "content": "[49] CPLEX ILOG. User's manual 12.7[Z]. 2016."
        },
        {
            "type": "list",
            "bbox": [
                0.181,
                0.156,
                0.74,
                0.841
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "540"
        },
        {
            "type": "header",
            "bbox": [
                0.753,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.156,
                0.826,
                0.215
            ],
            "angle": 0,
            "content": "[50] DAI Y H, YUAN Y X. A nonlinear conjugate gradient method with a strong global convergence property[J]. SIAM Journal on Optimization, 1999, 10(1): 177-182."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.223,
                0.826,
                0.28
            ],
            "angle": 0,
            "content": "[51] DAI Y, HAN J, LIU G, et al. Convergence properties of nonlinear conjugate gradient methods[J]. SIAM Journal on Optimization, 2000, 10(2):345-358."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.289,
                0.824,
                0.327
            ],
            "angle": 0,
            "content": "[52] DANTZIG G. Linear programming and extensions[M]. Princeton: Princeton University Press, 2016."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.334,
                0.825,
                0.392
            ],
            "angle": 0,
            "content": "[53] DAVIS T A. Algorithm 1000: SuiteSparse: GraphBLAS: graph algorithms in the language of sparse linear algebra[J]. ACM Trans on Mathematical Software, 2019, 45(4)."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.4,
                0.825,
                0.457
            ],
            "angle": 0,
            "content": "[54] DE CONINCK A, DE BAETS B, KOUROUNIS D, et al. Needles: toward large-scale genomic prediction with marker-by-environment interaction[J]. Genetics, 2016, 203(1): 543-555."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.466,
                0.826,
                0.544
            ],
            "angle": 0,
            "content": "[55] DEFAZIO A, BACH F, LACOSTE-JULIEN S. SAGA: a fast incremental gradient method with support for non-strongly convex composite objectives[C]//Advances in neural information processing systems. Cambridge, Massachusetts: MIT Press, 2014: 1646-1654."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.552,
                0.824,
                0.589
            ],
            "angle": 0,
            "content": "[56] DEMMEL J W. Applied numerical linear algebra[M]. Philadelphia: SIAM, 1997."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.597,
                0.826,
                0.656
            ],
            "angle": 0,
            "content": "[57] DENG W, YIN W. On the global and linear convergence of the generalized alternating direction method of multipliers[J]. Journal of Scientific Computing, 2016, 66(3):889-916."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.663,
                0.826,
                0.722
            ],
            "angle": 0,
            "content": "[58] DENNIS J E, MEI H. Two new unconstrained optimization algorithms which use function and gradient values[J]. Journal of Optimization Theory and Applications, 1979, 28(4): 453-482."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.729,
                0.853,
                0.785
            ],
            "angle": 0,
            "content": "[59] DENNIS JR J E, SCHNABEL R B. Numerical methods for unconstrained optimization and nonlinear equations[M]. Philadelphia: SIAM, 1996."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.269,
                0.794,
                0.826,
                0.853
            ],
            "angle": 0,
            "content": "[60] DIAMOND S, BOYD S. CVXPY: a Python-embedded modeling language for convex optimization[J]. The Journal of Machine Learning Research, 2016, 17(1): 2909-2913."
        },
        {
            "type": "list",
            "bbox": [
                0.268,
                0.156,
                0.853,
                0.853
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.173,
                0.117,
                0.244,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.735,
                0.131
            ],
            "angle": 0,
            "content": "541"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.18,
                0.156,
                0.739,
                0.215
            ],
            "angle": 0,
            "content": "[61] DONGARRA J. Basic linear algebra subprograms technical forum standard[J]. High Performance Applications and Supercomputing, 2002, 16(1): 1-199."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.181,
                0.224,
                0.737,
                0.283
            ],
            "angle": 0,
            "content": "[62] DUBOVITSKII A Y, MILYUTIN A A. Extremum problems in the presence of restrictions[J]. USSR Computational Mathematics and Mathematical Physics, 1965, 5(3): 1-80."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.291,
                0.737,
                0.33
            ],
            "angle": 0,
            "content": "[63] DURRETT R. Probability: theory and examples[M]. 5th ed. Cambridge, UK: Cambridge University Press, 2019."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.338,
                0.739,
                0.415
            ],
            "angle": 0,
            "content": "[64] ESSER E, ZHANG X, CHANT F. A general framework for a class of first order primal-dual algorithms for convex optimization in imaging science[J]. SIAM Journal on Imaging Sciences, 2010, 3(4): 1015-1046."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.425,
                0.741,
                0.484
            ],
            "angle": 0,
            "content": "[65] FALGOUT R D, YANG U M. Hypre: a library of high performance preconditioners[C]//International conference on computational science. Amsterdam: [s.n.], 2002: 632-641."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.493,
                0.739,
                0.569
            ],
            "angle": 0,
            "content": "[66] FAZEL M, PONG T K, SUN D, et al. Hankel matrix rank minimization with applications to system identification and realization[J]. SIAM Journal on Matrix Analysis and Applications, 2013, 34(3): 946-977."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.58,
                0.737,
                0.619
            ],
            "angle": 0,
            "content": "[67] FEI Y, RONG G, WANG B, et al. Parallel L-BFGS-B algorithm on GPU[J]. Computers & Graphics, 2014, 40: 1-9."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.627,
                0.737,
                0.665
            ],
            "angle": 0,
            "content": "[68] FLETCHER R, REEVES C M. Function minimization by conjugate gradients[J]. The Computer Journal, 1964, 7(2): 149-154."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.673,
                0.737,
                0.711
            ],
            "angle": 0,
            "content": "[69] FLETCHER R. Practical methods of optimization[M]. New Jersey: John Wiley & Sons, 2013."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.719,
                0.737,
                0.777
            ],
            "angle": 0,
            "content": "[70] FLETCHER R, FREEMAN T. A modified Newton method for minimization[J]. Journal of Optimization Theory and Applications, 1977, 23(3): 357-372."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.786,
                0.741,
                0.843
            ],
            "angle": 0,
            "content": "[71] FOURER R, GAY D M, KERNIGHAN B W. A modeling language for mathematical programming[J]. Management Science, 1990, 36(5): 519-554."
        },
        {
            "type": "list",
            "bbox": [
                0.18,
                0.156,
                0.741,
                0.843
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "542"
        },
        {
            "type": "header",
            "bbox": [
                0.754,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.156,
                0.826,
                0.196
            ],
            "angle": 0,
            "content": "[72] FRANK M, WOLFE P. An algorithm for quadratic programming[J]. Naval Research Logistics Quarterly, 1956, 3(1-2): 95-110."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.267,
                0.203,
                0.825,
                0.242
            ],
            "angle": 0,
            "content": "[73] FRIEDMAN J, HASTIE T, TIBSHIRANI R. The elements of statistical learning[M]. 2nd ed. New York, NY: Springer, 2009."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.249,
                0.825,
                0.31
            ],
            "angle": 0,
            "content": "[74] GABAY D, MERCIER B. A dual algorithm for the solution of non linear variational problems via finite element approximation[M]. [S.l.]: Institut de recherche d'informatique et d'automatique, 1975."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.317,
                0.836,
                0.376
            ],
            "angle": 0,
            "content": "[75] GAO Y, SUN D. A majorized penalty approach for calibrating rank constrained correlation matrix problems[R]. Singapore: National University of Singapore, 2010."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.383,
                0.826,
                0.442
            ],
            "angle": 0,
            "content": "[76] GHADIMI S, LAN G. Stochastic first-and zeroth-order methods for nonconvex stochastic programming[J]. SIAM Journal on Optimization, 2013, 23(4): 2341-2368."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.451,
                0.826,
                0.51
            ],
            "angle": 0,
            "content": "[77] GHADIMI S, LAN G. Accelerated gradient methods for nonconvex nonlinear and stochastic programming[J]. Mathematical Programming, 2016, 156(1-2): 59-99."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.517,
                0.826,
                0.577
            ],
            "angle": 0,
            "content": "[78] GHADIMIS, LAN G, ZHANG H. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization[J]. Mathematical Programming, 2016, 155(1-2): 267-305."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.584,
                0.826,
                0.643
            ],
            "angle": 0,
            "content": "[79] GILBERT J C, NOCEDAL J. Global convergence properties of conjugate gradient methods for optimization[J]. SIAM Journal on Optimization, 1992, 2(1): 21-42."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.652,
                0.824,
                0.691
            ],
            "angle": 0,
            "content": "[80] GILL P E, MURRAY W, WRIGHT M H. Practical optimization[M]. Philadelphia: Society for Industrial, 2019."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.699,
                0.87,
                0.798
            ],
            "angle": 0,
            "content": "[81] GLOWINSKI R, MARROCO A. Sur l'approximation, par éléments finis d'ordre un, et la résolution, par pénalisation-dualité d'une classe de problèmes de dirichlet non linéaires[J]. Revue française d'automatique, informatique, recherche opérationnelle. Analyse numérique, 1975, 9(R2): 41-76."
        },
        {
            "type": "list",
            "bbox": [
                0.267,
                0.156,
                0.87,
                0.798
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.244,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "543"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.156,
                0.749,
                0.234
            ],
            "angle": 0,
            "content": "[82] GOEMANS M X, WILLIAMSON D P. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming[J]. Journal of the ACM (JACM), 1995, 42(6): 1115-1145."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.245,
                0.739,
                0.303
            ],
            "angle": 0,
            "content": "[83] GOLDFARB D. Curvilinear path steplength algorithms for minimization which use directions of negative curvature[J]. Mathematical Programming, 1980, 18(1):31-40."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.312,
                0.741,
                0.348
            ],
            "angle": 0,
            "content": "[84] GOLDSTEIN A, PRICE J. An effective algorithm for minimization[J]. Numerische Mathematik, 1967, 10(3): 184-189."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.183,
                0.359,
                0.736,
                0.396
            ],
            "angle": 0,
            "content": "[85] GOLUB G H, VAN LOAN C F. Matrix computations[M]. 4th ed. Baltimore: Johns Hopkins University Press, 2012."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.405,
                0.736,
                0.442
            ],
            "angle": 0,
            "content": "[86] GOODFELLOW I, BENGIO Y, COURVILLE A. Deep learning[M]. Cambridge, Massachusetts: MIT press, 2016."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.183,
                0.451,
                0.737,
                0.509
            ],
            "angle": 0,
            "content": "[87] GOODFELLOW I, POUGET-ABADIE J, MIRZA M, et al. Generative adversarial nets[C]//Advances in neural information processing systems. Cambridge, Massachusetts: MIT Press, 2014: 2672-2680."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.183,
                0.518,
                0.736,
                0.556
            ],
            "angle": 0,
            "content": "[88] GRANT M, BOYD S, YE Y. CVX: MATLAB software for disciplined convex programming[Z]. 2008."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.565,
                0.756,
                0.641
            ],
            "angle": 0,
            "content": "[89] GRIppo L, LAMPARIELLO F, LUCIDI S. A truncated Newton method with nonmonotone line search for unconstrained optimization[J]. Journal of Optimization Theory and Applications, 1989, 60(3): 401-419."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.652,
                0.741,
                0.71
            ],
            "angle": 0,
            "content": "[90] GRIppo L, SCIANDRONE M. On the convergence of the block nonlinear Gauss-Seidel method under convex constraints[J]. Operations Research Letters, 2000, 26(3): 127-136."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.719,
                0.746,
                0.777
            ],
            "angle": 0,
            "content": "[91] GRIppo L, LAMPARIELLOF, LUCIDI S. A nonmonotone line search technique for Newton's method[J]. SIAM Journal on Numerical Analysis, 1986, 23(4):707-716."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.786,
                0.634,
                0.804
            ],
            "angle": 0,
            "content": "[92] GUENNEBAUD G, JACOB B, et al. Eigen v3[Z]. 2010."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.182,
                0.812,
                0.735,
                0.848
            ],
            "angle": 0,
            "content": "[93] Gurobi Optimization LLC. Gurobi optimizer reference manual[Z]. 2018."
        },
        {
            "type": "list",
            "bbox": [
                0.182,
                0.156,
                0.756,
                0.848
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "544"
        },
        {
            "type": "header",
            "bbox": [
                0.753,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.156,
                0.826,
                0.195
            ],
            "angle": 0,
            "content": "[94] HAGER W W, ZHANG H. A survey of nonlinear conjugate gradient methods[J]. Pacific Journal of Optimization, 2006, 2(1): 35-58."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.267,
                0.201,
                0.827,
                0.263
            ],
            "angle": 0,
            "content": "[95] HALE E T, YIN W, ZHANG Y. Fixed-point continuation for \\(\\ell_1\\)-minimization: methodology and convergence[J]. SIAM Journal on Optimization, 2008, 19(3): 1107-1130."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.267,
                0.269,
                0.827,
                0.328
            ],
            "angle": 0,
            "content": "[96] HAN S P, MANGASARIAN O L. Exact penalty functions in nonlinear programming[J]. Mathematical Programming, 1979, 17(1):251-269."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.336,
                0.827,
                0.398
            ],
            "angle": 0,
            "content": "[97] HE B, YOU Y, YUAN X. On the convergence of primal-dual hybrid gradient algorithm[J]. SIAM Journal on Imaging Sciences, 2014, 7(4): 2526-2537."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.404,
                0.827,
                0.466
            ],
            "angle": 0,
            "content": "[98] HE B, YUAN X. Convergence analysis of primal-dual algorithms for a saddle-point problem: from contraction perspective[J]. SIAM Journal on Imaging Sciences, 2012, 5(1): 119-149."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.268,
                0.47,
                0.827,
                0.532
            ],
            "angle": 0,
            "content": "[99] HE B, YUAN X. On the \\(\\mathcal{O}(1/n)\\) convergence rate of the Douglas-Rachford alternating direction method[J]. SIAM Journal on Numerical Analysis, 2012, 50(2):700-709."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.537,
                0.836,
                0.598
            ],
            "angle": 0,
            "content": "[100] HERNANDEZ V, ROMAN J E, VIDAL V. SLEPc: a scalable and flexible toolkit for the solution of eigenvalue problems[J]. ACM Trans. Math. Software, 2005, 31(3):351-362."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.605,
                0.827,
                0.646
            ],
            "angle": 0,
            "content": "[101] HESTENES M R, STIEFEL E. Methods of conjugate gradients for solving linear systems[M]. Washington: NBS, 1952."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.651,
                0.827,
                0.711
            ],
            "angle": 0,
            "content": "[102] HIRIART-URRUTY J B, LEMARECHAL C. Convex analysis and minimization algorithms i: Fundamentals[M]. Berlin: Springer Science & Business Media, 2013."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.718,
                0.827,
                0.758
            ],
            "angle": 0,
            "content": "[103] HOLMSTRÖM K, GÖRAN A O, EDVALL M M. User's guide for TOMLAB/KNITRO V6[Z]. Seattle, 2009."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.765,
                0.827,
                0.824
            ],
            "angle": 0,
            "content": "[104] HONG M, LUO Z Q. On the linear convergence of the alternating direction method of multipliers[J]. Mathematical Programming, 2017, 162(1-2): 165-199."
        },
        {
            "type": "list",
            "bbox": [
                0.26,
                0.156,
                0.836,
                0.824
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.244,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "545"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.171,
                0.156,
                0.739,
                0.216
            ],
            "angle": 0,
            "content": "[105] HONG M, LUO Z Q, RAZAVIYAYN M. Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems[J]. SIAM Journal on Optimization, 2016, 26(1): 337-364."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.171,
                0.223,
                0.738,
                0.283
            ],
            "angle": 0,
            "content": "[106] HU J, JIANG B, LIN L, et al. Structured quasi-Newton methods for optimization with orthogonality constraints[J]. SIAM Journal on Scientific Computing, 2019, 41(4): A2239-A2269."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.289,
                0.739,
                0.348
            ],
            "angle": 0,
            "content": "[107] HU J, MILZAREK A, WEN Z, et al. Adaptive quadratically regularized Newton method for Riemannian optimization[J]. SIAM Journal on Matrix Analysis and Applications, 2018, 39(3): 1181-1207."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.355,
                0.749,
                0.454
            ],
            "angle": 0,
            "content": "[108] JAGGI M. Revisiting Frank-Wolfe: projection-free sparse convex optimization[C]//DASGUPTA S, MCALLESTER D. Proceedings of Machine Learning Research: Proceedings of the 30th international conference on machine learning: vol. 28: 1. Atlanta: PMLR, 2013: 427-435."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.463,
                0.739,
                0.523
            ],
            "angle": 0,
            "content": "[109] JIANG B, MA S, SO A M C, et al. Vector transport-free SVRG with general retraction for Riemannian optimization: complexity analysis and practical implementation[J]. ArXiv:1705.09059, 2017."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.172,
                0.529,
                0.74,
                0.609
            ],
            "angle": 0,
            "content": "[110] JOHNSON R, ZHANG T. Accelerating stochastic gradient descent using predictive variance reduction[C]//Advances in neural information processing systems. Cambridge, Massachusetts: MIT Press, 2013: 315-323."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.171,
                0.616,
                0.741,
                0.696
            ],
            "angle": 0,
            "content": "[111] KARMARKAR N. A new polynomial-time algorithm for linear programming[C]/Proceedings of the sixteenth annual ACM symposium on theory of computing. New York: Association for Computer Machinery, 1984:302-311."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.171,
                0.703,
                0.739,
                0.741
            ],
            "angle": 0,
            "content": "[112] KIM S J, KOH K, BOYD S, et al. \\(\\ell_1\\) trend filtering[J]. SIAM Review, 2009, 51(2): 339-360."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.171,
                0.749,
                0.74,
                0.807
            ],
            "angle": 0,
            "content": "[113] LANDWEBER L. An iteration formula for Fredholm integral equations of the first kind[J]. American Journal of Mathematics, 1951, 73(3):615-624."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.171,
                0.815,
                0.739,
                0.855
            ],
            "angle": 0,
            "content": "[114] LARSEN R. Lanczos bidiagonalization with partial reorthogonalization[J]. DAIMI Report Series, 1998, 27(537)."
        },
        {
            "type": "list",
            "bbox": [
                0.171,
                0.156,
                0.749,
                0.855
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "546"
        },
        {
            "type": "header",
            "bbox": [
                0.754,
                0.117,
                0.824,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.156,
                0.827,
                0.215
            ],
            "angle": 0,
            "content": "[115] LEHOUCQR B, SORENSEND C, YANG C. ARPACK users' guide: solution of large-scale eigenvalue problems with implicitly restarted Arnoldi methods[M].Philadelphia:SIAM, 1998."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.222,
                0.835,
                0.28
            ],
            "angle": 0,
            "content": "[116] LEVENBERG K. A method for the solution of certain non-linear problems in least squares[J]. Quarterly of Applied Mathematics, 1944, 2(2): 164-168."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.288,
                0.827,
                0.348
            ],
            "angle": 0,
            "content": "[117] LI X S, DEMMEL J W, BAILEY D H, et al. Design, implementation and testing of extended and mixed precision BLAS[J]. ACM Transactions on Mathematical Software (TOMS), 2002, 28(2): 152-205."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.354,
                0.824,
                0.393
            ],
            "angle": 0,
            "content": "[118] LI X S, DEMMEL J, GILBERT J, et al. SuperLU[M]//Encyclopedia of parallel computing. Boston: Springer US, 2011: 1955-1962."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.399,
                0.825,
                0.457
            ],
            "angle": 0,
            "content": "[119] LI X, SUN D, TOH K C. A highly efficient semismooth Newton augmented Lagrangian method for solving Lasso problems[J]. SIAM Journal on Optimization, 2018, 28(1):433-458."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.465,
                0.826,
                0.524
            ],
            "angle": 0,
            "content": "[120] LI X, SUN D, TOH K C. An asymptotically superlinearly convergent semismooth Newton augmented Lagrangian method for linear programming[J]. ArXiv:1903.09546, 2019."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.531,
                0.827,
                0.609
            ],
            "angle": 0,
            "content": "[121] LI Y, WEN Z, YANG C, et al. A semismooth Newton method for semidefinite programs and its applications in electronic structure calculations[J]. SIAM Journal on Scientific Computing, 2018, 40(6): A4131-A4157."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.617,
                0.825,
                0.657
            ],
            "angle": 0,
            "content": "[122] LIANG S, SRIKANT R. Why deep neural networks for function approximation?[J]. ArXiv:1610.04161, 2016."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.663,
                0.825,
                0.721
            ],
            "angle": 0,
            "content": "[123] LIU D C, NOCEDAL J. On the limited memory BFGS method for large scale optimization[J]. Mathematical Programming, 1989, 45(1-3): 503-528."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.728,
                0.825,
                0.786
            ],
            "angle": 0,
            "content": "[124] LIU J, WRIGHT S J. Asynchronous stochastic coordinate descent: parallelism and convergence properties[J]. SIAM Journal on Optimization, 2015, 25(1):351-376."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.794,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "[125] LIU J, WRIGHT S J, RÉ C, et al. An asynchronous parallel stochastic coordinate descent algorithm[J]. The Journal of Machine Learning Research, 2015, 16(1): 285-322."
        },
        {
            "type": "list",
            "bbox": [
                0.26,
                0.156,
                0.835,
                0.853
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.244,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "547"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.156,
                0.739,
                0.234
            ],
            "angle": 0,
            "content": "[126] LIU X, WEN Z, ZHANG Y. Limited memory block Krylov subspace optimization for computing dominant singular value decompositions[J]. SIAM Journal on Scientific Computing, 2013, 35(3): A1641-A1668."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.244,
                0.739,
                0.323
            ],
            "angle": 0,
            "content": "[127] LOFBERG J. YALMIP: a toolbox for modeling and optimization in MATLAB[C]//Computer aided control systems design, 2004 IEEE international symposium on robotics and automation. Washington: IEEE, 2004: 284-289."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.332,
                0.737,
                0.372
            ],
            "angle": 0,
            "content": "[128] LUENBERGER D G, YE Y. Linear and nonlinear programming[M]. 4th ed. Berlin: Springer Publishing Company, Incorporated, 2015."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.379,
                0.74,
                0.438
            ],
            "angle": 0,
            "content": "[129] LUO Z Q, TSENG P. On the convergence of the coordinate descent method for convex differentiable minimization[J]. Journal of Optimization Theory and Applications, 1992, 72(1): 7-35."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.446,
                0.739,
                0.505
            ],
            "angle": 0,
            "content": "[130] MA S, GOLDFARB D, CHEN L. Fixed point and Bregman iterative methods for matrix rank minimization[J]. Mathematical Programming, 2011, 128(1-2): 321-353."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.512,
                0.74,
                0.571
            ],
            "angle": 0,
            "content": "[131] MCCORMICK G P. A modification of Armijo's step-size rule for negative curvature[J]. Mathematical Programming, 1977, 13(1): 111-115."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.172,
                0.58,
                0.756,
                0.639
            ],
            "angle": 0,
            "content": "[132] MILZAREK A, XIAO X, CENS, et al. A stochastic semismooth Newton method for nonsmooth nonconvex optimization[J]. SIAM Journal on Optimization, 2019, 29(4): 2916-2948."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.172,
                0.646,
                0.74,
                0.706
            ],
            "angle": 0,
            "content": "[133] MITLIAGKASI, CARAMANISC, JAIN P. Memory limited, streaming PCA[C]//Advances in neural information processing systems. Cambridge Massachusetts: MIT Press, 2013: 2886-2894."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.172,
                0.714,
                0.741,
                0.773
            ],
            "angle": 0,
            "content": "[134] MORALES J L, NOCEDAL J. Remark on “algorithm 778: L-BFGS-B: fortran subroutines for large-scale bound constrained optimization” [J]. ACM Transactions on Mathematical Software, 2011, 38(1)."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.172,
                0.781,
                0.741,
                0.841
            ],
            "angle": 0,
            "content": "[135] MORDUKHOVICH BS, NAM NM, YEN N. Fréchet subdifferential calculus and optimality conditions in nondifferentiable programming[J]. Optimization, 2006, 55(5-6): 685-708."
        },
        {
            "type": "list",
            "bbox": [
                0.172,
                0.156,
                0.756,
                0.841
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "548"
        },
        {
            "type": "header",
            "bbox": [
                0.753,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.156,
                0.826,
                0.215
            ],
            "angle": 0,
            "content": "[136] MORE J J, SORENSEN D C. On the use of directions of negative curvature in a modified Newton method[J]. Mathematical Programming, 1979, 16(1): 1-20."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.224,
                0.859,
                0.282
            ],
            "angle": 0,
            "content": "[137] MORITZ P, NISHIHARA R, JORDAN M. A linearly-convergent stochastic L-BFGS algorithm[C]//Artificial intelligence and statistics. [S.l.: s.n.], 2016: 249-258."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.29,
                0.753,
                0.309
            ],
            "angle": 0,
            "content": "[138] MOSEK ApS. The MOSEK optimization toolbox[Z]. 2017."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.317,
                0.825,
                0.375
            ],
            "angle": 0,
            "content": "[139] NASH S G, NOCEDAL J. A numerical study of the limited memory BFGS method and the truncated-Newton method for large scale optimization[J]. SIAM Journal on Optimization, 1991, 1(3): 358-372."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.383,
                0.824,
                0.422
            ],
            "angle": 0,
            "content": "[140] NESTEROV Y. Smooth minimization of non-smooth functions[J]. Mathematical Programming, 2005, 103(1): 127-152."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.43,
                0.826,
                0.488
            ],
            "angle": 0,
            "content": "[141] NESTEROV Y. On an approach to the construction of optimal methods of minimization of smooth convex functions[J]. Ekonomika i Mateaticheskie Metody, 1988, 24(3):509-517."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.497,
                0.824,
                0.535
            ],
            "angle": 0,
            "content": "[142] NESTEROV Y. Introductory lectures on convex programming volume i: basic course[J]. Lecture notes, 1998, 3(4): 5."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.543,
                0.833,
                0.58
            ],
            "angle": 0,
            "content": "[143] NESTEROV Y. Lectures on convex optimization[M]. Berlin: Springer, 2018."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.59,
                0.826,
                0.629
            ],
            "angle": 0,
            "content": "[144] NOCEDAL J. Updating quasi-Newton matrices with limited storage[J]. Mathematics of computation, 1980, 35(151): 773-782."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.636,
                0.838,
                0.674
            ],
            "angle": 0,
            "content": "[145] NOCEDAL J, WRIGHT S. Numerical optimization[M]. 2nd ed. Berlin: Springer Science & Business Media, 2006."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.682,
                0.826,
                0.741
            ],
            "angle": 0,
            "content": "[146] OCHS P, CHEN Y, BROX T, et al. iPiano: inertial proximal algorithm for nonconvex optimization[J]. SIAM Journal on Imaging Sciences, 2014, 7(2): 1388-1419."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.749,
                0.826,
                0.808
            ],
            "angle": 0,
            "content": "[147] OUYANG Y, CHEN Y, LAN G, et al. An accelerated linearized alternating direction method of multipliers[J]. SIAM Journal on Imaging Sciences, 2015, 8(1): 644-681."
        },
        {
            "type": "list",
            "bbox": [
                0.26,
                0.156,
                0.859,
                0.808
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.244,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "549"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.156,
                0.752,
                0.215
            ],
            "angle": 0,
            "content": "[148] PAATERO P, TAPPER U. Positive matrix factorization: a non-negative factor model with optimal utilization of error estimates of data values[J]. Environmetrics, 1994, 5(2): 111-126."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.224,
                0.737,
                0.261
            ],
            "angle": 0,
            "content": "[149] PARIKH N, BOYD S, et al. Proximal algorithms[J]. Foundations and Trendső in Optimization, 2014, 1(3): 127-239."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.27,
                0.737,
                0.307
            ],
            "angle": 0,
            "content": "[150] PETERSEN K B, PEDERSEN M S. The matrix cookbook[Z]. Version 20121115. 2012."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.316,
                0.737,
                0.374
            ],
            "angle": 0,
            "content": "[151] PEYRé G, CUTURI M, et al. Computational optimal transport[J]. Foundations and Trendsö in Machine Learning, 2019, 11(5-6): 355-607."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.383,
                0.737,
                0.442
            ],
            "angle": 0,
            "content": "[152] PILANCI M, WAINWRIGHT M J. Newton sketch: a near linear-time optimization algorithm with linear-quadratic convergence[J]. SIAM Journal on Optimization, 2017, 27(1):205-245."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.451,
                0.753,
                0.529
            ],
            "angle": 0,
            "content": "[153] POLAK E, RIBIERE G. Note sur la convergence de méthodes de directions conjuguées[J]. ESAIM: Mathematical Modelling and Numerical Analysis-Modélisation Mathématique et Analyse Numérique, 1969, 3(R1): 35-43."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.538,
                0.737,
                0.577
            ],
            "angle": 0,
            "content": "[154] POLIZZI E. Density-matrix-based algorithm for solving eigenvalue problems[J]. Physical Review B, 2009, 79(11): 115112."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.585,
                0.737,
                0.643
            ],
            "angle": 0,
            "content": "[155] POLYAK B T. Some methods of speeding up the convergence of iteration methods[J]. USSR Computational Mathematics and Mathematical Physics, 1964, 4(5): 1-17."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.652,
                0.737,
                0.731
            ],
            "angle": 0,
            "content": "[156] POULSON J, MARKER B, VAN DE GEIJN R A, et al. Elemental: a new framework for distributed memory dense matrix computations[J]. ACM Transactions on Mathematical Software (TOMS), 2013, 39(2): 13."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.74,
                0.745,
                0.778
            ],
            "angle": 0,
            "content": "[157] POWELL M J D. On search directions for minimization algorithms[J]. Mathematical Programming, 1973, 4(1): 193-201."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.786,
                0.737,
                0.825
            ],
            "angle": 0,
            "content": "[158] POWELL M J. A new algorithm for unconstrained optimization[G] // Nonlinear programming. Amsterdam: Elsevier, 1970: 31-65."
        },
        {
            "type": "list",
            "bbox": [
                0.173,
                0.156,
                0.753,
                0.825
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "550"
        },
        {
            "type": "header",
            "bbox": [
                0.753,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.156,
                0.826,
                0.214
            ],
            "angle": 0,
            "content": "[159] POWELL M. A note on quasi-Newton formulae for sparse second derivative matrices[J]. Mathematical Programming, 1981, 20(1): 144-151."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.222,
                0.827,
                0.28
            ],
            "angle": 0,
            "content": "[160] RECHT B, FAZEL M, PARRILO P. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization[J]. SIAM Review, 2010, 52(3): 471-501."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.288,
                0.827,
                0.348
            ],
            "angle": 0,
            "content": "[161] RICHTÁRIK P, TAKÁ M. Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function[J]. Mathematical Programming, 2014, 144(1-2): 1-38."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.354,
                0.833,
                0.392
            ],
            "angle": 0,
            "content": "[162] ROBBINS H, MONROS, et al. A stochastic approximation method[J]. The Annals of Mathematical Statistics, 1951, 22(3): 400-407."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.399,
                0.827,
                0.457
            ],
            "angle": 0,
            "content": "[163] ROCKAFELLAR R T. Augmented Lagrangians and applications of the proximal point algorithm in convex programming[J]. Mathematics of Operations Research, 1976, 1(2): 97-116."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.465,
                0.826,
                0.503
            ],
            "angle": 0,
            "content": "[164] ROCKAFELLAR R. Convex analysis[M]. Princeton: Princeton University Press, 1970."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.51,
                0.832,
                0.568
            ],
            "angle": 0,
            "content": "[165] RUDIN L I, OSHER S, FATEMI E. Nonlinear total variation based noise removal algorithms[J]. Physica D: Nonlinear Phenomena, 1992, 60(1-4): 259-268."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.576,
                0.825,
                0.615
            ],
            "angle": 0,
            "content": "[166] RUSZCZYSKI A P, RUSZCZYNSKI A. Nonlinear optimization[M]. Princeton: Princeton University Press, 2006."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.622,
                0.838,
                0.699
            ],
            "angle": 0,
            "content": "[167] SCHEINBERG K, MA S, GOLDFARB D. Sparse inverse covariance selection via alternating linearization methods[C]//Advances in neural information processing systems. Cambridge, Massachusetts: MIT Press, 2010: 2101-2109."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.707,
                0.832,
                0.766
            ],
            "angle": 0,
            "content": "[168] SCHMIDT M, LE ROUX N, BACH F. Minimizing finite sums with the stochastic average gradient[J]. Mathematical Programming, 2017, 162(1-2): 83-112."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.773,
                0.827,
                0.851
            ],
            "angle": 0,
            "content": "[169] SHALEV-SHWARTZ S, ZHANG T. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization[C]// International conference on machine learning. [S.l.: s.n.], 2014: 64-72."
        },
        {
            "type": "list",
            "bbox": [
                0.26,
                0.156,
                0.838,
                0.851
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.244,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.735,
                0.131
            ],
            "angle": 0,
            "content": "551"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.156,
                0.738,
                0.215
            ],
            "angle": 0,
            "content": "[170] SIMON D, ABELL J. A majorization algorithm for constrained correlation matrix approximation[J]. Linear Algebra and its Applications, 2010, 432(5): 1152-1164."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.224,
                0.772,
                0.261
            ],
            "angle": 0,
            "content": "[171] SNYMAN J A. Practical mathematical optimization[M]. Berlin: Springer, 2005."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.27,
                0.739,
                0.328
            ],
            "angle": 0,
            "content": "[172] STEIHAUG T. The conjugate gradient method and trust regions in large scale optimization[J]. SIAM Journal on Numerical Analysis, 1983, 20(3): 626-637."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.337,
                0.739,
                0.416
            ],
            "angle": 0,
            "content": "[173] SU W, BOYD S, CANDES E. A differential equation for modeling Nesterov's accelerated gradient method: theory and insights[C]// Advances in neural information processing systems. Cambridge, Massachusetts: MIT Press, 2014: 2510-2518."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.425,
                0.739,
                0.464
            ],
            "angle": 0,
            "content": "[174] SUN R, LUO Z Q, YE Y. On the expected convergence of randomly permuted ADMM[J]. ArXiv:1503.06387, 2015."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.471,
                0.739,
                0.51
            ],
            "angle": 0,
            "content": "[175] SUN W, YUAN Y X. Optimization theory and methods: nonlinear programming[M]. Berlin: Springer Science & Business Media, 2006."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.518,
                0.739,
                0.577
            ],
            "angle": 0,
            "content": "[176] SUTSKEVER I, MARTENS J, DAHL G, et al. On the importance of initialization and momentum in deep learning[C]//International conference on machine learning. [S.l.: s.n.], 2013: 1139-1147."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.584,
                0.739,
                0.624
            ],
            "angle": 0,
            "content": "[177] SUTTON R S, BARTO A G. Reinforcement learning: an introduction[M]. Cambridge, Massachusetts: MIT press, 2018."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.631,
                0.739,
                0.669
            ],
            "angle": 0,
            "content": "[178] TAHA H A. Operations research: an introduction[M]. New Jersey: Pearson/Prentice Hall, 2011."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.677,
                0.739,
                0.735
            ],
            "angle": 0,
            "content": "[179] TIBSHIRANI R. Regression shrinkage and selection via the Lasso[J]. Journal of the Royal Statistical Society. Series B (Methodological), 1996:267-288."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.744,
                0.742,
                0.783
            ],
            "angle": 0,
            "content": "[180] TIKHONOV A N, ARSENIN V I. Solutions of ill-posed problems[M]. Washington: Vh Winston, 1977."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.791,
                0.739,
                0.85
            ],
            "angle": 0,
            "content": "[181] TOH K C, YUN S. An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems[J]. Pacific Journal of Optimization, 2010, 6(615-640): 15."
        },
        {
            "type": "list",
            "bbox": [
                0.173,
                0.156,
                0.772,
                0.85
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "552"
        },
        {
            "type": "header",
            "bbox": [
                0.753,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.156,
                0.829,
                0.195
            ],
            "angle": 0,
            "content": "[182] TOINT P. A note about sparsity exploiting quasi-Newton updates[J]. Mathematical Programming, 1981, 21(1): 172-181."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.203,
                0.826,
                0.242
            ],
            "angle": 0,
            "content": "[183] TSENG P. Dual coordinate ascent methods for non-strictly convex minimization[J]. Mathematical Programming, 1993, 59(1): 231-247."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.249,
                0.827,
                0.307
            ],
            "angle": 0,
            "content": "[184] TSENG P. On accelerated proximal gradient methods for convex-concave optimization[J]. submitted to SIAM Journal on Optimization, 2008, 2:3."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.316,
                0.853,
                0.374
            ],
            "angle": 0,
            "content": "[185] TÜTÜNCÜ R, TOH K C, TODD M. SDPT3—a MATLAB software package for semidefinite-quadratic-linear programming, version 3.0[Z]. 2001."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.383,
                0.836,
                0.443
            ],
            "angle": 0,
            "content": "[186] UDELL M, MOHAN K, ZENG D, et al. Convex optimization in Julia[C]//Proceedings of the 1st first workshop for high performance technical computing in dynamic languages. [S.l.: s.n.], 2014: 18-28."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.451,
                0.826,
                0.509
            ],
            "angle": 0,
            "content": "[187] VIRTANEN P, GOMMERS R, OLIPHANT T E, et al. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python[J]. Nature Methods, 2020, 17: 261-272."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.517,
                0.826,
                0.577
            ],
            "angle": 0,
            "content": "[188] WÄCHTER A, BIEGLER L T. On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming[J]. Mathematical Programming, 2006, 106(1): 25-57."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.584,
                0.83,
                0.623
            ],
            "angle": 0,
            "content": "[189] WALKER H F, NI P. Anderson acceleration for fixed-point iterations[J]. SIAM Journal on Numerical Analysis, 2011, 49(4): 1715-1735."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.631,
                0.841,
                0.688
            ],
            "angle": 0,
            "content": "[190] WANG E, ZHANG Q, SHEN B, et al. Intel math kernel library[G]// High-performance computing on the intelo xeon phi. Berlin: Springer, 2014: 167-188."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.697,
                0.89,
                0.755
            ],
            "angle": 0,
            "content": "[191] WANG P W, CHANG W C, KOLTER J Z. The mixing method: coordinate descent for low-rank semidefinite programming[J]. ArXiv:1706.00476, 2017."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.765,
                0.826,
                0.824
            ],
            "angle": 0,
            "content": "[192] WANG Y X, ZHANG Y J. Nonnegative matrix factorization: A comprehensive review[J]. IEEE Transactions on Knowledge and Data Engineering, 2012, 25(6): 1336-1353."
        },
        {
            "type": "list",
            "bbox": [
                0.261,
                0.156,
                0.89,
                0.824
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.244,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "553"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.156,
                0.737,
                0.214
            ],
            "angle": 0,
            "content": "[193] WARGA J. Minimizing certain convex functions[J]. Journal of the Society for Industrial and Applied Mathematics, 1963, 11(3):588-593."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.224,
                0.737,
                0.262
            ],
            "angle": 0,
            "content": "[194] WELLING M, WEBER M. Positive tensor factorization[J]. Pattern Recognition Letters, 2001, 22: 1255-1261."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.27,
                0.737,
                0.33
            ],
            "angle": 0,
            "content": "[195] WEN F, CHU L, LIU P, et al. A survey on nonconvex regularization-based sparse and low-rank recovery in signal processing, statistics, and machine learning[J]. IEEE Access, 2018, 6: 69883-69906."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.337,
                0.737,
                0.395
            ],
            "angle": 0,
            "content": "[196] WEN Z, YIN W. A feasible method for optimization with orthogonality constraints[J]. Mathematical Programming, 2013, 142(1-2): 397-434."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.404,
                0.737,
                0.464
            ],
            "angle": 0,
            "content": "[197] WHALEY R C, DONGARRA J. Automatically Tuned Linear Algebra Software[C]//Ninth SIAM conference on parallel processing for scientific computing. [S.l.: s.n.], 1999."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.471,
                0.737,
                0.51
            ],
            "angle": 0,
            "content": "[198] WRIGHT S J. Coordinate descent algorithms[J]. Mathematical Programming, 2015, 151(1): 3-34."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.518,
                0.737,
                0.575
            ],
            "angle": 0,
            "content": "[199] XIAO L, ZHANG T. A proximal stochastic gradient method with progressive variance reduction[J]. SIAM Journal on Optimization, 2014, 24(4): 2057-2075."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.584,
                0.737,
                0.662
            ],
            "angle": 0,
            "content": "[200] XU P, ROOSTA F, MAHONEY M W. Second-order optimization for non-convex machine learning: an empirical study[M]/Proceedings of the 2020 siam international conference on data mining. [S.l.: s.n.]: 199-207."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.672,
                0.737,
                0.751
            ],
            "angle": 0,
            "content": "[201] XU P, YANG J, ROOSTA-KHORASANI F, et al. Sub-sampled Newton methods with non-uniform sampling[C]// Advances in neural information processing systems. Cambridge, Massachusetts: MIT Press, 2016: 3000-3008."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.173,
                0.76,
                0.737,
                0.84
            ],
            "angle": 0,
            "content": "[202] XU Y, YIN W. A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion[J]. SIAM Journal on Imaging Sciences, 2013, 6(3): 1758-1789."
        },
        {
            "type": "list",
            "bbox": [
                0.173,
                0.156,
                0.737,
                0.84
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "554"
        },
        {
            "type": "header",
            "bbox": [
                0.753,
                0.117,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.156,
                0.826,
                0.215
            ],
            "angle": 0,
            "content": "[203] YANG J, SUN D, TOH K C. A proximal point algorithm for log-determinant optimization with group Lasso regularization[J]. SIAM Journal on Optimization, 2013, 23(2):857-893."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.222,
                0.828,
                0.302
            ],
            "angle": 0,
            "content": "[204] YANG L, SUN D, TOH K C. SDPNAL+: a majorized semismooth Newton-CG augmented Lagrangian method for semidefinite programming with nonnegative constraints[J]. Mathematical Programming Computation, 2015, 7(3): 331-366."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.26,
                0.309,
                0.872,
                0.348
            ],
            "angle": 0,
            "content": "[205] YIN W. Analysis and generalizations of the linearized bregman method[J]. SIAM Journal on Imaging Sciences, 2010, 3(4): 856-877."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.354,
                0.827,
                0.414
            ],
            "angle": 0,
            "content": "[206] YIN W, OSHER S, GOLDFARB D, et al. Bregman iterative algorithms for \\(\\ell_1\\)-minimization with applications to compressed sensing[J]. SIAM Journal on Imaging Sciences, 2008, 1(1): 143-168."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.421,
                0.825,
                0.459
            ],
            "angle": 0,
            "content": "[207] YUAN Y X. On the truncated conjugate gradient method[J]. Mathematical Programming, 2000, 87(3): 561-573."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.466,
                0.826,
                0.504
            ],
            "angle": 0,
            "content": "[208] YUAN Y X. Recent advances in trust region algorithms[J]. Mathematical Programming, 2015, 151(1): 249-281."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.51,
                0.826,
                0.569
            ],
            "angle": 0,
            "content": "[209] ZHANG H, HAGER W W. A nonmonotone line search technique and its application to unconstrained optimization[J]. SIAM Journal on Optimization, 2004, 14(4): 1043-1056."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.577,
                0.826,
                0.655
            ],
            "angle": 0,
            "content": "[210] ZHANG H, REDDI S J, SRA S. Riemannian SVRG: fast stochastic optimization on Riemannian manifolds[C]//Advances in neural information processing systems. Cambridge, Massachusetts: MIT Press, 2016: 4592-4600."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.663,
                0.826,
                0.723
            ],
            "angle": 0,
            "content": "[211] ZHANG J, LIU H, WEN Z, et al. A sparse completely positive relaxation of the modularity maximization for community detection[J]. SIAM Journal on Scientific Computing, 2018, 40(5): A3091-A3120."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.728,
                0.85,
                0.787
            ],
            "angle": 0,
            "content": "[212] ZHANG L, ZHOU W, LI D. Global convergence of a modified Fletcher-Reeves conjugate gradient method with Armijo-type line search[J]. Numerische Mathematik, 2006, 104(4): 561-572."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.261,
                0.794,
                0.826,
                0.854
            ],
            "angle": 0,
            "content": "[213] ZHANG N, ZHANG Y, SUN D, et al. An efficient linearly convergent regularized proximal point algorithm for fused multiple graphical Lasso problems[J]. ArXiv:1902.06952, 2019."
        },
        {
            "type": "list",
            "bbox": [
                0.26,
                0.156,
                0.872,
                0.854
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.245,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.738,
                0.131
            ],
            "angle": 0,
            "content": "555"
        },
        {
            "type": "ref_text",
            "bbox": [
                0.171,
                0.156,
                0.745,
                0.215
            ],
            "angle": 0,
            "content": "[214] ZHAO XY, SUN D, TOH KC. A Newton-CG augmented Lagrangian method for semidefinite programming[J]. SIAM Journal on Optimization, 2010, 20(4): 1737-1765."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.171,
                0.223,
                0.741,
                0.302
            ],
            "angle": 0,
            "content": "[215] ZHU C, BYRD R H, LU P, et al. Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization[J]. ACM Transactions on Mathematical Software (TOMS), 1997, 23(4): 550-560."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.171,
                0.311,
                0.692,
                0.33
            ],
            "angle": 0,
            "content": "[216] 徐树方，钱江. 矩阵计算六讲[M]. 北京: 高等教育出版社, 2011."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.171,
                0.337,
                0.739,
                0.374
            ],
            "angle": 0,
            "content": "[217] 徐树方，高立，张平文. 数值线性代数[M]. 北京：北京大学出版社，2013."
        },
        {
            "type": "ref_text",
            "bbox": [
                0.171,
                0.383,
                0.709,
                0.402
            ],
            "angle": 0,
            "content": "[218] 袁亚湘，孙文瑜. 最优化理论与方法[M]. 北京: 科学出版社, 2011."
        },
        {
            "type": "list",
            "bbox": [
                0.171,
                0.156,
                0.745,
                0.402
            ],
            "angle": 0,
            "content": null
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.261,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "556"
        },
        {
            "type": "header",
            "bbox": [
                0.755,
                0.118,
                0.825,
                0.133
            ],
            "angle": 0,
            "content": "参考文献"
        }
    ],
    [
        {
            "type": "title",
            "bbox": [
                0.417,
                0.252,
                0.491,
                0.283
            ],
            "angle": 0,
            "content": "索引"
        },
        {
            "type": "title",
            "bbox": [
                0.259,
                0.333,
                0.341,
                0.352
            ],
            "angle": 0,
            "content": "Symbols"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.355,
                0.428,
                0.372
            ],
            "angle": 0,
            "content": "\\(\\alpha\\) -下水平集 36"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.376,
                0.428,
                0.392
            ],
            "angle": 0,
            "content": "\\(\\ell_1\\) 罚函数 310"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.395,
                0.427,
                0.413
            ],
            "angle": 0,
            "content": "\\(L^2\\) 空间 525"
        },
        {
            "type": "title",
            "bbox": [
                0.291,
                0.431,
                0.309,
                0.444
            ],
            "angle": 0,
            "content": "A"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.452,
                0.427,
                0.466
            ],
            "angle": 0,
            "content": "AdaDelta 468"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.472,
                0.427,
                0.487
            ],
            "angle": 0,
            "content": "AdaGrad 466"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.493,
                0.427,
                0.507
            ],
            "angle": 0,
            "content": "Adam 468"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.513,
                0.427,
                0.529
            ],
            "angle": 0,
            "content": "ADMM . . . 见交替方向乘子法"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.534,
                0.427,
                0.548
            ],
            "angle": 0,
            "content": "AMPL 150"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.555,
                0.427,
                0.571
            ],
            "angle": 0,
            "content": "Azuma-Hoeffding 不等式 … 533"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.576,
                0.427,
                0.591
            ],
            "angle": 0,
            "content": "鞍点 163"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.596,
                0.427,
                0.611
            ],
            "angle": 0,
            "content": "凹函数 46"
        },
        {
            "type": "title",
            "bbox": [
                0.293,
                0.629,
                0.308,
                0.642
            ],
            "angle": 0,
            "content": "B"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.651,
                0.427,
                0.665
            ],
            "angle": 0,
            "content": "BB步长 230"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.672,
                0.427,
                0.686
            ],
            "angle": 0,
            "content": "BB方法 229,262"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.693,
                0.427,
                0.707
            ],
            "angle": 0,
            "content": "Bellman方程 115"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.713,
                0.427,
                0.728
            ],
            "angle": 0,
            "content": "BP问题……见基追踪问题"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.734,
                0.427,
                0.749
            ],
            "angle": 0,
            "content": "Bregman 距离 327"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.755,
                0.427,
                0.77
            ],
            "angle": 0,
            "content": "Bregman算法 328"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.776,
                0.427,
                0.79
            ],
            "angle": 0,
            "content": "半定规划 136"
        },
        {
            "type": "text",
            "bbox": [
                0.206,
                0.796,
                0.427,
                0.811
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的对偶 177"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.817,
                0.427,
                0.832
            ],
            "angle": 0,
            "content": "半空间 41"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.837,
                0.427,
                0.852
            ],
            "angle": 0,
            "content": "半正定锥 43"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.335,
                0.736,
                0.35
            ],
            "angle": 0,
            "content": "闭函数 37"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.356,
                0.736,
                0.371
            ],
            "angle": 0,
            "content": "步长 216"
        },
        {
            "type": "title",
            "bbox": [
                0.602,
                0.389,
                0.619,
                0.402
            ],
            "angle": 0,
            "content": "C"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.41,
                0.736,
                0.425
            ],
            "angle": 0,
            "content": "Chambolle-Pock算法 420"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.431,
                0.736,
                0.446
            ],
            "angle": 0,
            "content": "Cholesky分解 249,506"
        },
        {
            "type": "text",
            "bbox": [
                0.516,
                0.452,
                0.736,
                0.466
            ],
            "angle": 0,
            "content": "修正的 \\(\\sim\\) 248"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.473,
                0.736,
                0.487
            ],
            "angle": 0,
            "content": "continuation . 见连续化"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.493,
                0.736,
                0.507
            ],
            "angle": 0,
            "content": "CVX 149"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.514,
                0.736,
                0.529
            ],
            "angle": 0,
            "content": "超平面 41"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.535,
                0.736,
                0.549
            ],
            "angle": 0,
            "content": "次梯度 61"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.555,
                0.736,
                0.57
            ],
            "angle": 0,
            "content": "次微分 61,167"
        },
        {
            "type": "title",
            "bbox": [
                0.601,
                0.589,
                0.618,
                0.601
            ],
            "angle": 0,
            "content": "D"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.609,
                0.736,
                0.643
            ],
            "angle": 0,
            "content": "Douglas-Rachford Splitting 算法 436"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.651,
                0.736,
                0.666
            ],
            "angle": 0,
            "content": "Dubovitskii-Milyutin 定理 ..... 70"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.672,
                0.736,
                0.686
            ],
            "angle": 0,
            "content": "大残量问题 281"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.693,
                0.736,
                0.707
            ],
            "angle": 0,
            "content": "带有微分方程约束优化问题 ..130"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.713,
                0.736,
                0.728
            ],
            "angle": 0,
            "content": "单调映射 50"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.734,
                0.736,
                0.749
            ],
            "angle": 0,
            "content": "低秩矩阵恢复 7"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.755,
                0.736,
                0.77
            ],
            "angle": 0,
            "content": "电子结构计算 143"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.776,
                0.736,
                0.79
            ],
            "angle": 0,
            "content": "动量方法 464"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.796,
                0.736,
                0.811
            ],
            "angle": 0,
            "content": "动作价值函数 128"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.817,
                0.736,
                0.832
            ],
            "angle": 0,
            "content": "对称不定矩阵分解 506"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.837,
                0.736,
                0.852
            ],
            "angle": 0,
            "content": "对偶变量……见拉格朗日乘子"
        },
        {
            "type": "page_number",
            "bbox": [
                0.44,
                0.869,
                0.469,
                0.882
            ],
            "angle": 0,
            "content": "557"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "558"
        },
        {
            "type": "header",
            "bbox": [
                0.787,
                0.118,
                0.823,
                0.133
            ],
            "angle": 0,
            "content": "索引"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.157,
                0.516,
                0.174
            ],
            "angle": 0,
            "content": "对偶间隙 171"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.178,
                0.516,
                0.194
            ],
            "angle": 0,
            "content": "对偶近似点梯度法 413"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.199,
                0.515,
                0.216
            ],
            "angle": 0,
            "content": "对偶可行解 171"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.22,
                0.516,
                0.236
            ],
            "angle": 0,
            "content": "对偶锥 172"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.241,
                0.516,
                0.257
            ],
            "angle": 0,
            "content": "对数罚函数 308"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.262,
                0.515,
                0.278
            ],
            "angle": 0,
            "content": "多层感知机 8,471"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.283,
                0.516,
                0.3
            ],
            "angle": 0,
            "content": "多面体 43"
        },
        {
            "type": "title",
            "bbox": [
                0.381,
                0.319,
                0.396,
                0.333
            ],
            "angle": 0,
            "content": "E"
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.34,
                0.351,
                0.357
            ],
            "angle": 0,
            "content": "二次罚函数"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.361,
                0.516,
                0.377
            ],
            "angle": 0,
            "content": "不等式约束的 \\(\\sim\\) 304"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.382,
                0.516,
                0.398
            ],
            "angle": 0,
            "content": "等式约束的 \\(\\sim\\) 298"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.403,
                0.516,
                0.418
            ],
            "angle": 0,
            "content": "一般约束的 \\(\\sim\\) 304"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.424,
                0.516,
                0.44
            ],
            "angle": 0,
            "content": "二次共轭函数 60"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.444,
                0.516,
                0.461
            ],
            "angle": 0,
            "content": "二次上界 28"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.465,
                0.516,
                0.482
            ],
            "angle": 0,
            "content": "二次下界 57"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.486,
                0.516,
                0.503
            ],
            "angle": 0,
            "content": "二次约束二次规划问题 137"
        },
        {
            "type": "title",
            "bbox": [
                0.381,
                0.523,
                0.395,
                0.537
            ],
            "angle": 0,
            "content": "F"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.544,
                0.516,
                0.56
            ],
            "angle": 0,
            "content": "Farkas引理 187"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.565,
                0.516,
                0.581
            ],
            "angle": 0,
            "content": "Fenchel 不等式 ..... 58"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.586,
                0.516,
                0.602
            ],
            "angle": 0,
            "content": "Frechet可微 26,30"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.607,
                0.516,
                0.623
            ],
            "angle": 0,
            "content": "Frobenius 内积 25"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.628,
                0.516,
                0.644
            ],
            "angle": 0,
            "content": "罚函数法 297"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.649,
                0.516,
                0.665
            ],
            "angle": 0,
            "content": "法锥 197"
        },
        {
            "type": "title",
            "bbox": [
                0.26,
                0.67,
                0.298,
                0.685
            ],
            "angle": 0,
            "content": "范数"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.691,
                0.516,
                0.708
            ],
            "angle": 0,
            "content": "\\(\\ell_p\\sim\\) 23"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.712,
                0.516,
                0.728
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的相容性 25"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.733,
                0.516,
                0.748
            ],
            "angle": 0,
            "content": "Frobenius \\(\\sim\\) 24"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.754,
                0.516,
                0.77
            ],
            "angle": 0,
            "content": "核\\~ 25"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.774,
                0.516,
                0.79
            ],
            "angle": 0,
            "content": "矩阵 \\(2\\sim\\) 25"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.794,
                0.516,
                0.811
            ],
            "angle": 0,
            "content": "矩阵\\~ 24"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.816,
                0.516,
                0.832
            ],
            "angle": 0,
            "content": "算子\\~ 24"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.837,
                0.516,
                0.853
            ],
            "angle": 0,
            "content": "向量\\~ 23"
        },
        {
            "type": "text",
            "bbox": [
                0.569,
                0.157,
                0.825,
                0.174
            ],
            "angle": 0,
            "content": "方差减小技术 482"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.178,
                0.825,
                0.194
            ],
            "angle": 0,
            "content": "仿射包 40"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.199,
                0.825,
                0.216
            ],
            "angle": 0,
            "content": "仿射集 39"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.22,
                0.825,
                0.236
            ],
            "angle": 0,
            "content": "非负矩阵分解 142"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.241,
                0.825,
                0.257
            ],
            "angle": 0,
            "content": "分布函数 521"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.261,
                0.825,
                0.277
            ],
            "angle": 0,
            "content": "分布式鲁棒优化 135"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.282,
                0.825,
                0.298
            ],
            "angle": 0,
            "content": "分块坐标下降法 390, 392"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.303,
                0.825,
                0.319
            ],
            "angle": 0,
            "content": "分离超平面定理 45"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.323,
                0.825,
                0.339
            ],
            "angle": 0,
            "content": "分组 LASSO 90,388"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.344,
                0.825,
                0.36
            ],
            "angle": 0,
            "content": "复合优化问题 130,343"
        },
        {
            "type": "text",
            "bbox": [
                0.603,
                0.365,
                0.825,
                0.381
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的一阶必要条件 165"
        },
        {
            "type": "title",
            "bbox": [
                0.688,
                0.397,
                0.706,
                0.411
            ],
            "angle": 0,
            "content": "G"
        },
        {
            "type": "text",
            "bbox": [
                0.569,
                0.419,
                0.825,
                0.435
            ],
            "angle": 0,
            "content": "Gateaux可微 30"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.439,
                0.825,
                0.455
            ],
            "angle": 0,
            "content": "概率测度 519"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.46,
                0.825,
                0.476
            ],
            "angle": 0,
            "content": "概率空间 518"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.481,
                0.825,
                0.496
            ],
            "angle": 0,
            "content": "概率密度函数 521"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.501,
                0.825,
                0.517
            ],
            "angle": 0,
            "content": "概率图模型 95"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.522,
                0.825,
                0.538
            ],
            "angle": 0,
            "content": "概率质量函数 522"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.543,
                0.825,
                0.558
            ],
            "angle": 0,
            "content": "高斯-牛顿算法 282"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.564,
                0.825,
                0.58
            ],
            "angle": 0,
            "content": "割线方程 253"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.584,
                0.825,
                0.6
            ],
            "angle": 0,
            "content": "共轭函数 58"
        },
        {
            "type": "text",
            "bbox": [
                0.569,
                0.605,
                0.825,
                0.621
            ],
            "angle": 0,
            "content": "广义不等式 172"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.625,
                0.825,
                0.641
            ],
            "angle": 0,
            "content": "广义实值函数 35"
        },
        {
            "type": "title",
            "bbox": [
                0.688,
                0.658,
                0.706,
                0.672
            ],
            "angle": 0,
            "content": "H"
        },
        {
            "type": "text",
            "bbox": [
                0.569,
                0.679,
                0.825,
                0.695
            ],
            "angle": 0,
            "content": "Huber 损失函数 232, 385"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.7,
                0.825,
                0.716
            ],
            "angle": 0,
            "content": "海瑟矩阵 27"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.721,
                0.825,
                0.736
            ],
            "angle": 0,
            "content": "互补松弛条件 188"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.741,
                0.825,
                0.757
            ],
            "angle": 0,
            "content": "回归分析 86"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.762,
                0.825,
                0.778
            ],
            "angle": 0,
            "content": "回退法 218"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.783,
                0.825,
                0.799
            ],
            "angle": 0,
            "content": "混合整数规划 145"
        },
        {
            "type": "title",
            "bbox": [
                0.691,
                0.816,
                0.702,
                0.832
            ],
            "angle": 0,
            "content": "J"
        },
        {
            "type": "text",
            "bbox": [
                0.569,
                0.837,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "Jensen 不等式 532"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.208,
                0.132
            ],
            "angle": 0,
            "content": "索引"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "559"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.157,
                0.428,
                0.173
            ],
            "angle": 0,
            "content": "基追踪问题..4,124,205,305,323"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.178,
                0.427,
                0.194
            ],
            "angle": 0,
            "content": "积极集 181"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.199,
                0.427,
                0.214
            ],
            "angle": 0,
            "content": "极锥 197"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.219,
                0.427,
                0.235
            ],
            "angle": 0,
            "content": "几何最优性条件 181"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.24,
                0.427,
                0.256
            ],
            "angle": 0,
            "content": "价值函数 128"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.261,
                0.427,
                0.277
            ],
            "angle": 0,
            "content": "交替方向乘子法 433"
        },
        {
            "type": "text",
            "bbox": [
                0.206,
                0.282,
                0.427,
                0.297
            ],
            "angle": 0,
            "content": "多块\\~ 442"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.303,
                0.427,
                0.318
            ],
            "angle": 0,
            "content": "线性化\\~ 440"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.324,
                0.427,
                0.339
            ],
            "angle": 0,
            "content": "交替极小化方法 416"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.344,
                0.427,
                0.36
            ],
            "angle": 0,
            "content": "截断共轭梯度法 274"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.365,
                0.427,
                0.381
            ],
            "angle": 0,
            "content": "近似点交替线性化方法 ..... 400"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.386,
                0.427,
                0.401
            ],
            "angle": 0,
            "content": "近似点算法 374"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.407,
                0.427,
                0.422
            ],
            "angle": 0,
            "content": "近似点梯度法 349"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.428,
                0.427,
                0.443
            ],
            "angle": 0,
            "content": "精度矩阵 96"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.448,
                0.427,
                0.464
            ],
            "angle": 0,
            "content": "精确罚函数 310"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.469,
                0.427,
                0.485
            ],
            "angle": 0,
            "content": "局部极小解 16"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.49,
                0.427,
                0.505
            ],
            "angle": 0,
            "content": "矩阵分离问题 103"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.511,
                0.427,
                0.526
            ],
            "angle": 0,
            "content": "矩阵内积……见Frobenius 内积"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.532,
                0.427,
                0.547
            ],
            "angle": 0,
            "content": "矩阵优化 140"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.552,
                0.427,
                0.568
            ],
            "angle": 0,
            "content": "卷积神经网络 10"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.573,
                0.427,
                0.588
            ],
            "angle": 0,
            "content": "决策变量 1"
        },
        {
            "type": "title",
            "bbox": [
                0.291,
                0.608,
                0.309,
                0.622
            ],
            "angle": 0,
            "content": "K"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.629,
                0.427,
                0.644
            ],
            "angle": 0,
            "content": "K-均值聚类 106,388,394"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.65,
                0.427,
                0.665
            ],
            "angle": 0,
            "content": "KKT对 188"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.671,
                0.249,
                0.686
            ],
            "angle": 0,
            "content": "KKT条件"
        },
        {
            "type": "text",
            "bbox": [
                0.206,
                0.692,
                0.427,
                0.707
            ],
            "angle": 0,
            "content": "凸优化问题的 \\(\\sim\\) 195"
        },
        {
            "type": "text",
            "bbox": [
                0.206,
                0.713,
                0.427,
                0.728
            ],
            "angle": 0,
            "content": "一般约束优化问题的 \\(\\sim\\) .189"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.733,
                0.427,
                0.768
            ],
            "angle": 0,
            "content": "KL 性质 见 Kurdyka-Lojasiewicz 性质"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.774,
                0.427,
                0.79
            ],
            "angle": 0,
            "content": "Kurdyka-Lojasiewicz 性质 .. 407"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.795,
                0.427,
                0.81
            ],
            "angle": 0,
            "content": "柯西不等式 24,26,63"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.816,
                0.427,
                0.831
            ],
            "angle": 0,
            "content": "柯西点 276"
        },
        {
            "type": "text",
            "bbox": [
                0.173,
                0.837,
                0.427,
                0.852
            ],
            "angle": 0,
            "content": "可行点 见可行解"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.157,
                0.735,
                0.173
            ],
            "angle": 0,
            "content": "可行解 1"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.178,
                0.735,
                0.194
            ],
            "angle": 0,
            "content": "可行域 1"
        },
        {
            "type": "title",
            "bbox": [
                0.602,
                0.213,
                0.617,
                0.227
            ],
            "angle": 0,
            "content": "L"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.234,
                0.736,
                0.25
            ],
            "angle": 0,
            "content": "L-BFGS方法 260,263"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.255,
                0.736,
                0.271
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的紧形式. 263, 264"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.276,
                0.735,
                0.291
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的双循环递归算法 ..... 261"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.297,
                0.736,
                0.312
            ],
            "angle": 0,
            "content": "LASSO 问题 6,89"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.318,
                0.613,
                0.333
            ],
            "angle": 0,
            "content": "LASSO问题求解"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.338,
                0.736,
                0.373
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的Chambolle-Pock算法421"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.38,
                0.736,
                0.395
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的Nesterov算法……367"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.401,
                0.736,
                0.416
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的PDHG算法 420"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.421,
                0.736,
                0.436
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的次梯度算法 242"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.442,
                0.736,
                0.457
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的分块坐标下降法 ..... 393"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.463,
                0.736,
                0.478
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的交替方向乘子法 443, 445"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.484,
                0.736,
                0.499
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的近似点算法 377"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.505,
                0.736,
                0.52
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的近似点梯度法 351"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.526,
                0.736,
                0.541
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的梯度法 231"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.547,
                0.736,
                0.582
            ],
            "angle": 0,
            "content": "Levenberg-Marquardt 方法 ……见 LM 方法"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.588,
                0.736,
                0.604
            ],
            "angle": 0,
            "content": "LICQ……见线性无关约束品性"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.609,
                0.736,
                0.624
            ],
            "angle": 0,
            "content": "LM方法 285"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.629,
                0.736,
                0.644
            ],
            "angle": 0,
            "content": "LMF方法 288"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.65,
                0.736,
                0.665
            ],
            "angle": 0,
            "content": "信赖域型\\~ 285"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.671,
                0.736,
                0.686
            ],
            "angle": 0,
            "content": "LU分解 506"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.691,
                0.736,
                0.706
            ],
            "angle": 0,
            "content": "拉格朗日乘子 169"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.712,
                0.736,
                0.728
            ],
            "angle": 0,
            "content": "拉格朗日对偶函数 170"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.733,
                0.736,
                0.748
            ],
            "angle": 0,
            "content": "拉格朗日对偶问题 171"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.754,
                0.736,
                0.769
            ],
            "angle": 0,
            "content": "拉格朗日函数 170"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.774,
                0.736,
                0.81
            ],
            "angle": 0,
            "content": "广义不等式约束优化问题的\\~ 173"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.816,
                0.736,
                0.831
            ],
            "angle": 0,
            "content": "离散的全变差 109"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.837,
                0.736,
                0.852
            ],
            "angle": 0,
            "content": "离散散度算子 110"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.13
            ],
            "angle": 0,
            "content": "560"
        },
        {
            "type": "header",
            "bbox": [
                0.787,
                0.117,
                0.823,
                0.133
            ],
            "angle": 0,
            "content": "索引"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.156,
                0.516,
                0.174
            ],
            "angle": 0,
            "content": "连续化 300,307"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.178,
                0.516,
                0.194
            ],
            "angle": 0,
            "content": "临界锥 189"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.198,
                0.516,
                0.215
            ],
            "angle": 0,
            "content": "邻近算子 344,357"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.219,
                0.516,
                0.236
            ],
            "angle": 0,
            "content": "岭回归 89"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.24,
                0.516,
                0.257
            ],
            "angle": 0,
            "content": "鲁棒主成分分析. 见矩阵分离问题"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.261,
                0.516,
                0.278
            ],
            "angle": 0,
            "content": "路径追踪算法 337"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.282,
                0.516,
                0.3
            ],
            "angle": 0,
            "content": "逻辑回归 91"
        },
        {
            "type": "title",
            "bbox": [
                0.376,
                0.319,
                0.4,
                0.334
            ],
            "angle": 0,
            "content": "M"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.34,
                0.516,
                0.357
            ],
            "angle": 0,
            "content": "MFCQ 186"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.361,
                0.516,
                0.378
            ],
            "angle": 0,
            "content": "Momentum . 见动量方法"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.382,
                0.516,
                0.398
            ],
            "angle": 0,
            "content": "Moreau分解 415"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.403,
                0.516,
                0.42
            ],
            "angle": 0,
            "content": "Moreau-Rockafellar 定理 ..... 69"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.423,
                0.516,
                0.44
            ],
            "angle": 0,
            "content": "Moreau-Yosida 正则化 384"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.444,
                0.516,
                0.461
            ],
            "angle": 0,
            "content": "马尔可夫不等式 532"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.465,
                0.516,
                0.482
            ],
            "angle": 0,
            "content": "马尔可夫决策过程 114"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.486,
                0.516,
                0.503
            ],
            "angle": 0,
            "content": "马尔可夫随机场 96"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.507,
                0.516,
                0.524
            ],
            "angle": 0,
            "content": "目标函数 1"
        },
        {
            "type": "title",
            "bbox": [
                0.378,
                0.543,
                0.397,
                0.558
            ],
            "angle": 0,
            "content": "N"
        },
        {
            "type": "text",
            "bbox": [
                0.259,
                0.565,
                0.371,
                0.581
            ],
            "angle": 0,
            "content": "Nesterov算法"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.586,
                0.516,
                0.602
            ],
            "angle": 0,
            "content": "FISTA算法 359"
        },
        {
            "type": "text",
            "bbox": [
                0.308,
                0.606,
                0.516,
                0.623
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的等价变形 360"
        },
        {
            "type": "text",
            "bbox": [
                0.308,
                0.628,
                0.516,
                0.644
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的线搜索 362"
        },
        {
            "type": "text",
            "bbox": [
                0.308,
                0.649,
                0.516,
                0.665
            ],
            "angle": 0,
            "content": "下降的 \\(\\sim\\) 363"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.669,
                0.516,
                0.686
            ],
            "angle": 0,
            "content": "第二类\\~ 364"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.69,
                0.516,
                0.707
            ],
            "angle": 0,
            "content": "第三类\\~ 365"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.711,
                0.516,
                0.728
            ],
            "angle": 0,
            "content": "非凸问题的 \\(\\sim\\) 365"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.732,
                0.516,
                0.749
            ],
            "angle": 0,
            "content": "随机优化问题的 \\(\\sim\\) 465"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.753,
                0.516,
                0.77
            ],
            "angle": 0,
            "content": "内点罚函数 308"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.774,
                0.349,
                0.79
            ],
            "angle": 0,
            "content": "拟牛顿格式"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.794,
                0.516,
                0.811
            ],
            "angle": 0,
            "content": "BFGS公式 256"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.815,
                0.516,
                0.832
            ],
            "angle": 0,
            "content": "DFP公式 257"
        },
        {
            "type": "text",
            "bbox": [
                0.292,
                0.836,
                0.516,
                0.853
            ],
            "angle": 0,
            "content": "SR1公式 255"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.157,
                0.624,
                0.173
            ],
            "angle": 0,
            "content": "牛顿法"
        },
        {
            "type": "text",
            "bbox": [
                0.602,
                0.178,
                0.825,
                0.195
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的收敛性. 245, 250"
        },
        {
            "type": "text",
            "bbox": [
                0.602,
                0.199,
                0.825,
                0.216
            ],
            "angle": 0,
            "content": "非精确 \\(\\sim\\) 249"
        },
        {
            "type": "text",
            "bbox": [
                0.602,
                0.221,
                0.825,
                0.236
            ],
            "angle": 0,
            "content": "经典\\~ 245"
        },
        {
            "type": "text",
            "bbox": [
                0.602,
                0.242,
                0.825,
                0.258
            ],
            "angle": 0,
            "content": "修正 \\(\\sim\\) 247"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.263,
                0.825,
                0.28
            ],
            "angle": 0,
            "content": "牛顿方程 245"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.284,
                0.825,
                0.301
            ],
            "angle": 0,
            "content": "牛顿方向 245"
        },
        {
            "type": "title",
            "bbox": [
                0.689,
                0.326,
                0.706,
                0.342
            ],
            "angle": 0,
            "content": "P"
        },
        {
            "type": "text",
            "bbox": [
                0.569,
                0.348,
                0.825,
                0.385
            ],
            "angle": 0,
            "content": "PDHG算法见原始对偶混合梯度法"
        },
        {
            "type": "title",
            "bbox": [
                0.687,
                0.412,
                0.708,
                0.429
            ],
            "angle": 0,
            "content": "Q"
        },
        {
            "type": "text",
            "bbox": [
                0.569,
                0.433,
                0.825,
                0.45
            ],
            "angle": 0,
            "content": "QR分解 286,506"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.454,
                0.825,
                0.471
            ],
            "angle": 0,
            "content": "奇异值分解 512"
        },
        {
            "type": "text",
            "bbox": [
                0.602,
                0.476,
                0.825,
                0.492
            ],
            "angle": 0,
            "content": "奇异向量 512"
        },
        {
            "type": "text",
            "bbox": [
                0.602,
                0.497,
                0.825,
                0.513
            ],
            "angle": 0,
            "content": "奇异值 512"
        },
        {
            "type": "text",
            "bbox": [
                0.602,
                0.518,
                0.825,
                0.534
            ],
            "angle": 0,
            "content": "约化\\~ 512"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.539,
                0.825,
                0.556
            ],
            "angle": 0,
            "content": "前馈神经网络……见多层感知机"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.56,
                0.825,
                0.577
            ],
            "angle": 0,
            "content": "强对偶原理 171, 193"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.582,
                0.825,
                0.598
            ],
            "angle": 0,
            "content": "强化学习 113"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.603,
                0.825,
                0.619
            ],
            "angle": 0,
            "content": "强拟凸函数 159"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.624,
                0.825,
                0.64
            ],
            "angle": 0,
            "content": "强凸函数 46"
        },
        {
            "type": "text",
            "bbox": [
                0.602,
                0.645,
                0.825,
                0.662
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的等价定义 47"
        },
        {
            "type": "text",
            "bbox": [
                0.602,
                0.667,
                0.825,
                0.683
            ],
            "angle": 0,
            "content": "强凸参数 47"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.688,
                0.825,
                0.705
            ],
            "angle": 0,
            "content": "切比雪夫不等式 532"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.709,
                0.825,
                0.726
            ],
            "angle": 0,
            "content": "切锥 180"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.731,
                0.59,
                0.746
            ],
            "angle": 0,
            "content": "球"
        },
        {
            "type": "text",
            "bbox": [
                0.602,
                0.752,
                0.825,
                0.768
            ],
            "angle": 0,
            "content": "范数\\~ 42"
        },
        {
            "type": "text",
            "bbox": [
                0.602,
                0.773,
                0.825,
                0.789
            ],
            "angle": 0,
            "content": "欧几里得 \\(\\sim\\) 42"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.794,
                0.825,
                0.811
            ],
            "angle": 0,
            "content": "曲率条件 254"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.816,
                0.825,
                0.832
            ],
            "angle": 0,
            "content": "全变差 108"
        },
        {
            "type": "text",
            "bbox": [
                0.57,
                0.836,
                0.825,
                0.853
            ],
            "angle": 0,
            "content": "全局极小解 16"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.208,
                0.132
            ],
            "angle": 0,
            "content": "索引"
        },
        {
            "type": "page_number",
            "bbox": [
                0.708,
                0.118,
                0.735,
                0.131
            ],
            "angle": 0,
            "content": "561"
        },
        {
            "type": "title",
            "bbox": [
                0.291,
                0.156,
                0.309,
                0.171
            ],
            "angle": 0,
            "content": "R"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.178,
                0.428,
                0.194
            ],
            "angle": 0,
            "content": "ReLU函数 9,127"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.198,
                0.428,
                0.215
            ],
            "angle": 0,
            "content": "RMSProp 467"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.219,
                0.427,
                0.236
            ],
            "angle": 0,
            "content": "融合 LASSO 91"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.24,
                0.428,
                0.256
            ],
            "angle": 0,
            "content": "弱对偶原理 170"
        },
        {
            "type": "title",
            "bbox": [
                0.293,
                0.275,
                0.308,
                0.29
            ],
            "angle": 0,
            "content": "s"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.296,
                0.427,
                0.312
            ],
            "angle": 0,
            "content": "SAG方法 483"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.317,
                0.427,
                0.333
            ],
            "angle": 0,
            "content": "SAGA 方法 484"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.338,
                0.427,
                0.354
            ],
            "angle": 0,
            "content": "Sigmoid函数 9,91"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.359,
                0.427,
                0.374
            ],
            "angle": 0,
            "content": "Slater 约束品性 192"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.38,
                0.427,
                0.396
            ],
            "angle": 0,
            "content": "Steinhaug-CG. 见截断共轭梯度法"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.401,
                0.427,
                0.416
            ],
            "angle": 0,
            "content": "SVRG 方法 ..... 486"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.421,
                0.427,
                0.437
            ],
            "angle": 0,
            "content": "上方图 36"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.442,
                0.427,
                0.458
            ],
            "angle": 0,
            "content": "深度Q学习 128"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.463,
                0.427,
                0.478
            ],
            "angle": 0,
            "content": "深度学习 8"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.484,
                0.427,
                0.499
            ],
            "angle": 0,
            "content": "事件域 518,520"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.504,
                0.427,
                0.52
            ],
            "angle": 0,
            "content": "似然函数 82"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.525,
                0.427,
                0.541
            ],
            "angle": 0,
            "content": "对数\\~ 83"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.546,
                0.427,
                0.561
            ],
            "angle": 0,
            "content": "示性函数 59,348"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.567,
                0.427,
                0.582
            ],
            "angle": 0,
            "content": "适当函数 35"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.588,
                0.427,
                0.603
            ],
            "angle": 0,
            "content": "适当锥 172"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.608,
                0.243,
                0.624
            ],
            "angle": 0,
            "content": "收敛速度"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.629,
                0.427,
                0.644
            ],
            "angle": 0,
            "content": "Q- \\(\\sim\\) 20"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.65,
                0.427,
                0.665
            ],
            "angle": 0,
            "content": "\\(\\mathrm{R - \\sim}\\) 20"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.671,
                0.427,
                0.686
            ],
            "angle": 0,
            "content": "收敛准则 17"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.691,
                0.427,
                0.706
            ],
            "angle": 0,
            "content": "数据插值 127"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.712,
                0.427,
                0.728
            ],
            "angle": 0,
            "content": "数据拟合 124"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.733,
                0.278,
                0.749
            ],
            "angle": 0,
            "content": "数值计算软件"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.754,
                0.427,
                0.769
            ],
            "angle": 0,
            "content": "ARPACK 518"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.774,
                0.427,
                0.789
            ],
            "angle": 0,
            "content": "ATLAS 515"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.795,
                0.427,
                0.81
            ],
            "angle": 0,
            "content": "BLAS 514, 515"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.816,
                0.427,
                0.831
            ],
            "angle": 0,
            "content": "Intel MKL 515, 516"
        },
        {
            "type": "text",
            "bbox": [
                0.205,
                0.836,
                0.427,
                0.851
            ],
            "angle": 0,
            "content": "LAPACK 515"
        },
        {
            "type": "text",
            "bbox": [
                0.514,
                0.157,
                0.737,
                0.174
            ],
            "angle": 0,
            "content": "SciPy 515"
        },
        {
            "type": "text",
            "bbox": [
                0.482,
                0.178,
                0.736,
                0.194
            ],
            "angle": 0,
            "content": "松弛 84,85"
        },
        {
            "type": "text",
            "bbox": [
                0.482,
                0.199,
                0.737,
                0.215
            ],
            "angle": 0,
            "content": "搜索方向 216"
        },
        {
            "type": "text",
            "bbox": [
                0.482,
                0.219,
                0.57,
                0.236
            ],
            "angle": 0,
            "content": "算法收敛性"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.24,
                0.736,
                0.256
            ],
            "angle": 0,
            "content": "全局依点列收敛 18"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.261,
                0.736,
                0.277
            ],
            "angle": 0,
            "content": "依点列收敛 18"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.282,
                0.736,
                0.298
            ],
            "angle": 0,
            "content": "依函数值收敛 19"
        },
        {
            "type": "text",
            "bbox": [
                0.482,
                0.303,
                0.736,
                0.319
            ],
            "angle": 0,
            "content": "随机变量 519"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.324,
                0.736,
                0.34
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的边缘分布 523"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.344,
                0.736,
                0.36
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的独立性 524"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.366,
                0.736,
                0.381
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的方差 523"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.386,
                0.736,
                0.402
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的几乎必然收敛 529"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.407,
                0.736,
                0.423
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的可测性 519"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.428,
                0.736,
                0.444
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的联合分布 523"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.449,
                0.736,
                0.464
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的数学期望 522"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.47,
                0.736,
                0.485
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的相关性 524"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.491,
                0.736,
                0.506
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的依概率收敛 529"
        },
        {
            "type": "text",
            "bbox": [
                0.482,
                0.511,
                0.553,
                0.527
            ],
            "angle": 0,
            "content": "随机过程"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.532,
                0.736,
                0.548
            ],
            "angle": 0,
            "content": "离散鞅 530"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.553,
                0.736,
                0.568
            ],
            "angle": 0,
            "content": "连续鞅 531"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.573,
                0.736,
                0.588
            ],
            "angle": 0,
            "content": "马尔可夫过程（马氏过程）530"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.594,
                0.736,
                0.61
            ],
            "angle": 0,
            "content": "鞅差序列 531"
        },
        {
            "type": "text",
            "bbox": [
                0.482,
                0.615,
                0.736,
                0.631
            ],
            "angle": 0,
            "content": "随机梯度下降算法 463"
        },
        {
            "type": "text",
            "bbox": [
                0.482,
                0.636,
                0.736,
                0.652
            ],
            "angle": 0,
            "content": "随机优化问题 134"
        },
        {
            "type": "text",
            "bbox": [
                0.482,
                0.656,
                0.736,
                0.672
            ],
            "angle": 0,
            "content": "随机主成分分析 134"
        },
        {
            "type": "title",
            "bbox": [
                0.602,
                0.69,
                0.618,
                0.705
            ],
            "angle": 0,
            "content": "T"
        },
        {
            "type": "text",
            "bbox": [
                0.482,
                0.712,
                0.736,
                0.728
            ],
            "angle": 0,
            "content": "Tikhonov正则化 89"
        },
        {
            "type": "text",
            "bbox": [
                0.482,
                0.733,
                0.736,
                0.749
            ],
            "angle": 0,
            "content": "泰勒展开 27,52"
        },
        {
            "type": "text",
            "bbox": [
                0.482,
                0.754,
                0.736,
                0.77
            ],
            "angle": 0,
            "content": "特征值分解 511"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.774,
                0.736,
                0.79
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的外积形式 511"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.795,
                0.736,
                0.811
            ],
            "angle": 0,
            "content": "特征向量 511"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.816,
                0.736,
                0.832
            ],
            "angle": 0,
            "content": "特征值 511"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.836,
                0.736,
                0.852
            ],
            "angle": 0,
            "content": "正交对角化 511"
        }
    ],
    [
        {
            "type": "page_number",
            "bbox": [
                0.26,
                0.118,
                0.29,
                0.131
            ],
            "angle": 0,
            "content": "562"
        },
        {
            "type": "header",
            "bbox": [
                0.788,
                0.118,
                0.822,
                0.133
            ],
            "angle": 0,
            "content": "索引"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.157,
                0.516,
                0.173
            ],
            "angle": 0,
            "content": "梯度 26,30"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.178,
                0.516,
                0.194
            ],
            "angle": 0,
            "content": "梯度利普希茨连续 28"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.199,
                0.516,
                0.215
            ],
            "angle": 0,
            "content": "梯度映射 354"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.22,
                0.516,
                0.236
            ],
            "angle": 0,
            "content": "条件数学期望 526"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.241,
                0.516,
                0.257
            ],
            "angle": 0,
            "content": "停机准则 18"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.262,
                0.516,
                0.278
            ],
            "angle": 0,
            "content": "透视函数 54,67"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.283,
                0.516,
                0.299
            ],
            "angle": 0,
            "content": "凸包 39"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.304,
                0.516,
                0.319
            ],
            "angle": 0,
            "content": "凸函数 46"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.324,
                0.516,
                0.34
            ],
            "angle": 0,
            "content": "凸函数的方向导数 66"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.345,
                0.516,
                0.361
            ],
            "angle": 0,
            "content": "凸集 39"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.366,
                0.516,
                0.382
            ],
            "angle": 0,
            "content": "凸组合 39"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.387,
                0.366,
                0.403
            ],
            "angle": 0,
            "content": "图像处理模型"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.407,
                0.514,
                0.423
            ],
            "angle": 0,
            "content": "\\(\\mathrm{TV - L}^1\\) 模型 110,421"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.429,
                0.515,
                0.444
            ],
            "angle": 0,
            "content": "反卷积模型 423"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.45,
                0.515,
                0.465
            ],
            "angle": 0,
            "content": "盲反卷积模型 133"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.471,
                0.515,
                0.486
            ],
            "angle": 0,
            "content": "全变差模型 108"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.492,
                0.515,
                0.507
            ],
            "angle": 0,
            "content": "图像去噪 132"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.513,
                0.515,
                0.528
            ],
            "angle": 0,
            "content": "图像填充 423"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.534,
                0.515,
                0.549
            ],
            "angle": 0,
            "content": "小波模型 111"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.554,
                0.516,
                0.57
            ],
            "angle": 0,
            "content": "椭球 42"
        },
        {
            "type": "title",
            "bbox": [
                0.377,
                0.59,
                0.398,
                0.605
            ],
            "angle": 0,
            "content": "W"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.612,
                0.516,
                0.628
            ],
            "angle": 0,
            "content": "Weierstrass定理 157,345"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.633,
                0.516,
                0.649
            ],
            "angle": 0,
            "content": "外点罚函数 298"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.654,
                0.516,
                0.67
            ],
            "angle": 0,
            "content": "唯一性定理 159"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.675,
                0.515,
                0.69
            ],
            "angle": 0,
            "content": "稳定点 161"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.695,
                0.435,
                0.711
            ],
            "angle": 0,
            "content": "无约束优化最优性条件"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.717,
                0.515,
                0.732
            ],
            "angle": 0,
            "content": "二阶必要条件 162"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.737,
                0.515,
                0.753
            ],
            "angle": 0,
            "content": "二阶充分条件 162"
        },
        {
            "type": "text",
            "bbox": [
                0.293,
                0.758,
                0.515,
                0.773
            ],
            "angle": 0,
            "content": "一阶必要条件 160"
        },
        {
            "type": "title",
            "bbox": [
                0.379,
                0.794,
                0.396,
                0.808
            ],
            "angle": 0,
            "content": "X"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.816,
                0.516,
                0.832
            ],
            "angle": 0,
            "content": "稀疏优化 3"
        },
        {
            "type": "text",
            "bbox": [
                0.26,
                0.836,
                0.516,
                0.852
            ],
            "angle": 0,
            "content": "下半连续函数 37"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.157,
                0.825,
                0.173
            ],
            "angle": 0,
            "content": "下降方向 160,216"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.178,
                0.658,
                0.194
            ],
            "angle": 0,
            "content": "线搜索准则"
        },
        {
            "type": "text",
            "bbox": [
                0.603,
                0.199,
                0.825,
                0.215
            ],
            "angle": 0,
            "content": "Armijo准则 217"
        },
        {
            "type": "text",
            "bbox": [
                0.603,
                0.22,
                0.825,
                0.235
            ],
            "angle": 0,
            "content": "Goldstein准则 219"
        },
        {
            "type": "text",
            "bbox": [
                0.603,
                0.241,
                0.825,
                0.256
            ],
            "angle": 0,
            "content": "Wolfe准则 220"
        },
        {
            "type": "text",
            "bbox": [
                0.603,
                0.262,
                0.825,
                0.277
            ],
            "angle": 0,
            "content": "非单调\\~ 220"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.282,
                0.823,
                0.298
            ],
            "angle": 0,
            "content": "线性规划 2,121"
        },
        {
            "type": "text",
            "bbox": [
                0.603,
                0.303,
                0.825,
                0.319
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的KKT条件.333"
        },
        {
            "type": "text",
            "bbox": [
                0.603,
                0.324,
                0.825,
                0.34
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的对偶 174"
        },
        {
            "type": "text",
            "bbox": [
                0.603,
                0.345,
                0.825,
                0.36
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的对偶间隙 334, 338"
        },
        {
            "type": "text",
            "bbox": [
                0.603,
                0.366,
                0.825,
                0.381
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的扰动KKT条件.....334"
        },
        {
            "type": "text",
            "bbox": [
                0.603,
                0.387,
                0.825,
                0.402
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的KKT条件. 204"
        },
        {
            "type": "text",
            "bbox": [
                0.603,
                0.407,
                0.823,
                0.423
            ],
            "angle": 0,
            "content": "标准形\\~ 121"
        },
        {
            "type": "text",
            "bbox": [
                0.603,
                0.428,
                0.825,
                0.443
            ],
            "angle": 0,
            "content": "不等式形\\~ 122"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.448,
                0.823,
                0.464
            ],
            "angle": 0,
            "content": "线性规划内点法 332"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.47,
                0.823,
                0.485
            ],
            "angle": 0,
            "content": "线性化可行方向锥 181"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.491,
                0.825,
                0.506
            ],
            "angle": 0,
            "content": "线性无关约束品性 184"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.511,
                0.825,
                0.526
            ],
            "angle": 0,
            "content": "线性约束品性 186"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.532,
                0.825,
                0.547
            ],
            "angle": 0,
            "content": "相对内点 192"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.553,
                0.823,
                0.568
            ],
            "angle": 0,
            "content": "相关系数矩阵估计 143"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.573,
                0.825,
                0.588
            ],
            "angle": 0,
            "content": "相位恢复 98,290"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.594,
                0.825,
                0.609
            ],
            "angle": 0,
            "content": "相位提升 100"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.615,
                0.823,
                0.63
            ],
            "angle": 0,
            "content": "小残量问题 281"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.636,
                0.823,
                0.651
            ],
            "angle": 0,
            "content": "信赖域 267"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.656,
                0.825,
                0.671
            ],
            "angle": 0,
            "content": "信赖域半径 267"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.677,
                0.825,
                0.692
            ],
            "angle": 0,
            "content": "信赖域算法 269"
        },
        {
            "type": "title",
            "bbox": [
                0.688,
                0.712,
                0.705,
                0.725
            ],
            "angle": 0,
            "content": "Y"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.733,
                0.825,
                0.748
            ],
            "angle": 0,
            "content": "压缩感知 3"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.754,
                0.825,
                0.769
            ],
            "angle": 0,
            "content": "雅可比矩阵 27"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.774,
                0.825,
                0.789
            ],
            "angle": 0,
            "content": "严格分离定理 45"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.796,
                0.825,
                0.811
            ],
            "angle": 0,
            "content": "严格互补松弛条件 188"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.816,
                0.825,
                0.832
            ],
            "angle": 0,
            "content": "严格局部极小解 16"
        },
        {
            "type": "text",
            "bbox": [
                0.571,
                0.836,
                0.825,
                0.852
            ],
            "angle": 0,
            "content": "严格凸函数 46"
        }
    ],
    [
        {
            "type": "header",
            "bbox": [
                0.172,
                0.117,
                0.208,
                0.132
            ],
            "angle": 0,
            "content": "索引"
        },
        {
            "type": "page_number",
            "bbox": [
                0.707,
                0.118,
                0.737,
                0.131
            ],
            "angle": 0,
            "content": "563"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.157,
                0.428,
                0.173
            ],
            "angle": 0,
            "content": "样本空间 518"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.178,
                0.427,
                0.194
            ],
            "angle": 0,
            "content": "优化算法软件 148"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.199,
                0.428,
                0.235
            ],
            "angle": 0,
            "content": "有限内存BFGS方法．见L-BFGS 方法"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.24,
                0.427,
                0.256
            ],
            "angle": 0,
            "content": "余强制性 227"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.262,
                0.427,
                0.277
            ],
            "angle": 0,
            "content": "原始-对偶算法 333"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.282,
                0.427,
                0.298
            ],
            "angle": 0,
            "content": "原始对偶混合梯度算法 419"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.304,
                0.427,
                0.319
            ],
            "angle": 0,
            "content": "约束集合 见可行域"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.324,
                0.427,
                0.34
            ],
            "angle": 0,
            "content": "约束品性 180"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.345,
                0.329,
                0.361
            ],
            "angle": 0,
            "content": "约束优化最优性条件"
        },
        {
            "type": "text",
            "bbox": [
                0.206,
                0.366,
                0.427,
                0.381
            ],
            "angle": 0,
            "content": "二阶必要条件 190"
        },
        {
            "type": "text",
            "bbox": [
                0.206,
                0.387,
                0.427,
                0.402
            ],
            "angle": 0,
            "content": "二阶充分条件 190"
        },
        {
            "type": "text",
            "bbox": [
                0.206,
                0.408,
                0.427,
                0.424
            ],
            "angle": 0,
            "content": "一阶最优性条件见KKT条件"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.429,
                0.427,
                0.444
            ],
            "angle": 0,
            "content": "运输问题 122"
        },
        {
            "type": "title",
            "bbox": [
                0.291,
                0.466,
                0.308,
                0.479
            ],
            "angle": 0,
            "content": "Z"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.487,
                0.427,
                0.503
            ],
            "angle": 0,
            "content": "Zoutendijk条件 223"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.508,
                0.427,
                0.523
            ],
            "angle": 0,
            "content": "增广拉格朗日函数 311,318"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.529,
                0.427,
                0.544
            ],
            "angle": 0,
            "content": "增广拉格朗日函数法 311"
        },
        {
            "type": "text",
            "bbox": [
                0.206,
                0.55,
                0.427,
                0.565
            ],
            "angle": 0,
            "content": "半定规划问题的 \\(\\sim\\) 330"
        },
        {
            "type": "text",
            "bbox": [
                0.206,
                0.571,
                0.427,
                0.586
            ],
            "angle": 0,
            "content": "等式约束优化问题的 \\(\\sim .311\\)"
        },
        {
            "type": "text",
            "bbox": [
                0.206,
                0.592,
                0.427,
                0.607
            ],
            "angle": 0,
            "content": "凸问题的 \\(\\sim\\) 320"
        },
        {
            "type": "text",
            "bbox": [
                0.206,
                0.613,
                0.427,
                0.628
            ],
            "angle": 0,
            "content": "一般约束优化问题的 \\(\\sim\\) .317"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.634,
                0.427,
                0.649
            ],
            "angle": 0,
            "content": "整数规划 145"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.655,
                0.427,
                0.67
            ],
            "angle": 0,
            "content": "正则化 81"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.676,
                0.427,
                0.691
            ],
            "angle": 0,
            "content": "支撑超平面 45"
        },
        {
            "type": "text",
            "bbox": [
                0.172,
                0.696,
                0.427,
                0.711
            ],
            "angle": 0,
            "content": "支撑超平面定理 45"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.157,
                0.736,
                0.173
            ],
            "angle": 0,
            "content": "支撑函数 55,59"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.178,
                0.736,
                0.194
            ],
            "angle": 0,
            "content": "支持向量机 93"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.199,
                0.736,
                0.214
            ],
            "angle": 0,
            "content": "中心路径 336"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.22,
                0.736,
                0.235
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 方程 336"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.24,
                0.736,
                0.256
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 邻域 336"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.261,
                0.735,
                0.277
            ],
            "angle": 0,
            "content": "主成分分析 101"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.282,
                0.502,
                0.297
            ],
            "angle": 0,
            "content": "锥"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.303,
                0.736,
                0.318
            ],
            "angle": 0,
            "content": "二次\\~ 42"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.323,
                0.736,
                0.338
            ],
            "angle": 0,
            "content": "范数\\~ 42"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.344,
                0.736,
                0.359
            ],
            "angle": 0,
            "content": "凸\\~ 41"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.365,
                0.735,
                0.38
            ],
            "angle": 0,
            "content": "锥包 200"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.386,
                0.735,
                0.401
            ],
            "angle": 0,
            "content": "锥组合 41"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.406,
                0.735,
                0.422
            ],
            "angle": 0,
            "content": "字典学习 105,389,397"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.427,
                0.736,
                0.442
            ],
            "angle": 0,
            "content": "自动微分 32"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.447,
                0.735,
                0.463
            ],
            "angle": 0,
            "content": "自对偶锥 173"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.468,
                0.57,
                0.484
            ],
            "angle": 0,
            "content": "最大割问题"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.489,
                0.736,
                0.504
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的对偶 179"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.51,
                0.736,
                0.525
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的非凸松弛 207,398"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.531,
                0.736,
                0.546
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的凸松弛 207"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.552,
                0.736,
                0.567
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的最优性条件 208"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.572,
                0.736,
                0.587
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的凸松弛 139"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.593,
                0.736,
                0.608
            ],
            "angle": 0,
            "content": "\\(\\sim\\) 的原始形式 138"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.613,
                0.736,
                0.628
            ],
            "angle": 0,
            "content": "最大似然估计 82"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.634,
                0.736,
                0.649
            ],
            "angle": 0,
            "content": "最小二乘法 80"
        },
        {
            "type": "text",
            "bbox": [
                0.483,
                0.655,
                0.736,
                0.67
            ],
            "angle": 0,
            "content": "最小二乘问题 125"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.676,
                0.736,
                0.691
            ],
            "angle": 0,
            "content": "非线性 \\(\\sim\\) 125,281"
        },
        {
            "type": "text",
            "bbox": [
                0.515,
                0.696,
                0.736,
                0.711
            ],
            "angle": 0,
            "content": "线性\\~ 125"
        }
    ]
]